Non,"WasmFX: Stack Switching via Eect Handlers in WebAssembly
Daniel HillerstrÃ¶m
Laboratory for Foundations of Computer Science The University of EdinburghScotland, United Kingdom
January 13, 2023      

I am but one of many
Sam Lindley Andreas Rossberg Daan Leijen
KC Sivaramakrishnan
Matija Pretnar Luna Phipps-Costin Arjun Guha
https://wasmfx.dev 

WebAssembly: neither web nor assembly (Haas et al. 2017)
What is Wasm? A universal compilation target
A virtual stack machine (source language agnostic)
A predictable performance model
Code format A Wasm program is a structured module
Designed for streaming compilation
The term language is
statically typedand block-structuredControl ow is structured (
i.e.all CFGs are reducible)
Wasm MVP 1.0 is tailored for C/C ++
https://webassembly.org        

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control ow abstractions to Wasm?Solution
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)
Add each abstraction as a primitive to Wasm
Use
eect handlers as a unied modular basis for control in Wasm         

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control ow abstractions to Wasm?Solution
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)
Add each abstraction as a primitive to Wasm
Use
eect handlers as a unied modular basis for control in Wasm         

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control ow abstractions to Wasm?Solution
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)
Add each abstraction as a primitive to Wasm
Use
eect handlers as a unied modular basis for control in Wasm         

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control ow abstractions to Wasm?Solution
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)
Add each abstraction as a primitive to Wasm
Use
eect handlers as a unied modular basis for control in Wasm         

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control ow abstractions to Wasm?Solution
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)
Add each abstraction as a primitive to Wasm
Use
eect handlers as a unied modular basis for control in Wasm         

Perspectives on eect handlers
Operational interpretation First-class resumable exceptions
Software engineering interpretation Composable monads builders (monads as a design pattern)
Functional programming interpretation Folds over computation trees
Mathematical interpretation Homomorphisms between free algebraic models 

Eect handlers are a proven technology
A modular and extensible basis Structured form of delimited control
Easy encoding of
your favourite abstraction via eect handlersTrivially compatible with typed representations
Practical evidence 100
+
peer reviewed papers Available in many programming languages (e.g. C
++
, Haskell, Pyro, OCaml, Unison) Deployed in industrial technologies (e.g. GitHub's semantic, Meta's React, Uber's Pyro)       

Running example: coroutines (1)
;; interface for running two coroutines
;; non-interleaving implementation
(
module $co2;; type alias task = [] -> []
(
type $task ( func)) ;; yield : [] -> []
(
func $yield ( export""yield"")(nop))
;; run : [(ref $task) (ref $task)] -> []
(
func $run ( export ""run"") ( param$task1 ( ref$task)) ( param$task2 ( ref$task)) ;; run the tasks sequentially
(call
_
ref ( local .get $task1)) (call
_
ref ( local .get $task2)) )
) 

Running example: coroutines (2)
(
module $example ;; main example: streams of odd and even naturals...
;; imports yield : [] -> []
(
func $yield ( import""co2"" ""yield""))...
) 

Running example: coroutines (3)
(
module $example...
;; odd : [i32] -> []
;; prints the first $niter odd natural numbers
(
func $odd ( param$niter i32) (
local $ni32 ) ;; next odd number(
local $ii32 ) ;; iterator(
local .set $n (i32.const 1)) ;; initialise locals(
local .set $i (i32.const 1)) ;; ...(
block $b (
loop $l (
br _
if $b ( i32.gt _
u (local .get $i) ( local .get $niter))) ;; termination condition (
call $print ( local.get $n)) ;; print the current odd number(
local .set $n (i32.add (local .get $n) ( i32.const 2))) ;; compute next odd number (
local .set $i (i32.add (local .get $i) ( i32.const 1))) ;; increment the iterator (
call $yield) ;; yield control(
br $l)))) ;; repeat;; even : [i32] -> []
;; prints the first $niter even natural numbers
(
func $even ( param$niter i32) ...) ...
) 

Running example: coroutines (4)
(
module $example...
;; odd5, even5 : [] -> []
(
func $odd5 ( export ""odd5"") (
call $odd ( i32.const 5)))(
func $even5 ( export""even5"") (
call $even ( i32.const 5))))
;; calling $run with $odd5 and $even5...
(
call $run ( ref.func $odd5) ( ref.func $even5));; ... prints 1 3 5 7 9 2 4 6 8 10 

Instructions: declaring control tags
Control tag declaration (tag $tag (param 
) ( result 
))
it's a mild extension of Wasm's exception tags
(known in the literature as an `operation symbol' (Plotkin and Pretnar 2013)) 

Refactoring the
co2module (1)(
module $co2;; type alias task = [] -> []
(
type $task ( func)) ;; yield : [] -> []
(
tag $yield) ;; yield : [] -> []
(
func $yield ( export""yield"")(nop))
;; run : [(ref $task) (ref $task)] -> []
(
func $run ( export ""run"") ( param$task1 ( ref$task)) ( param$task2 ( ref$task)) ...)
) 

Instructions: creating continuations
Continuation type (cont $ft )
cont is a new reference type constructor parameterised by a function type, $ft : [
] ! [ 
]
Continuation allocation cont.new: [(ref null $ft )] ! [(ref $ct )]
where $ft : [
] ! [ 
]
and $ct :cont $ft 

Refactoring the
co2module (2)(
module $co2;; type alias $task = [] -> []
(
type $task ( func)) ;; type alias $ct = $task
(
type $ct ( cont$task))...
;; run : [(ref $task) (ref $task)] -> []
;; implements a 'seesaw' (c.f. Ganz et al. (ICFP@99))
(
func $run ( export ""run"") ( param$task1 ( ref$task)) ( param$task2 ( ref$task)) ;; locals to manage continuations
(
local $up ( ref null $ct))(
local $down ( ref null $ct))(
local $isOtherDone i32) ;; initialise locals
(
local .set $up ( cont.new (type $ct) ( local.get $task1))) (
local .set $down ( cont.new (type $ct) ( local.get $task2))) ...)
) 

Thinking of
cont.newin terms of stacksPP (null)
SP up
down
   red zone PP (null)
SP $
task 1 red zone
PP (null)
SP $
task 2 red zone cont.new
allocates a new stack segment
New segments are initially suspended 

Thinking of
cont.newin terms of stacksPP (null)
SP up
down
   red zone PP (null)
SP $
task 1 red zone
PP (null)
SP $
task 2 red zone cont.new
allocates a new stack segment
New segments are initially suspended 

Thinking of
cont.newin terms of stacksPP (null)
SP up
down
   red zone PP (null)
SP $
task 1 red zone
PP (null)
SP $
task 2 red zone cont.new
allocates a new stack segment
New segments are initially suspended 

Instructions: invoking continuations
Continuation resumption resume(tag $tag $h )
: [ 
(ref null $ct )] ! [ 
]
where f$tag
i: [

i ]
! [ 
i ]
and $h
i : [

i (
ref null $ct
i)]
and
$ ct
i :
cont $ft
i and
$ft
i: [

i ]
! [ 
]g
i
and $ct :cont $ft
and $ft : [
] ! [ 
]
The instruction fully consume the continuation argument 

Refactoring the
co2module (3)(
module $co2...
;; declarations of $task, $yield, etc;; run : [(ref $task) (ref $task)] -> []
(
func $run ( export ""run"") ( param$task1 ( ref$task)) ( param$task2 ( ref$task)) ...
;; initialisation of $up and $down;; run $up
(
loop $h ;; handling loop(
block $on_
yield ( result (ref $ct)) (
resume (tag $yield $on _
yield) ( local.get $up)) ;; resume $up; handle $yield using $on _
yield (
if (i32 .eq (local .get $isOtherDone) ( i32.const 1));; $up finished; $down is already done? (
then (return ))) ;; ... then exit(
local .get $down) ;; ... otherwise prepare to run $down(
local .set $up) ;; $up := $down(
local .set $isOtherDone ( i32.const 1)) ;; mark other as done(
br $h) ;; repeat)
;; yield-case definition; stack: [(cont $ct)](
local .set $up) ;; set $up to the current continuation(
if (i32 .eqz (local .get $isOtherDone)) ;; is $down already done?(
then (local .get $down) ;; ... then swap $up and $down(
local .set $down ( local.get $up)) (
local .set $up))) (
br $h))) ;; repeat) 

Thinking of
resumein terms of stacksPP (null)
SP up
down
   red zone PP
SP $
task 1 red zone
PP (null)
SP $
task 2 red zone resume
transfers control from the parent to the
child stack
The pointer between parent and child is inverted 

Thinking of
resumein terms of stacksPP (null)
SP up
down
   red zone PP
SP $
task 1 red zone
PP (null)
SP $
task 2 red zone resume
transfers control from the parent to the
child stack
The pointer between parent and child is inverted 

Instructions: suspending continuations
Continuation suspension suspend$tag : [
] ! [ 
]
where $tag : [
] ! [ 
] 

Refactoring the
co2module (4)(
module $co2;; type alias task = [] -> []
(
type $task ( func)) ;; type alias ct = $task
(
type $ct ( cont$task));; yield : [] -> []
(
tag $yield ( param) (result ));; yield : [] -> []
(
func $yield ( export""yield"")(
suspend $yield));; run : [(ref $task) (ref $task)] -> []
;; implements a 'seesaw' (c.f. Ganz et al. (ICFP@99))
(
func $run ( export ""run"") ( param$task1 ( ref$task)) ( param$task2 ( ref$task)) ... )
)
Now
(call $run ( ref.func $odd5) ( ref.func $even5)) prints1 2 3 4 5 6 7 8 9 10 

Thinking of
suspendin terms of stacksPP (null)
SP up
down
  
cont up
   red zone PP
SP   
suspend red zone
PP (null)
SP $
task 2 red zone suspend
transfers control a child to a (grand)parent
The pointer between child and parent is inverted 

Thinking of
suspendin terms of stacksPP (null)
SP up
down
  
cont up
   red zone PP
SP   
suspend red zone
PP (null)
SP $
task 2 red zone suspend
transfers control a child to a (grand)parent
The pointer between child and parent is inverted 

Current status of the proposal
What has already been done Formal specication
Informal explainer documentation
Reference implementation
What is happening now An implementation in Wasmtime, a production-grade engine
What is going to happen next Fine-tune the implementation
Gathering performance evidence       

Summary
Summary Eect handlers provide a modular and extensible basis for stack switching in Wasm
Eect handlers are a proven technology
The extension to Wasm is minimal and compatible
Working on a production-grade implementation in Wasmtime
The work is actively being turned into a proposal; for more details see https://wasmfx.dev
Comments and feedback are welcome!     

References
Sitaram, Dorai (1993). Handling Control.In:
PLDI. ACM, pp. 147155.Ganz, Steven E., Daniel P. Friedman, and Mitchell Wand (1999). Trampolined Style.In:
ICFP. ACM,
pp. 1827. Plotkin, Gordon D. and Matija Pretnar (2013). Handling Algebraic Eects.In:
Logical Methods in
Computer Science 9.4.Haas, Andreas et al. (2017). Bringing the web up to speed with WebAssembly.In:
PLDI. ACM,
pp. 185200. Forster, Yannick et al. (2019). On the expressive power of user-dened eects: Eect handlers,
monadic reection, delimited control.In: J. Funct. Program.29, e15.HillerstrÃ¶m, Daniel (2021). Foundations for Programming and Implementing Eect Handlers.
PhD thesis. The University of Edinburgh, Scotland, UK. Sivaramakrishnan, K. C. et al. (2021). Retrotting eect handlers onto OCaml.In:
PLDI. ACM,
pp. 206221. Ghica, Dan et al. (2022). High-Level Type-Safe Eect Handlers in C
++
.In: Proc. ACM Program.
Lang. 6.OOPSLA, pp. 130.Thomson, Patrick et al. (2022). Fusing industry and academia at GitHub (experience report).In:
Proc. ACM Program. Lang. 6.ICFP, pp. 496511.                                     

Continuation binding, cancellation, and trapping
Partial continuation application cont.bind(type $ct ) : [ 
0 (
ref null $ct )] ! [(ref $ct 0
)]
where $ct :cont $ft and $ft : [
0 
1 ]
! [ 
]
and $ct 0
:cont $ft 0
and $ft 0
: [ 
1 ]
! [ 
]
Continuation cancellation
resume_
throw (tag $exn ) (tag $tag $h )
: [ 
0 (
ref null $ct )] ! [ 
]
where $exn : [
0 ]
! [], f$tag
i: [

i ]
! [ 
i ]
and $h
i : [

i (
ref null $ct
i)]
and
$ ct
i :
cont $ft
i and
$ft
i: [

i ]
! [ 
]g
i
and $ct :cont ([
] ! [ 
]
Control barriers barrier$lbl (type $bt )instr 
: [ 
] ! [ 
]
where $bt = [ 
] ! [ 
] and instr 
: [ 
] ! [ 
] 

"
Non,"IBM Security Guardium 11.3:Whatâs New in December? Functionality drives value Enrique GutiÃ©rrez ÃlvarezData Security Sales LeaderPublic Sector, Federal, Healthcare  MarketEnrique_Gutierrez@us.ibm.comDecember 10th2020 

Guardium Data Protection empowers you to meet your critical data protection needs with smarter capabilities2IBM Security / Â© 2020 IBM CorporationComplete visibilityActionable insightsReal-time controlsAutomated complianceAdvanced analyticsReal-time monitoring and alertingBlocking, masking, quarantining and encryptionPurpose-built patterns, reports, policies and workflowsVulnerability and risk assessment, issue remediationDiscovery and classificationCloudApplicationsMainframeFiles |Unstructured dataDatabases and warehouses | Structured dataBig data |  Semi-structured dataIBM Security Guardium Data ProtectionRisk-basedapproachDiscoverComplyProtectTransform 

The best of Data Security:  Guardium Data Protection with Guardium InsightsGuardiumInsights â¦Retainscollected data for years Finds threats faster or as they happenMakes it easy to prioritize response Makes it easy to protect data across environmentsEliminates silos Delivers a modernized architectureDeploys and scales flexibly âon premises or in public or private cloudsGuardiumData Protection â¦Simplifies compliance with monitoring, pre-built templates, policies & workflowsDeep capabilities with Discovery & classification, Real-time monitoring ,Dynamic data protection & separation of duties, Vulnerability assessmentBroad platform support across on-premises and cloud environments Scales to support the largest environments 

Guardium:Whatâs new on 12/7?MORE OF THE RIGHT STUFF!Group Name / DOC ID / Month XX, 2018 / Â© 2018 IBM Corporation4 

1)Game changer!! Introducing the Universal Connector: A lightweight new way to monitorÃ Supports agentless streaming of audit logs & provides a common framework toquickly develop new connectorsÃ Unlike GDDI agentless streaming, we normalize the data for reporting & analyticsÃ For traditional & modern sources: MongoDB and AWS S3 support in v11.32)More support for modern Cloud environments. Drives down TCO & helps with modernization.Ã Support for AWS Secrets Manager (AWSâs cloud-based cyber vault) Ã Backup/restore on Azure & AWS3)Easier compliance with policy rules tagging. Faster time to value, lower TCO.Ã Accelerators â¦ only better! New flexible way to apply rules & policies for multiple regulationsÃ Out-of-the box and custom tags  4)Enhanced Unified Health & Deployment Dashboard. Helps reduce TCO. Terrific client feedback!!Ã More tiles provide more insight into STAP & GIM health, at-a-glance 5)Expanding âconnected Securityâ. Automation. Reduced TCO. Enhanced collaboration with SOC.Ã Ticketing integration with IBM SOARÃ Expanding integration with IBM Cloud Pak for Data to enforce governance policiesÃ Classification support for MongoDB, vulnerability assessment for CouchbaseIBM Security / Â© 2020 IBM Corporation5Exciting capabilities GA in Guardium Data Protection 11.3!Announce 10/20, GA 12/712345 

GuardiumNative Audit LogsData source writes or pushes logs to storageSnifferPulls (or receives from Push) logsfrom data sourceUniversal ConnectorTransforms logs into a universal format that the Sniffer understands* Varies based on what is written to the native audit logs Agentlessmonitoring using the Guardium Universal Connector Ã With the introduction of the Universal Connector, Guardium can support the most flexible options for monitoring and data collection: STAP for real-time monitoring of on-premsourcesETAP/Proxy for real-time monitoring of Cloud sourcesAgentless for passive monitoring for Cloud & on-prem sourcesAPI streamingfor Cloud sourcesCaptures: Session, Request, and Error info 1 

Next Gen Accelerators: Flexible tagging for faster, easier complianceSupports two use cases1) Create a new  policy containing rules to support a regulation (PIPEDA) or multiple regulations2) As a Guardium admin, I want to create a new  policy containing rules to support PIPEDA. Note: Users can select multiple tags.3 

New tiles on the Health Dashboard share more health insights4 

Connected Security Example: Guardium DP & CP4D: Help CP4D customers easily enforce their governance policies 5ValueÃ¼Simplified out-of-box config in CP4D to secure data sourcesÃ¼Monitor access and protect dataÃ¼Simplify security and compliance reportingÃ¼Detect anomalous data usage & block suspicious activityCompetitive DifferentiationÃ¼CP4D + Guardium provides unique value âno competitors match upÃ¼Tighter integration w/ IBM & Open Source DBs vs. competitorsÃ¼Improved total cost of ownership with integrated common platformÃ¼End-to-end visibility & protection, which competitors lack in breadthCP4D 3.0.1 data sources supported by GDP: Ã¼DB2Ã¼DB2 WarehouseÃ¼Netezza Performance Server Ã¼Big SQLâ¢Data Virtualization (1Q20)â¢PostgreSQL (1Q21)â¢Cockroach DB (1Q21)â¢Mongo DB (1Q21)  

Guardium Insights v2.5Announce Oct. 10th, GA Dec. 7thHelp Guardium Data Protection clients reduce TCO by reducing reliance on aggregators and collectors. New in Guardium Insights v2.5:âCustomizable reports for power usersâSchedule and send reports to support audit workflowsâCreate & manage groups in Guardium InsightsIntegrate vulnerability assessment results in Guardium Insights for greater context and more efficient reporting Simplify maintenance and enhance ease of use with streamlined installation & deployment, consolidated Guardium health APIsâImprove Guardium operational maintenance with agent health infoEnable security analysts to respond faster to data-related threats with a seamless integration with the security operations center (SOC)âAutomated ticketing with SOAR in Cloud Pak for Security101234 

Meet compliance standards faster and more easily with flexible, customizable & repeatable reports âbuilt from scratch  Support automated audit processes with scheduled reporting  Create and manage groups in Guardium Insights, consolidated vulnerability assessment directly in InsightsBottom line: Customers can phase out aggregators & collectors to streamline their legacy costsReduce the need for GDP aggregators, improve TCOIBM Security / Â© 2019 IBM Corporation111 

Dramatically streamlined installation and deployment âÂ½ as many steps, get started fasterSimplify maintenance of Guardium infrastructure with consolidated Guardium health APIs âclients can feed their operational dashboards directly Simplified maintenance & deploymentIBM Security / Â© 2019 IBM Corporation123 

Reminder! Clients can leverage data insights as they modernize their SOC13Clients can now:Simplify their IT architecture by using the same OpenShift environment for IBM Cloud Pak for Security and Guardium InsightsEnable their security operations center to respond faster to data-related threats â3 clicks to open a ticket in cases or SOARâTicket may be auto-populated with asset & risk info for the SOC team  4 

ibm.com/securitysecurityintelligence.comxforce.ibmcloud.com@ibmsecurityyoutube/user/ibmsecuritysolutionsÂ© Copyright IBM Corporation 2019. All rights reserved. The information contained in these materials is provided for informational purposes only, and is provided AS IS without warranty of any kind, express or im plied.  Any statem ent of direction represents IBM's current intent, is subject to change or withdrawal, and represent only goals and objectives.  IBM, the IBM logo, and other IBM products and services are trademarks of the International Business Machines Corporation, in the United States, other countries or both. Other company, product,or service names may be tradem arks or service m arks of others.Statement of Good Security Practices: IT system security involves protecting systems and information through prevention, detection and response to improper access from within and outside your enterprise. Improper access can result in information being altered, destroyed, m isappropriated or misused or can result in damage to or misuse of your systems, including for use in attacks on others. No IT system or productshould be considered completely secure and no single product, service or security measure can be completely effective in preventing improper use or access. IBM systems, products and services are designed to be part of a lawful, comprehensive security approach, which will necessarily involve additional operational procedures, and m ay require other system s, products or services to be m ost effective. IBM  does notwarrant that any system s, products or services are immune from, or will make your enterprise immune from, the malicious or illegal conduct of any party.FOLLOW US ON:THANK YOUibm.com/security/communityIBM Security 

V8, V9, V10 âGuardium Implementation ArchitectureAggregatorsCollectorsMainframesDatabases (w/ S-TAPS-E-TAPS)Multi-cloudClient Server Tiered Monitoring:S-TAP & E-TAP âImmediate Action, Regulatory ComplianceModerate SOC IntegrationS-TAP or E-TAPS-TAPCentral Manager Apply PoliciesHealth StatusManage EnvironmentKey Challenges:Ease of Use âInfrastructure Management, Limited DashboardNot âContainerizedâ Modern ArchitectureRetention Periods hours/daysModerate SOC IntegrationModerate Service/Identity IntegrationModerate AnalyticsReporting Performance Variesz DB2Cloud Servers MonitoredScope: AWS, Azure, IBM, GoogleOn-Prem Servers Monitored 

Guardium v11 with Insights âNext Generation ArchitectureIBM VERIFYCentralManagerIBMSecurity ResilientCollectorsMainframesDatabases (w/ S-TAPS-E-TAPS)Multi-cloudGuardium InsightsData Security Hub(Replaces Aggregation Layer)IBMSecurity QRadarFlexible Tiered Monitoring:S-TAP & E-TAP âImmediate ActionStreaming âThreat AwarenessLogs âCompliance/RegulationExpanded Monitoring ChoicesSOC IntegrationDeep Identity & Service IntegrationRisk SpottingThreat AnalysisAlert RefinementLong Term RetentionPolicy TuningDeep AnalyticsDeep SOC IntegrationEnterprise DashboardStreamlined InfrastructureCloud Servers MonitoredScope: AWS RDS Posgres, SQL and Oracle Azure CosmoDBOn-Prem Servers Monitoredz DB2 

IBM Security Guardium Insights DifferentiatorsIBM Security / Â© 2020 IBM CorporationOur Guardium Platform is purpose-built to accelerate Data Security Programs to Identify, Classify, and Protect sensitive data across the enterpriseEnterprise Data Security Hub and DashboardRisk-Based Normalized Analytics  EngineContextual Integration with SIEM/SOCLong-Term Retention Advanced Cloud-Ready  Data SecurityCompliance Reporting   AcceleratorsAdvanced relationships with Cloud Providers enables timely flexible pre-built high-performance normalized Cloud Data Security Protection and Monitoring for both IaaS and DBaaS Cloud environments.  Use Machine-Learning to identify Data Security Risks. Establish effective granular policies and rules to alert on potential data security hot spots. Surface exposures based on weighted data sensitivity.Pre-built Integrations to SIEM/SOC tooling with advanced contextual alerting to reduce false-positives and provide easy âdrill-downâ functionality for SOC Analysts. Intuitive GUI for enriched  investigations. Retain years of Data Security Data in a normalized Data Lake. Perform advanced user behavior and threshold analytics across time. Capture and retain critical Privacy data. Cloud-Ready, Micro-Services, Enterprise Data Security Hub and Dashboard leveraging flexible data source monitoring, Machine Learning and Advanced Analytics for Outlier Detection, Risk Spotting and Privileged User Access Control.Tried and tested Pre-Built Identification and Classification algorithms to speed time to value. Pre-Built Accelerators for critical regulatory reporting and audit response. 

Blog:Slowing Data Security Tool Sprawl in a Hybrid Multicloud WorldMarketplace page:IBM Security Guardium InsightsData Sheet:IBM Guardium Insights Data SheetExplainer Video:ExploringGuardiumInsightsDemo: Product tourInfographic:Unifying Data Security with IBM Security Guardium InsightsFAQs:Frequently Asked QuestionsIDC Next Gen  Data Security Assets: White paper, Webcast, BlogEbooks: 5 Pitfalls of Data Security& Overcoming data security challenges in a hybrid multicloud worldGuardium Insights customer-facing assets 

Public-facing assets for Guardium Data ProtectionMarketing assets:â¢Smarter Data Security ebook and PDFâ¢5 Common Data Security Pitfalls to Avoid e-book â¢Overcoming data security challengesâ¦ e-book â¢Guardium Data Protection solution briefâ¢Guardium Data Protection for SAP HANA data sheetWebinars and tech talks:â¢Webinar: âContainers and Data Security: What You Need to Knowââ¢Webinar: âEnd-to-end data activity monitoring services and solution provide smarter data securityââ¢Tech Talks:â¢Hybrid multi-cloud data protection with IBM Security Guardium â¢Securing SAP Hana deployments with IBM Security Guardiumâ¢How CSOs can leverage their security infrastructure to comply with privacy mandatesProduct tours and demos:â¢Guardium Data Protection Product Tour(QwikTour)â¢Guardium Data Protection interactive demo 

"
Non,"                           	  	International Journal of Engineering Applied Sciences 	and Technology, 201	6      	  	
                         	Vol. 1, Issue 1	0, ISSN No. 2455	-2143, Pages 185	-18	9 	 	
                                         	Published Online August 	- September 2016 in IJEAST (http://www.ijeast.com)	 	
185	 	
 
 
SPEED PERFORMANCE BETWEEN SWIFT AND 	
OBJECTIVE	-C	 	
 	
Harwinder Singh	 	
Department	 of 	CSE	 	
DIET, Regional 	Centre	 PTU,	 	
Mohali, INDIA	 	
 
 
Abstracts	: The  appearance  of  a  new  programming 	
language  gives  the  necessity  to  contrast  its  contribution 
with the existing programming languages to evaluate the 
novelties  and  improvements  that  the  new  programming 
language  offers  for  developers.  Intended  to  eventually	 	
replace Objective	-C  as Appleâs language  of choice, Swift 	
needs  to  convince  developers  to  switch  over  to  the  new 
language.  Apple  has  promised  that  Swift  will  be  faster 
than Objective	-C, as well as offer more modern language 	
features,  be  very  safe,  and  be  e	asy  to  learn  and  use.  In 	
this thesis developer  test  these  claims by creating an iOS 
application  entirely  in  Swift  as  well  as  benchmarking 
two  di	ï¬erent  algorithms.  Developer  ï¬nds  that  while 	
Swift  is  faster  than  Objective	-C,  it  does  not  see  the 	
speedup proje	cted by Apple. Swift was launched to offer 	
an  alternative  to  Objective	-C  because  this  has  a  syntax 	
which barely evolved from it was created and has a great 
difference  with  other  programming  languages  that  have 
appeared in the latest years, because these ha	ve based on 	
the  C++  syntax.  For  this,  Swift  is  inspired  in  new 
programming  languages  like  C++11,  C#,  F#,  Go, 
Haskell,  Java,  JavaScript,  Python,  Ruby.  Then  his 
syntax  is  totally  different  than  its  predecessor.  The 
Swiftâs  syntax  is  more  simplified  because  i	t  does  not  use 	
pointers  and  includes  improvements  in  its  data 
structures and in its syntax.	 	
Keywords: 	Swift  vs  Objective	-C,  Swift  in  iOS  mobile  app, 	
Swift,	 	
I. 	INTRODUCTION	 	
In  the  summer  of  2008  Apple  launched  the  App  Store  for 
the  iPhone  and  iPod  touch.    Originally  containing  only  522 
apps,  as  of  2014  the  App  store  houses  over  1  million  apps 
and has seen over 75 billion app downloads.    This platform 
has  attracted  thousands  of	 developers  to  create  applications 	
for  iOS  devices,  and  has  launched  thousands  of  careers  and 
companies.    As    Objective	-C    aged    it    became    harder   	
for    new    developers,    unfamiliar with C and SmallTalk, to 
learn and understand.    Languages such as Java, P	ython, and 	
JavaScript  became  widely  used  and  began  to  set  the 
standard  for  modern  programming  languages.      Developers 
began to complain that Objective	-C  was di cult  to learn and 	
uncomfortable to use.   	 	
Swift  is  a  new  programming  language  for  iOS  and  OS  X 
apps  that  builds  on  the  best  of  C  and  Objective	-C,  without 	
the  constraints  of  C  compatibility.  Swift  adopts  safe 
programming  patterns  and  adds  modern  features  to  make 
programming  easier,  more  flexible,  and  more  fun.  Swiftâs 
clean  slate,  backed  by  the  mature	 and  much  loved  Cocoa 	
and  Cocoa  Touch  frameworks,  is  an  opportunity  to 
reimaging how software development works.	 	
II. 	LITERATURE SURVEY	 	
By  Christ  Lattner(2015)	Released    in    June    of    2014    by   	
Apple    Swift    is    a    statically    typed    language  and   
compiled   	language    that    uses    the    LLVM    compiler   	
infrastructure    and  the  Objective	-C  runtime  .    Since  Swift 	
uses the same runtime as Objective	-C    the    two    languages   	
can    be    intermixed    in    a    single    program    or  project, 
as  both  will  compile  down  to  native  ma	chine  code.    Swift 	
can  access  Objective	-C  classes,  types,  functions,  and 	
variables  through  a  ""bridging  header"",  as  well  as  by 
extension  C  and  C++  code.    Similarly  Objective	-C  can 	
access  code  written  in  Swift,  with  some  exceptions.    This 
allows  Swift  to  wor	k  with  the  Cocoa  and  Cocoa  Touch 	
frameworks  and  existing  Objective	-C  apps  and  libraries 	
without rewriting the large body of code that was written for 
iOS  devices.  Swift  is  heavily  influenced  by  many  other 
languages such as Rust, Haskell, Ruby, Python, and 	C#, and 	
offers  many  of  the  object	-oriented  and  functional  features 	
found  in  these  languages.    Swift  also  includes  a 
read	-eval	-print	-loop  (REPL)  that  can  be  accessed  in  Xcode 	
as well as on the command line.	 	
 
Prof.  Diwakar  Gupta: 	Many  previous  researchers  ha	ve 	
proposed  methods  for  evaluating  and  com	-paring  new 	
programming  languages.    Languages  are  often  compared 
against  one  another  on  a  number  of  different  criteria.     
Developers  compared  C++,  Java,  Perl,  and  Lisp  together 
and their approach was extended to ev	en more languages by 	
other  developers.    Both  of  these    papers    conclude    that   
each    language    has    various    pros    and    cons    and  are 
suited  to  different  types  of  tasks,  with  Java  and  C  receiving 
the most favourable reviews.    Many programming language 
eval	uations  examine  a  language  holistically  and 	
qualitatively,  although  attempts  have  been  made  to  be  more  

                           	  	International Journal of Engineering Applied Sciences 	and Technology, 201	6      	  	
                         	Vol. 1, Issue 1	0, ISSN No. 2455	-2143, Pages 185	-18	9 	 	
                                         	Published Online August 	- September 2016 in IJEAST (http://www.ijeast.com)	 	
186	 	
 
rigorous  and  quantitative.  Urban  propose  a  qualitative 
framework  for  assessing  languages  in  terms  of  twelve 
different  attributes  including  regularity,  re	adability, 	
reliability,  portability,  and  Input/output.      This    framework   
provides    a    standardized    way    to    evaluate    a   
language    in    isolation    and    describes    the    key   
attributes    important    in    any    language design.    Although 
these  frameworks  and  co	mparisons  help  unveil  the 	
important  aspects  of  a  programming  language,  they  are  too 
high  level  to  be  appropriately  applied  to  Swift.  Other 
researchers  have  looked  at  programming  languages  as  they 
apply  to  a  specific  domain.      Since    swift    is    meant    to   
be    used    primarily    for    mobile  devices,  this  type  of 
research  is  more  applicable.    Gupta  discusses  the 
appropriateness  of  programming  languages  for  teaching 
beginners  or  teaching,  ultimately  recommending  basic  or  C. 
Howatt  recommends  evaluating  a  language	 based  on  how 	
well  it  solves  a  given  project  or  task  on  hand  although  he 
has  doubts  about  the  real  world  relevance  of  this  approach. 
Oppermann  and  Compos  discuss  several  popular  languages 
used  for  mobile  clients  and  server	-side  development.    They 	
conclude 	that  using  single  language  on  both  the  mobile 	
client  and  server  offers  a  distinct  advantage  and  that  Java 
and  Python  are  the  best  choices  for  this  approach. 
Developers      use    design    patterns    to    evaluate    the    Go   
programming language .They implement a s	ubset of the Hot 	
Draw  framework  in  Go  and  use  their  implementation  to 
motivate  a  discussion  of  the  language  .    This  project  was 
the  main  source  of  inspiration  for  my  analysis  of  Swift, 
since  the  authors  used  a  large  project  to  demonstrate  their 
view  on  a  n	ew  language.    While  I  do  not  use  design 	
patterns  or  the  Hot  Draw  frame	-work  in  my  Swift 	
application.	 	
By  Appleâs  Developers  Team(2014)	Development  on  Swift 	
was  begun  in  July  2010  by	 Chris  Lattner,  with  the  eventual 	
collaboration  of  many  other  programmers  at	 Apple.  Swift 	
took  language  ideas 
""from	 Objective	-C,Rust,	 Haskell,	 Ruby,	 Python,	 C#,	 CLU, 	
and far too many others to list"".	 On June  2, 2014, the	 Apple 	
Worldwide  Developers  Conference	 (WWDC)  application 	
became  the  first  publicly  released  app  written  in 
Swif	t. A beta  version	 of  the	 programming  language	 was 	
released  to  registered  Apple  developers  at  the  conference, 
but  the  company  did  not  promise  that  the  final  version  of 
Swift  would  be	 source  code	 compatible  with  the  test 	
version.  Apple  planned  to  make  source	 code  converters 	
available if needed for the full release.	 	
The Swift Programming Language, a  free 500	-page  manual, 	
was  also  released  at  WWDC,  and  is  available  on 
the	 iBooks	 Store and the official website.	 	
Swift reached the  1.0 milestone  on September 9, 2014, with 
the	 Gold  Master	 of Xcode	 6.0  for	 iOS.Swift  1.1  was 	
released  on  October  22,  2014,  alongside  the  launch  of 
Xcode  6.1.	 Swift  1.2  was  released  on  April  8,  2015,  along 	
with  Xcode  6.3.	 Swift  2.0  was  anno	unced  at  WWDC  2015, 	
and was made available for publishing apps in the App Store 
in September 21, 2015.A Swift 3.0 roadmap was announced 
on  the  Swift  blog  on  December  3,  2015.	 However,  before 	
that,  an  intermediate  Swift  2.2	 embracing  new  syntax  and 	
features	 was  introduced.  This  also  omits  some  outdated 	
components including Tuple splat syntax, C	-style for loops.	 	
Swift  won  first  place  for	 Most  Loved  Programming 	
Language	 in  the	 Stack  Overflow	 Developer  Survey 	
2015	 and second place in 2016.	 	
Google  is  said  to  be  considering  using  swift  as  a  first  class 
language fo	r its operating system android.	 	During  the 	
wwd  2016,  apple  announced  an	 ipad  exclusive  app,  named 	
swift  playgrounds  that  will  easily  teach  people  how  to  code 
in swift. 	The	 app is presented in an interface which provides 	
feedback  when  lines  of  code  are  placed  in  a  certain  order 
and executed.	 	
III.	 	RESEARCH WORK	 	
Swift  needs  to  convince  develope	rs  to  switch  over  to  the 	
new  language.   	As	 it  assumed      that  swift  will  be  faster 	
than objective	-c, as well as swift is safe, and      easy to learn 	
and  use.   	In this  thesis  developers,  developing  an  ios 	
application  entirely  in  swift  as  well  as  benchmarking  t	wo 	
different  algorithms. 	As	 it's  mentioned  earlier  that  swift  is 	
faster  than  objective	-c  and  it  does  not  find  the  performance 	
projected  by	 DEVELOPERS	.  I ALSO  CONCLUDE  THAT 	SWIFT 	
HAS  MANY  ADVANTAGES 	OVER 	OBJECTIVE	-C, AND  IS  EASY 	
for  developers  to  learn  and  use.    However  there  are  some 
weak areas of Swift involving interactions with Objective	-C 	
and  the  strictn	ess  of  the  compiler  that  can  make  the 	
language  bit  difficult  to  work  with.      Apart  from  all  these 
drawbacks, Swift is overall a  successful software  generating 
language for us.	 	
The four stated goals of swift (safety, clarity, modernity, and 
performance)  reflect  both  an  accurate  analysis  of  the 
deficiencies  of  objective	-c  and  an  fair  assessment  of  the 	
various  developments  that  have  taken  place  ov	er  the  last 	
three  decades  in  programming  language  design	--mostly 	
object  oriented  languages  like  c++,  java,  python,  and  ruby, 
but  also  more  specialized  languages  such  as  the  functional 
language haskell.	 	
If  you  have  experience  using  Objective	-C  to  develop  for 	
Apple  platforms,  you  may  be  wondering:  âWhy  did  Apple 
release  a  new  language?â  After  all,  developers  had  been 
pro	ducing  high	-quality  apps  for  Mac  OS  X  and  iOS  for 	
years.  Apple  has  a  few  things  in  mind.  First,  Objective	-C  is 	
an older language. And while this is not always a problem, it 
leads  to  some  difficulty  in  this  case.  The  syntax  of 
Objective	-C  was  solidified  pri	or  to  the  rise  of  prominent 	
scripting  languages  in  the  1990s  that  popularized  more 
streamlined  and  elegant  syntax  (e.g.,  JavaScript,  Python, 
PHP,  Ruby,  and  others).  This  means  Objective	-C  feels 	
strange  to  most  developers  when  they  get  started,  so  its 
synta	x  can  be  an  impediment  to  developer  productivity. 	
Additionally,  as  an  older  language,  Objective	-C  is  missing  

                           	  	International Journal of Engineering Applied Sciences 	and Technology, 201	6      	  	
                         	Vol. 1, Issue 1	0, ISSN No. 2455	-2143, Pages 185	-18	9 	 	
                                         	Published Online August 	- September 2016 in IJEAST (http://www.ijeast.com)	 	
187	 	
 
many  advancements  developers  in  modern  languages 
currently enjoy. Also, Swift aims to be safe. Objective	-C did 	
not  aim  to  be  unsafe,  but  things  hav	e  changed  quite  a  bit 	
since  Objective	-C  was  released  in  the  1980s.  For  example, 	
the  Swift  compiler  aims  to  minimize  undefined  behavior, 
which  is  intended  to  save  the  developer  time  debugging 
code  that  failed  during  the  runtime  of  an  application. 
Another go	al of Swift is to be a suitable replacement for the 	
C  family  of  languages  (C,  C++,  and  Objective	-C).  That 	
means  Swift  has  be  fast.  Indeed,  Swift's  performance  is 
comparable  to  these  languages  in  most  cases.  Swift  gives 
you  safety  and  performance  all  in  a  c	lean,  modern  syntax. 	
The  language  is quite expressive;  developers can  write  code 
that  feels  natural.  This  feature  makes  Swift  a  joy  to  write 
and  easy  to  read,  which  makes  it  great  for  collaborating  on 
larger projects.	 	
3.1 Objectives	 	
â¢ To  calculate  speed  o	f  code  execution  with  swift 	
Language  over  Objective	-c  and  other  Programming 	
Language used in iOS mobile application.	 	
To  measure  performance  of  swift  language  on  iOS  mobile 
application platform.	 	
Finding  out  Feature  set  and  safety  course  for  Swift 
Langua	ge.	 	
3.2 Software And Hardware Requirements	 	
3.2.1 	Software Used:	 	
â¢ Mac operating system El	-Capitan	 	
IDE Xcode(Latest 7.3)	 	
Language Swift (Version 2.2)	 	
3.2.2 	Hardware Used:	 	
â¢ Mac  mini  system  form  Apple  or  Other  Mac  CPU  with 
OSX installed.	â¨	 	
 
3.4 	Methodology	 	
Swift  and  Objective	-C  compilers  are  based  on  the  LLVM 	
Compiler  Infrastructure,  and  there  is  a  single  iOS  SDK  for 
both  Swift  and  Objective	-C.  That's  why  there  isnâ	t  much 	
difference  between  the  ways  the  programming  languages 
work with the Cocoa frameworks.	 	
We  decided  to  examine  both  Swift  and  Objective	-C 	
performance by comparing their data structures.  For that we 
took  Objective	-C  Foundation  framework  and  Swiftâs  nati	ve 	
solutions. 	 	
In  Swift,  all  classes  are  created  during  compile	-time. 	
Methods cannot be added on	-the	-fly and all types are known 	
before  the  run  time.  Since  everything  is  known  beforehand, 
a compiler can optimize code without any problem.	 	
Objective	-C,  on  t	he  other  hand,  canât  optimize  as 	
effectively, because all dynamic languages work slower than 
static.	 	
Swift drops the  two	-file requirement.  Xcode  and the  LLVM 	
compiler  can  figure  out  dependencies  and  perform 	
incremental  builds  automatically  in  Swift  1.2.  As	 a  result, 	
the repetitive task of separating the table of	 contents (header 	
file)  from  the  body  (implementation  file)  is  a  thing  of  the 
past.  Swift  combines  the  Objective	-C  header  (.h)  and 	
implementation files (.m) into a single code file (.swift).	 	
Objectiv	e-Câs  two	-file  system  imposes  additional  work  on 	
programmers 	-- and  its  work  that  distracts  programmers 	
from  the  bigger  picture.  In  Objective	-C  you  have  to 	
manually  synchronize  method  names  and  comments 
between  files,  hopefully  using  a  standard  convention,	 but 	
this  isnât  guaranteed  unless  the  team  has  rules  and  code 
reviews in place.	 	
IV.	 	RESULT	 	
Test performed with an algorithm in each language to sort a 
list  of  1,000,000  objects  in  ascending  order.  The  objects 
were  sorted  in  order  based  on  a  randomly  generated 
numerical instance variable that ranged from â1000 to 1000. 
Since C does not support ob	jects a struct was used instead. I 	
ran  each  algorithm  25  times  and  plotted  the  results.  Swift 
and  Objective	-C  both  performed  approximately  equal, 	
running  on  average  1.4x  faster  than  Python.  However  both 
languages  paled  in  comparison  to  Java  and  C,  with  Jav	a 	
being on average twice as fast as either language.	 	
 
~xdeN lxpmkpfx N ereoN reN N flue lu x N e kN errderrN frep txeN
ekoxeexpN   eeN rON iOmhN ruokp eefN  ON xrteN kmN eexN egxN
urOolroxesN ror ON plOO OoN xrteN ruokp eefN  eN e fxeN  ON xrteN	
urOolrox NyONee eNexeeNÂ  meNmrpNklel	xpmkpfx Ndexte gx-	sNd N
rN e oO etrOeN frpo OsN plOO OoN kON rgxproxN rufkeeN nrdN mreexpN	
eerON l eekON rO N edN mreexpN eerON dexte gx-	s N Âe uxN ee uuN OkeN	
tl exN reN mreeN reN ssN Â  meâ	eN lxpmkpfrOtxN  reN kON lrpN   eeN
zrgr N Âe uxN eexexN pxelueeN rpxN rllpkd frexN eex N ee uuN eek N
eereN  eN  eN lOu oxu N eereN Â  meN  eN tl exN reN mreeN reN lpkextex N d N
qllux 	 	
 
Apart  from  the  performance  benchmark  swift  contains  the 
feature  list,  taken  from    other  programming  languages  as 
they are listed below.	â¨	 	
 
SWIFT 	FEATURE	 	
LANGUAGES WITH 	 	
SIMILAR FEATURES	 	
CLOSURES	 	JAVA	SCRIPT	 	
GENERICS	 	JAVA 	 	
TYPE 	INFERENCE	 	HASKELL	 	
TUPLES	 	PYTHON	 	
FUNCTIONS  AS 	FIRST 	
CLASS 	OBJECTS	 	
JAVA	SCRIPT	 	
OPERATOR 	OVERLOADING	 	C++	 	
PATTERN 	MATCHING 	 	SCALA	,HASKELL	  

                           	  	International Journal of Engineering Applied Sciences 	and Technology, 201	6      	  	
                         	Vol. 1, Issue 1	0, ISSN No. 2455	-2143, Pages 185	-18	9 	 	
                                         	Published Online August 	- September 2016 in IJEAST (http://www.ijeast.com)	 	
188	 	
 
OPTIONAL 	TYPES	 	HASKELL	, RUST	 	
AUTOMATIC 	REFERENCE 	
COUNTING	 	
OBJECTIVE	-C 	
PROTOCOLS	 	JAVA	,C++	 	
READ	-EVAL	-PRINT	-LOOP 	
(REPL)	 	PYTHON	 	
 
 	
V. 	CONCLUSION	 	
Swift's  level  of  raw  processing  performance  has  yet  to  be 
credibly  demonstrated.  But  there  is  good  reason  to  believe 
that  this  will  be  achieved,  with  Swift  performing  far  faster 
than  Python  and  Ruby,  at  least  modestly  faster  than 
Objective	-C,  and  even  slig	htly  faster  than  C  in  some 	
circumstances.	 	
Swift  is  not  yet  a  production  language  for  large	-scale 	
projects.  Small  and  perhaps  medium	-scale  apps  have  been 	
successfully  built  and  are  in  the  App  Store,  built  by 
developers  with  a  certain  persistence  and  willing	ness  to 	
work  around  problems.  The  compiler  is  slow,  with  delays 
apparently  exponentially  related  to  the  amount  of  code  and 
dependent  upon  which  files  and  modules  the  code  is 
packaged in. Error messages  are  not very informative, often 
at  an  extremely  low  le	vel.  Automatic  bridging  that  is 	
intended to make Swift work ""seamlessly"" with the iOS API 
doesn't  always  work.  As  discussed  earlier,  the  compiler 
often  produces  poorly  optimised  code,  including  code  with 
extraneous  retain	-release  statements.  The  Xcode  inte	ractive 	
development  environment  can  be  slow  to  respond  with 
autocompletion  and  with  flagging  errors  and  turning  error 
flags off when they have been fixed.	 	
The  reaction  to  Swift  from  the  developer  community  has 
generally been very positive. Criticism has be	en minor. This 	
is perhaps surprising given the programmer culture in which 
people  have  strong  and  often  crazy  opinions  about 
everything, no matter how ill	-informed.	 	
Some  of  the  negative  reactions  have  been  predictable: 
Enthusiasts  of  scripting  languages  li	ke  JavaScript  think  that 	
not  having  implicit  type  conversion  (automatically 
converting  a  data  value's  type  when  necessary)  makes  the 
language  too  inflexible.  This  is  mostly  a  philosophical 
disagreement  that  has  no  solution.  Some  programmers  feel 
that  error	s  are  inevitable  and  want  their  programs  to  run  no 	
matter  what.  Others  want  them  to  quickly  fail  in  hopes  of 
getting every	 LAST BUG OUT	. 	
O Some major projects that were started in Swift reverted to 
Objective	-C  when  compiler  and  other  issues  developed. 	
Most	 of  these  projects  are  looking  to  migrate  to  Swift  as 	
soon as they reasonably can.	 	
Some  programmers  will  not  be  so  quick  to  switch. 
Automatic  Reference  Counting  has  solved  the  most 
significant  annoyance  with  Objective	-C  (having  to  allocate 	
and deallocate m	emory manually) and its source of the most 	
critical  errors  (memory  leaks  when  memory  not  deallocated 
properly  and  crashes  when  deallocation  is  done 
unnecessarily).  Many  programmers  will  have  worked  with 
Objective	-C so long that they have adapted to its qui	rks, are 	
blind  to  its  confusing  aspects,  and  can  work  productively 
with it (although they surely spend a lot of time debugging.) 
There  are  other  programmers  who  like  doing  tricky  (and 
arguably unsafe) things with low level pointers. 	 	
Apple  is quick to depr	ecate  APIs that  it  no longer considers 	
those  it  wants  developers  to  use,  and  is  aggressive  about 
pushing  users  and  developers  to  the  latest  versions  of  iOS 
and  to  relatively  recent  hardware.  But  Apple  clearly  needs 
Objective	-C  to  maintain  the  legacy  APIs, 	and  parts  of  the 	
iOS  and  OS  X  operating  systems,  that  have  been  written  in 
it.  And  there  is  a  large  base  of  app  code  that  has  been 
written in Objective	-C.  Apple is  unlikely to be  very quickly 	
aggressive  about  getting  to  developers  to  switch  to  Swift. 
But  A	pple  has  made  it  easy  to  mix  Swift  and  Objective	-C 	
code.  It  is  quite  possible  that  Apple  will  slowly  nudge 
developers  in  the  direction  of  using  Swift,  without 
prohibiting  Objective	-C.  This  might  include  requiring  aÃpps 	
being  submitted  to  the  App  Store  to  b	e  have  their  root 	
controller  written  in  Swift  but  allow  calling  on  Objective	-C 	
code.  It  could  also  involve  developing  new  APIs  for  Swift 
that do not work in Objective	-C. 	
VI.	 	REFERENCES	 	
 
 [1]	 	@ADAMJLEONARD	, 	@THINKCLAY	, 	AND 	
@CESAR	_DEVERS	, 	âSWIFT 	TOOLBOX	,â 	
HTT	P://WWW	.SWIFTTOOLBOX	.IO/, 	2014.	 	[O	NLINE	]. 	
AVAILABLE	: HTTP	://WWW	.SWIFTTOOLBOX	.IO/. [A	CCESSED	: 	
17	-APR-2015].	 	
 [2]	 E. GONZÃLEZ	, H. FERNÃNDEZ	, AND 	V. DÃAZ	, âG	ENERAL 	
PURPOSE 	MDE	 TOOLS	,â INT	. J. INTERACT	. MULTIMED	. 	
ARTIF	. INTELL	., VOL	. 1, PP. 72â75,	 2008.	  	
[3]	 E. R. NÃÃEZ	-VALDEZ	, O. SANJUAN	-MARTINEZ	, C. P. G. 	
BUSTELO	, J. M.	 C. LOVELLE	, AND 	G. INFANTE	-HERNANDEZ	, 	
âG	ADE	4ALL	: DEVELOPING 	MULTIPLATFORM 	VIDEOGAMES 	
BASED  ON 	DOMAIN 	SPECIFIC 	LANGUAGES  AND 	MODEL 	
DRIVEN 	ENGINEERING	,â INT	. J. INTERACT	. MULTIMED	. 	
ARTIF	. INTELL	., VOL	. 2, NO	. REGULAR 	ISSUE	, PP. 33	â42,	 	
2013.	 	
 [4]	 R. GONZALEZ	-CRESPO	, S. R. AGUILAR	, R. F. ESCOBAR	, 	
AND 	N. TORRES	, âDYNAMIC	, ECOLOGICAL	, ACCESSIBLE  AND 	
3D	 VIRTUAL 	WORLDS	-BASED 	LIBRARIES  USING 	OPEN	SIM 	
AND 	SLOODLE  ALONG  WITH  MO	BILE  LOCATION  AND 	NFC	 	
FOR  CHECKING  IN	,â INT	. J. INTERACT	. MULTIMED	. ARTIF	. 	
INTELL	., VOL	. 1, NO	. 7, PP. 63â69,	 2012.	  	
[5]	 	A. 	PUDER  AND 	I.Y	OON	, 	âSMAR	TPHONE 	
CROSS	-COMPILATION 	FRAMEWORK  FOR 	MULTIPLAYER 	
ONLINE 	GAMES	â IN 	2ND 	INT	. CONF	. ON 	MOBILE	, HYBRID	, 	
AND 	ON-LINE 	LEARNING	, 2010	 Â© IEEE	 COMPUTER 	SOCIETY	.  

                           	  	International Journal of Engineering Applied Sciences 	and Technology, 201	6      	  	
                         	Vol. 1, Issue 1	0, ISSN No. 2455	-2143, Pages 185	-18	9 	 	
                                         	Published Online August 	- September 2016 in IJEAST (http://www.ijeast.com)	 	
189	 	
 
DOI	: 10.1109/	ELML.2010.13.	 	
 [6]	 A. PUDER AND 	P. ANTEBI	, âC	ROSS	-COMPILING 	ANDROID 	
APPLICATIONS TO I	OS	 AND 	WINDOWS 	PHONE 	7â IN 	MOBILE   	 	
NETW	APPL	, 2012	 Â© SPRINGER 	SCIENCE	+B	USINESS 	MEDIA	, 	
LLC.	 DOI	: 10.1107/	S11036	-012	-0374	-2  	
[7]I	NDERJEET 	SINGH AND 	MANUEL 	PALMIERI	, âC	OMPARISON 	
OF CROSS	-PLATFORM 	MOBILE 	DEVELOPMENT 	TOOLS	â, IDT	 	
MALARDALEN 	UNIVERSITY	 	
 [8]	 C. XIN, âCROSS	-PLATFORM 	MOBILE 	PHONE 	GAME 	
DEVELOPMENT 	ENVIRONMENT	â 	IN 	INT	. 	CONF	. 	ON 	
INDUSTRIAL  AND 	INFORMATION 	SYSTEMS	, 2009	 Â© IEEE	 	
COMPUTER 	SOCIETY	. DOI	: 10.1109/IIS.2009.96	 	
[9]I	NC	. 	APPLE	. 	USING 	SWIFT  WITH 	COCOA  AND 	
OBJECTIVE	-C. 2. APPLE	, INC	., 2014.	 	
[10]	 SULTAN 	S AL-QAHTANI	, PAWEL	PIETRZYNSKI	, LUIS 	F 	
GUZMAN	, RAï¬K	ARIF	, AND 	ADRIEN 	TEVOEDJRE	. COMPARING 	
SELECTED  CRITERIA  OF	 PROGRAMMING  LANGUAGE	S  JAVA	, 	
PHP	, C++,	 PERL	, HASKELL	, ASPECTJ	, RUBY	, COBOL	, BASH 	
SCRIPTS  AND  SCHEME  R	EVISION 	1.0	-A  TEAM  CPLGROUP 	
COMP	6411	-S10 TERM R	EPORT	. 2012.	 	
[11]	 LEX 	FRIEDMAN	. THE  APP  STORE  TURNS  ï¬	VE	: A LOOK 	
BACKWARD  AND  FORWARD	. 	
HTTP	://WWW	.MACWORLD	.COM	/ARTICLE	/2043841/	 	
THE	-APP	-STORE	-TURNS	-FIVE	-A-LOOK	-BACK	-AND	-FORWARD	. 	
HTML	, JULY 	2013.	 	
[12]	       	JAMES 	HOWATT	. A PROJECT	-BASED  APPROACH  TO 	
PROGRAMMING 	LANGUAGE  EVALUATION	. ACM	 SIGPLA	N 	
NOTICES	, 30(7):37	â40,	 1995.	 	
[13]P	RASHANT 	KULKARNI	, 	HD	 	KAILASH	, 	VAIBHAV 	
SHANKAR	, 	SHASHI	NAGARAJAN	, 	AND 	DL	 	GOUTHAM	. 	
PROGRAMMING LANGUAGES	: A COMPARATIVE STUDY	. 2008.	 	
[14]	 STEPHEN 	G. KOCHAN	. PROGRAMMING  IN 	OBJECTIVE	-C. 	
SAMS 	PUBLISHING	, 1999.	 	
 
 
 
 
 
 
 
 	
 	
 
  

"
Non,"Micro	Focus	
Fortify	Static	Code	Analyzer	
Software	Version:	20.2.0	
User	Guide	
Document	Release	Date:	Revision	1:Â December	2020	
Software	Release	Date:	November	2020 

Legal	Notices	
Micro	Focus	
The	Lawn	
22-	30	Old	Bath	Road	
Newbury,	Berkshire	RG14	1QN	
UK
https://www.microfocus.com
Warranty
The	only	warranties	for	products	and	services	of	Micro	Focus	and	its	affiliates	and	licensors	(âMicro	Focusâ)	are	set	forth	in	
the	express	warranty	statements	accompanying	such	products	and	services.	Nothing	herein	should	be	construed	as	
constituting	an	additional	warranty.	Micro	Focus	shall	not	be	liable	for	technical	or	editorial	errors	or	omissions	contained	
herein.	The	information	contained	herein	is	subject	to	change	without	notice.	
Restricted	Rights	Legend	
Confidential	computer	software.	Except	as	specifically	indicated	otherwise,	avalid	license	from	Micro	Focus	is	required	for	
possession,	use	or	copying.	Consistent	with	FAR	12.211	and	12.212,	Commercial	Computer	Software,	Computer	Software	
Documentation,	and	Technical	Data	for	Commercial	Items	are	licensed	to	the	U.S.	Government	under	vendor's	standard	
commercial	license.	
Copyright	Notice	
Â©	Copyright	2003	-2020	Micro	Focus	or	one	of	its	affiliates	
Trademark	Notices	
All	trademarks,	service	marks,	product	names,	and	logos	included	in	this	document	are	the	property	of	their	respective	
owners.
Documentation	Updates	
The	title	page	of	this	document	contains	the	following	identifying	information:	
l	Software	Version	number	
l	Document	Release	Date,	which	changes	each	time	the	document	is	updated	
l	Software	Release	Date,	which	indicates	the	release	date	of	this	version	of	the	software	
This	document	was	produced	on	December	09,	2020	.To	check	for	recent	updates	or	to	verify	that	you	are	using	the	most	
recent	edition	of	adocument,	go	to:	
https://www.microfocus.com/support/documentation	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	2	of	216 

Contents	
Preface	12	
Contacting	Micro	Focus	Fortify	Customer	Support	12	
For	More	Information	12	
About	the	Documentation	Set	12	
Change	Log	13	
Chapter	1:	Introduction	17	
Fortify	Static	Code	Analyzer	17	
Fortify	ScanCentral	SAST	18	
Fortify	Scan	Wizard	18	
Fortify	Software	Security	Content	18	
About	the	Analyzers	19	
Related	Documents	20	
All	Products	21	
Micro	Focus	Fortify	ScanCentral	SAST	21	
Micro	Focus	Fortify	Software	Security	Center	22	
Micro	Focus	Fortify	Static	Code	Analyzer	22	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	25	
Fortify	Static	Code	Analyzer	Component	Applications	25	
About	Downloading	the	Software	27	
About	Installing	Fortify	Static	Code	Analyzer	and	Applications	27	
Installing	Fortify	Static	Code	Analyzer	and	Applications	28	
Installing	Fortify	Static	Code	Analyzer	and	Applications	Silently	(Unattended)	29	
Installing	Fortify	Static	Code	Analyzer	and	Applications	in	Text-	Based	Mode	on	
Non-Windows	Platforms	32	
Manually	Installing	Fortify	Security	Content	32	
Using	Docker	to	Install	and	Run	Fortify	Static	Code	Analyzer	33	
Creating	aDockerfile	to	Install	Fortify	Static	Code	Analyzer	33	
Running	the	Container	34	
Example	Docker	Run	Commands	for	Translation	and	Scan	35	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	3	of	216 

About	Upgrading	Fortify	Static	Code	Analyzer	and	Applications	35	
Notes	About	Upgrading	the	Fortify	Extension	for	Visual	Studio	36	
About	Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	36	
Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	36	
Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	Silently	37	
Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	in	Text-	Based	Mode	on	Non-	
Windows	Platforms	37	
Post-	Installation	Tasks	38	
Running	the	Post-	Install	Tool	38	
Migrating	Properties	Files	38	
Specifying	aLocale	39	
Configuring	for	Security	Content	Updates	39	
Configuring	the	Connection	to	Fortify	Software	Security	Center	40	
Removing	Proxy	Server	Settings	40	
Chapter	3:	Analysis	Process	Overview	41	
Analysis	Process	41	
Parallel	Processing	42	
Translation	Phase	42	
Mobile	Build	Sessions	43	
Mobile	Build	Session	Version	Compatibility	43	
Creating	aMobile	Build	Session	43	
Importing	aMobile	Build	Session	43	
Analysis	Phase	44	
Higher-	Order	Analysis	45	
Modular	Analysis	45	
Modular	Command-	Line	Examples	45	
Translation	and	Analysis	Phase	Verification	46	
Chapter	4:	Translating	Java	Code	47	
Java	Command-	Line	Syntax	47	
Java	Command-	Line	Options	48	
Java	Command-	Line	Examples	50	
Handling	Resolution	Warnings	50	
Java	Warnings	50	
Using	FindBugs	51	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	4	of	216 

Translating	Java	EE	Applications	52	
Translating	Java	Files	52	
Translating	JSP	Projects,	Configuration	Files,	and	Deployment	Descriptors	52	
Java	EE	Translation	Warnings	53	
Translating	Java	Bytecode	53	
Troubleshooting	JSP	Translation	Issues	54	
Chapter	5:	Translating	Kotlin	Code	55	
Kotlin	Command-	Line	Syntax	55	
Kotlin	Command-	Line	Options	56	
Kotlin	Command-	Line	Examples	57	
Kotlin	and	Java	Translation	Interoperability	57	
Chapter	6:	Translating	Visual	Studio	and	MSBuild	Projects	58	
Visual	Studio	and	MSBuild	Project	Translation	Prerequisites	58	
Visual	Studio	and	MSBuild	Project	Translation	Command-	Line	Syntax	59	
Handling	Special	Cases	for	Translating	Visual	Studio	and	MSBuild	Projects	59	
Running	Translation	From	aScript	59	
Translating	Plain	.NET	and	ASP.NET	Projects	60	
Translating	C/C++	and	Xamarin	Projects	60	
Translating	Projects	with	Settings	Containing	Spaces	60	
Translating	aSingle	Project	from	aVisual	Studio	Solution	60	
Translating	Visual	Studio	Solutions	with	Excluded	or	Skipped	Projects	61	
Working	with	Multiple	Targets	and	Projects	for	MSBuild	Command	61	
Analyzing	Projects	That	Build	Multiple	Executable	Files	61	
Alternative	Ways	to	Translate	Visual	Studio	and	MSBuild	Projects	62	
Alternative	Translation	Options	for	Visual	Studio	Solutions	62	
Translating	Without	Explicitly	Running	Fortify	Static	Code	Analyzer	62	
Chapter	7:	Translating	C	and	C++	Code	64	
C	and	C++	Â Code	Translation	Prerequisites	64	
C	and	C++	Command-	Line	Syntax	64	
Scanning	Pre-	processed	C	and	C++	Code	65	
C/C++	Precompiled	Header	Files	65	
Troubleshooting	Translation	Failed	Message	65	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	5	of	216 

Chapter	8:	Translating	JavaScript	and	TypeScript	Code	67	
Translating	Pure	JavaScript	Projects	67	
Excluding	Dependencies	67	
Excluding	NPM	Â Dependencies	68	
Translating	JavaScript	Projects	with	HTML	Files	68	
Including	External	JavaScript	or	HTML	in	the	Translation	69	
Chapter	9:	Translating	Python	Code	71	
Python	Translation	Command-	Line	Syntax	71	
Including	Import	Files	71	
Including	Namespace	Packages	72	
Using	the	Django	Framework	with	Python	72	
Python	Command-	Line	Options	72	
Python	Command-	Line	Examples	73	
Chapter	10:	Translating	Code	for	Mobile	Platforms	74	
Translating	Apple	iOS	Projects	74	
iOS	Â Project	Translation	Prerequisites	74	
iOS	Â Code	Analysis	Command-	Line	Syntax	75	
Translating	Android	Projects	75	
Android	Project	Translation	Prerequisites	75	
Android	Code	Analysis	Command-	Line	Syntax	76	
Filtering	Issues	Detected	in	Android	Layout	Files	76	
Chapter	11:	Translating	Go	Code	77	
Go	Command-	Line	Syntax	77	
Go	Command-	Line	Options	77	
Resolving	Dependencies	78	
Chapter	12:	Translating	Ruby	Code	79	
Ruby	Command-	Line	Syntax	79	
Ruby	Command-	Line	Options	79	
Adding	Libraries	80	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	6	of	216 

Adding	Gem	Paths	80	
Chapter	13:	Translating	Apex	and	Visualforce	Code	81	
Apex	Translation	Prerequisites	81	
Apex	and	Visualforce	Command-	Line	Syntax	81	
Apex	and	Visualforce	Command-	Line	Options	82	
Downloading	Customized	Salesforce	Database	Structure	Information	82	
Chapter	14:	Translating	COBOL	Code	84	
Preparing	COBOL	Source	and	Copybook	Files	for	Translation	84	
COBOL	Command-	Line	Syntax	85	
Translating	COBOL	Â Source	Files	Without	File	Extensions	86	
COBOL	Â Command-	Line	Options	86	
Chapter	15:	Translating	Other	Languages	and	Configurations	88	
Translating	PHP	Code	88	
PHP	Â Command-	Line	Options	88	
Translating	ABAP	Code	89	
INCLUDE	Processing	90	
Importing	the	Transport	Request	90	
Adding	Fortify	Static	Code	Analyzer	to	Your	Favorites	List	91	
Running	the	Fortify	ABAP	Extractor	92	
Uninstalling	the	Fortify	ABAP	Extractor	96	
Translating	Flex	and	ActionScript	97	
Flex	and	ActionScript	Command-	Line	Options	97	
ActionScript	Command-	Line	Examples	98	
Handling	Resolution	Warnings	99	
ActionScript	Warnings	99	
Translating	ColdFusion	Code	99	
ColdFusion	Command-	Line	Syntax	99	
ColdFusion	Command-	Line	Options	100	
Translating	SQL	100	
PL/SQL	Command-	Line	Example	101	
T-	SQL	Command-	Line	Example	101	
Translating	Scala	Code	101	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	7	of	216 

Translating	ASP/VBScript	Virtual	Roots	101	
Translating	Dockerfiles	103	
Classic	ASP	Command-	Line	Example	104	
VBScript	Command-	Line	Example	104	
Chapter	16:	Integrating	into	aBuild	105	
Build	Integration	105	
Make	Example	106	
Modifying	aBuild	Script	to	Invoke	Fortify	Static	Code	Analyzer	106	
Touchless	Build	Integration	107	
Ant	Integration	107	
Gradle	Integration	107	
Including	Verbose	and	Debug	Options	108	
Maven	Integration	109	
Installing	and	Updating	the	Fortify	Maven	Plugin	109	
Testing	the	Fortify	Maven	Plugin	Installation	109	
Using	the	Fortify	Maven	Plugin	110	
Chapter	17:	Command-	Line	Interface	112	
Translation	Options	112	
Analysis	Options	114	
Output	Options	117	
Other	Options	120	
Directives	122	
Specifying	Files	and	Directories	123	
Chapter	18:	Command-	Line	Utilities	125	
Fortify	Static	Code	Analyzer	Utilities	125	
About	Updating	Security	Content	126	
Updating	Security	Content	127	
fortifyupdate	Command-	Line	Options	127	
Working	with	FPR	Files	from	the	Command	Line	128	
Merging	FPR	Files	129	
Displaying	Analysis	Results	Information	from	an	FPR	File	130	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	8	of	216 

Extracting	aSource	Archive	from	an	FPR	File	134	
Allocating	More	Memory	for	FPRUtility	135	
Generating	Reports	from	the	Command	Line	135	
Generating	aBIRT	Report	136	
Generating	aLegacy	Report	138	
Checking	the	Fortify	Static	Code	Analyzer	Scan	Status	139	
SCAState	Utility	Command-	Line	Options	140	
Chapter	19:	Improving	Performance	142	
Hardware	Considerations	142	
Sample	Scans	143	
Tuning	Options	144	
Quick	Scan	145	
Limiters	145	
Using	Quick	Scan	and	Full	Scan	146	
Configuring	Scan	Speed	with	Speed	Dial	146	
Breaking	Down	Codebases	147	
Limiting	Analyzers	and	Languages	148	
Disabling	Analyzers	148	
Disabling	Languages	149	
Optimizing	FPR	Â Files	149	
Filter	Files	149	
Excluding	Issues	from	the	FPR	Â with	Filter	Sets	150	
Excluding	Source	Code	from	the	FPR	150	
Reducing	the	FPR	Â File	Size	151	
Opening	Large	FPR	Â Files	152	
Monitoring	Long	Running	Scans	153	
Using	the	SCAState	Utility	154	
Using	JMX	Tools	154	
Using	JConsole	154	
Using	Java	VisualVM	154	
Chapter	20:	Troubleshooting	156	
Exit	Codes	156	
Memory	Tuning	157	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	9	of	216 

Java	Heap	Exhaustion	157	
Native	Heap	Exhaustion	158	
Stack	Overflow	158	
Scanning	Complex	Functions	159	
Dataflow	Analyzer	Limiters	160	
Control	Flow	and	Null	Pointer	Analyzer	Limiters	161	
Issue	Non-	Determinism	161	
Accessing	Log	Files	162	
Configuring	Log	Files	162	
Understanding	Log	Levels	163	
Reporting	Issues	and	Requesting	Enhancements	164	
Appendix	A:	Filtering	the	Analysis	165	
Filter	Files	165	
Filter	File	Example	165	
Appendix	B:	Fortify	Scan	Wizard	168	
Preparing	to	use	the	Fortify	Scan	Wizard	168	
Starting	the	Fortify	Scan	Wizard	169	
Appendix	C:	Sample	Projects	170	
Basic	Samples	170	
Advanced	Samples	172	
Appendix	D:	Fortify	Java	Annotations	175	
Dataflow	Annotations	175	
Source	Annotations	176	
Passthrough	Annotations	176	
Sink	Annotations	177	
Validate	Annotations	178	
Field	and	Variable	Annotations	178	
Password	and	Private	Annotations	178	
Non-	Negative	and	Non-	Zero	Annotations	179	
Other	Annotations	179	
Check	Return	Value	Annotation	179	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	10	of	216 

Dangerous	Annotations	179	
Appendix	E:	Configuration	Options	180	
Fortify	Static	Code	Analyzer	Properties	Files	180	
Properties	File	Format	180	
Precedence	of	Setting	Properties	181	
fortify-	sca.properties	182	
fortify-	sca-	quickscan.properties	211	
Send	Documentation	Feedback	216	
User	Guide	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	11	of	216 

Preface
Contacting	Micro	Focus	Fortify	Customer	Support	
Visit	the	Support	website	to:	
l	Manage	licenses	and	entitlements	
l	Create	and	manage	technical	assistance	requests	
l	Browse	documentation	and	knowledge	articles	
l	Download	software	
l	Explore	the	Community	
https://www.microfocus.com/support
For	More	Information	
For	more	information	about	Fortify	software	products:	
https://www.microfocus.com/solutions/application-	security	
About	the	Documentation	Set	
The	Fortify	Software	documentation	set	contains	installation,	user,	and	deployment	guides	for	all	
Fortify	Software	products	and	components.	In	addition,	you	will	find	technical	notes	and	release	notes	
that	describe	new	features,	known	issues,	and	last-	minute	updates.	You	can	access	the	latest	versions	of	
these	documents	from	the	following	Micro	Focus	Product	Documentation	website:	
https://www.microfocus.com/support/documentation
User	Guide	
Preface
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	12	of	216 

Change	Log	
The	following	table	lists	changes	made	to	this	document	.Revisions	to	this	document	are	published	
between	software	releases	only	if	the	changes	made	affect	product	functionality.	
Software	Release	/	
Document	Version	Changes	
20.2.0	/Revision	
1:	Â December	2020	
Updated:
l	""Running	the	Fortify	ABAP	Extractor""	on	page	Â 92	-New	option	to	
export	SAP	standard	code	in	addition	to	custom	code	
20.2.0	Added:
l	""About	Installing	Fortify	Static	Code	Analyzer	and	Applications""	on	
page	Â 27	and	""Using	Docker	to	Install	and	Run	Fortify	Static	Code	
Analyzer""	on	page	Â 33	
l	""Translating	Dockerfiles""	on	page	Â 103	
l	""Configuring	Scan	Speed	with	Speed	Dial""	on	page	Â 146	
l	""Fortify	Java	Annotations""	on	page	Â 175	-Incorporated	information	
previously	available	in	the	javaAnnotations	sample	README.txt	to	this	
guide	
Updated:
l	""Translating	Visual	Studio	and	MSBuild	Projects""	on	page	Â 58	-Updated	
to	reflect	the	translation	improvements	made	over	the	past	couple	
releases	(former	chapter	title:	Â Translating	.NET	Â Code)	
l	""Translating	COBOL	Code""	on	page	Â 84	-Describes	the	changes	
introduced	for	analyzing	COBOL	Â code	
l	""Generating	aBIRT	Report""	on	page	Â 136	-New	supported	report	
template	added:	Â OWASP	Â ASVS	Â 4.0	
l	""Sample	Projects""	on	page	Â 170	-Added	two	new	samples	
l	All	references	to	Fortify	ScanCentral	were	replaced	with	Fortify	
ScanCentral	SAST	(product	name	change)	
20.1.2	Added:
l	""Translating	Kotlin	Code""	on	page	Â 55	
Updated:	
User	Guide	
Change	Log	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	13	of	216 

Software	Release	/	
Document	Version	Changes
l	""Gradle	Integration""	on	page	Â 107	-Added	information	about	using	the	
Gradle	Wrapper	
20.1.0	Added:
l	.NET	Â Command-	Line	Options	-Added	anew	option	to	exclude	from	
translation	any	disabled	projects	in	asolution	
Updated:
l	""About	Installing	Fortify	Static	Code	Analyzer	and	Applications""	on	
page	Â 27	-Removed	all	mentions	of	Solaris	as	this	operating	system	is	no	
longer	supported	
l	""Installing	Fortify	Static	Code	Analyzer	and	Applications	Silently	
(Unattended)""	on	page	Â 29	-Â Added	more	information	to	the	instructions	
for	different	operating	systems	
l	""Generating	aBIRT	Report""	on	page	Â 136	-Added	support	for	new	
report:	Â CWE	Â Top	25	2019	
l	""Generating	aLegacy	Report""	on	page	Â 138	-Removed	RTF	Â as	apossible	
output	format	
l	All	references	to	Fortify	CloudScan	were	replaced	with	Fortify	
ScanCentral	(product	name	change)	
Removed:
l	Incremental	Analysis	-Feature	to	be	removed	in	the	next	release	
19.2.0	Added:
l	""Modular	Analysis""	on	page	Â 45	-New	feature	to	scan	Java/Java	EE	
libraries	separately	from	the	core	project	(related	updated	topics:	
""Analysis	Options""	on	page	Â 114	and	""fortify-	sca.properties""	on	
page	Â 182	)	
l	""Translating	Go	Code""	on	page	Â 77	
Updated:
l	""About	Upgrading	Fortify	Static	Code	Analyzer	and	Applications""	on	
page	Â 35	-Provided	additional	information	
l	""Translating	JavaScript	and	TypeScript	Code""	on	page	Â 67	-Added	
instructions	for	excluding	NPM	Â dependencies	
l	""Generating	aBIRT	Report""	on	page	Â 136	-Support	added	for	GDPR,	
User	Guide	
Change	Log	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	14	of	216 

Software	Release	/	
Document	Version	Changes	
MISRA	Â and	PCI	Â SSF	Â reports	
l	""Translation	Options""	on	page	Â 112	and	""fortify-	sca.properties""	on	
page	Â 182	-Updated	the	list	of	valid	languages	for	
com.fortify.sca.DISabledLanguages	and	added	adescription	for	
com.fortify.sca.EnabledLanguages	
Removed:
l	""Translating	AngularJS	Code""	-Analysis	of	AngularJS	Â 1.x	is	no	longer	
supported	
l	""Translating	.NET	Â Binaries""	and	""Adding	Custom	Tasks	to	your	MSBuild	
Project""	-These	features	will	be	removed	from	Fortify	Static	Code	
Analyzer	in	the	next	release	
19.1.0	Added:
l	This	document	now	includes	all	content	from	the	Micro	Focus	Fortify	
Static	Code	Analyzer	Installation	Guide	and	the	Micro	Focus	Fortify	
Static	Code	Analyzer	User	Guide	,which	are	no	longer	published	as	of	
this	release.	
Updated:
l	""Translating	JavaScript	and	TypeScript	Code""	on	page	Â 67	-The	Higher	
Order	Analyzer	is	now	enabled	by	default	for	JavaScript	and	TypeScript	
l	""Using	the	Django	Framework	with	Python""	on	page	Â 72	Â and	""Python	
Command-	Line	Options""	on	page	Â 72	-Â Added	adescription	of	anew	
feature	to	automatically	discover	Django	template	locations	
l	""iOS	Â Code	Analysis	Command-	Line	Syntax""	on	page	Â 75	and	""Android	
Code	Analysis	Command-	Line	Syntax""	on	page	Â 76	-Added	examples	
for	translating	property	list	files	and	configuration	files	
l	""Importing	the	Transport	Request""	on	page	Â 90	-Clarified	the	supported	
SAP	Â version	for	the	Fortify	ABAP	Extractor	transport	request	and	
added	asuggestion	if	the	import	fails	
l	""Running	the	Fortify	ABAP	Extractor""	on	page	Â 92	-Updated	to	provide	
more	details	
l	""Using	the	Fortify	Maven	Plugin""	on	page	Â 110	-Clarified	the	two	
different	ways	to	analyze	amaven	project	
l	""Output	Options""	on	page	Â 117	-Added	adescription	of	the	FVDL	
User	Guide	
Change	Log	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	15	of	216 

Software	Release	/	
Document	Version	Changes	
output	format	and	added	options	to	exclude	information	from	the	
FVDL	file	
l	""Sample	Scans""	on	page	Â 143	-Table	updated	to	show	data	for	the	
current	release	
User	Guide	
Change	Log	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	16	of	216 

Chapter	1:	Introduction	
This	guide	provides	instructions	for	using	Micro	Focus	Fortify	Static	Code	Analyzer	to	scan	code	on	
most	major	programming	platforms.	This	guide	is	intended	for	people	responsible	for	security	audits	
and	secure	coding.	
This	section	contains	the	following	topics:	
Fortify	Static	Code	Analyzer	17	
About	the	Analyzers	19	
Related	Documents	20	
Fortify	Static	Code	Analyzer	
Fortify	Static	Code	Analyzer	is	aset	of	software	security	analyzers	that	search	for	violations	of	security-	
specific	coding	rules	and	guidelines	in	avariety	of	languages.	The	Fortify	Static	Code	Analyzer	language	
technology	provides	rich	data	that	enables	the	analyzers	to	pinpoint	and	prioritize	violations	so	that	
fixes	are	fast	and	accurate.	Fortify	Static	Code	Analyzer	produces	analysis	information	to	help	you	
deliver	more	secure	software,	as	well	as	make	security	code	reviews	more	efficient,	consistent,	and	
complete.	Its	design	enables	you	to	quickly	incorporate	new	third-	party	and	customer-	specific	security	
rules.
At	the	highest	level,	using	Fortify	Static	Code	Analyzer	involves:	
1.	Running	Fortify	Static	Code	Analyzer	as	astand-	alone	process	or	integrating	Fortify	Static	Code	
Analyzer	in	abuild	tool	
2.	Translating	the	source	code	into	an	intermediate	translated	format	
3.	Scanning	the	translated	code	and	producing	security	vulnerability	reports	
4.	Auditing	the	results	of	the	scan,	either	by	opening	the	results	(typically	an	FPR	file)	in	Micro	Focus	
Fortify	Audit	Workbench	or	uploading	them	to	Micro	Focus	Fortify	Software	Security	Center	for	
analysis,	or	working	directly	with	the	results	displayed	on	screen.	
Note:	For	information	about	how	to	open	and	view	results	in	Fortify	Audit	Workbench	or	Fortify	
Software	Security	Center	,see	the	Micro	Focus	Fortify	Audit	Workbench	User	Guide	or	the	Micro	
Focus	Fortify	Software	Security	Center	User	Guide	respectively.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	17	of	216 

Fortify	ScanCentral	SAST	
You	can	use	Micro	Focus	Fortify	ScanCentral	SAST	to	manage	your	resources	by	offloading	the	Fortify	
Static	Code	Analyzer	scan	phase	from	build	machines	to	acollection	of	machines	provisioned	for	this	
purpose.	For	some	languages,	Fortify	ScanCentral	SAST	can	perform	both	the	translation	and	the	
analysis	(scan)	phases.	You	can	analyze	your	code	in	one	of	two	ways:	
l	Perform	the	translation	phase	on	alocal	build	machine	and	generate	amobile	build	session	(MBS).	
Start	the	scan	with	Fortify	ScanCentral	SAST	using	the	MBS	file.	In	addition	to	freeing	up	the	build	
machines,	this	process	makes	it	easy	to	expand	the	system	by	adding	more	resources	as	needed,	
without	having	to	interrupt	the	build	process.	In	addition,	users	of	Micro	Focus	Fortify	Software	
Security	Center	can	direct	Fortify	ScanCentral	SAST	to	output	the	FPR	file	directly	to	the	server.	
l	If	your	application	is	written	in	alanguage	supported	for	Fortify	ScanCentral	SAST	translation,	you	
can	also	offload	the	translation	phase	of	the	analysis	to	Fortify	ScanCentral	SAST	.For	information	
about	the	specific	supported	language,	see	the	Micro	Focus	Fortify	Software	System	Requirements	
document.	
For	detailed	information	about	how	to	use	Fortify	ScanCentral	SAST	,see	the	Micro	Focus	Fortify	
ScanCentral	SAST	Installation,	Configuration,	and	Usage	Guide	.	
Fortify	Scan	Wizard	
Micro	Focus	Fortify	Scan	Wizard	is	autility	that	enables	you	to	quickly	and	easily	generate	ascript	to	
run	on	Windows	or	Linux/	macOS	so	Â that	you	can	scan	project	code	with	Fortify	Static	Code	Analyzer	.	
With	the	Scan	Wizard,	you	can	run	your	scans	locally,	or,	if	you	are	using	Micro	Focus	Fortify	
ScanCentral	SAST	,in	acollection	of	computers	provisioned	to	manage	the	processor-	intensive	scan	
phase	of	the	analysis.	
For	more	information,	see	""Fortify	Scan	Wizard""	on	page	Â 168	.	
Fortify	Software	Security	Content	
Fortify	Static	Code	Analyzer	uses	aknowledge	base	of	rules	to	enforce	secure	coding	standards	
applicable	to	the	codebase	for	static	analysis.	Micro	Focus	Fortify	Software	Security	Content	is	required	
for	both	translation	and	analysis.	You	can	download	and	install	security	content	when	you	install	Fortify	
Static	Code	Analyzer	(see	""Installing	Fortify	Static	Code	Analyzer""	on	page	Â 25	).	Alternatively,	you	can	
download	or	import	previously	downloaded	Fortify	Security	Content	with	the	fortifyupdate	utility	as	a	
post-	installation	task	(see	""Manually	Installing	Fortify	Security	Content""	on	page	Â 32	).	
Fortify	Software	Security	Content	(security	content)	consists	of	Secure	Coding	Rulepacks	and	external	
metadata:
l	Secure	Coding	Rulepacks	describe	general	secure	coding	idioms	for	popular	languages	and	public	
APIs	
l	External	metadata	includes	mappings	from	the	Fortify	categories	to	alternative	categories	(such	as	
CWE,	OWASP	Top	10,	and	PCI)	
User	Guide	
Chapter	1:	Introduction	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	18	of	216 

Fortify	provides	the	ability	to	write	custom	rules	that	add	to	the	functionality	of	Fortify	Static	Code	
Analyzer	and	the	Secure	Coding	Rulepacks	.For	example,	you	might	need	to	enforce	proprietary	
security	guidelines	or	analyze	aproject	that	uses	third-	party	libraries	or	other	pre-	compiled	binaries	that	
are	not	already	covered	by	the	Secure	Coding	Rulepacks	.You	can	also	customize	the	external	metadata	
to	map	Fortify	issues	to	different	taxonomies,	such	as	internal	application	security	standards	or	
additional	compliance	obligations.	For	instructions	on	how	to	create	your	own	custom	rules	or	custom	
external	metadata,	see	the	Micro	Focus	Fortify	Static	Code	Analyzer	Custom	Rules	Guide	.	
Fortify	recommends	that	you	periodically	update	the	security	content.	You	can	use	the	fortifyupdate	
utility	to	obtain	the	latest	security	content.	For	more	information,	see	""Updating	Security	Content""	on	
page	Â 127	.	
About	the	Analyzers	
Fortify	Static	Code	Analyzer	comprises	eight	vulnerability	analyzers:	Buffer,	Configuration,	Content,	
Control	Flow,	Dataflow,	Higher	Order,	Semantic,	and	Structural.	Each	analyzer	accepts	adifferent	type	
of	rule	specifically	tailored	to	provide	the	information	necessary	for	the	corresponding	type	of	analysis	
performed.	Rules	are	definitions	that	identify	elements	in	the	source	code	that	might	result	in	security	
vulnerabilities	or	are	otherwise	unsafe.	
The	following	table	lists	and	describes	each	analyzer.	
Analyzer	Description	
Buffer	The	Buffer	Analyzer	detects	buffer	overflow	vulnerabilities	that	involve	writing	or	
reading	more	data	than	abuffer	can	hold.	The	buffer	can	be	either	stack-	allocated	
or	heap-	allocated.	The	Buffer	Analyzer	uses	limited	interprocedural	analysis	to	
determine	whether	there	is	acondition	that	causes	the	buffer	to	overflow.	If	any	
execution	path	to	abuffer	leads	to	abuffer	overflow,	Fortify	Static	Code	Analyzer	
reports	it	as	abuffer	overflow	vulnerability	and	points	out	the	variables	that	could	
cause	the	overflow.	If	the	value	of	the	variable	causing	the	buffer	overflow	is	tainted	
(user-	controlled),	then	Fortify	Static	Code	Analyzer	reports	it	as	well	and	displays	
the	dataflow	trace	to	show	how	the	variable	is	tainted.	
Configuration	The	Configuration	Analyzer	searches	for	mistakes,	weaknesses,	and	policy	violations	
in	application	deployment	configuration	files.	For	example,	the	Configuration	
Analyzer	checks	for	reasonable	timeouts	in	user	sessions	in	aweb	application.	
Content	The	Content	Analyzer	searches	for	security	issues	and	policy	violations	in	HTML	
content.	In	addition	to	static	HTML	pages,	the	Content	Analyzer	performs	these	
checks	on	files	that	contain	dynamic	HTML,	such	as	PHP,	JSP,	and	classic	ASP	files.	
User	Guide	
Chapter	1:	Introduction	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	19	of	216 

Analyzer	Description	
Control	Flow	The	Control	Flow	Analyzer	detects	potentially	dangerous	sequences	of	operations.	
By	analyzing	control	flow	paths	in	aprogram,	the	Control	Â Flow	Analyzer	determines	
whether	aset	of	operations	are	executed	in	acertain	order.	For	example,	the	Control	
Flow	Analyzer	detects	time	of	check/time	of	use	issues	and	uninitialized	variables,	
and	checks	whether	utilities,	such	as	XML	readers,	are	configured	properly	before	
being	used.	
Dataflow	The	Dataflow	Analyzer	detects	potential	vulnerabilities	that	involve	tainted	data	
(user-	controlled	input)	put	to	potentially	dangerous	use.	The	Dataflow	Analyzer	
uses	global,	interprocedural	taint	propagation	analysis	to	detect	the	flow	of	data	
between	asource	(site	of	user	input)	and	asink	(dangerous	function	call	or	
operation).	For	example,	the	Dataflow	Analyzer	detects	whether	auser-	controlled	
input	string	of	unbounded	length	is	copied	into	astatically	sized	buffer,	and	detects	
whether	auser-	controlled	string	is	used	to	construct	SQL	query	text.	
Null	Pointer	The	Null	Pointer	Analyzer	detects	dereferences	of	pointer	variables	that	are	
assigned	the	null	value.	The	Null	Pointer	Â Analyzer	detection	is	performed	at	the	
intra-	procedural	level.	Issues	are	detected	only	when	the	null	assignment,	the	
dereference,	and	all	the	paths	between	them	occur	within	asingle	function.	
Semantic	The	Semantic	Analyzer	detects	potentially	dangerous	uses	of	functions	and	APIs	at	
the	intra-	procedural	level.	Its	specialized	logic	searches	for	buffer	overflow,	format	
string,	and	execution	path	issues,	but	is	not	limited	to	these	categories.	For	example,	
the	Semantic	Analyzer	detects	deprecated	functions	in	Java	and	unsafe	functions	in	
C/C++	,such	as	gets	()	.	
Structural	The	Structural	Analyzer	detects	potentially	dangerous	flaws	in	the	structure	or	
definition	of	the	program.	By	understanding	the	way	programs	are	structured,	the	
Structural	Analyzer	identifies	violations	of	secure	programming	practices	and	
techniques	that	are	often	difficult	to	detect	through	inspection	because	they	
encompass	awide	scope	involving	both	the	declaration	and	use	of	variables	and	
functions.	For	example,	the	Structural	Analyzer	detects	assignment	to	member	
variables	in	Java	servlets,	identifies	the	use	of	loggers	that	are	not	declared	static	
final,	and	flags	instances	of	dead	code	that	is	never	executed	because	of	apredicate	
that	is	always	false.	
Related	Documents	
This	topic	describes	documents	that	provide	information	about	Micro	Focus	Fortify	software	products.	
User	Guide	
Chapter	1:	Introduction	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	20	of	216 

Note:	You	can	find	the	Micro	Focus	Fortify	Product	Documentation	at	
https://www.microfocus.com/support/documentation	.All	guides	are	available	in	both	PDF	and	
HTML	formats.	
All	Products	
The	following	documents	provide	general	information	for	all	products.	Unless	otherwise	noted,	these	
documents	are	available	on	the	Micro	Focus	Product	Documentation	website.	
Document	/File	Name	Description	
About	Micro	Focus	Fortify	Product	
Software	Documentation	
About_	Fortify_	Docs_	<version>	.pdf	
This	paper	provides	information	about	how	to	access	Micro	
Focus	Fortify	product	documentation.	
Note:	This	document	is	included	only	with	the	product	
download.	
Micro	Focus	Fortify	Software	System	
Requirements
Fortify_	Sys_	Reqs_	<version>	.pdf	
This	document	provides	the	details	about	the	
environments	and	products	supported	for	this	version	of	
Fortify	Software.	
Micro	Focus	Fortify	Software	Release	
Notes
FortifySW_	RN_	<version>	.pdf	
This	document	provides	an	overview	of	the	changes	made	
to	Fortify	Software	for	this	release	and	important	
information	not	included	elsewhere	in	the	product	
documentation.	
Whatâs	New	in	Micro	Focus	Fortify	
Software	<version>	
Fortify_	Whats_	New_	<version>	.pdf	
This	document	describes	the	new	features	in	Fortify	
Software	products.	
Micro	Focus	Fortify	ScanCentral	SAST	
The	following	document	provides	information	about	Fortify	ScanCentral	SAST.	Unless	otherwise	
noted,	these	documents	are	available	on	the	Micro	Focus	Product	Documentation	website	at	
https://www.microfocus.com/documentation/fortify-	software-	security-	center	.	
Document	/File	Name	Description	
Micro	Focus	Fortify	ScanCentral	
SAST	Installation,	Configuration,	and	
Usage	Guide	
This	document	provides	information	about	how	to	install,	
configure,	and	use	Fortify	ScanCentral	SAST	to	streamline	
the	static	code	analysis	process.	It	is	written	for	anyone	who	
User	Guide	
Chapter	1:	Introduction	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	21	of	216 

Document	/File	Name	Description	
SC_	SAST_	Guide_	<version>	.pdf	intends	to	install,	configure,	or	use	Fortify	ScanCentral	
SAST	to	offload	the	resource-	intensive	translation	and	
scanning	phases	of	their	Fortify	Static	Code	Analyzer	
process.	
Micro	Focus	Fortify	Software	Security	Center	
The	following	document	provides	information	about	Fortify	Software	Security	Center.	Unless	otherwise	
noted,	these	documents	are	available	on	the	Micro	Focus	Product	Documentation	website	at	
https://www.microfocus.com/documentation/fortify-	software-	security-	center	.	
Document	/File	Name	Description	
Micro	Focus	Fortify	Software	Security	
Center	User	Guide	
SSC_	Guide_	<version>	.pdf	
This	document	provides	Fortify	Software	Security	Center	
users	with	detailed	information	about	how	to	deploy	and	
use	Software	Security	Center.	It	provides	all	of	the	
information	you	need	to	acquire,	install,	configure,	and	use	
Software	Security	Center.	
It	is	intended	for	use	by	system	and	instance	
administrators,	database	administrators	(DBAs),	enterprise	
security	leads,	development	team	managers,	and	
developers.	Software	Security	Center	provides	security	
team	leads	with	ahigh-	level	overview	of	the	history	and	
current	status	of	aproject.	
Micro	Focus	Fortify	Static	Code	Analyzer	
The	following	documents	provide	information	about	Fortify	Static	Code	Analyzer.	Unless	otherwise	
noted,	these	documents	are	available	on	the	Micro	Focus	Product	Documentation	website	at	
https://www.microfocus.com/documentation/fortify-	static-	code	.	
Document	/File	Name	Description	
Micro	Focus	Fortify	Static	Code	
Analyzer	User	Guide	
SCA_	Guide_	<version>	.pdf	
This	document	describes	how	to	install	and	use	Fortify	
Static	Code	Analyzer	to	scan	code	on	many	of	the	major	
programming	platforms.	It	is	intended	for	people	
responsible	for	security	audits	and	secure	coding.	
User	Guide	
Chapter	1:	Introduction	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	22	of	216 

Document	/File	Name	Description	
Micro	Focus	Fortify	Static	Code	
Analyzer	Custom	Rules	Guide	
SCA_	Cust_	Rules_	Guide_	<version>	.zip	
This	document	provides	the	information	that	you	need	to	
create	custom	rules	for	Fortify	Static	Code	Analyzer.	This	
guide	includes	examples	that	apply	rule-	writing	concepts	to	
real-	world	security	issues.	
Note:	This	document	is	included	only	with	the	product	
download.	
Micro	Focus	Fortify	Audit	Workbench	
User	Guide	
AWB_	Guide_	<version>	.pdf	
This	document	describes	how	to	use	Fortify	Audit	
Workbench	to	scan	software	projects	and	audit	analysis	
results.	This	guide	also	includes	how	to	integrate	with	bug	
trackers,	produce	reports,	and	perform	collaborative	
auditing.	
Micro	Focus	Fortify	Plugins	for	Eclipse	
User	Guide	
Eclipse_	Plugins_	Guide_	<version>	.pdf	
This	document	provides	information	about	how	to	install	
and	use	the	Fortify	Complete	and	the	Fortify	Remediation	
Plugins	for	Eclipse.	
Micro	Focus	Fortify	Plugins	for	
JetBrains	IDEs	and	Android	Studio	
User	Guide	
JetBrains_	AndStud_	Plugins_	Guide_	
<version>	.pdf	
This	document	describes	how	to	install	and	use	both	the	
Fortify	Analysis	Plugin	for	IntelliJ	IDEA	Â and	Android	Studio	
and	the	Fortify	Remediation	Plugin	for	IntelliJ	Â IDEA,	
Android	Studio,	and	other	JetBrains	IDEs.	
Micro	Focus	Fortify	Jenkins	Plugin	
User	Guide	
Jenkins_	Plugin_	Guide_	<version>	.pdf	
This	document	describes	how	to	install,	configure,	and	use	
the	plugin.	This	documentation	is	available	at	
https://www.microfocus.com/documentation/fortify-
jenkins-	plugin	.	
Micro	Focus	Fortify	Security	Assistant	
Plugin	for	Eclipse	User	Guide	
SecAssist_	Eclipse_	Guide_	
<version>	.pdf	
This	document	describes	how	to	install	and	use	Fortify	
Security	Assistant	plugin	for	Eclipse	to	provide	alerts	to	
security	issues	as	you	write	your	Java	code.	
Micro	Focus	Fortify	Extension	for	
Visual	Studio	User	Guide	
VS_	Ext_	Guide_	<version>	.pdf	
This	document	provides	information	about	how	to	install	
and	use	the	Fortify	extension	for	Visual	Studio	to	analyze,	
audit,	and	remediate	your	code	to	resolve	security-	related	
issues	in	solutions	and	projects.	
Micro	Focus	Fortify	Static	Code	This	document	describes	the	properties	used	by	Fortify	
User	Guide	
Chapter	1:	Introduction	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	23	of	216 

Document	/File	Name	Description	
Analyzer	Tools	Properties	Reference	
Guide
SCA_	Tools_	Props_	Ref_	<version>	.pdf	
Static	Code	Analyzer	tools.	
User	Guide	
Chapter	1:	Introduction	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	24	of	216 

Chapter	2:	Installing	Fortify	Static	Code	
Analyzer
This	chapter	describes	how	to	install	and	uninstall	Fortify	Static	Code	Analyzer	and	Fortify	Static	Code	
Analyzer	tools.	This	chapter	also	describes	basic	post-	installation	tasks.	See	the	Micro	Focus	Fortify	
Software	System	Requirements	document	to	be	sure	that	your	system	meets	the	minimum	
requirements	for	each	software	component	installation.	
This	section	contains	the	following	topics:	
Fortify	Static	Code	Analyzer	Component	Applications	25	
About	Downloading	the	Software	27	
About	Installing	Fortify	Static	Code	Analyzer	and	Applications	27	
Using	Docker	to	Install	and	Run	Fortify	Static	Code	Analyzer	33	
About	Upgrading	Fortify	Static	Code	Analyzer	and	Applications	35	
About	Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	36	
Post-	Installation	Tasks	38	
Fortify	Static	Code	Analyzer	Component	Applications	
The	installation	consists	of	Fortify	Static	Code	Analyzer	,which	analyzes	your	build	code	according	to	a	
set	of	rules	specifically	tailored	to	provide	the	information	necessary	for	the	type	of	analysis	performed.	
A	Fortify	Static	Code	Analyzer	installation	might	also	include	one	or	more	component	applications.	
The	following	table	describes	the	components	that	are	available	for	installation	with	the	Fortify	Static	
Code	Analyzer	and	Applications	installer.	
Component	Description	
Micro	Focus	Fortify	Audit	
Workbench	
Provides	agraphical	user	interface	for	Fortify	Static	Code	Analyzer	that	
helps	you	organize,	investigate,	and	prioritize	analysis	results	so	that	
developers	can	fix	security	flaws	quickly.	
Micro	Focus	Fortify	
Plugin	for	Eclipse	
Adds	the	ability	to	scan	and	analyze	the	entire	codebase	of	aproject	and	
apply	software	security	rules	that	identify	the	vulnerabilities	in	your	Java	
code	from	the	Eclipse	IDE.	The	results	are	displayed,	along	with	
descriptions	of	each	of	the	security	issues	and	suggestions	for	their	
elimination.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	25	of	216 

Component	Description	
Micro	Focus	Fortify	
Analysis	Plugin	for	IntelliJ	
and	Android	Studio	
Adds	the	ability	to	run	Fortify	Static	Code	Analyzer	scans	on	the	entire	
codebase	of	aproject	and	apply	software	security	rules	that	identify	the	
vulnerabilities	in	your	code	from	the	IntelliJ	and	Android	Studio	IDEs.	
Micro	Focus	Fortify	
Extension	for	Visual	
Studio	
Adds	the	ability	to	scan	and	locate	security	vulnerabilities	in	your	
solutions	and	projects	and	displays	the	scan	results	in	Visual	Studio.	The	
results	include	alist	of	issues	uncovered,	descriptions	of	the	type	of	
vulnerability	each	issue	represents,	and	suggestions	on	how	to	fix	them.	
This	extension	also	includes	remediation	functionality	that	works	with	
audit	results	stored	on	aMicro	Focus	Fortify	Software	Security	Center	
server.	
Micro	Focus	Fortify	
Custom	Rules	Editor	
AÂ tool	to	create	and	edit	custom	rules.	
Micro	Focus	Fortify	Scan	
Wizard	
A	tool	to	quickly	prepare	ascript	that	you	can	use	to	scan	your	code	with	
Fortify	Static	Code	Analyzer	and	optionally,	upload	the	results	directly	
to	Fortify	Software	Security	Center	.	
Note:	This	tool	is	installed	automatically	with	Fortify	Static	Code	
Analyzer	.	
Command-	line	utilities	There	are	several	command-	line	utilities	that	are	installed	automatically	
with	Fortify	Static	Code	Analyzer	.For	more	information,	see	""Command-	
Line	Utilities""	on	page	Â 125	.	
The	following	table	describes	the	components	that	are	included	in	the	Fortify	Static	Code	Analyzer	and	
Applications	package.	You	install	these	components	separately	from	the	Fortify	Static	Code	Analyzer	
and	Applications	installer.	
Component	Description	
Micro	Focus	Fortify	Remediation	Plugin	for	
Eclipse	
Works	with	Fortify	Software	Security	Center	Â for	
developers	who	want	to	remediate	issues	detected	in	
source	code	from	the	Eclipse	IDE.	
Micro	Focus	Fortify	Remediation	Plugin	for	
JetBrains	IDEs	
Works	in	several	JetBrains	IDEs	and	Android	Studio	
together	with	Fortify	Software	Security	Center	to	
add	remediation	functionality	to	your	security	
analysis.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	26	of	216 

Component	Description	
Micro	Focus	Fortify	Security	Assistant	Plugin	
for	Eclipse	
Provides	alerts	to	potential	security	issues	as	you	
write	your	Java	code.	It	provides	detailed	
information	about	security	risks	and	
recommendations	for	how	to	secure	the	potential	
issue.	
About	Downloading	the	Software	
Fortify	Static	Code	Analyzer	and	Applications	is	available	as	adownloadable	application	or	package.	For	
details	on	how	to	acquire	the	software	and	alicense	for	the	Fortify	Software,	see	the	Micro	Focus	
Fortify	Software	System	Requirements	document.	
About	Installing	Fortify	Static	Code	Analyzer	and	
Applications
This	section	describes	how	to	install	Fortify	SCA	and	Applications.	You	need	aFortify	license	file	to	
complete	the	process.	The	following	table	lists	the	different	methods	of	installing	Fortify	SCA	and	
Applications.
Installation	Method	Instructions	
Perform	the	installation	using	astandard	install	
wizard	
""Installing	Fortify	Static	Code	Analyzer	and	
Applications""	on	the	next	page	
Perform	the	installation	silently	(unattended)	""Installing	Fortify	Static	Code	Analyzer	and	
Applications	Silently	(Unattended)""	on	page	Â 29	
Perform	atext-	based	installation	on	non-	
Windows	systems	
""Installing	Fortify	Static	Code	Analyzer	and	
Applications	in	Text-	Based	Mode	on	
Non-Windows	Platforms""	on	page	Â 32	
Perform	the	installation	using	Docker	""Using	Docker	to	Install	and	Run	Fortify	Static	
Code	Analyzer""	on	page	Â 33	
For	best	performance,	install	Fortify	Static	Code	Analyzer	on	the	same	local	file	system	where	the	code	
that	you	want	to	scan	resides.	
Note:	On	non-	Windows	systems,	you	must	install	Fortify	SCA	and	Applications	as	auser	that	has	a	
home	directory	with	write	permission.	Do	not	install	Fortify	SCA	and	Applications	as	anon-	root	
user	that	has	no	home	directory.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	27	of	216 

After	you	complete	the	installation,	see	""Post-	Installation	Tasks""	on	page	Â 38	for	additional	steps	you	
can	perform	to	complete	your	system	setup.	You	can	also	configure	settings	for	runtime	analysis,	
output,	and	performance	of	Fortify	Static	Code	Analyzer	and	its	components	by	updating	the	installed	
configuration	files.	For	information	about	the	configuration	options	for	Fortify	Static	Code	Analyzer	,	
see	""Configuration	Options""	on	page	Â 180	.For	information	about	configuration	options	for	Fortify	
Static	Code	Analyzer	component	applications,	see	the	Micro	Focus	Fortify	Static	Code	Analyzer	Tools	
Properties	Reference	Guide	.	
Installing	Fortify	Static	Code	Analyzer	and	Applications	
To	install	Fortify	Static	Code	Analyzer	and	Applications:	
1.	Run	the	installer	file	that	corresponds	to	your	operating	system:	
l	Windows:	Fortify_	SCA_	and_	Apps_	<version>	_windows_	x64.exe	
l	Linux:	Fortify_	SCA_	and_	Apps_	<version>	_linux_	x64.run	
l	macOS	:Fortify_	SCA_	and_	Apps_	<version>	_osx_	x64.app.zip	
where	<version	>	is	the	software	release	version.	
2.	Accept	the	license	agreement,	and	then	click	Next	.	
3.	Choose	where	to	install	Fortify	Static	Code	Analyzer	and	applications,	and	then	click	Next	.	
Note:	If	you	are	using	Micro	Focus	Fortify	ScanCentral	SAST	,you	must	specify	alocation	that	
does	not	include	spaces	in	the	path.	
4.	(Optional)	Â Select	the	components	to	install,	and	then	click	Next	.	
Note:	Component	selection	is	not	available	for	all	operating	systems.	
5.	If	you	are	installing	the	Fortify	extension	for	Visual	Studio	2015	or	2017,	you	are	prompted	to	
specify	whether	to	install	the	extensions	for	the	current	install	user	or	for	all	users.	
The	default	is	to	install	the	extensions	for	the	current	install	user.	
6.	Specify	the	path	to	the	fortify.license	file,	and	then	click	Next	.	
7.	Specify	the	settings	required	to	update	your	security	content.	
To	update	the	security	content	for	your	installation:	
Note:	For	installations	on	non-	Windows	platforms	and	for	deployment	environments	that	do	
not	have	access	to	the	Internet	during	installation,	you	can	update	the	security	content	using	
the	fortifyupdate	utility.	See	""Manually	Installing	Fortify	Security	Content""	on	page	Â 32	.	
a.	Specify	the	URL	address	of	the	update	server.	To	use	the	Fortify	Rulepack	update	server	for	
security	content	updates,	specify	the	URL	as:	https://update.fortify.com	.	
b.	(Optional)	Specify	the	proxy	host	and	port	number	of	the	update	server.	
c.	Click	Next	.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	28	of	216 

8.	Specify	if	you	want	to	migrate	from	aprevious	installation	of	Fortify	Static	Code	Analyzer	on	your	
system.
Migrating	from	aprevious	Fortify	Static	Code	Analyzer	installation	preserves	Fortify	Static	Code	
Analyzer	artifact	files.	For	more	information,	see	""About	Upgrading	Fortify	Static	Code	Analyzer	
and	Applications""	on	page	Â 35	.	
Note:	You	can	also	migrate	Fortify	Static	Code	Analyzer	artifacts	using	the	scapostinstall	
command-	line	utility.	For	information	on	how	to	use	the	post-	install	tool	to	migrate	from	a	
previous	Fortify	Static	Code	Analyzer	installation,	see	""Migrating	Properties	Files""	on	page	Â 38	.	
To	migrate	artifacts	from	aprevious	installation:	
a.	In	the	SCA	Â Migration	step,	select	Yes	,and	then	click	Next	.	
b.	Specify	the	location	of	the	existing	Fortify	Static	Code	Analyzer	installation	on	your	system,	
and	then	click	Next	.	
9.	Specify	if	you	want	to	install	sample	source	code	projects,	and	then	click	Next	.	
See	""Sample	Projects""	on	page	Â 170	for	descriptions	of	these	samples.	
Note:	If	you	do	not	install	the	samples	and	decide	later	that	you	want	to	install	them,	you	must	
uninstall	and	then	re-	install	Fortify	Static	Code	Analyzer	and	Applications.	
10.	Click	Next	to	proceed	to	install	Fortify	Static	Code	Analyzer	and	applications.	
11.	After	Fortify	Static	Code	Analyzer	is	installed,	select	Update	security	content	after	installation	
if	you	want	to	update	the	security	content,	and	then	click	Finish	.	
The	Security	Content	Update	Result	window	displays	the	security	content	update	results.	
Installing	Fortify	Static	Code	Analyzer	and	Applications	Silently	
(Unattended)
A	silent	installation	enables	you	to	complete	the	installation	without	any	user	prompts.	To	install	
silently,	you	need	to	create	an	option	file	to	provide	the	necessary	information	to	the	installer.	Using	the	
silent	installation,	you	can	replicate	the	installation	parameters	on	multiple	machines.	When	you	install	
Fortify	Static	Code	Analyzer	and	Applications	silently,	the	installer	does	not	download	the	Micro	Focus	
Fortify	Software	Security	Content	.For	instructions	on	how	to	install	the	Fortify	security	content	,see	
""Manually	Installing	Fortify	Security	Content""	on	page	Â 32	.	
To	install	Fortify	Static	Code	Analyzer	silently:	
1.	Create	an	options	file.	
a.	Create	atext	file	that	contains	the	following	line:	
fortify_	license_	path=	<license_	file_	location>	
where	<license_	file_	location>	is	the	full	path	to	your	fortify.license	file.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	29	of	216 

b.	If	you	are	using	adifferent	location	for	the	Fortify	Security	Content	updates	than	the	default	of	
https://update.fortify.com	,add	the	following	line:	
UpdateServer=	<update_	server_	url>	
Note:	As	previously	mentioned,	Fortify	security	content	is	not	downloaded	with	asilent	
installation.	However,	this	information	and	the	proxy	information	in	the	following	step	is	
added	to	the	<sca_	install_	dir>Core/config/server.properties	file	to	use	for	
manually	installing	Fortify	security	content	.	
c.	If	you	require	aproxy	server,	add	the	following	lines:	
UpdateProxyServer=	<proxy_	server>	
UpdateProxyPort=	<port_	number>	
d.	If	you	do	not	want	to	install	the	sample	source	code	projects,	add	the	following	line.	
On	Windows:	
InstallSamples=0
On	Linux	and	macOS	:	
enable-	components=Samples	
e.	Add	more	information,	as	needed,	to	the	options	file.	
For	list	of	installation	options	that	you	can	add	to	your	options	file,	type	the	installer	file	name	
and	the	--help	option.	This	command	displays	each	available	command-	line	option	preceded	
with	adouble	dash	and	optional	file	parameters	enclosed	in	angle	brackets.	For	example,	if	you	
want	to	see	the	progress	of	the	install	displayed	at	the	command	line,	add	
unattendedmodeui=minimal	to	your	options	file.	
Note:	The	installation	options	are	not	the	same	on	all	supported	operating	systems.	Run	
the	installer	with	--help	to	see	the	options	available	for	your	operating	system.	
For	the	enable-	components	option	on	Windows,	you	can	specify	the	AWB_	group	parameter	
to	install	Fortify	Audit	Workbench	,Fortify	Custom	Rules	Editor,	and	associate	FPR	files	with	
Fortify	Audit	Workbench	.To	install	specific	plugins,	list	each	one	by	parameter	name	(the	
Plugins_	group	parameter	does	not	install	all	plugins	and	you	do	not	need	to	include	it).	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	30	of	216 

The	following	example	Windows	options	file	specifies	the	location	of	the	license	file,	the	location	
and	proxy	information	for	obtaining	the	Fortify	Security	Content	,arequest	to	migrate	from	a	
previous	release,	installation	of	Audit	Workbench,	installation	of	Micro	Focus	Fortify	Extension	
for	Visual	Studio	2019	for	all	users,	and	the	location	of	the	Fortify	SCA	and	Applications	
installation	directory:	
fortify_	license_	path=C:\Users\admin\Desktop\fortify.license	
UpdateServer=https://internalserver.abc.com
UpdateProxyServer=webproxy.abc.company.com
UpdateProxyPort=8080
MigrateSCA=1
enable-	components=AWB_	group,VS2019	
VS_	all_	users=1	
installdir=C:\Fortify
The	following	options	file	example	is	for	Linux	and	macOS	:	
fortify_	license_	path=/opt/Fortify/fortify.license	
UpdateServer=https://internalserver.abc.com
UpdateProxyServer=webproxy.abc.company.com
UpdateProxyPort=8080
MigrateSCA=1
enable-	components=Samples	
installdir=/opt/Fortify	
2.	Save	the	options	file.	
3.	Run	the	silent	install	command	for	your	operating	system.	
Windows	Fortify_	SCA_	and_	Apps_	<version>	_windows_	x64.exe	--mode	unattended	--	
optionfile	<full_	path_	to_	option_	file>	
Linux	./	Fortify_	SCA_	and_	Apps_	<version>	_linux_	x64.run	--mode	unattended	--	
optionfile	<full_	path_	to_	option_	file>	
macOS	You	must	uncompress	the	ZIP	file	before	you	run	the	command.	
Fortify_	SCA_	and_	Apps_	<version>	_osx_	x64.app	/Contents/	
MacOS/installbuilder.sh	--mode	unattended	--optionfile	<full_	
path_	to_	option_	file>	
The	installer	creates	an	installer	log	file	when	the	installation	is	complete.	This	log	file	is	located	in	the	
following	location	depending	on	your	operating	system.	
Windows	C:\Users\	<username>	\AppData\Local\Temp\FortifySCAandApps-	
<version>	-install.log	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	31	of	216 

Linux	/tmp/FortifySCAandApps-	<version>	-install.log	
macOS	/tmp/FortifySCAandApps-	<version>	-install.log	
Installing	Fortify	Static	Code	Analyzer	and	Applications	in	Text-	
Based	Mode	on	Non-Windows	Platforms	
You	perform	atext-	based	installation	on	the	command	line.	During	the	installation,	you	are	prompted	
for	information	required	to	complete	the	installation.	Text-	based	installations	are	not	supported	on	
Windows	systems.	
To	perform	atext-	based	installation	of	Fortify	Static	Code	Analyzer	and	Applications,	run	the	text-	
based	install	command	for	your	operating	system	as	listed	in	the	following	table.	
Linux	./	Fortify_	SCA_	and_	Apps_	<version>	_linux_	x64.run	--mode	text	
macOS	You	must	uncompress	the	provided	ZIP	file	before	you	run	the	command.	
Fortify_	SCA_	and_	Apps_	<version>	_osx_	x64.app	/Contents/	
MacOS/installbuilder.sh	--mode	text	
Manually	Installing	Fortify	Security	Content	
You	can	install	Micro	Focus	Fortify	Software	Security	Content	(Secure	Coding	Rulepacks	and	
metadata)	Â automatically	during	the	Windows	installation	procedure.	However,	you	can	also	download	
Fortify	security	content	from	the	Fortify	Rulepack	update	server	,and	then	use	the	fortifyupdate	utility	
to	install	it.	This	option	is	provided	for	installations	on	non-	Windows	platforms	and	for	deployment	
environments	that	do	not	have	access	to	the	Internet	during	installation.	
Use	the	fortifyupdate	utility	to	install	Fortify	security	content	from	either	aremote	server	or	alocally	
downloaded	file.	
To	install	security	content:	
1.	Open	acommand	window.	
2.	Navigate	to	the	<sca_	install_	dir>	/bin	directory.	
3.	At	the	command	prompt,	type	fortifyupdate	.	
If	you	have	previously	downloaded	the	Fortify	security	content	from	the	Fortify	Customer	Portal	,	
run	fortifyupdate	with	the	-import	option	and	the	path	to	the	directory	where	you	
downloaded	the	ZIP	file.	
You	can	also	use	this	same	utility	to	update	your	security	content.	For	more	information	about	the	
fortifyupdate	utility,	see	""Updating	Security	Content""	on	page	Â 127	.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	32	of	216 

Using	Docker	to	Install	and	Run	Fortify	Static	Code	
Analyzer
You	can	install	Fortify	Static	Code	Analyzer	in	aDocker	image	and	then	run	Fortify	Static	Code	
Analyzer	as	aDocker	container.	
Note:	You	can	only	run	Fortify	Static	Code	Analyzer	in	Docker	on	supported	Linux	platforms.	
Creating	a	Dockerfile	to	Install	Fortify	Static	Code	Analyzer	
This	topic	describes	how	to	create	aDockerfile	to	install	Fortify	Static	Code	Analyzer	in	Docker	image.	
The	Dockerfile	must	include	the	following	instructions:	
1.	Set	aLinux	system	to	use	for	the	base	image.	
Note:	If	you	intend	to	use	build	tools	when	you	run	Fortify	Static	Code	Analyzer	,make	sure	
that	the	required	build	tools	are	installed	in	the	image.	For	information	about	using	the	
supported	build	tools,	see	""Build	Integration""	on	page	Â 105	.	
2.	Copy	the	Fortify	SCA	and	Applications	installer,	the	Fortify	license	file,	and	installation	options	file	
to	the	Docker	image	using	the	COPY	instruction.	
For	instructions	on	how	to	create	an	installation	options	file,	see	""Installing	Fortify	Static	Code	
Analyzer	and	Applications	Silently	(Unattended)""	on	page	Â 29	.	
3.	Run	the	Fortify	SCA	and	Applications	installer	using	the	RUN	instruction.	
You	must	run	the	installer	in	unattended	mode.	For	more	information,	see	""Installing	Fortify	Static	
Code	Analyzer	and	Applications	Silently	(Unattended)""	on	page	Â 29	.	
4.	Run	fortifyupdate	to	download	the	Fortify	Security	Content	using	the	RUN	instruction.	
For	more	information	about	this	utility,	see	""Manually	Installing	Fortify	Security	Content""	on	the	
previous	page	.	
5.	To	configure	the	image	so	you	can	run	Fortify	Static	Code	Analyzer	,set	the	entry	point	to	the	
location	of	the	installed	sourceanalyzer	executable	using	the	ENTRYPOINT	instruction.	
The	default	sourceanalyzer	installation	path	is:	/opt/Fortify/Fortify_	SCA_	and_	Apps_	
<version>	/bin/sourceanalyzer	.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	33	of	216 

The	following	is	an	example	of	aDockerfile	to	install	Fortify	SCA	and	Applications:	
FROM	registry.suse.com/suse/sles12sp4	
COPY	fortify.license	./	
COPY	Fortify_	SCA_	and_	Apps_	20.2.0_	linux_	x64.run	./	
COPY	installerSettings	./	
RUN	zypper	-n	install	rpm-	build	
RUN	./Fortify_	SCA_	and_	Apps_	20.2.0_	linux_	x64.run	--mode	unattended	\	
Â Â -	-optionfile	./installerSettings	&&	\	
Â Â /opt/Fortify/Fortify_	SCA_	and_	Apps_	20.2.0/bin/fortifyupdate	&&	\	
Â Â rm	Fortify_	SCA_	and_	Apps_	20.2.0_	linux_	x64.run	fortify.license	installerSettings	
ENTRYPOINT	[Â 	""/opt/Fortify/Fortify_	SCA_	and_	Apps_	20.2.0/bin/sourceanalyzer""	]	
Note:	The	rpm-	build	package	in	SUSE	is	needed	by	the	installer.	
To	create	the	docker	image	using	the	Dockerfile	from	the	current	directory,	you	must	use	the	docker	
build	command.	For	example:	
docker	build	-t	<image_	name>	
Running	the	Container	
This	topic	describes	how	to	run	the	Fortify	Static	Code	Analyzer	image	as	acontainer	and	provides	
example	Docker	run	commands	for	translation	and	scan.	
To	run	the	Fortify	Static	Code	Analyzer	image	as	acontainer,	you	must	mount	two	directories	from	the	
host	file	system	to	the	container:	
l	The	directory	that	contains	the	source	files	you	want	to	analyze.	
l	A	temporary	directory	to	store	the	SCA	build	session	between	the	translate	and	scan	phases	and	to	
share	the	output	files	(logs,	FPR)	with	the	host.	
Specify	this	directory	using	the	âproject-	root	command-	line	option	in	both	the	Fortify	Static	
Code	Analyzer	translate	and	scan	commands.	
The	following	example	commands	mount	the	input	directory	/sources	in	/src	and	the	temporary	
directory	in	/scratch_	docker	.The	image	name	in	the	example	is	fortify-	sca	.	
Important!	Include	the	Fortify	Static	Code	Analyzer	âfcontainer	option	in	both	the	translate	
and	scan	commands	so	that	Fortify	Static	Code	Analyzer	detects	and	uses	only	the	memory	
dedicated	to	the	container.	Otherwise,	by	default	Fortify	Static	Code	Analyzer	detects	the	total	
system	memory	because	-autoheap	is	enabled.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	34	of	216 

Example	Docker	Run	Commands	for	Translation	and	Scan	
The	following	example	mounts	the	temporary	directory	and	the	sources	directory,	and	then	runs	Fortify	
Static	Code	Analyzer	from	the	container	for	the	translation	phase:	
docker	run	-v	/scratch_	local/:/scratch_	docker	-v	/sources/:/src	
-it	fortify-	sca	âb	<build_	id>	-project-	root	/scratch_	docker	-fcontainer	
[<sca_	options>	]	/src	
The	following	example	mounts	the	temporary	directory,	and	then	runs	Fortify	Static	Code	Analyzer	
from	the	container	for	the	analysis	phase:	
docker	run	-v	/scratch_	local/:/scratch_	docker	
-it	fortify-	sca	âb	<build_	id>	-project-	root	/scratch_	docker	âscan	-	
fcontainer	[<sca_	options>	]	âf	/scratch_	docker/results.fpr	
The	results.fpr	file	is	created	in	the	host's	/scratch_	local	directory.	
About	Upgrading	Fortify	Static	Code	Analyzer	and	
Applications
To	upgrade	Fortify	Static	Code	Analyzer	and	Applications,	install	the	new	version	in	adifferent	location	
than	where	your	current	version	is	installed	and	choose	to	migrate	settings	from	the	previous	
installation.	This	migration	preserves	and	updates	the	Fortify	Static	Code	Analyzer	artifact	files	located	
in	the	<sca_	install_	dir>	/Core/config	directory.	
If	you	choose	not	to	migrate	any	settings	from	aprevious	release,	Fortify	recommends	that	you	save	a	
backup	of	the	following	data:	
l	<sca_	install_	dir>	/Core/config/rules	folder	
l	<sca_	install_	dir>	/Core/config/customrules	folder	
l	<sca_	install_	dir>	/Core/config/ExternalMetadata	folder	
l	<sca_	install_	dir>	/Core/config/CustomExternalMetadata	folder	
l	<sca_	install_	dir>	/Core/config/server.properties	file	
After	you	install	the	new	version,	you	can	uninstall	the	previous	version.	For	more	information,	see	
""About	Uninstalling	Fortify	Static	Code	Analyzer	and	Applications""	on	the	next	page	.	
Note:	You	can	leave	the	previous	version	installed.	If	you	have	multiple	versions	installed	on	the	
same	system,	the	most	recently	installed	version	is	invoked	when	you	run	the	command	from	the	
command	line.	Scanning	source	code	from	the	Fortify	Secure	Code	Plugins	also	uses	the	most	
recently	installed	version	of	Fortify	Static	Code	Analyzer	.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	35	of	216 

Notes	About	Upgrading	the	Fortify	Extension	for	Visual	Studio	
If	you	have	administrative	privileges	and	are	upgrading	from	aprevious	version	of	the	Fortify	Static	
Code	Analyzer	for	any	supported	version	of	Visual	Studio,	the	installer	will	overwrite	the	existing	Micro	
Focus	Fortify	Extension	for	Visual	Studio	.If	the	previous	version	was	installed	without	administrative	
privileges,	the	installer	will	also	overwrite	the	existing	Fortify	Extension	for	Visual	Studio	without	
requiring	administrative	privileges.	
Note:	If	you	do	not	have	administrative	privileges	and	you	are	upgrading	the	Fortify	Extension	for	
Visual	Studio	2015,	2017,	or	2019	that	was	previously	installed	using	an	administrative	privileged	
user	account,	you	must	first	uninstall	the	Fortify	Extension	for	Visual	Studio	from	Visual	Studio	
2015,	2017,	or	2019	using	an	administrative	privilege	account.	
About	Uninstalling	Fortify	Static	Code	Analyzer	and	
Applications
This	section	describes	how	to	uninstall	Fortify	Static	Code	Analyzer	and	Applications.	You	can	use	the	
standard	install	wizard	or	you	can	perform	the	uninstallation	silently.	You	can	also	perform	atext-	based	
uninstallation	on	non-	Windows	systems.	
Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	
Uninstalling	on	Windows	Platforms	
To	uninstall	the	Fortify	Static	Code	Analyzer	and	applications	software:	
1.	Select	Start	>	Control	Panel	>	Add	or	Remove	Programs	.	
2.	From	the	list	of	programs,	select	Fortify	SCA	Â and	Applications	<version	>,and	then	click	
Remove	.	
3.	You	are	prompted	to	indicate	whether	to	remove	all	application	settings.	Do	one	of	the	following:	
l	Click	Yes	to	remove	the	application	setting	folders	for	the	tools	installed	with	the	version	of	
Fortify	Static	Code	Analyzer	that	you	are	uninstalling.	The	Fortify	Static	Code	Analyzer	
(sca	<version>	)folder	Â is	not	removed.	
l	Click	No	to	retain	the	application	settings	on	your	system.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	36	of	216 

Uninstalling	on	Other	Platforms	
To	uninstall	Fortify	Static	Code	Analyzer	software	on	Linux	and	macOS	platforms:	
1.	Back	up	your	configuration,	including	any	important	files	you	have	created.	
2.	Run	the	uninstall	command	located	in	the	<sca_	install_	dir>	for	your	operating	system:	
Linux	Uninstall_	FortifySCAandApps_	<version>	
macOS	Uninstall_	FortifySCAandApps_	<version>	.app	
3.	You	are	prompted	to	indicate	whether	to	remove	all	application	settings.	Do	one	of	the	following:	
l	Click	Yes	to	remove	the	application	setting	folders	for	the	tools	installed	with	the	version	of	
Fortify	Static	Code	Analyzer	that	you	are	uninstalling.	The	Fortify	Static	Code	Analyzer	
(sca	<version>	)folder	Â is	not	removed.	
l	Click	No	to	retain	the	application	settings	on	your	system.	
Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	Silently	
To	uninstall	Fortify	Static	Code	Analyzer	silently:	
1.	Navigate	to	the	installation	directory.	
2.	Type	one	of	the	following	commands	based	on	your	operating	system:	
Windows	Uninstall_	FortifySCAandApps_	<version>	.exe	--mode	unattended	
Linux	./Uninstall_	FortifySCAandApps_	<version>	--mode	unattended	
macOS	Uninstall_	FortifySCAandApps_	
<version>	.app/Contents/MacOS/installbuilder.sh	
--mode	unattended	
Note:	The	uninstaller	removes	the	application	setting	folders	for	the	tools	installed	with	the	version	
of	Fortify	Static	Code	Analyzer	that	you	are	uninstalling.	
Uninstalling	Fortify	Static	Code	Analyzer	and	Applications	in	
Text-	Based	Mode	on	Non-	Windows	Platforms	
To	uninstall	Fortify	Static	Code	Analyzer	in	text-	based	mode,	run	the	text-	based	install	command	for	
your	operating	system,	as	follows:	
1.	Navigate	to	the	installation	directory.	
2.	Type	one	of	the	following	commands	based	on	your	operating	system:	
Linux	./Uninstall_	FortifySCAandApps_	<version>	--mode	text	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	37	of	216 

macOS	Uninstall_	FortifySCAandApps_	
<version>	.app/Contents/MacOS/installbuilder.sh	--mode	text	
Post-	Installation	Tasks	
Post-	installation	tasks	prepare	you	to	start	using	Fortify	Static	Code	Analyzer	and	tools.	
Running	the	Post-	Install	Tool	
To	run	the	Fortify	Static	Code	Analyzer	post-	install	tool:	
1.	Navigate	to	the	<sca_	install_	dir>	/bin	directory	from	the	command	line.	
2.	At	the	command	prompt,	type	scapostinstall	.	
3.	Type	one	of	the	following:	
l	To	display	settings,	type	s.	
l	To	return	to	aprevious	prompt,	type	r.	
l	To	exit	the	tool,	type	q.	
Migrating	Properties	Files	
To	migrate	properties	files	from	aprevious	version	of	Fortify	Static	Code	Analyzer	to	the	current	
version	of	Fortify	Static	Code	Analyzer	installed	on	your	system:	
1.	Navigate	to	the	<sca_	install_	dir>	/bin	directory	from	the	command	line.	
2.	At	the	command	prompt,	type	scapostinstall	.	
3.	Type	1	to	select	Migration	.	
4.	Type	1	to	select	SCA	Migration	.	
5.	Type	1	to	select	Migrate	from	an	existing	Fortify	installation	.	
6.	Type	1	to	select	Set	previous	Fortify	installation	directory	.	
7.	Type	the	previous	install	directory.	
8.	Type	s	to	confirm	the	settings.	
9.	Type	2	to	perform	the	migration.	
10.	Type	y	to	confirm.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	38	of	216 

Specifying	a	Locale	
English	is	the	default	locale	for	aFortify	Static	Code	Analyzer	installation.	
To	change	the	locale	for	your	Fortify	Static	Code	Analyzer	installation:	
1.	Navigate	to	the	bin	directory	from	the	command	line.	
2.	At	the	command	prompt,	type	scapostinstall	.	
3.	Type	2	to	select	Settings	.	
4.	Type	1	to	select	General	.	
5.	Type	1	to	select	Locale	.	
6.	Type	one	of	the	following	locale	codes:	
l	English:	en	
l	Spanish:	es	
l	Japanese:	ja	
l	Korean:	ko	
l	Brazilian	Portuguese:	pt_	BR	
l	Simplified	Chinese:	zh_	CN	
l	Traditional	Chinese:	zh_	TW	
Configuring	for	Security	Content	Updates	
Specify	how	you	want	to	obtain	Micro	Focus	Fortify	Software	Security	Content	.You	must	also	specify	
proxy	information	if	it	is	required	to	reach	the	server.	
To	specify	settings	for	Fortify	Security	Content	updates:	
1.	Navigate	to	the	bin	directory	from	the	command	line.	
2.	At	the	command	prompt,	type	scapostinstall	.	
3.	Type	2	to	select	Settings	.	
4.	Type	2	to	select	Fortify	Update	.	
5.	To	change	the	Fortify	Rulepack	update	server	URL,	type	1	and	then	type	the	URL.	
The	default	Fortify	Rulepack	update	server	URL	is	https://update.fortify.com	.	
6.	To	specify	aproxy	for	Fortify	Security	Content	updates,	do	the	following:	
a.	Type	2	to	select	Proxy	Server	Host	,and	then	type	the	name	of	the	proxy	server.	
b.	Type	3	to	select	Proxy	Server	Port	,and	then	type	the	proxy	server	port	number.	
c.	(Optional)	Â You	can	also	specify	the	proxy	server	user	name	(option	4)and	password	
(option	Â 5).	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	39	of	216 

Configuring	the	Connection	to	Fortify	Software	Security	Center	
Specify	how	to	connect	to	Micro	Focus	Fortify	Software	Security	Center	.If	your	network	uses	aproxy	
server	to	reach	the	Fortify	Software	Security	Center	server,	you	must	specify	the	proxy	information.	
To	specify	settings	for	connecting	to	Fortify	Software	Security	Center	:	
1.	Navigate	to	the	bin	directory	from	the	command	line.	
2.	At	the	command	prompt,	type	scapostinstall	.	
3.	Type	2	to	select	Settings	.	
4.	Type	3	to	select	Software	Security	Center	Settings	.	
5.	Type	1	to	select	Server	URL	,and	then	type	the	Fortify	Software	Security	Center	server	URL.	
For	example,	https://mywebserver/ssc	.	
6.	To	specify	proxy	settings	for	the	connection,	do	the	following:	
a.	Type	2	to	select	Proxy	Server	,and	then	type	the	proxy	server	path.	
b.	Type	3	to	select	Proxy	Server	Port	,and	then	type	the	proxy	server	port	number.	
c.	To	specify	the	proxy	server	username	and	password,	use	option	4	for	the	username	and	
option	Â 5	for	the	password.	
7.	(Optional)	Â You	can	also	specify	the	following:	
l	Whether	to	update	security	content	from	your	Fortify	Software	Security	Center	server	
(option	Â 6)	
l	The	Fortify	Software	Security	Center	user	name	(option	Â 7)	
Removing	Proxy	Server	Settings	
If	you	previously	specified	proxy	server	settings	for	the	Fortify	Security	Content	update	server	or	Micro	
Focus	Fortify	Software	Security	Center	and	it	is	no	longer	required,	you	can	remove	these	settings.	
To	remove	the	proxy	settings	for	Fortify	Security	Content	updates	or	Fortify	Software	Security	Center	:	
1.	Navigate	to	the	bin	directory	from	the	command	line.	
2.	At	the	command	prompt,	type	scapostinstall	.	
3.	Type	2	to	select	Settings	.	
4.	Type	2	to	select	Fortify	Update	or	type	3	to	select	Software	Security	Center	Settings	.	
5.	Type	the	number	that	corresponds	to	the	proxy	setting	you	want	to	remove,	and	then	type	-	
(hyphen)	to	remove	the	setting.	
6.	Repeat	step	5	for	each	proxy	setting	you	want	to	remove.	
User	Guide	
Chapter	2:	Installing	Fortify	Static	Code	Analyzer	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	40	of	216 

Chapter	3:	Analysis	Process	Overview	
This	section	contains	the	following	topics:	
Analysis	Process	41	
Translation	Phase	42	
Mobile	Build	Sessions	43	
Analysis	Phase	44	
Translation	and	Analysis	Phase	Verification	46	
Analysis	Process	
There	are	four	distinct	phases	that	make	up	the	analysis	process:	
1.	Build	Integration	â	Choose	whether	to	integrate	Fortify	Static	Code	Analyzer	into	your	build	tool.	
For	descriptions	of	build	integration	options,	see	""Integrating	into	aBuild""	on	page	Â 105	.	
2.	Translation	â	Gathers	source	code	using	aseries	of	commands	and	translates	it	into	an	
intermediate	format	associated	with	abuild	ID.	The	build	ID	is	usually	the	name	of	the	project	you	
are	translating.	For	more	information,	see	""Translation	Phase""	on	the	next	page	.	
3.	Analysis	â	Scans	source	files	identified	in	the	translation	phase	and	generates	an	analysis	results	
file	(typically	in	the	Fortify	Project	Results	(FPR)	format).	FPR	files	have	the	.fpr	file	extension.	
For	more	information,	see	""Analysis	Phase""	on	page	Â 44	.	
4.	Verification	of	translation	and	analysis	â	Verifies	that	the	source	files	were	scanned	using	the	
correct	Rulepacks	and	that	no	errors	were	reported.	For	more	information,	see	""Translation	and	
Analysis	Phase	Verification""	on	page	Â 46	.	
The	following	is	an	example	of	the	sequence	of	commands	you	use	to	translate	and	analyze	code:	
sourceanalyzer	-b	<build_	id>	-clean	
sourceanalyzer	-b	<build_	id>	...	
sourceanalyzer	-b	<build_	id>	-scan	-f	myresults.fpr	
The	three	commands	in	the	previous	example	illustrates	the	following	steps	in	the	analysis	process:	
1.	Remove	all	existing	Fortify	Static	Code	Analyzer	temporary	files	for	the	specified	build	ID.	
Always	begin	an	analysis	with	this	step	to	analyze	aproject	with	apreviously	used	build	ID.	
2.	Translate	the	project	code.	
This	step	can	consist	of	multiple	calls	to	sourceanalyzer	with	the	same	build	ID.	
3.	Analyze	the	project	code	and	produce	the	Fortify	Project	Results	file	(FPR).	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	41	of	216 

Parallel	Processing	
Fortify	Static	Code	Analyzer	runs	in	parallel	analysis	mode	to	reduce	the	scan	time	of	large	projects.	This	
takes	advantage	of	all	CPU	cores	available	on	your	system.	When	you	run	Fortify	Static	Code	Analyzer	,	
avoid	running	other	substantial	processes	during	the	Fortify	Static	Code	Analyzer	execution	because	it	
expects	to	have	the	full	resources	of	your	hardware	available	for	the	scan.	
Translation	Phase	
To	successfully	translate	aproject	that	is	normally	compiled,	make	sure	that	you	have	any	dependencies	
required	to	build	the	project	available.	The	chapters	for	each	source	code	type	describe	any	specific	
requirements.
The	basic	command-	line	syntax	to	perform	the	first	step	of	the	analysis	process,	file	translation,	is:	
sourceanalyzer	-b	<build_	id>	...	<files>	
or
sourceanalyzer	-b	<build_	id>	...	<compiler_	command>	
The	translation	phase	consists	of	one	or	more	invocations	of	Fortify	Static	Code	Analyzer	using	the	
sourceanalyzer	command.	Fortify	Static	Code	Analyzer	uses	abuild	ID	(-b	option)	to	tie	the	
invocations	together.	Subsequent	invocations	of	sourceanalyzer	add	any	newly	specified	source	or	
configuration	files	to	the	file	list	associated	with	the	build	ID.	
After	translation,	you	can	use	the	-show-	build-	warnings	directive	to	list	any	warnings	and	errors	
that	occurred	in	the	translation	phase:	
sourceanalyzer	-b	<build_	id>	-show-	build-	warnings	
To	view	the	files	associated	with	abuild	ID,	use	the	-show-	files	directive:	
sourceanalyzer	-b	<build_	id>	-show-	files	
The	following	chapters	describe	how	to	translate	different	types	of	source	code:	
l	""Translating	Java	Code""	on	page	Â 47	
l	""Translating	Kotlin	Code""	on	page	Â 55	
l	""Translating	Visual	Studio	and	MSBuild	Projects""	on	page	Â 58	
l	""Translating	C	and	C++	Code""	on	page	Â 64	
l	""Translating	JavaScript	and	TypeScript	Code""	on	page	Â 67	
l	""Translating	Python	Code""	on	page	Â 71	
l	""Translating	Code	for	Mobile	Platforms""	on	page	Â 74	
l	""Translating	Go	Code""	on	page	Â 77	
User	Guide	
Chapter	3:	Analysis	Process	Overview	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	42	of	216 

l	""Translating	Ruby	Code""	on	page	Â 79	
l	""Translating	Apex	and	Visualforce	Code""	on	page	Â 81	
l	""Translating	COBOL	Code""	on	page	Â 84	
l	""Translating	Other	Languages	and	Configurations""	on	page	Â 88	
Mobile	Build	Sessions	
With	aFortify	Static	Code	Analyzer	mobile	build	session	(MBS),	you	can	translate	aproject	on	one	
machine	and	scan	it	on	another.	A	mobile	build	session	(MBS	file)	includes	all	the	files	needed	for	the	
analysis	phase.	To	improve	scan	time,	you	can	perform	the	translation	on	the	original	computer	and	
then	move	the	build	session	(MBS	Â file)	Â to	abetter	equipped	computer	for	the	scan.	The	developers	can	
run	translations	on	their	own	computers	and	use	only	one	powerful	computer	to	run	large	scans.	
You	must	have	the	same	version	of	Fortify	Security	Content	(Rulepacks)	installed	on	both	the	system	
where	you	are	performing	the	translation	and	the	system	where	you	are	performing	the	analysis.	
Mobile	Build	Session	Version	Compatibility	
The	Fortify	Static	Code	Analyzer	version	on	the	translate	machine	must	be	compatible	with	the	Fortify	
Static	Code	Analyzer	version	on	the	analysis	machine.	The	version	number	format	is:	
major.minor.patch.buildnumber	(for	example,	20.2.0	.0240).	The	major	and	minor	portions	of	the	Fortify	
Static	Code	Analyzer	version	numbers	on	both	the	translation	and	the	analysis	machines	must	match.	
For	example,	20.2.0	and	20.1.x	are	compatible.	
Note:	Before	version	16.10,	the	major	portion	of	the	Fortify	Static	Code	Analyzer	version	number	
was	not	the	same	as	the	Micro	Focus	Fortify	Software	Security	Center	version	number.	
To	determine	the	Fortify	Static	Code	Analyzer	version	number,	type	sourceanalyzer	-version	on	
the	command	line.	
Creating	a	Mobile	Build	Session	
On	the	machine	where	you	performed	the	translation,	issue	the	following	command	to	generate	a	
mobile	build	session:	
sourceanalyzer	-b	<build_	id>	-export-	build-	session	<file>	.mbs	
where	<file>	.mbs	is	the	file	name	you	provide	for	the	Fortify	Static	Code	Analyzer	mobile	build	
session.
Importing	a	Mobile	Build	Session	
After	you	move	the	<file>	.mbs	file	to	the	machine	where	you	want	to	perform	the	scan,	import	the	
mobile	build	session	into	the	Fortify	Static	Code	Analyzer	project	root	directory.	
User	Guide	
Chapter	3:	Analysis	Process	Overview	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	43	of	216 

Note:	If	necessary,	you	can	obtain	the	build	ID	and	Fortify	Static	Code	Analyzer	version	from	an	
MBS	Â file	with	the	following	command:	
sourceanalyzer	-import-	build-	session	<file>	.mbs	
-Dcom.fortify.sca.ExtractMobileInfo=true	
To	import	the	mobile	build	session,	type	the	following	command:	
sourceanalyzer	-import-	build-	session	<file>	.mbs	
After	you	import	your	Fortify	Static	Code	Analyzer	mobile	build	session,	you	can	proceed	to	the	
analysis	phase.	Perform	ascan	with	the	same	build	ID	that	was	used	in	the	translation.	
You	cannot	merge	multiple	mobile	build	sessions	into	asingle	MBS	file.	Each	exported	build	session	
must	have	aunique	build	ID.	However,	after	all	the	build	IDs	are	imported	on	the	same	Fortify	Static	
Code	Analyzer	installation,	you	can	scan	multiple	build	IDs	in	one	scan	with	the	-b	option	(see	""Analysis	
Phase""	below	).	
Analysis	Phase	
The	analysis	phase	scans	the	intermediate	files	created	during	translation	and	creates	the	vulnerability	
results	file	(FPR).	
The	analysis	phase	consists	of	one	invocation	of	sourceanalyzer	.You	specify	the	build	ID	and	
include	the	-scan	directive	with	any	other	required	analysis	or	output	options	(see	""Analysis	Options""	
on	page	Â 114	and	""Output	Options""	on	page	Â 117	).	
An	example	of	the	basic	command-	line	syntax	for	the	analysis	phase	is:	
sourceanalyzer	-b	<build_	id>	-scan	-f	myresults.fpr	
Note:	By	default,	Fortify	Static	Code	Analyzer	includes	the	source	code	in	the	FPR	file.	
To	combine	multiple	builds	into	asingle	scan	command,	add	the	additional	builds	to	the	command	line:	
sourceanalyzer	-b	<build_	id1>	-b	<build_	id2>	-b	<build_	id3>	-scan	-f	
myresults.fpr
The	use	of	antivirus	software	can	negatively	impact	Fortify	Static	Code	Analyzer	performance.	If	you	
notice	long	scan	times,	Fortify	recommends	that	you	temporarily	exclude	the	internal	Fortify	Static	
Code	Analyzer	files	from	your	antivirus	software	scan.	You	can	also	do	the	same	for	the	directories	
where	the	source	code	resides,	however	the	performance	impact	on	the	Fortify	analysis	is	less	than	with	
the	internal	directories.	
By	default,	Fortify	Static	Code	Analyzer	creates	internal	files	in	the	following	location:	
l	On	Windows:	c:\Users\	<user>	\AppData\Local\Fortify\sca	<version>	
l	On	non-	Windows:	$HOME/.fortify/sca	<version>	
where	<version>	is	the	version	of	Fortify	Static	Code	Analyzer	you	are	using.	
User	Guide	
Chapter	3:	Analysis	Process	Overview	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	44	of	216 

Higher-	Order	Analysis	
Higher-	Order	Analysis	(HOA)	improves	the	ability	to	track	dataflow	through	higher-	order	code.	Higher-	
order	code	manipulates	functions	as	values,	generating	them	with	anonymous	function	expressions	
(lambda	expressions),	passing	them	as	arguments,	returning	them	as	values,	and	assigning	them	to	
variables	and	to	fields	of	objects.	These	code	patterns	are	common	in	modern	dynamic	languages	such	
as	JavaScript,	TypeScript,	Python,	Ruby,	and	Swift.	
By	default,	Fortify	Static	Code	Analyzer	performs	Higher-	Order	Analysis	when	you	scan	JavaScript,	
TypeScript,	Python,	Ruby,	and	Swift	code.	For	adescription	of	the	Higher-	Order	Analysis	properties,	
see	""fortify-	sca.properties""	on	page	Â 182	and	search	for	""higher-	order	analysis.""	
Modular	Analysis	
This	release	includes	atechnology	preview	of	modular	analysis.	With	modular	analysis,	you	can	pre-	scan	
libraries	(and	sublibraries)	separately	from	your	core	project.	You	can	then	include	these	pre-	scanned	
libraries	when	you	scan	the	core	project.	Doing	this	might	improve	the	core	project	analysis	performance	
because	you	are	not	rescanning	the	libraries	every	time	you	scan	the	core	project.	Modular	analysis	also	
enables	you	to	scan	aproject	that	references	alibrary	without	requiring	the	library's	source	code,	Fortify	
Static	Code	Analyzer	translated	files,	or	custom	rules	used	to	scan	the	library.	This	has	the	added	
benefit	that	you	only	need	to	audit	issues	in	your	core	application.	The	analysis	results	are	more	
streamlined	to	code	that	you	directly	control	and	therefore	you	do	not	need	to	worry	about	issues	in	
code	that	you	do	not	own.	
Modular	analysis	is	currently	available	for	libraries	and	applications	developed	in	Java	and	Java	EE.	
Note:	In	this	release,	you	might	not	see	any	performance	improvements	from	modular	analysis.	
Fortify	is	working	to	optimize	the	performance	of	modular	analysis	in	future	releases.	
You	must	rescan	your	libraries	whenever	you:	
l	Update	to	anew	version	of	Fortify	Static	Code	Analyzer	
l	Update	your	Fortify	security	content	
l	Modify	the	libraries	
Modular	Command-	Line	Examples	
To	translate	and	scan	alibrary	separately,	type:	
sourceanalyzer	-b	LibA	MyLibs/A/*.java	
sourceanalyzer	-b	LibA	-scan-	module	
To	translate	and	scan	the	core	project	and	include	multiple	pre-	scanned	libraries:	
sourceanalyzer	-b	MyProj	MyProj/*.java	
User	Guide	
Chapter	3:	Analysis	Process	Overview	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	45	of	216 

sourceanalyzer	-b	MyProj	-scan	-include-	modules	LibA,LibB	
For	adescription	of	the	options	shown	in	the	previous	examples,	see	""Analysis	Options""	on	page	Â 114	.	
Translation	and	Analysis	Phase	Verification	
Micro	Focus	Fortify	Audit	Workbench	result	certification	indicates	whether	the	code	analysis	from	a	
scan	is	complete	and	valid.	The	project	summary	in	Fortify	Audit	Workbench	shows	the	following	
specific	information	about	Fortify	Static	Code	Analyzer	scanned	code:	
l	List	of	files	scanned,	with	file	sizes	and	timestamps	
l	Java	class	path	used	for	the	translation	(if	applicable)	
l	Rulepacks	used	for	the	analysis	
l	Fortify	Static	Code	Analyzer	runtime	settings	Â and	command-	line	options	
l	Any	errors	or	warnings	encountered	during	translation	or	analysis	
l	Machine	and	platform	information	
Note:	To	obtain	result	certification,	you	must	specify	FPR	for	the	analysis	phase	output	format.	
To	view	result	certification	information,	open	the	FPR	file	in	Fortify	Audit	Workbench	and	select	Tools	
>	Project	Summary	>	Certification	.For	more	information,	see	the	Micro	Focus	Fortify	Audit	
Workbench	User	Guide	.	
User	Guide	
Chapter	3:	Analysis	Process	Overview	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	46	of	216 

Chapter	4:	Translating	Java	Code	
This	section	describes	how	to	translate	Java	code.	
Fortify	Static	Code	Analyzer	supports	translation	of	Java	EE	Â applications	(including	JSP	files,	
configuration	files,	and	deployment	descriptors),	Â Java	Bytecode,	and	Java	code	with	Lombok	
annotations.
This	section	contains	the	following	topics:	
Java	Command-	Line	Syntax	47	
Handling	Resolution	Warnings	50	
Using	FindBugs	51	
Translating	Java	EE	Applications	52	
Translating	Java	Bytecode	53	
Troubleshooting	JSP	Translation	Issues	54	
Java	Command-	Line	Syntax	
To	translate	Java	code,	all	types	defined	in	alibrary	that	are	referenced	in	the	code	must	have	a	
corresponding	definition	in	the	source	code,	aclass	file,	or	aJAR	file.	Include	all	source	files	on	the	
Fortify	Static	Code	Analyzer	command	line.	
If	your	project	contains	Java	code	that	refers	to	Kotlin	code,	make	sure	that	the	Java	and	Kotlin	code	are	
translated	in	the	same	Fortify	Static	Code	Analyzer	instance	so	that	the	Java	references	to	Kotlin	
elements	are	resolved	correctly.	Kotlin	to	Java	interoperability	does	not	support	Kotlin	files	provided	by	
the	âsourcepath	option.	For	more	information	about	the	âsourcepath	option,	see	""Java	Command-	
Line	Options""	on	the	next	page	
The	basic	command-	line	syntax	to	translate	Java	code	is	shown	in	the	following	example:	
sourceanalyzer	-b	<build_	id>	-cp	<classpath>	<files>	
With	Java	code,	Fortify	Static	Code	Analyzer	can	either:	
l	Emulate	the	compiler,	which	might	be	convenient	for	build	integration	
l	Accept	source	files	directly,	which	is	convenient	for	command-	line	scans	
For	information	about	integrating	Fortify	Static	Code	Analyzer	with	Ant,	see	""Ant	Integration""	on	
page	Â 107	.	
To	have	Fortify	Static	Code	Analyzer	emulate	the	compiler,	type:	
sourceanalyzer	-b	<build_	id>	javac	[<translation_	options>	]	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	47	of	216 

To	pass	files	directly	to	Fortify	Static	Code	Analyzer	,type:	
sourceanalyzer	-b	<build_	id>	-cp	<classpath>	[<translation_	options	>]	
<files>	|	<file_	specifiers>	
where:
l	<translation_	options>	are	options	passed	to	the	compiler.	
l	-cp	<classpath>	specifies	the	class	path	to	use	for	the	Java	source	code.	
A	class	path	is	the	path	that	the	Java	runtime	environment	searches	for	classes	and	other	resource	
files.	Include	all	JAR	dependencies	normally	used	to	build	the	project.	The	format	is	the	same	as	what	
javac	expects	(colon-	or	semicolon-	separated	list	of	paths).	
Similar	to	javac,	Fortify	Static	Code	Analyzer	loads	classes	in	the	order	they	appear	in	the	class	path.	
If	there	are	multiple	classes	with	the	same	name	in	the	list,	Fortify	Static	Code	Analyzer	uses	the	first	
loaded	class.	In	the	following	example,	if	both	A.jar	and	B.jar	include	aclass	called	
MyData.class	,Fortify	Static	Code	Analyzer	uses	the	MyData.class	from	A.jar	.	
sourceanalyzer	-cp	A.jar:B.jar	myfile.java	
Fortify	strongly	recommends	that	you	avoid	using	duplicate	classes	with	the	-cp	option.	
Fortify	Static	Code	Analyzer	loads	JAR	files	in	the	following	order:	
a.	From	the	-cp	option	
b.	From	jre/lib	
c.	From	<sca_	install_	dir>	/Core/default_	jars	
This	enables	you	to	override	alibrary	class	by	including	the	similarly-	named	class	in	aJAR	Â specified	
with	the	-cp	option.	
For	descriptions	of	all	the	available	Java-	specific	command-	line	options,	see	""Java	Command-	Line	
Options""	below	.	
Java	Command-	Line	Options	
The	following	table	describes	the	Java	command-	line	options	(for	Java	Â SE	and	Java	Â EE).	
Java/Java	EE	Option	Description	
-appserver
weblogic	Â |Â websphere	
Specifies	the	application	server	to	process	JSP	files.	
Equivalent	Property	Name:	
com.fortify.sca.AppServer	
-appserver-	home	<dir>	Specifies	the	application	serverâs	home.	
l	For	WebLogic,	this	is	the	path	to	the	directory	that	
contains	the	server/lib	directory.	
l	For	WebSphere,	this	is	the	path	to	the	directory	that	
User	Guide	
Chapter	4:	Translating	Java	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	48	of	216 

Java/Java	EE	Option	Description	
contains	the	JspBatchCompiler	script.	
Equivalent	Property	Name:	
com.fortify.sca.AppServerHome	
-appserver-	version	
<version>	
Specifies	the	version	of	the	application	server.	See	the	Micro	
Focus	Fortify	Software	System	Requirements	document	for	
supported	versions.	
Equivalent	Property	Name:	
com.fortify.sca.AppServerVersion	
-cpÂ 	<dirs>	|	
-classpath	<dirs>	
Specifies	the	class	path	to	use	for	analyzing	Java	source	code.	
The	format	is	the	same	as	javac:	acolon-	or	semicolon-	
separated	list	of	directories.	You	can	use	Fortify	Static	Code	
Analyzer	file	specifiers	as	shown	in	the	following	example:	
-cp	""build/classes:lib/*.jar""	
For	information	about	file	specifiers,	see	""Specifying	Files	and	
Directories""	on	page	Â 123	.	
Equivalent	Property	Name:	
com.fortify.sca.JavaClasspath	
-extdirs	<dirs>	Similar	to	the	javac	extdirs	option,	accepts	acolon-	or	
semicolon-	separated	list	of	directories.	Any	JAR	Â files	found	in	
these	directories	are	included	implicitly	on	the	class	path.	
Equivalent	Property	Name:	
com.fortify.sca.JavaExtdirs	
-java-	build-	dir	<dirs>	Specifies	one	or	more	directories	that	contain	compiled	Java	
sources.	You	must	specify	this	for	FindBugs	results	as	
described	in	""Analysis	Options""	on	page	Â 114	.	
-sourceÂ 	<version>	|Â 	
-jdkÂ 	<version>	
Indicates	the	JDK	version	for	which	the	Java	code	is	written.	
See	the	Micro	Focus	Fortify	Software	System	Requirements	
document	for	supported	versions.	The	default	is	Java	8.	
Equivalent	Property	Name:	
com.fortify.sca.JdkVersion	
-sourcepath	<dirs>	Specifies	acolon-	or	semicolon-	separated	list	of	directories	
that	contain	source	code	that	is	not	included	in	the	scan	but	is	
User	Guide	
Chapter	4:	Translating	Java	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	49	of	216 

Java/Java	EE	Option	Description
used	for	name	resolution.	The	source	path	is	similar	to	class	
path,	except	it	uses	source	files	instead	of	class	files	for	
resolution.	Only	source	files	that	are	referenced	by	the	target	
file	list	are	translated.	
Equivalent	Property	Name:	
com.fortify.sca.JavaSourcePath	
Java	Command-	Line	Examples	
To	translate	asingle	file	named	MyServlet.java	with	javaee.jar	as	the	class	path,	type:	
sourceanalyzer	-b	MyServlet	-cp	lib/javaee.jar	MyServlet.java	
To	translate	all	.java	files	in	the	src	directory	using	all	JAR	Â files	in	the	lib	directory	as	aclass	path,	
type:
sourceanalyzer	-b	MyProject	-cp	""lib/*.jar""	""src/**/*.java""	
To	translate	and	compile	the	MyCode.java	file	with	the	javac	compiler,	type:	
sourceanalyzer	-b	MyProject	javac	-classpath	libs.jar	MyCode.java	
Handling	Resolution	Warnings	
To	see	all	warnings	that	were	generated	during	translation,	type	the	following	command	before	you	
start	the	scan	phase:	
sourceanalyzer	-b	<build_	id>	-show-	build-	warnings	
Java	Warnings	
You	might	see	the	following	warnings	for	Java:	
Unable	to	resolve	type...	
Unable	to	resolve	function...	
Unable	to	resolve	field...	
Unable	to	locate	import...	
Unable	to	resolve	symbol...	
User	Guide	
Chapter	4:	Translating	Java	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	50	of	216 

Multiple	definitions	found	for	function...	
Multiple	definitions	found	for	class...	
These	warnings	are	typically	caused	by	missing	resources.	For	example,	some	of	the	.jar	and	.class	
files	required	to	build	the	application	might	not	have	been	specified.	To	resolve	the	warnings,	make	sure	
that	you	include	all	the	required	files	that	your	application	uses.	
Using	FindBugs	
FindBugs	(http://findbugs.sourceforge.net	)is	astatic	analysis	tool	that	detects	quality	issues	in	Java	
code.	You	can	run	FindBugs	with	Fortify	Static	Code	Analyzer	and	the	results	are	integrated	into	the	
analysis	results	file.	Unlike	Fortify	Static	Code	Analyzer	,which	runs	on	Java	source	files,	FindBugs	runs	
on	Java	bytecode.	Therefore,	before	you	run	an	analysis	on	your	project,	first	compile	the	project	and	
produce	the	class	files.	
To	see	an	example	of	how	to	run	FindBugs	automatically	with	Fortify	Static	Code	Analyzer	,compile	the	
sample	code	Warning.java	as	follows:	
1.	Go	to	the	following	directory:	
<sca_	install_	dir>	/Samples/advanced/findbugs	
2.	Type	the	following	commands	to	compile	the	sample:	
mkdir	build	
javac	-d	build	Warning.java	
3.	Scan	the	sample	with	FindBugs	and	Fortify	Static	Code	Analyzer	as	follows:	
sourceanalyzer	-b	findbugs_	sample	-java-	build-	dir	build	Warning.java	
sourceanalyzer	-b	findbugs_	sample	-scan	-findbugs	-f	myresults.fpr	
4.	Examine	the	analysis	results	in	Micro	Focus	Fortify	Audit	Workbench	:	
auditworkbench	myresults.fpr	
The	output	contains	the	following	issue	categories:	
l	Bad	casts	of	Object	References	(1)	
l	Dead	local	store	(2)	
l	Equal	objects	must	have	equal	hashcodes	(1)	
l	Object	model	violation	(1)	
l	Unwritten	field	(2)	
l	Useless	self-	assignment	(2)	
User	Guide	
Chapter	4:	Translating	Java	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	51	of	216 

If	you	group	by	analyzer,	you	can	see	that	the	Structural	Analyzer	produced	one	issue	and	FindBugs	
produced	eight.	The	Object	model	violation	issue	Fortify	Static	Code	Analyzer	detected	on	line	
25	is	similar	to	the	Equal	objects	must	have	equal	hash	codes	issue	that	FindBugs	detected.	
In	addition,	FindBugs	produces	two	sets	of	issues	(Useless	self-	assignment	and	Dead	local	
store	)about	the	same	vulnerabilities	on	lines	6	and	7.	To	avoid	overlapping	results,	use	the	-filter	
option	during	the	scan	to	apply	the	filter.txt	filter	file.	Note	that	the	filtering	is	not	complete	
because	each	tool	filters	at	adifferent	level	of	granularity.	To	see	how	to	avoid	overlapping	results,	scan	
the	sample	code	using	filter.txt	as	follows:	
sourceanalyzer	-b	findbugs_	sample	-scan	-findbugs	-filter	filter.txt	
-f	myresults.fpr	
Translating	Java	EE	Applications	
To	translate	Java	EE	Â applications,	Fortify	Static	Code	Analyzer	processes	Java	source	files	and	
Java	Â EE	Â components	such	as	JSP	files,	deployment	descriptors,	and	configuration	files.	While	you	can	
process	all	the	pertinent	files	in	aJava	Â EE	application	in	one	step,	your	project	might	require	that	you	
break	the	procedure	into	its	components	for	integration	in	abuild	process	or	to	meet	the	needs	of	
various	stakeholders	in	your	organization.	
Translating	Java	Files	
To	translate	Java	EE	applications,	use	the	same	procedure	used	to	translate	Java	files.	For	examples,	see	
""Java	Command-	Line	Examples""	on	page	Â 50	.	
Translating	JSP	Projects,	Configuration	Files,	and	Deployment	
Descriptors
In	addition	to	translating	the	Java	files	in	your	Java	Â EE	application,	you	might	also	need	to	translate	JSP	
files,	configuration	files,	and	deployment	descriptors.	Your	JSP	files	must	be	part	of	aWeb	Application	
Archive	(WAR).	If	your	source	directory	is	already	organized	in	aWAR	file	format,	you	can	translate	the	
JSP	files	directly	from	the	source	directory.	If	not,	you	might	need	to	deploy	your	application	and	
translate	the	JSP	files	from	the	deployment	directory.	
For	example:	
sourceanalyzer	-b	MyJavaApp	""/**/*.jsp""	""/**/*.xml""	
where	/**/*.jsp	refers	to	the	location	of	your	JSP	project	files	and	/**/*.xml	refers	to	the	location	
of	your	configuration	and	deployment	descriptor	files.	
User	Guide	
Chapter	4:	Translating	Java	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	52	of	216 

Java	EE	Translation	Warnings	
You	might	see	the	following	warning	in	the	translation	of	Java	EE	Â applications:	
Could	not	locate	the	root	(WEB-	INF)	of	the	web	application.	Please	build	
your	web	application	and	try	again.	Failed	to	parse	the	following	jsp	
files:
<list_	of_	jsp_	files>	
This	warning	indicates	that	your	web	application	is	not	deployed	in	the	standard	WAR	directory	format	
or	does	not	contain	the	full	set	of	required	libraries.	To	resolve	the	warning,	make	sure	that	your	web	
application	is	in	an	exploded	WAR	directory	format	with	the	correct	WEB-	INF/lib	and	WEB-	
INF/classes	directories	containing	all	of	the	.jar	and	.class	files	required	for	your	application.	
Also	verify	that	you	have	all	of	the	TLD	files	for	all	of	your	tags	and	the	corresponding	JAR	files	with	
their	tag	implementations.	
Translating	Java	Bytecode	
In	addition	to	translating	source	code,	you	can	translate	the	bytecode	in	your	project.	You	must	specify	
two	configuration	properties	and	include	the	bytecode	files	in	the	Fortify	Static	Code	Analyzer	
translation	phase.	
For	best	results,	Fortify	recommends	that	the	bytecode	be	compiled	with	full	debug	information	
(javacÂ -	g).	
Fortify	recommends	that	you	do	not	translate	Java	bytecode	and	JSP/Java	code	in	the	same	call	to	
sourceanalyzer	.Use	multiple	invocations	of	sourceanalyzer	with	the	same	build	ID	to	translate	a	
project	that	contains	both	bytecode	and	JSP/Java	Â code.	
To	include	bytecode	in	the	Fortify	Static	Code	Analyzer	translation:	
1.	Add	the	following	properties	to	the	fortify-	sca.properties	file	(or	include	these	properties	
on	the	command	line	using	the	-D	option):	
com.fortify.sca.fileextensions.class=BYTECODE
com.fortify.sca.fileextensions.jar=ARCHIVE
This	specifies	how	Fortify	Static	Code	Analyzer	processes	.class	and	.jar	files.	
2.	In	the	Fortify	Static	Code	Analyzer	translation	phase,	specify	the	Java	bytecode	files	that	you	want	
to	translate.	For	best	performance,	specify	only	the	.jar	or	.class	files	that	require	scanning.	
In	the	following	example,	the	.class	files	are	translated:	
sourceanalyzer	-b	MyProject	-cp	""lib/*.jar""	""src/**/*.class""	
User	Guide	
Chapter	4:	Translating	Java	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	53	of	216 

Troubleshooting	JSP	Translation	Issues	
Fortify	Static	Code	Analyzer	uses	either	the	built-	in	compiler	or	your	specific	application	server	JSP	
compiler	to	translate	JSP	files	into	Java	files	for	analysis.	If	the	JSP	parser	encounters	problems	when	
Fortify	Static	Code	Analyzer	converts	JSP	files	to	Java	files,	you	will	see	amessage	similar	to	the	
following:
Failed	to	translate	the	following	jsps	into	analysis	model.	Please	see	the	
log	file	for	any	errors	from	the	jsp	parser	and	the	user	manual	for	hints	
on	fixing	those	
<list_	of_	jsp_	files>	
This	typically	happens	for	one	or	more	of	the	following	reasons:	
l	The	web	application	is	not	laid	out	in	aproper	deployable	WAR	directory	format	
l	You	are	missing	some	JAR	files	or	classes	required	for	the	application	
l	You	are	missing	some	tag	libraries	or	their	definitions	(TLD)	for	the	application	
To	obtain	more	information	about	the	problem,	perform	the	following	steps:	
1.	Open	the	Fortify	Static	Code	Analyzer	log	file	in	an	editor.	
2.	Search	for	the	strings	Jsp	parser	stdout:	and	Jsp	parser	stderr:	.	
The	JSP	parser	generates	these	errors.	Resolve	the	errors	and	rerun	Fortify	Static	Code	Analyzer	.	
For	more	information	about	scanning	Java	Â EE	applications,	see	""Translating	Java	EE	Applications""	on	
page	Â 52	.	
User	Guide	
Chapter	4:	Translating	Java	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	54	of	216 

Chapter	5:	Translating	Kotlin	Code	
This	section	describes	how	to	translate	Kotlin	code.	
This	section	contains	the	following	topics:	
Kotlin	Command-	Line	Syntax	55	
Kotlin	and	Java	Translation	Interoperability	57	
Kotlin	Command-	Line	Syntax	
The	translation	of	Kotlin	code	is	similar	to	the	translation	of	Java	code.	To	translate	Kotlin	code,	all	types	
defined	in	alibrary	that	are	referenced	in	the	code	must	have	acorresponding	definition	in	the	source	
code,	aclass	file,	or	aJAR	file.	Include	all	source	files	on	the	Fortify	Static	Code	Analyzer	command	line.	
The	basic	command-	line	syntax	to	translate	Kotlin	code	is	shown	in	the	following	example:	
sourceanalyzer	âb	<build_	id>	-cp	<classpath>	[<translation_	options>	]	
<files>
where
l	-cp	<classpath>	specifies	the	class	path	to	use	for	the	Kotlin	source	code.	
A	class	path	is	the	path	that	the	Java	runtime	environment	searches	for	classes	and	other	resource	
files.	Include	all	JAR	dependencies	normally	used	to	build	the	project.	The	format	is	acolon-	or	
semicolon-	separated	list	of	paths.	
Fortify	Static	Code	Analyzer	loads	classes	in	the	order	they	appear	in	the	class	path.	If	there	are	
multiple	classes	with	the	same	name	in	the	list,	Fortify	Static	Code	Analyzer	uses	the	first	loaded	class.	
In	the	following	example,	if	both	A.jar	and	B.jar	include	aclass	called	MyData.class	,Fortify	
Static	Code	Analyzer	uses	the	MyData.class	from	A.jar	.	
sourceanalyzer	âcp	""A.jar:B.jar""	myfile.kt	
Fortify	strongly	recommends	that	you	avoid	using	duplicate	classes	with	the	-cp	option.	
For	descriptions	of	all	the	available	Kotlin-	specific	command-	line	options,	see	""Kotlin	Command-	Line	
Options""	on	the	next	page	.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	55	of	216 

Kotlin	Command-	Line	Options	
The	following	table	describes	the	Kotlin-	specific	command-	line	options.	
Kotlin	Option	Description	
-cpÂ 	<paths>	|	
-classpath	<dirs>	
Specifies	the	class	path	to	use	for	translating	Kotlin	source	
code,	which	is	acolon-	or	semicolon-	separated	list	of	
directories.	You	can	use	Fortify	Static	Code	Analyzer	file	
specifiers	as	shown	in	the	following	example:	
-cp	""build/classes:lib/*.jar""	
For	information	about	file	specifiers,	see	""Specifying	Files	and	
Directories""	on	page	Â 123	.	
Equivalent	Property	Name:	
com.fortify.sca.JavaClasspath	
-sourcepath	<dirs>	Specifies	acolon-	or	semicolon-	separated	list	of	directories	
that	contain	source	code	that	is	not	included	in	the	scan	but	is	
used	for	name	resolution.	The	source	path	is	similar	to	class	
path,	except	it	uses	source	files	instead	of	class	files	for	
resolution.	Only	source	files	that	are	referenced	by	the	target	
file	list	are	translated.	
Equivalent	Property	Name:	
com.fortify.sca.JavaSourcePath	
User	Guide	
Chapter	5:	Translating	Kotlin	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	56	of	216 

Kotlin	Command-	Line	Examples	
To	translate	asingle	file	named	MyKotlin.kt	with	A.jar	as	the	class	path,	type:	
sourceanalyzer	-b	MyProject	-cp	lib/A.jar	MyKotlin.kt	
To	translate	all	.kt	files	in	the	src	directory	using	all	JAR	Â files	in	the	lib	directory	as	aclass	path,	type:	
sourceanalyzer	-b	MyProject	-cp	""lib/**/*.jar""	""src/**/*.kt""	
To	translate	and	scan	agradle	project	using	gradlew,	type:	Â 	
sourceanalyzer	-b	MyProject	gradlew	clean	assemble	
sourceanalyzer	-b	MyProject	-scan	âf	myresults.fpr	
To	translate	all	files	in	the	src	directory	using	Java	dependencies	from	src/java	and	all	JAR	files	in	
the	lib	directory	and	subdirectories	as	aclass	path,	type:	
sourceanalyzer	âb	MyProject	âcp	""lib/**/*.jar""	-sourcepath	""src/java""	
""src""
Kotlin	and	Java	Translation	Interoperability	
If	your	project	contains	Kotlin	code	that	refers	to	Java	code,	you	can	provide	Java	files	to	the	translator	
the	same	way	as	Kotlin	files	that	refers	to	another	Kotlin	file.	You	can	provide	them	as	part	of	the	
translated	project	source	or	as	âsourcepath	parameters.	
If	your	project	contains	Java	code	that	refers	to	Kotlin	code,	make	sure	that	the	Java	and	Kotlin	code	are	
translated	in	the	same	Fortify	Static	Code	Analyzer	instance	so	that	the	Java	references	to	Kotlin	
elements	are	resolved	correctly.	Kotlin	to	Java	interoperability	does	not	support	Kotlin	files	provided	by	
the	âsourcepath	option.	For	more	information	about	the	âsourcepath	option,	see	""Kotlin	
Command-	Line	Options""	on	the	previous	page	
User	Guide	
Chapter	5:	Translating	Kotlin	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	57	of	216 

Chapter	6:	Translating	Visual	Studio	and	
MSBuild	Projects	
Fortify	Static	Code	Analyzer	supports	translation	of	the	following	types	of	projects	built	with	Visual	
Studio	or	MSBuild:	
l	C/C++	projects	
l	.NET	projects	written	in	C#	or	Visual	Basic	(VB.NET)	
This	includes	projects	that	target	.NET	Â Framework,	.NET	Â Core,	and	.NET	Â Standard	
l	ASP.NET	applications	
This	includes	applications	that	make	use	of	the	ASP.NET	Â Core	framework	
l	Xamarin	applications	that	target	Android	and	iOS	platforms	
For	the	list	of	supported	versions	of	relevant	programming	languages	and	frameworks,	as	well	as	Visual	
Studio	and	MSBuild,	see	the	Micro	Focus	Fortify	Software	System	Requirements	document.	
This	section	contains	the	following	topics:	
Visual	Studio	and	MSBuild	Project	Translation	Prerequisites	58	
Visual	Studio	and	MSBuild	Project	Translation	Command-	Line	Syntax	59	
Handling	Special	Cases	for	Translating	Visual	Studio	and	MSBuild	Projects	59	
Alternative	Ways	to	Translate	Visual	Studio	and	MSBuild	Projects	62	
Visual	Studio	and	MSBuild	Project	Translation	
Prerequisites
Important!	Fortify	Static	Code	Analyzer	supports	translation	of	Visual	Studio	and	MSBuild	
projects	on	Windows	systems	only.	
Fortify	recommends	that	each	project	you	translate	is	complete	and	that	you	perform	the	translation	in	
an	environment	where	you	can	build	it	without	errors.	AÂ complete	project	contains	the	following:	
l	All	necessary	source	code	files	(C/C++,	C#,	or	VB.NET)	
l	All	required	reference	libraries	
This	includes	those	from	relevant	frameworks,	NuGet	packages,	and	third-	party	libraries.	
l	For	C/C++	projects,	include	all	necessary	header	files	that	do	not	belong	to	the	Visual	Studio	or	
MSBuild	installation	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	58	of	216 

l	For	ASP.NET	and	ASP.NET	Core	projects,	include	all	the	necessary	ASP.NET	page	files	
The	supported	ASP.NET	page	types	are	ASPX,	ASCX,	ASAX,	ASHX,	ASMX,	AXML,	Master,	
CSHTML,	VBHTML,	BAML,	and	XAML.	
Visual	Studio	and	MSBuild	Project	Translation	
Command-	Line	Syntax	
The	basic	syntax	to	translate	Visual	Studio	or	MSBuild	projects	is	to	append	an	MSBuild	command	that	
builds	the	project	to	the	Fortify	Static	Code	Analyzer	command.	The	following	command	translates	a	
Visual	Studio	solution	called	Sample.sln	:	
sourceanalyzer	âb	<build_	id>	msbuild	/t:rebuild	Sample.sln	
This	command	first	builds	the	solution	or	project	and	then	translates	it.	Fortify	strongly	recommends	
you	run	this	command	from	the	Developer	Command	Prompt	for	Visual	Studio	to	ensure	an	optimal	
environment	for	the	translation.	
Important!	When	you	translate	from	the	Developer	Command	Prompt	for	Visual	Studio	
environment,	Fortify	recommends	that	you	run	the	dotnet	restore	command	before	you	run	
the	Fortify	Static	Code	Analyzer	translation.	You	must	run	this	command	from	the	top-	level	folder	
of	the	project.	This	ensures	that	all	required	reference	libraries	are	downloaded	and	installed	in	the	
project.
After	the	translation	is	complete,	you	can	perform	the	analysis	phase	as	shown	in	the	following	example:	
sourceanalyzer	âb	<build_	id>	-scan	-f	<results>	.fpr	
Handling	Special	Cases	for	Translating	Visual	Studio	
and	MSBuild	Projects	
Running	Translation	From	a	Script	
As	stated	previously,	Fortify	recommends	that	you	run	the	Fortify	Static	Code	Analyzer	translation	of	
Visual	Studio	and	MSBuild	projects	from	the	Developer	Command	Prompt	for	Visual	Studio.	To	
perform	the	translation	in	anon-	interactive	mode	such	as	with	ascript,	establish	an	optimal	
environment	for	translation	by	executing	the	following	command	before	you	run	the	Fortify	Static	
Code	Analyzer	translation:	
cmd.exe	/k	<vs_	install_	dir>	/Common7/Tools/VSDevCmd.bat	
where	<vs_	install_	dir>	Â is	the	directory	where	you	installed	Visual	Studio.	
User	Guide	
Chapter	6:	Translating	Visual	Studio	and	MSBuild	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	59	of	216 

Translating	Plain	.NET	and	ASP.NET	Projects	
You	can	translate	plain	.NET	and	ASP.NET	projects	from	the	Windows	Command	Prompt	as	well	as	
from	aVisual	Studio	environment.	When	you	translate	from	the	Windows	Command	Prompt,	make	sure	
the	path	to	the	MSBuild	executable	required	to	build	your	project	is	included	in	PATH	environment	
variable.
Translating	C/C++	and	Xamarin	Projects	
Important!	You	must	translate	C/C++	and	Xamarin	projects	either	from	aDeveloper	Command	
Prompt	for	Visual	Studio	or	from	the	Micro	Focus	Fortify	Extension	for	Visual	Studio	.	
Additionally,	Fortify	recommends	that	you	translate	Xamarin	projects	from	the	Visual	Studio	2019	
environment,	even	if	you	created	the	project	with	adifferent	version	of	Visual	Studio.	Using	adifferent	
Visual	Studio	version	environment	for	translation	might	reduce	the	quality	of	the	analysis	results.	
Translating	Projects	with	Settings	Containing	Spaces	
If	your	project	is	built	with	aconfiguration	or	other	settings	file	that	contains	spaces,	make	sure	to	do	
the	following	in	the	MSBuild	command:	
l	Enclose	the	setting	value	in	quotes	(in	addition	to	the	quotes	around	the	appropriate	command-	line	
option)	
l	Quotes	are	escaped	
For	example,	to	translate	aVisual	Studio	solution	Sample.sln	that	is	built	with	configuration	My	
Configuration	,use	the	following	command:	
sourceanalyzer	âb	<build_	id>	msbuild	/t:rebuild	
""/p:Configuration=\"My	Configuration\"""	Sample.sln	
Translating	a	Single	Project	from	a	Visual	Studio	Solution	
If	your	Visual	Studio	solution	contains	multiple	projects,	you	have	the	option	to	translate	asingle	
project	instead	of	the	entire	solution.	Project	files	have	afile	name	extension	that	ends	with	proj	such	
as	.vcxproj	and	.csproj	.To	translate	asingle	project,	specify	the	project	file	instead	of	the	solution	
as	the	parameter	for	the	MSBuild	command.	
The	following	example	translates	the	Sample.vcxproj	project	file:	
sourceanalyzer	âb	<build_	id>	msbuild	/t:rebuild	Sample.vcxproj	
User	Guide	
Chapter	6:	Translating	Visual	Studio	and	MSBuild	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	60	of	216 

Translating	Visual	Studio	Solutions	with	Excluded	or	Skipped	
Projects
By	default,	Fortify	Static	Code	Analyzer	translates	all	projects	in	the	solution	specified	on	the	command	
line,	even	if	some	projects	are	explicitly	excluded	from	build	in	the	solution	or	skipped	during	the	build	
due	to	platform	and	configuration	settings	used.	To	skip	these	excluded	or	skipped	projects	from	the	
translation,	add	the	âexclude-	disabled-	projects	option	to	the	Fortify	Static	Code	Analyzer	
command	as	shown	in	the	following	example:	
sourceanalyzer	âb	<build_	id>	-exclude-	disabled-	projects	msbuild	/t:rebuild	
Sample.sln
Working	with	Multiple	Targets	and	Projects	for	MSBuild	Command	
Recent	versions	of	MSBuild	enable	you	to	build	multiple	targets	and	specific	projects	using	an	extended	
syntax	of	the	/t	option.	However,	Fortify	recommends	that	you	avoid	this	syntax	for	translating	Visual	
Studio	or	MSBuild	projects.	Specifying	asingle	rebuild	target	is	sufficient	to	translate	any	supported	
Visual	Studio	and	MSBuild	project.	
If	you	cannot	use	asingle	rebuild	target	to	translate	your	project,	Fortify	provides	limited	support	of	
multiple	targets	and	projects	specified	for	the	MSBuild	command.	To	use	this	feature,	add	the	â	
multiple-	msbuild-	targets	option	to	the	Fortify	Static	Code	Analyzer	command,	as	shown	in	the	
following	example:	
sourceanalyzer	âb	<build_	id>	-multiple-	msbuild-	targets	msbuild	
/t:Project1:build;Project2:build	Sample.sln	
Note:	Support	of	this	translation	mode	is	limited	and	Fortify	Static	Code	Analyzer	might	not	
properly	handle	all	possible	combinations	of	targets	and	projects.	
Important!	If	you	use	this	translation	mode,	do	not	specify	aclean	target	as	the	last	target	on	the	
list.	This	target	removes	dependencies	that	your	Visual	Studio	or	MSBuild	project	requires.	This	
drastically	reduces	the	translation	quality	and	can	negatively	impact	the	analysis	results.	
Analyzing	Projects	That	Build	Multiple	Executable	Files	
If	your	Visual	Studio	or	MSBuild	project	builds	multiple	executable	files	(such	as	files	with	the	file	name	
extension	*.exe	),	Fortify	strongly	recommends	that	you	run	the	analysis	phase	separately	for	each	
executable	file	to	avoid	false	positive	issues	in	the	analysis	results.	To	do	this,	use	âbinary-	name	
option	when	running	the	analysis	phase	and	specify	the	executable	file	name	or	.NET	assembly	name	as	
the	parameter.	
The	following	example	shows	how	to	translate	and	analyze	aVisual	Studio	solution	Sample.sln	that	
consists	of	two	projects,	Sample1	(a	C++	project	with	no	associated	.NET	assembly	name)	and	Sample2	
User	Guide	
Chapter	6:	Translating	Visual	Studio	and	MSBuild	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	61	of	216 

(a	.NET	project	with	.NET	assembly	name	Sample2	).	Each	project	builds	aseparate	executable	file,	
Sample1.exe	and	Sample2.exe	,respectively.	
sourceanalyzer	-b	<build_	id>	msbuild	/t:rebuild	Sample.sln	
sourceanalyzer	-b	<build_	id>	-scan	-binary-	name	Sample1.exe	-f	Sample1.fpr	
sourceanalyzer	-b	<build_	id>	-scan	-binary-	name	Sample2	-f	Sample2.fpr	
For	more	information	about	the	-binary-	name	option,	see	""Analysis	Options""	on	page	Â 114	.	
Alternative	Ways	to	Translate	Visual	Studio	and	
MSBuild	Projects	
This	section	describes	alternative	methods	of	translating	Visual	Studio	and	MSBuild	projects.	
This	section	contains	the	following	topics:	
Alternative	Translation	Options	for	Visual	Studio	Solutions	
The	following	are	two	alternative	ways	of	translation	available	only	for	Visual	Studio	solutions:	
l	Use	the	Micro	Focus	Fortify	Extension	for	Visual	Studio	
The	Fortify	Extension	for	Visual	Studio	runs	the	translation	and	analysis	(scan)	phases	together	in	
one	step.	
l	Append	adevenv	command	to	the	Fortify	Static	Code	Analyzer	command	
The	following	command	translates	aVisual	Studio	solution	called	Sample.sln	:	
sourceanalyzer	âb	<build_	id>	devenv	Sample.sln	/rebuild	
Note	that	Fortify	Static	Code	Analyzer	converts	adevenv	invocation	to	the	equivalent	MSBuild	
invocation,	therefore	in	this	case	the	solution	with	this	command	is	actually	built	by	MSBuild	instead	
of	the	devenv	tool.	
Translating	Without	Explicitly	Running	Fortify	Static	Code	
Analyzer
You	have	the	option	to	translate	your	Visual	Studio	or	MSBuild	project	without	invoking	Fortify	Static	
Code	Analyzer	directly.	This	requires	the	MSbuild	extension	file	named	
FortifyMSBuildTouchless.dll	located	in	the	\Core\lib	directory	where	you	have	installed	
Fortify	Static	Code	Analyzer	.Specify	this	MSBuild	extension	file	using	an	absolute	or	relative	path	in	
the	MSBuild	command	that	builds	your	project	with	the	/logger	option.	For	example:	
msbuild	/t:rebuild	/logger:	<sca_	install_	dir>	\Core\lib\FortifyMSBuildTouchless.dll	
Sample.sln
User	Guide	
Chapter	6:	Translating	Visual	Studio	and	MSBuild	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	62	of	216 

There	are	several	environment	variables	that	you	can	set	to	configure	the	translation	of	your	project.	
Most	of	them	have	default	values,	which	Fortify	Static	Code	Analyzer	uses	if	the	variable	is	not	set.	
These	variables	are	listed	in	the	following	table.	
Environment
Variable	Description	Default	Value	
FORTIFY_
MSBUILD_
BUILDID	
Specifies	the	Fortify	Static	Code	Analyzer	build	
ID	for	translation.	Make	sure	that	you	set	this	
value.
This	is	equivalent	to	the	Fortify	Static	Code	
Analyzer	's	-b	option.	
None	
FORTIFY_
MSBUILD_
DEBUG	
Enables	debug	mode.	This	is	equivalent	to	the	
Fortify	Static	Code	Analyzer	âdebug	option.	
False	
FORTIFY_
MSBUILD_
DEBUG_
VERBOSE	
Enables	verbose	debug	mode.	This	is	
equivalent	to	the	Fortify	Static	Code	Analyzer	â	
debug-	verbose	option.	Takes	precedence	
over	FORTIFY_	MSBUILD_	DEBUG	variable	if	
both	are	set	to	true.	
False	
FORTIFY_
MSBUILD_	MEM	
Specifies	the	memory	requirements	translation	
in	the	form	of	the	JVM	Â -Xmx	option.	For	
example,	-Xmx2G	.	
Automatic	allocation	based	on	
physical	memory	available	on	
the	system	
FORTIFY_
MSBUILD_
SCALOG	
Specifies	the	location	(absolute	path)	of	the	
Fortify	Static	Code	Analyzer	log	file.	
This	is	equivalent	to	the	Fortify	Static	Code	
Analyzer	's	-logfile	option.	
%LOCALAPPDATA%/Fortify/
sca/log/sca.log	
User	Guide	
Chapter	6:	Translating	Visual	Studio	and	MSBuild	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	63	of	216 

Chapter	7:	Translating	C	and	C++	Code	
This	section	describes	how	to	translate	C	and	C++	Â code.	
Important!	The	chapter	describes	how	to	translate	C	and	C++	code	that	is	not	apart	of	aVisual	
Studio	or	MSBuild	project.	For	instructions	on	translating	Visual	Studio	or	MSBuild	projects,	see	
""Translating	Visual	Studio	and	MSBuild	Projects""	on	page	Â 58	.	
This	section	contains	the	following	topics:	
C	and	C++	Â Code	Translation	Prerequisites	64	
C	and	C++	Command-	Line	Syntax	64	
Scanning	Pre-	processed	C	and	C++	Code	65	
C/C++	Precompiled	Header	Files	65	
Troubleshooting	Translation	Failed	Message	65	
C	and	C++	Â Code	Translation	Prerequisites	
Make	sure	that	you	have	any	dependencies	required	to	build	the	project	available,	including	headers	for	
third-	party	libraries.	Fortify	Static	Code	Analyzer	translation	does	not	require	object	files	and	
static/dynamic	library	files.	
C	and	C++	Command-	Line	Syntax	
Command-	line	options	passed	to	the	compiler	affect	preprocessor	execution	and	can	enable	or	disable	
language	features	and	extensions.	For	Fortify	Static	Code	Analyzer	to	interpret	your	source	code	in	the	
same	way	as	the	compiler,	the	translation	phase	for	C/C++	source	code	requires	the	complete	compiler	
command	line.	Prefix	your	original	compiler	command	with	the	sourceanalyzer	command	and	
options.
The	basic	command-	line	syntax	for	translating	asingle	file	is:	
sourceanalyzer	-b	<build_	id>	[<sca_	options>	]	<compiler>	[<compiler_	
options>	]	<file>	.c	
where:
l	<compiler>	is	the	name	of	the	C/C++	compiler	you	use,	such	as	gcc	,g++	,or	cl	.See	the	Micro	
Focus	Fortify	Software	System	Requirements	document	for	alist	of	supported	C/C++	compilers.	
l	<sca_	options>	are	options	passed	to	Fortify	Static	Code	Analyzer	.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	64	of	216 

l	<compiler_	options>	are	options	passed	to	the	C/C++	compiler.	
l	<file>	.c	must	be	in	ASCII	or	UTF-	8	encoding.	
Note:	All	Fortify	Static	Code	Analyzer	Â options	must	precede	the	compiler	options.	
The	compiler	command	must	successfully	complete	when	executed	on	its	own.	If	the	compiler	command	
fails,	then	the	Fortify	Static	Code	Analyzer	command	prefixed	to	the	compiler	command	also	fails.	
For	example,	if	you	compile	afile	with	the	following	command:	
gcc	-I.	-o	hello.o	-c	helloworld.c	
then	you	can	translate	this	file	with	the	following	command:	
sourceanalyzer	-b	<build_	id>	gcc	-I.	-o	hello.o	-c	helloworld.c	
Fortify	Static	Code	Analyzer	executes	the	original	compiler	command	as	part	of	the	translation	phase.	In	
the	previous	example,	the	command	produces	both	the	translated	source	suitable	for	scanning,	and	the	
object	file	hello.o	from	the	gcc	execution.	You	can	use	the	Fortify	Static	Code	Analyzer	-nc	option	
to	disable	the	compiler	execution.	
Scanning	Pre-	processed	C	and	C++	Code	
If,	before	compilation,	your	C/C++	build	executes	athird-	party	C	preprocessor	that	Fortify	Static	Code	
Analyzer	does	not	support,	you	must	invoke	the	Fortify	Static	Code	Analyzer	translation	on	the	
intermediate	file.	Fortify	Static	Code	Analyzer	touchless	build	integration	automatically	translates	the	
intermediate	file	provided	that	your	build	executes	the	unsupported	preprocessor	and	supported	
compiler	as	two	commands	connected	by	atemporary	file	rather	than	apipe	chain.	
C/C++	Precompiled	Header	Files	
Some	C/C++	compilers	support	Precompiled	Header	Files,	which	can	improve	compilation	performance.	
Some	compilers'	implementations	of	this	feature	have	subtle	side-	effects.	When	the	feature	is	enabled,	
the	compiler	might	accept	erroneous	source	code	without	warnings	or	errors.	This	can	result	in	a	
discrepancy	where	Fortify	Static	Code	Analyzer	reports	translation	errors	even	when	your	compiler	
does	not.	
If	you	use	your	compiler's	Precompiled	Header	feature,	disable	Precompiled	Headers,	and	then	perform	
afull	build	to	make	sure	that	your	source	code	compiles	cleanly.	
Troubleshooting	Translation	Failed	Message	
If	your	C	or	C++	application	builds	successfully	but	you	see	one	or	more	âtranslation	failedâ	messages	
during	the	Fortify	Static	Code	Analyzer	translation	phase,	edit	the	<sca_	install_	
dir>	/Core/config/fortify-	sca.properties	file	to	change	the	following	line:	
User	Guide	
Chapter	7:	Translating	C	and	C++	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	65	of	216 

com.fortify.sca.cpfe.options=	--remove_	unneeded_	entities	--suppress_	vtbl	
to:
com.fortify.sca.cpfe.options=	-w	--remove_	unneeded_	entities	--suppress_	
vtbl
Re-	run	the	translation	to	print	the	errors	that	the	translator	encountered.	If	the	output	indicates	an	
incompatibility	between	your	compiler	and	the	Fortify	Static	Code	Analyzer	translator,	send	your	
output	to	Micro	Focus	Fortify	Customer	Support	for	further	investigation.	
User	Guide	
Chapter	7:	Translating	C	and	C++	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	66	of	216 

Chapter	8:	Translating	JavaScript	and	
TypeScript	Code	
You	can	analyze	JavaScript	projects	that	contain	JavaScript,	TypeScript,	JSX,	and	TSX	Â source	files,	as	
well	as	JavaScript	embedded	in	HTML	files.	
Some	JavaScript	frameworks	are	transpiled	(source-	to-	source	compilation)	to	plain	JavaScript.	This	
generated	code	is	optimized.,	minimized,	or	both.	Therefore,	you	might	want	to	exclude	it	from	
translation	because	it	would	be	challenging	to	fix	any	vulnerabilities	Fortify	Static	Code	Analyzer	might	
report	in	this	code.	Use	the	-exclude	command-	line	option	to	manually	exclude	this	type	of	code.	
This	section	contains	the	following	topics:	
Translating	Pure	JavaScript	Projects	67	
Excluding	Dependencies	67	
Excluding	NPM	Â Dependencies	68	
Translating	JavaScript	Projects	with	HTML	Files	68	
Including	External	JavaScript	or	HTML	in	the	Translation	69	
Translating	Pure	JavaScript	Projects	
The	basic	command-	line	syntax	to	translate	JavaScript	is:	
sourceanalyzer	âb	<build_	id>	<js_	file_	or_	dir>	
where	<js_	file_	or_	dir>	is	the	either	the	name	of	the	JavaScript	file	to	be	translated	or	adirectory	
that	contains	multiple	JavaScript	files.	You	can	also	translate	multiple	files	by	specifying	*.js	for	the	
<js_	file_	or_	dir>	.	
Excluding	Dependencies	
You	can	avoid	translating	specific	dependencies	by	adding	them	to	the	appropriate	property	setting	in	
the	fortify-	sca.properties	file.	Files	specified	in	the	following	properties	are	not	translated:	
l	com.fortify.sca.skip.libraries.ES6	
l	com.fortify.sca.skip.libraries.jQuery	
l	com.fortify.sca.skip.libraries.javascript	
l	com.fortify.sca.skip.libraries.typescript	
Each	property	specifies	alist	of	comma-	or	colon-	separated	file	names	(without	path	information).	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	67	of	216 

The	files	specified	in	these	properties	apply	to	both	local	files	and	files	on	the	internet.	Suppose,	for	
example,	that	the	JavaScript	code	includes	the	following	file	reference:	
<script
src=""https://ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js"">
</script>
By	default,	the	com.fortify.sca.skip.libraries.jQuery	property	in	the	fortify-	
sca.properties	file	includes	jquery.min.js	,and	therefore	Fortify	Static	Code	Analyzer	does	not	
translate	the	file	shown	in	the	previous	example.	Also,	any	local	copy	of	the	jquery.min.js	file	is	not	
translated.
You	can	use	regular	expressions	for	the	file	names.	Note	that	Fortify	Static	Code	Analyzer	automatically	
inserts	the	regular	expression	'(-?\d+\.\d+\.\d+)?'	before	.min.js	or	.js	for	each	file	name	
included	in	the	com.fortify.sca.skip.libraries.jQuery	property	value.	
Note:	You	can	also	exclude	local	files	or	entire	directories	with	the	-exclude	command-	line	option.	
For	more	information	about	this	option,	see	""Translation	Options""	on	page	Â 112	.	
Excluding	NPM	Â Dependencies	
By	default,	Fortify	Static	Code	Analyzer	translates	only	the	NPM	Â dependences	that	are	imported	in	the	
code.	You	can	change	this	behavior	with	the	following	two	properties:	
l	The	com.fortify.sca.follow.imports	property	directs	Fortify	Static	Code	Analyzer	to	resolve	
all	imported	files	and	include	them	in	the	translation.	
This	property	is	enabled	by	default.	Setting	this	property	to	false	prevents	NPM	Â dependencies	that	
are	not	explicitly	included	on	the	command-	line	from	being	included	in	the	translation.	
l	The	com.fortify.sca.exclude.unimported.node.modules	property	directs	Fortify	Static	
Code	Analyzer	to	exclude	all	files	in	any	node_	modules	directory	from	the	translation	except	files	that	
are	specifically	imported	by	the	com.fortify.sca.follow.imports	property.	
This	property	is	enabled	by	default	to	avoid	translating	dependencies	that	are	not	needed	for	the	
final	project	such	as	those	only	required	for	the	build	system.	
Translating	JavaScript	Projects	with	HTML	Files	
If	the	project	contains	HTML	files	in	addition	to	JavaScript	files,	set	the	
com.fortify.sca.EnableDOMModeling	property	to	true	in	the	fortify-	sca.properties	file	or	
on	the	command	line	as	follows:	
sourceanalyzer	âb	<build_	id>	<js_	file_	or_	dir>	
-Dcom.fortify.sca.EnableDOMModeling=true	
When	you	set	the	com.fortify.sca.EnableDOMModeling	property	to	true,	this	can	decrease	false	
negative	reports	of	DOM-	related	attacks,	such	as	DOM-	related	cross-	site	scripting	issues.	
User	Guide	
Chapter	8:	Translating	JavaScript	and	TypeScript	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	68	of	216 

Note:	If	you	enable	this	option,	Fortify	Static	Code	Analyzer	generates	JavaScript	code	to	model	
the	DOM	tree	structure	in	the	HTML	files.	The	duration	of	the	analysis	phase	might	increase	
(because	there	is	more	translated	code	to	analyze).	
If	you	set	the	com.fortify.sca.EnableDOMModeling	property	to	true,	you	can	also	specify	
additional	HTML	Â tags	for	Fortify	Static	Code	Analyzer	to	include	in	the	DOM	Â modeling	with	the	
com.fortify.sca.DOMModeling.tags	property.	By	default,	Fortify	Static	Code	Analyzer	includes	
the	following	HTML	Â tags:	body	,button	,div	,form	,iframe	,input	,head	,html	,and	p.	
For	example,	to	include	the	HTML	tags	ul	and	li	in	the	DOM	Â model,	use	the	following	command:	
sourceanalyzer	âb	<build_	id>	<js_	file_	or_	dir>	
-Dcom.fortify.sca.DOMModeling.tags=ul,li	
Including	External	JavaScript	or	HTML	in	the	
Translation
To	include	external	JavaScript	or	HTML	Â files	that	are	specified	with	the	src	attribute,	you	can	specify	
which	domains	Fortify	Static	Code	Analyzer	can	download	and	include	in	the	translation	phase.	To	do	
this,	specify	one	or	more	domains	with	the	
com.fortify.sca.JavaScript.src.domain.whitelist	property.	
Note:	You	can	also	set	this	property	globally	in	the	fortify-	sca.properties	file.	
For	example,	you	might	have	the	following	statement	in	your	HTML	file:	
<script	src='http://xyzdomain.com/foo/bar.js'	language='text/javascript'/>	
</script>
If	you	are	confident	that	the	xyzdomain.com	domain	is	asafe	location	from	which	to	download	files,	
then	you	can	include	them	in	the	translation	phase	by	adding	the	following	property	specification	on	
the	command	line:	
-Dcom.fortify.sca.JavaScript.src.domain.whitelist=""xyzdomain.com/foo""
Note:	You	can	omit	the	www.	prefix	from	the	domain	in	the	property	value.	For	example,	if	the	src	
tag	in	the	original	HTML	file	specifies	to	download	files	from	www.google.com	,you	can	specify	
just	the	google.com	domain.	
To	trust	more	than	one	domain,	include	each	domain	separated	by	the	vertical	bar	character	(|)as	
shown	in	the	following	example:	
-Dcom.fortify.sca.JavaScript.src.domain.whitelist=
""xyzdomain.com/foo|abcdomain.com|123.456domain.comâ
User	Guide	
Chapter	8:	Translating	JavaScript	and	TypeScript	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	69	of	216 

If	you	are	using	aproxy	server,	then	you	need	to	include	the	proxy	server	information	on	the	command	
line	as	shown	in	the	following	example:	
-Dhttp.proxyHost=example.proxy.com	-Dhttp.proxyPort=8080	
For	acomplete	list	of	proxy	server	options,	see	the	Networking	Properties	Java	documentation.	
User	Guide	
Chapter	8:	Translating	JavaScript	and	TypeScript	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	70	of	216 

Chapter	9:	Translating	Python	Code	
Fortify	Static	Code	Analyzer	translates	Python	applications,	and	processes	files	with	the	.py	extension	
as	Python	source	code.	
This	section	contains	the	following	topics:	
Python	Translation	Command-	Line	Syntax	71	
Including	Import	Files	71	
Including	Namespace	Packages	72	
Using	the	Django	Framework	with	Python	72	
Python	Command-	Line	Options	72	
Python	Command-	Line	Examples	73	
Python	Translation	Command-	Line	Syntax	
The	basic	command-	line	syntax	to	translate	Python	code	is:	
sourceanalyzer	-b	<build_	id>	-python-	version	<python_	version>	
-python-	pathÂ 	<dirs>	<files>	
Including	Import	Files	
To	translate	Python	applications	and	prepare	for	ascan,	Fortify	Static	Code	Analyzer	searches	for	any	
import	files	used	by	the	application.	Fortify	Static	Code	Analyzer	does	not	respect	the	PYTHONPATH	
environment	variable,	which	the	Python	runtime	system	uses	to	find	imported	files.	Specify	all	the	paths	
to	search	for	import	files	with	the	-python-	path	option.	
Fortify	Static	Code	Analyzer	includes	asubset	of	modules	from	the	standard	Python	library	(module	
""builtins"",	all	modules	originally	written	in	C,	and	others)	in	the	translation.	Fortify	Static	Code	Analyzer	
first	searches	for	astandard	Python	library	module	in	the	set	included	with	Fortify	Static	Code	Analyzer	
and	then	in	the	paths	specified	with	the	-python-	path	option.	If	your	Python	code	imports	any	
module	that	Fortify	Static	Code	Analyzer	cannot	find,	it	produces	awarning.	To	make	sure	that	all	
modules	of	standard	Python	library	are	found,	add	the	path	to	your	standard	Python	library	to	the	-	
python-	path	list.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	71	of	216 

Including	Namespace	Packages	
To	translate	namespace	packages,	include	all	the	paths	to	the	namespace	package	directories	in	the	-	
python-	path	option.	For	example,	if	you	have	two	subpackages	for	anamespace	package	package_	
name	in	multiple	folders	as	in	this	example:	
/path_	1/package_	name/subpackageA	
/path_	2/package_	name/subpackageB	
Include	the	following	with	the	-python-	path	option:	/path_	1;/path_	2.	
Using	the	Django	Framework	with	Python	
Fortify	Static	Code	Analyzer	supports	the	Django	framework.	To	translate	code	created	using	the	
Django	framework,	add	the	following	properties	to	the	<sca_	install_	
dir>	/Core/config/fortify-	sca.properties	configuration	file:	
com.fortify.sca.limiters.MaxPassthroughChainDepth=8
com.fortify.sca.limiters.MaxChainDepth=8
By	default,	Fortify	Static	Code	Analyzer	attempts	to	discover	Django	templates	in	the	project	root	
folder.	Any	Django	templates	found	are	automatically	added	to	the	translation.	If	you	do	not	want	
Fortify	Static	Code	Analyzer	to	automatically	discover	Django	templates,	use	the	-django-	disable-	
autodiscover	option.	If	your	project	requires	Django	templates,	but	the	project	is	configured	such	
that	Django	templates	are	in	an	unexpected	location,	use	the	-django-	template-	dirs	option	to	
specify	the	directories	that	contain	the	templates	in	addition	to	the	-django-	disable-	
autodiscover	option.	
You	can	specify	additional	locations	of	Django	template	files	by	adding	the	-django-	template-	dirs	
option	to	the	sourceanalyzer	command:	
-django-	template-	dirs	<dirs>	
Python	Command-	Line	Options	
The	following	table	describes	the	Python	options.	
Python	Option	Description	
-python-	version	
<version>	
Specifies	the	Python	source	code	version	you	want	to	scan.	The	valid	
values	for	<version>	are	2	and	3.The	default	value	is	2.	
User	Guide	
Chapter	9:	Translating	Python	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	72	of	216 

Python	Option	Description
Equivalent	Property	Name:	
com.fortify.sca.PythonVersion	
-python-	path	
<dirs>	
Specifies	acolon-	separated	Â (non-	Windows)	or	semicolon-	separated	Â 	
(Windows)	list	of	additional	import	directories.	You	can	use	the	-python-	
path	option	to	specify	all	paths	used	to	import	packages	or	modules.	
Include	all	paths	to	namespace	package	directories	with	this	option.	
Fortify	Static	Code	Analyzer	sequentially	searches	the	specified	paths	for	
each	imported	file	and	uses	the	first	file	encountered.	
Equivalent	Property	Name:	
com.fortify.sca.PythonPath	
-django-	template-	
dirs	<dirs>	
Specifies	acolon-	separated	Â (non-	Windows)	or	semicolon-	separated	Â 	
(Windows)	list	of	directories	that	contain	Django	templates.	Fortify	Static	
Code	Analyzer	sequentially	searches	Â the	specified	paths	for	each	Django	
template	file	and	uses	the	first	template	file	encountered.	
Equivalent	Property	Name:	
com.fortify.sca.DjangoTemplateDirs	
-django-	disable-	
autodiscover	
Specifies	that	Fortify	Static	Code	Analyzer	does	not	automatically	
discover	Django	templates.	
Equivalent	Property	Name:	
com.fortify.sca.DjangoDisableAutodiscover	
Python	Command-	Line	Examples	
To	translate	Python	3	code,	type:	
sourceanalyzer	-b	Python3Proj	-python-	version	3	-python-	path	
/usr/lib/python3.4:/usr/local/lib/python3.4/site-	packages	src/*.py	
To	translate	Python	2	code,	type:	
sourceanalyzer	-b	MyPython2	-python-	path	
/usr/lib/python2.7:/usr/local/lib/python2.7/site-	packages	src/*.py	
User	Guide	
Chapter	9:	Translating	Python	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	73	of	216 

Chapter	10:	Translating	Code	for	Mobile	
Platforms
Fortify	Static	Code	Analyzer	supports	analysis	of	the	following	mobile	application	source	languages:	Â 	
l	Swift,	Objective-	C,	and	Objective-	C++	for	iOS	applications	developed	using	Xcode	
l	Java	for	Android	applications	
For	information	about	translating	Xamarin	applications,	see	""Translating	Visual	Studio	and	MSBuild	
Projects""	on	page	Â 58	.	
This	section	contains	the	following	topics:	
Translating	Apple	iOS	Projects	74	
Translating	Android	Projects	75	
Translating	Apple	iOS	Projects	
This	section	describes	how	to	translate	Swift,	Objective-	C,	and	Objective-	C++	source	code	for	iOS	
applications.	Fortify	Static	Code	Analyzer	automatically	integrates	with	the	Xcode	Command	Line	Tool,	
Xcodebuild,	to	identify	the	project	source	files.	
iOS	Â Project	Translation	Prerequisites	
The	following	are	the	prerequisites	for	translating	iOS	projects:	
l	Objective-	C++	projects	must	use	the	non-	fragile	Objective-	C	runtime	(ABI	version	2	or	3).	
l	Use	Appleâs	xcode-	select	command-	line	tool	to	set	your	Xcode	path.	Fortify	Static	Code	Analyzer	
uses	the	system	global	Xcode	configuration	to	find	the	Xcode	toolchain	and	headers.	
l	Make	sure	that	you	have	any	dependencies	required	to	build	the	project	available.	
l	To	translate	Swift	code,	make	sure	that	you	have	available	all	third-	party	modules,	including	
CocoaPods.	Bridging	headers	must	also	be	available.	However,	Xcode	usually	generates	them	
automatically	during	the	build.	
l	If	your	project	includes	property	list	files	in	binary	format,	you	must	first	convert	them	to	XML	format.	
You	can	do	this	with	the	Xcode	putil	command.	
l	To	translate	Objective-	C	projects,	ensure	that	the	headers	for	third-	party	libraries	are	available.	
l	To	translate	WatchKit	applications,	make	sure	that	you	translate	both	the	iPhone	application	target	
and	the	WatchKit	extension	target.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	74	of	216 

iOS	Â Code	Analysis	Command-	Line	Syntax	
The	command-	line	syntax	to	translate	iOS	code	using	Xcodebuild	is:	
sourceanalyzer	-b	<build_	id>	xcodebuild	[<compiler_	options>	]	
where	<compiler_	options>	are	the	supported	options	that	are	passed	to	the	Xcode	compiler.	
Note:	Xcodebuild	compiles	the	source	code	when	you	run	this	command.	
If	your	application	uses	any	property	list	files	(for	example,	<file>	.plist	),	translate	these	files	with	a	
separate	sourceanalyzer	command.	Use	the	same	build	ID	that	you	used	to	translate	the	project	files.	
The	following	is	an	example:	
sourceanalyzer	-b	<build_	id>	<path_	to_	plist_	files>	
If	your	project	uses	CocoaPods,	include	-workspace	to	build	the	project.	For	example:	
sourceanalyzer	-b	DemoAppSwift	xcodebuild	clean	build	-workspace	
DemoAppSwift.xcworkspace	-scheme	DemoAppSwift	-sdk	iphonesimulator	
You	can	then	perform	the	analysis	phase,	as	shown	in	the	following	example:	
sourceanalyzer	-b	DemoAppSwift	-scan	-f	myresults.fpr	
Translating	Android	Projects	
This	section	describes	how	to	translate	Java	source	code	for	Android	applications.	You	can	use	Fortify	
Static	Code	Analyzer	to	scan	the	code	with	Gradle	from	either:	
l	Your	operating	system's	command	line	
l	A	terminal	window	running	in	Android	Studio	
The	way	you	use	Gradle	is	the	same	for	either	method.	
Note:	You	can	also	scan	Android	code	directly	from	Android	Studio	with	the	Micro	Focus	Fortify	
Analysis	Plugin	for	IntelliJ	and	Android	Studio	.For	more	information,	see	the	Micro	Focus	Fortify	
Plugins	for	JetBrains	IDEs	and	Android	Studio	User	Guide	.	
Android	Project	Translation	Prerequisites	
The	following	are	the	prerequisites	for	translating	Android	projects:	
l	Android	Studio	and	the	relevant	Android	SDKs	are	installed	on	the	system	where	you	will	run	the	
scans	
User	Guide	
Chapter	10:	Translating	Code	for	Mobile	Platforms	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	75	of	216 

l	Your	Android	project	uses	Gradle	for	builds.	
If	you	have	an	older	project	that	does	not	use	Gradle,	you	must	add	Gradle	support	to	the	associated	
Android	Studio	project	
Use	the	same	version	of	Gradle	that	is	provided	with	the	version	of	Android	Studio	that	you	use	to	
create	your	Android	project	
l	Make	sure	you	have	available	all	dependencies	that	are	required	to	build	the	Android	code	in	the	
application's	project	
l	To	translate	your	Android	code	from	acommand	window	that	is	not	displayed	within	Android	
Studio,	make	sure	that	Gradle	Wrapper	(gradlew	)Â is	defined	on	the	system	path	
Android	Code	Analysis	Command-	Line	Syntax	
Use	gradlew	to	scan	Android	projects,	which	is	similar	to	using	Gradle	except	that	you	use	the	Gradle	
Wrapper.	For	information	about	how	to	translate	your	Android	project	using	the	Gradle	Wrapper,	see	
""Gradle	Integration""	on	page	Â 107	.	
Filtering	Issues	Detected	in	Android	Layout	Files	
If	your	Android	project	contains	layout	files	(used	to	design	the	user	interface),	your	project	files	might	
include	R.java	source	files	that	are	automatically	generated	by	Android	Studio.	When	you	scan	the	
project,	Fortify	Static	Code	Analyzer	can	detect	issues	associated	with	these	layout	files.	
Fortify	recommends	that	Issues	reported	in	any	layout	file	be	included	in	your	standard	audit	so	you	can	
carefully	determine	if	any	of	them	are	false	positives.	After	you	identify	issues	in	layout	files	that	you	are	
not	interested	in,	you	can	filter	them	out	as	described	in	""Filtering	the	Analysis""	on	page	Â 165	.You	can	
filter	out	the	issues	based	on	the	Instance	ID.	
User	Guide	
Chapter	10:	Translating	Code	for	Mobile	Platforms	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	76	of	216 

Chapter	11:	Translating	Go	Code	
This	section	describes	how	to	translate	Go	code.	
This	section	contains	the	following	topics:	
Go	Command-	Line	Syntax	77	
Go	Command-	Line	Options	77	
Resolving	Dependencies	78	
Go	Command-	Line	Syntax	
For	best	results,	your	project	must	be	compilable	and	you	must	have	all	required	dependencies	available.	
The	following	entities	are	excluded	from	the	translation	(and	the	scan):	
l	Vendor	folder	
l	All	projects	defined	by	any	go.mod	files	in	subfolders,	except	the	project	defined	by	the	go.mod	file	
under	the	%PROJECT_	ROOT%	
l	All	files	with	the	_test.go	suffix	(unit	tests)	
The	basic	command-	line	syntax	to	translate	Go	code	is:	
sourceanalyzer	-b	<build_	id>	[-	gopath	<path>	]	[-	goroot	<path>	]	<files>	
Go	Command-	Line	Options	
The	following	table	describes	the	command-	line	options	that	are	specific	for	translating	Go	code.	
Go	Option	Description	
-gopathÂ 	<path>	Specifies	the	root	directory	of	your	project.	Make	sure	that	the	directory	
structure	adheres	to	the	Go	workspace	hierarchy	
(https://golang.org/doc/gopath_	code.html)	.	
If	this	option	is	not	specified,	then	the	GOPATH	system	environment	
variable	is	used.	
You	must	specify	the	root	directory	as	an	absolute	path.	The	following	
examples	are	valid	values	for	<path>	:	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	77	of	216 

Go	Option	Description
/home/projects/go_	workspace/my_	proj	
C:\projects\go_	workspace\my_	proj	
The	following	example	is	an	invalid	value	for	<path>	:	
go_	workspace/my_	proj	
If	the	environment	variable	is	not	set,	then	the	gopath	defaults	to	a	
subdirectory	named	go	in	the	user's	home	directory	($HOME/go	on	Linux	
and	%USERPROFILE%\go	on	Windows),	unless	that	directory	contains	a	
Go	distribution.	
-gorootÂ 	<path>	Specifies	the	location	of	the	Go	installation.	If	this	option	is	not	specified,	
the	GOROOT	system	environment	variable	is	used.	
If	this	option	is	not	specified	and	the	GOROOT	system	environment	
variable	is	not	set,	then	Fortify	Static	Code	Analyzer	uses	the	Go	compiler	
included	in	the	Fortify	Static	Code	Analyzer	installation.	
Resolving	Dependencies	
Fortify	Static	Code	Analyzer	supports	two	dependency	management	systems	built-	in	into	Go:	
l	GOPATH	dependency	resolution	
If	you	are	using	athird-	party	dependency	management	system	such	as	GoDeps	or	dep),	you	must	
download	all	dependencies	before	you	start	the	translation.	
l	Modules
Fortify	Static	Code	Analyzer	downloads	all	required	dependencies	using	the	native	Go	toolchain.	If	
access	to	the	internet	is	restricted	on	the	machine	where	you	run	Fortify	Static	Code	Analyzer	,then	
do	one	of	the	following:	
l	If	you	are	using	an	artifact	management	system	such	as	Artifactory,	set	the	GOPROXY	
environment	variable.	
l	Download	all	required	dependencies	using	modules	and	vendoring.	
User	Guide	
Chapter	11:	Translating	Go	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	78	of	216 

Chapter	12:	Translating	Ruby	Code	
This	section	contains	the	following	topics:	
Ruby	Command-	Line	Syntax	79	
Adding	Libraries	80	
Adding	Gem	Paths	80	
Ruby	Command-	Line	Syntax	
The	basic	command-	line	syntax	to	translate	Ruby	code	is:	
sourceanalyzer	âb	<build_	id>	<file>	
where	<file>	is	the	name	of	the	Ruby	file	you	want	to	scan.	To	include	multiple	Ruby	files,	separate	
them	with	aspace,	as	shown	in	the	following	example:	
sourceanalyzer	âb	<build_	id>	file1.rb	file2.rb	file3.rb	
In	addition	to	listing	individual	Ruby	files,	you	can	use	the	asterisk	(*)	wildcard	to	select	all	Ruby	files	in	a	
specified	directory.	For	example,	to	find	all	of	the	Ruby	files	in	adirectory	called	src	,use	the	following	
sourceanalyzer	command:	
sourceanalyzer	âb	<build_	id>	src/*.rb	
Ruby	Command-	Line	Options	
The	following	table	describes	the	Ruby	translation	options.	
Ruby	Option	Description	
-ruby-	path	<dirs>	Specifies	one	or	more	paths	to	directories	that	contain	Ruby	libraries	(see	
""Adding	Libraries	""on	the	next	page	)	
Equivalent	Property	Name:	
com.fortify.sca.RubyLibraryPaths	
-rubygem-	path	
<dirs>	
Specifies	the	path	(s)	to	aRubyGems	location	(see	""Adding	Gem	Paths""	on	
the	next	page	)	
Equivalent	Property	Name:	
com.fortify.sca.RubyGemPaths	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	79	of	216 

Adding	Libraries	
If	your	Ruby	source	code	requires	aspecific	library,	add	the	Ruby	library	to	the	sourceanalyzer	
command.	Include	all	ruby	libraries	that	are	installed	with	ruby	gems.	For	example,	if	you	have	a	
utils.rb	file	that	resides	in	the	/usr/share/ruby/myPersonalLibrary	directory,	then	add	the	
following	to	the	sourceanalyzer	command:	
-ruby-	path	/usr/share/ruby/myPersonalLibrary	
To	use	multiple	libraries,	use	adelimited	list.	On	Windows,	separate	the	paths	with	asemicolon;	and	on	
all	other	platforms	use	acolon,	as	in	the	following	non-	Windows	example:	
-ruby-	path	/path/one:/path/two:/path/three	
Adding	Gem	Paths	
To	add	all	RubyGems	and	their	dependency	paths,	import	all	RubyGems.	To	obtain	the	Ruby	gem	paths,	
run	the	gem	env	command.	Under	GEM	PATHS	,look	for	adirectory	similar	to:	
/home/myUser/gems/ruby-	version	
This	directory	contains	another	directory	called	gems	,which	contains	directories	for	all	the	gem	files	
installed	on	the	system.	For	this	example,	use	the	following	in	your	command	line:	
-rubygem-	path	/home/myUser/gems/ruby-	version/gems	
If	you	have	multiple	gems	directories,	add	them	by	specifying	adelimited	list	of	directories	such	as:	
-rubygem-	path	/path/to/gems:/another/path/to/more/gems	
Note:	On	Windows	systems,	separate	the	gems	directories	with	asemicolon.	
User	Guide	
Chapter	12:	Translating	Ruby	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	80	of	216 

Chapter	13:	Translating	Apex	and	
Visualforce	Code	
This	section	contains	the	following	topics:	
Apex	Translation	Prerequisites	81	
Apex	and	Visualforce	Command-	Line	Syntax	81	
Apex	and	Visualforce	Command-	Line	Options	82	
Downloading	Customized	Salesforce	Database	Structure	Information	82	
Apex	Translation	Prerequisites	
l	All	the	source	code	to	scan	is	available	on	the	same	machine	where	you	have	installed	Fortify	Static	
Code	Analyzer	
To	scan	your	custom	Salesforce	app,	download	it	to	your	local	computer	from	your	Salesforce	
organization	(org)	Â where	you	develop	and	deploy	it.	The	downloaded	version	of	your	app	consists	
of:
l	Apex	classes	in	files	with	the	.cls	extension	
l	Visualforce	web	pages	in	files	with	the	.page	extension	
l	Apex	code	files	called	database	âtriggerâ	functions	are	in	files	with	the	.trigger	extension	
Use	the	Force.com	Migration	Tool	available	on	the	Salesforce	website	to	download	your	app	from	
your	org	in	the	Salesforce	cloud	to	your	local	computer.	
l	If	you	customized	the	standard	Salesforce	database	structures	to	support	your	app,	then	you	must	
also	download	adescription	of	the	changes	so	that	Fortify	Static	Code	Analyzer	knows	how	your	
modified	version	of	Salesforce	interacts	with	your	app.	See	""Downloading	Customized	Salesforce	
Database	Structure	Information""	on	the	next	page	.	
Apex	and	Visualforce	Command-	Line	Syntax	
The	basic	command-	line	syntax	to	translate	Apex	and	Visualforce	code	is:	
sourceanalyzer	-b	<build_	id>	-apex	<files>	
where	<files>	is	an	Apex	or	Visualforce	file	or	apath	to	the	source	files.	
Important!	Supported	file	extensions	for	the	source	code	files	are:	Â .cls	,.trigger	,.page	,and	
.component	.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	81	of	216 

For	descriptions	of	all	the	Apex-	and	Visualforce-	specific	command-	line	options,	see	""Apex	and	
Visualforce	Command-	Line	Options""	below	.	
Apex	and	Visualforce	Command-	Line	Options	
The	following	table	describes	the	Apex	and	Visualforce	translation	command-	line	options.	
Apex	or	Visualforce	Option	Description	
-apex	Directs	Fortify	Static	Code	Analyzer	to	use	the	Apex	and	
Visualforce	translation	for	files	with	the	.cls	extension.	
Without	this	option,	Fortify	Static	Code	Analyzer	translates	
*.cls	files	as	Visual	Basic	code.	
Note:	Alternatively,	you	can	set	the	
com.fortify.sca.fileextension.cls	property	to	
APEX	either	on	the	command	line	(include	-	
Dcom.fortify.sca.fileextensions.cls=APEX	)or	
in	the	<sca_	install_	dir>	/Core/config/fortify-	
sca.properties	file.	
Equivalent	Property	Name:	
com.fortify.sca.Apex	
-apex-	sobject-	path	<path>	Specifies	the	location	of	the	custom	sObject	JSON	file	
sobjects.json	.	
For	instructions	on	how	to	use	the	sf_	extractor	tool,	see	
""Downloading	Customized	Salesforce	Database	Structure	
Information""	below	.	
Equivalent	Property	Name:	
com.fortify.sca.ApexObjectPath	
Downloading	Customized	Salesforce	Database	
Structure	Information	
Use	the	sf_	extractor	tool	to	download	adescription	of	any	customized	Salesforce	database	structures.	
Fortify	Static	Code	Analyzer	requires	this	information	to	perform	amore	complete	analysis.	The	sf_	
extractor	creates	acustom	sObject	JSON	file	that	you	include	with	the	sourceanalyzer	translation	phase.	
(For	information	about	how	to	provide	this	information	to	Fortify	Static	Code	Analyzer	,see	""Apex	and	
Visualforce	Command-	Line	Options""	above	.)	
User	Guide	
Chapter	13:	Translating	Apex	and	Visualforce	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	82	of	216 

The	following	table	describes	the	contents	of	the	sf_	extractor.zip	file,	which	is	located	in	<sca_	
install_	dir>	/Tools	.	
Folder	or	File	Name	Description	
lib	Folder	containing	JAR	Â dependencies	
src	Source	code	
partner.wsdl	Partner	WSDL	file	version	37.0	
sf_	extractor.jar	Compiled	JAR	file	(dependencies	included)	
The	command-	line	syntax	to	run	sf_	extractor	is:	
java	-jar	sf_	extractor.jar	<username>	<password>	<security_	token>	<org>	
where:
l	<username>	is	your	Salesforce	cloud	user	name.	For	example,	test@test.test	.	
l	<password>	is	your	Salesforce	cloud	password.	
l	<security_	token>	is	the	25	alphanumeric	character	security	token	
l	<org>	is	y	if	you	are	using	asandbox	org	or	n	if	you	are	using	aproduction	org	
The	sf_	extractor	tool	uses	the	credentials	to	access	the	Salesforce	SOAP	API.	It	downloads	all	the	
sObjects	with	additional	information	from	the	current	org,	and	then	it	downloads	information	about	
fields	in	the	sObjects.	This	is	required	to	properly	resolve	types	represented	in	current	org.	
This	tool	produces	an	sobjects.json	file	that	you	provide	to	Fortify	Static	Code	Analyzer	in	the	
translation	command	using	the	-apex-	sobject-	path	option.	
User	Guide	
Chapter	13:	Translating	Apex	and	Visualforce	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	83	of	216 

Chapter	14:	Translating	COBOL	Code	
The	20.2.0	release	of	Fortify	Static	Code	Analyzer	introduces	updated	COBOL	code	translation,	which	is	
now	the	default	translation	method.	The	previous	translation	method,	referred	to	now	as	legacy	
COBOL	Â translation,	is	still	available	for	use	with	acommand-	line	option.	Use	the	legacy	
COBOL	Â translation	method	if	either	of	the	following	is	true:	
l	You	run	Fortify	Static	Code	Analyzer	on	anon-	Windows	operating	system	
l	Your	COBOL	Â dialect	is	unsupported.	For	example,	your	COBOL	Â source	code	is	free	format.	
The	following	sections	describe	the	default	COBOL	Â code	translation.	Information	that	pertains	only	to	
the	legacy	COBOL	translation	is	indicated	as	such.	
For	alist	of	supported	technologies	for	translating	COBOL	Â code,	see	the	Micro	Focus	Fortify	Software	
System	Requirements	document.	Fortify	Static	Code	Analyzer	does	not	currently	support	custom	rules	
for	COBOL	applications.	
Note:	To	scan	COBOL	with	Fortify	Static	Code	Analyzer	,you	must	have	aspecialized	Fortify	license	
specifically	for	COBOL	scanning	capabilities.	Contact	Micro	Focus	Fortify	Customer	Support	for	
more	information	about	scanning	COBOL	and	the	required	license.	
This	section	contains	the	following	topics:	
Preparing	COBOL	Source	and	Copybook	Files	for	Translation	84	
COBOL	Command-	Line	Syntax	85	
COBOL	Â Command-	Line	Options	86	
Preparing	COBOL	Source	and	Copybook	Files	for	
Translation
Fortify	Static	Code	Analyzer	supports	translation	of	COBOL	source	files	on	Windows	systems	only.	
Legacy	COBOL	Â Translation:	Fortify	Static	Code	Analyzer	Â supports	translation	of	COBOL	Â source	
files	on	the	supported	platforms	and	architectures	listed	in	the	Micro	Focus	Fortify	Software	
System	Requirements	document.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	84	of	216 

Before	you	can	analyze	aCOBOL	program,	you	must	copy	the	following	program	components	to	the	
Windows	system	where	you	run	Fortify	Static	Code	Analyzer	:	
l	COBOL	source	code	
Note:	Fortify	Static	Code	Analyzer	translates	COBOL	Â source	code	files	with	or	without	file	
extensions.	
l	All	copybook	files	that	the	COBOL	source	code	uses	
l	All	SQL	INCLUDE	files	that	the	COBOL	source	code	references	(a	SQL	Â INCLUDE	file	is	technically	a	
copybook	file)	
Important!	The	copybook	files	must	have	the	file	extension	.CPY	or	.cpy	.	
Legacy	COBOL	Â Translation:	Fortify	Static	Code	Analyzer	Â translates	copybook	files	with	or	
without	file	extensions.	
If	your	COBOL	source	code	contains:	
COPY	FOO	
or
EXEC	SQL	INCLUDE	FOO	END-	EXEC	
then	FOO	is	the	name	of	aCOBOL	Â copybook	and	the	corresponding	copybook	file	has	the	name	
FOO.CPY	or	FOO.cpy	.	
Legacy	COBOL	Â Translation:	
l	The	corresponding	copybook	file	has	the	name	FOO	with	or	without	afile	extension.	If	the	
copybook	files	have	file	extensions,	use	the	-copy-	extensions	command-	line	option.	For	
more	information,	see	""Legacy	COBOL	Â Translation	Command-	Line	Options""	on	page	Â 87	.	
l	The	COPY	command	can	also	accept	adirectory-	file-	path	structure	instead	of	afile	name.	
Fortify	Static	Code	Analyzer	processes	only	top-	level	COBOL	sources.	Do	not	include	copybook	files	in	
the	directory	or	the	subdirectory	where	the	COBOL	sources	reside.	Fortify	recommends	that	you	place	
your	COBOL	source	code	files	in	adirectory	called	sources	and	your	copybook	files	in	adirectory	called	
copybooks	.Create	these	directories	at	the	same	level.	
COBOL	Command-	Line	Syntax	
The	basic	syntax	used	to	translate	asingle	COBOL	source	code	file	is:	
sourceanalyzer	-b	<build_	id>	<path>	
The	basic	syntax	used	to	scan	atranslated	COBOL	program	is:	
sourceanalyzer	-b	<build_	id>	-scan	-f	<results>	.fpr	
User	Guide	
Chapter	14:	Translating	COBOL	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	85	of	216 

Legacy	COBOL	Â Translation:	Free-	format	COBOL	is	the	default	translation	mode.	Fortify	Static	
Code	Analyzer	supports	the	translation	of	fixed-	format	COBOL.	To	translate	fixed-	format	COBOL,	
you	must	specify	the	-fixed-	format	command-	line	option.	For	more	information,	see	""Legacy	
COBOL	Â Translation	Command-	Line	Options""	on	the	next	page	.	
Translating	COBOL	Â Source	Files	Without	File	Extensions	
If	you	have	COBOL	source	files	retrieved	from	amainframe	without	.COB	or	.CBL	file	extensions	(which	
is	typical	for	COBOL	file	names),	then	you	must	include	the	following	in	the	translation	command	line:	
-noextension-	type	COBOL	
If	you	have	COBOL	Â source	files	with	an	arbitrary	extension	.xyz	,then	you	must	include	the	following	in	
the	translation	command	line:	
-Dcom.fortify.sca.fileextension.xyz=COBOL	
The	following	example	command	translates	COBOL	Â source	code	without	file	extensions:	
sourceanalyzer	-b	MyProject	-noextension-	type	COBOL	-copydirs	copybooks	
sources
COBOL	Â Command-	Line	Options	
The	following	table	describes	the	COBOL	Â command-	line	options.	
COBOL	Â Option	Description	
-copydirs	<dirs>	Specifies	one	or	more	semicolon-	separated	directory	paths	in	which	
Fortify	Static	Code	Analyzer	looks	for	copybook	files.	
Equivalent	Property	Name:	
com.fortify.sca.CobolCopyDirs	
-checker-
directives
<directives>	
Specifies	one	or	more	semicolon-	separated	COBOL	checker	directives.	
Note:	This	option	is	intended	for	advanced	users	of	Micro	Focus	
Server	Express.	
Equivalent	property	name:	
com.fortify.sca.CobolCheckerDirectives	
User	Guide	
Chapter	14:	Translating	COBOL	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	86	of	216 

Legacy	COBOL	Â Translation	Command-	Line	Options	
The	following	table	describes	the	command-	line	options	for	the	legacy	COBOL	Â translation.	
Legacy	Â COBOL	Â Option	Description	
-cobol-	legacy	Specifies	translation	of	COBOL	Â code	using	legacy	COBOL	Â translation.	This	
option	is	required	to	enable	legacy	COBOL	translation.	
Equivalent	Property	Name:	
com.fortify.sca.CobolLegacy	
-copydirs	<dirs>	Specifies	one	or	more	colon-	or	semicolon-	separated	directory	paths	in	
which	Fortify	Static	Code	Analyzer	looks	for	copybook	files.	
Equivalent	Property	Name:	
com.fortify.sca.CobolCopyDirs	
-copy-	extensions	
<ext>	
Specifies	one	or	more	colon-	or	semicolon-	separated	copybook	file	
extensions.
Equivalent	Property	Name:	
com.fortify.sca.CobolCopyExtensions	
-fixed-	format	Specifies	fixed-	format	COBOL	to	direct	Fortify	Static	Code	Analyzer	to	
only	look	for	source	code	between	columns	8â	72	in	all	lines	of	code.	
IBM	Enterprise	COBOL	code	is	typically	fixed-	format.	The	following	are	
indications	that	you	might	need	the	-fixed-	format	option:	
l	The	COBOL	translation	appears	to	hang	indefinitely	
l	Fortify	Static	Code	Analyzer	reports	numerous	parsing	errors	in	the	
COBOL	Â translation	
Equivalent	Property	Name:	
com.fortify.sca.CobolFixedFormat	
User	Guide	
Chapter	14:	Translating	COBOL	Code	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	87	of	216 

Chapter	15:	Translating	Other	Languages	
and	Configurations	
This	section	contains	the	following	topics:	
Translating	PHP	Code	88	
Translating	ABAP	Code	89	
Translating	Flex	and	ActionScript	97	
Translating	ColdFusion	Code	99	
Translating	SQL	100	
Translating	Scala	Code	101	
Translating	ASP/VBScript	Virtual	Roots	101	
Translating	Dockerfiles	103	
Classic	ASP	Command-	Line	Example	104	
VBScript	Command-	Line	Example	104	
Translating	PHP	Code	
The	syntax	to	translate	asingle	PHP	Â file	named	MyPHP.php	is	shown	in	the	following	example:	
sourceanalyzer	-b	<build_	id>	MyPHP.php	
To	translate	afile	where	the	source	or	the	php.ini	file	entry	includes	arelative	path	name	(starts	with	
./	or	../	),	consider	setting	the	PHP	source	root	as	shown	in	the	following	example:	
sourceanalyzer	-php-	source-	root	<path>	-b	<build_	id>	MyPHP.php	
For	more	information	about	the	-php-	source-	root	option,	see	the	description	in	""PHP	Â Command-	
Line	Options""	below	.	
PHP	Â Command-	Line	Options	
The	following	table	describes	the	PHP-	specific	command-	line	options.	
PHP	Â Option	Description	
-php-	source-	root	Specifies	an	absolute	path	to	the	project	root	directory.	The	relative	path	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	88	of	216 

PHP	Â Option	Description	
<path>	name	first	expands	from	the	current	directory.	If	the	file	is	not	found,	then	
the	path	expands	from	the	specified	PHP	Â source	root	directory.	
Equivalent	Property	Name:	
com.fortify.sca.PHPSourceRoot	
-php-	version	
<version>	
Specifies	the	PHP	Â version.	The	default	version	is	7.0.	For	alist	of	valid	
versions,	see	the	Micro	Focus	Fortify	Software	System	Requirements	.	
Equivalent	Property	Name:	
com.fortify.sca.PHPVersion	
Translating	ABAP	Code	
Translating	ABAP	code	is	similar	to	translating	other	operating	language	code.	However,	it	requires	
additional	steps	to	extract	the	code	from	the	SAP	database	and	prepare	it	for	scanning.	See	""Importing	
the	Transport	Request""	on	the	next	page	for	more	information.	This	section	assumes	you	have	abasic	
understanding	of	SAP	and	ABAP.	
To	translate	ABAP	code,	the	Fortify	ABAP	Extractor	program	downloads	source	files	to	the	
presentation	server,	and	optionally,	invokes	Fortify	Static	Code	Analyzer	.You	need	to	use	an	account	
with	permission	to	download	files	to	the	local	system	and	execute	operating	system	commands.	
Because	the	extractor	program	is	executed	online,	you	might	receive	amax	dialog	work	process	
time	reached	exception	message	if	the	volume	of	source	files	selected	for	extraction	exceeds	the	
allowable	process	run	time.	To	work	around	this,	download	large	projects	as	aseries	of	smaller	Extractor	
tasks.	Â For	example,	if	your	project	consists	of	four	different	packages,	download	each	package	
separately	into	the	same	project	directory.	Â If	the	exception	occurs	frequently,	work	with	your	SAP	Basis	
administrator	to	increase	the	maximum	time	limit	(rdisp/max_	wprun_	time	).	
When	aPACKAGE	is	extracted	from	ABAP,	the	Fortify	ABAP	Extractor	extracts	everything	from	TDEVC	
with	aparentcl	field	that	matches	the	package	name.	It	then	recursively	extracts	everything	else	from	
TDEVC	with	aparentcl	field	equal	to	those	already	extracted	from	TDEVC	.The	field	extracted	from	
TDEVC	is	devclass	.	
The	devclass	values	are	treated	as	aset	of	program	names	and	handled	the	same	way	as	aprogram	
name,	which	you	can	provide.	
Programs	are	extracted	from	TRDIR	by	comparing	the	name	field	with	either:	
l	The	program	name	specified	in	the	selection	screen	
l	The	list	of	values	extracted	from	TDEVC	if	apackage	was	provided	
The	rows	from	TRDIR	are	those	for	which	the	name	field	has	the	given	program	name	and	the	
expression	LIKE	programname	is	used	to	extract	rows.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	89	of	216 

This	final	list	of	names	is	used	with	READ	REPORT	to	get	code	out	of	the	SAP	system.	This	method	does	
read	classes	and	methods	out	as	well	as	merely	REPORTS	,for	the	record.	
Each	READ	REPORT	call	produces	afile	in	the	temporary	folder	on	the	local	system.	This	set	of	files	is	
what	Fortify	Static	Code	Analyzer	translates	and	scans,	producing	an	FPR	file	that	you	can	open	with	
Micro	Focus	Fortify	Audit	Workbench	.	
INCLUDE	Processing	
As	source	code	is	downloaded,	the	Fortify	ABAP	Extractor	detects	INCLUDE	statements	in	the	source.	
When	found,	it	downloads	the	include	targets	to	the	local	machine	for	analysis.	
Importing	the	Transport	Request	
To	scan	ABAP	Â code,	you	need	to	import	the	Fortify	ABAP	Extractor	transport	request	on	your	SAP	
Server.
The	Fortify	transport	request	is	located	in	the	SAP_	Extractor.zip	package.	The	package	is	located	in	
the	Tools	directory:	
<sca_	install_	dir>	/Tools/SAP_	Extractor.zip	
The	Fortify	ABAP	Extractor	package,	SAP_	Extractor.zip	,contains	the	following	files:	
l	K900XXX.S9S	(where	the	âXXXâ	is	the	release	number)	
l	R900XXX.S9S	(where	the	âXXXâ	is	the	release	number)	
These	files	make	up	the	SAP	transport	request	that	you	must	import	into	your	SAP	system	from	
outside	your	local	Transport	Domain.	Have	your	SAP	administrator	or	an	individual	authorized	to	install	
transport	requests	on	the	system	import	the	transport	request.	
The	NSP	files	contain	aprogram,	atransaction	(YSCA),	and	the	program	user	interface.	After	you	
import	them	into	your	system,	you	can	extract	your	code	from	the	SAP	database	and	prepare	it	for	
Fortify	Static	Code	Analyzer	scanning.	
Installation	Note	
The	Fortify	ABAP	Extractor	transport	request	is	supported	on	asystem	running	SAP	release	7.02,	SP	
level	0006.	If	you	are	running	adifferent	SAP	version	and	you	get	the	transport	request	import	error:	
Install	release	does	not	match	the	current	version	,then	the	transport	request	
installation	has	failed.	
To	try	to	resolve	this	issue,	perform	the	following	steps:	
1.	Re-	run	the	transport	request	import.	
The	Import	Transport	Request	dialog	box	opens.	
2.	Click	the	Options	tab.	
3.	Select	the	Ignore	Invalid	Component	Version	check	box.	
4.	Complete	the	import	procedure.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	90	of	216 

If	this	does	not	resolve	the	issue	or	if	your	system	is	running	on	an	SAP	version	with	adifferent	table	
structure,	Fortify	recommends	that	you	export	your	ABAP	Â file	structure	using	your	own	technology	so	
that	Fortify	Static	Code	Analyzer	can	scan	the	ABAP	Â code.	
Adding	Fortify	Static	Code	Analyzer	to	Your	Favorites	List	
Adding	Fortify	Static	Code	Analyzer	to	your	Favorites	list	is	optional,	but	doing	so	can	make	it	quicker	
to	access	and	launch	Fortify	Static	Code	Analyzer	scans.	The	following	steps	assume	that	you	use	the	
user	menu	in	your	day-	to-	day	work.	If	your	work	is	done	from	adifferent	menu,	add	the	Favorites	link	
to	the	menu	that	you	use.	Before	you	create	the	Fortify	Static	Code	Analyzer	entry,	make	sure	that	the	
SAP	server	is	running	and	you	are	in	the	SAP	Easy	Access	area	of	your	web-	based	client.	
To	add	Fortify	Static	Code	Analyzer	to	your	Favorites	list:	
1.	From	the	SAP	Easy	Access	menu,	type	S000	in	the	transaction	box.	
The	SAP	Menu	opens.	
2.	Right-	click	the	Favorites	folder	and	select	Insert	transaction	.	
The	Manual	entry	of	a	transaction	dialog	box	opens.	
3.	Type	YSCA	in	the	Transaction	Code	box.	
4.	Click	the	green	check	mark	button	.	
The	Extract	ABAP	code	and	launch	SCA	item	appears	in	the	Favorites	list.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	91	of	216 

5.	Click	the	Extract	ABAP	code	and	launch	SCA	link	to	launch	the	Fortify	ABAP	Extractor	.	
Running	the	Fortify	ABAP	Extractor	
To	run	the	Fortify	ABAP	Extractor	:	
1.	Start	the	program	from	the	Favorites	link,	the	transaction	code,	or	manually	start	the	Extractor	
object.
This	opens	the	Fortify	ABAP	Extractor	.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	92	of	216 

2.	Select	the	code	to	download.	
Provide	the	start	and	end	name	for	the	range	of	software	components,	packages,	programs,	or	
BSP	Â applications	that	you	want	to	scan.	
Note:	You	can	specify	multiple	objects	or	ranges.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	93	of	216 

3.	Provide	the	Fortify	Static	Code	Analyzer	specific	information	described	in	the	following	table.	
Field	Description	
FPR	File	
Path	
(Optional)	Type	or	select	the	directory	where	you	want	to	store	the	scan	results	
file	(FPR).	Include	the	name	for	the	FPR	file	in	the	path	name.	You	must	provide	
the	FPR	Â file	path	if	you	want	to	automatically	scan	the	downloaded	code	on	the	
same	machine	where	you	are	running	the	extraction	process.	
Working
Directory	
Type	or	select	the	directory	where	you	want	to	store	the	extracted	source	code.	
Build-	ID	(Optional)	Â Type	the	build	ID	for	the	scan.	Fortify	Static	Code	Analyzer	uses	the	
build	ID	to	identify	the	translated	source	code,	which	is	necessary	to	scan	the	code.	
You	must	specify	the	build	ID	Â if	you	want	to	automatically	translate	the	
downloaded	code	on	the	same	machine	where	you	are	running	the	extraction	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	94	of	216 

Field	Description
process.	
Translation
Parameters	
(Optional)	Â Type	any	additional	Fortify	Static	Code	Analyzer	command-	line	
translation	options.	You	must	specify	translation	parameters	if	you	want	to	
automatically	translate	the	downloaded	code	on	the	same	machine	where	you	are	
running	the	extraction	process	or	you	want	to	customize	the	translation	options.	
Scan
Parameters	
(Optional)	Â Type	any	Fortify	Static	Code	Analyzer	command-	line	scan	options.	You	
must	specify	scan	parameters	if	you	want	to	scan	the	downloaded	code	
automatically	on	the	same	machine	where	you	are	running	the	process	or	you	
want	to	customize	the	scan	options.	
ZIP	File	
Name	
(Optional)	Â Type	aZIP	file	name	if	you	want	your	output	in	acompressed	package.	
Maximum
Call-	chain	
Depth	
A	global	SAP-	function	F	is	not	downloaded	unless	F	was	explicitly	selected	or	
unless	F	can	be	reached	through	achain	of	function	calls	that	start	in	explicitly-	
selected	code	and	whose	length	is	this	number	or	less.	Fortify	recommends	that	
you	do	not	specify	avalue	greater	than	2	unless	directed	to	do	so	by	Micro	Focus	
Fortify	Customer	Support	.	
4.	Provide	action	information	described	in	the	following	table.	
Field	Description	
Download	Select	this	check	box	to	have	Fortify	Static	Code	Analyzer	download	the	source	
code	extracted	from	your	SAP	database.	
Build	Select	this	check	box	to	have	Fortify	Static	Code	Analyzer	translate	all	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	95	of	216 

Field	Description
downloaded	ABAP	code	and	store	it	using	the	specified	build	ID.	This	action	
requires	that	you	have	an	installed	version	of	Fortify	Static	Code	Analyzer	on	
the	machine	where	you	are	running	the	Fortify	ABAP	Extractor	.It	is	often	
easier	to	move	the	downloaded	source	code	to	apredefined	Fortify	Static	Code	
Analyzer	machine.	
Scan	Select	this	check	box	to	have	Fortify	Static	Code	Analyzer	run	ascan	of	the	
specified	build	ID.	This	action	requires	that	the	translate	(build)	action	was	
previously	performed.	This	action	requires	that	you	have	an	installed	version	of	
Fortify	Static	Code	Analyzer	on	the	machine	where	you	are	running	the	Fortify	
ABAP	Extractor	.It	is	often	easier	to	move	the	downloaded	source	code	to	a	
predefined	Fortify	Static	Code	Analyzer	machine.	
Launch	AWB	Select	this	check	box	to	start	Micro	Focus	Fortify	Audit	Workbench	and	open	
the	specified	FPR	file.	
Create	ZIP	
File	
Select	this	check	box	to	compress	the	output.	You	can	also	manually	compress	
the	output	after	the	source	code	is	extracted	from	your	SAP	database.	
Export
SAP	Â standard	
code	
Select	this	check	box	to	export	SAP	standard	code	in	addition	to	custom	code.	
5.	Click	Execute	.	
Uninstalling	the	Fortify	ABAP	Extractor	
To	uninstall	the	ABAP	extractor:	
1.	In	ABAP	Â Workbench,	open	the	Object	Navigator.	
2.	Select	package	Y_	FORTIFY_	ABAP.	
3.	Expand	the	Programs	tab.	
4.	Right-	click	the	following	element,	and	then	select	Delete	.	
l	Program:	Y_	FORTIFY_	SCA	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	96	of	216 

Translating	Flex	and	ActionScript	
The	basic	command-	line	syntax	for	translating	ActionScript	is:	
sourceanalyzer	-b	<build_	id>	-flex-	libraries	<libs>	<files>	
where:
<libs>	is	asemicolon-	separated	list	(Windows)	or	acolon-	separated	list	(non-	Windows	systems)	of	
library	names	to	which	you	want	to	""link""	and	<files>	Â are	the	files	to	translate.	
Flex	and	ActionScript	Command-	Line	Options	
Use	the	following	command-	line	options	to	translate	Flex	files.	You	can	also	specify	this	information	in	
the	properties	configuration	file	(fortify-	sca.properties	)Â as	noted	in	each	description.	
Flex	and	ActionScript	
Option	Description	
-flex-	sdk-	root	
<path>	
The	location	of	the	root	of	avalid	Flex	SDK.	This	folder	must	contain	a	
frameworks	folder	that	contains	aflex-	config.xml	file.	It	must	also	
contain	abin	folder	that	contains	an	MXMLC	executable.	
Equivalent	Property	Name:	
com.fortify.sca.FlexSdkRoot	
-flex-	libraries	
<libs>	
A	colon-	or	semicolon-	separated	list	(colon	on	most	platforms,	semicolon	
on	Windows)	of	library	names	that	you	want	to	âlinkâ	to.	In	most	cases,	this	
list	includes	flex.swc	,framework.swc	,and	playerglobal.swc	
(usually	found	in	frameworks/libs/	in	your	Flex	SDK	root).	
Note:	You	can	specify	SWC	or	SWF	files	as	Flex	libraries	(SWZ	is	not	
currently	supported).	
Equivalent	Property	Name:	
com.fortify.sca.FlexLibraries	
-flex-	source-	roots	
<dirs>	
A	colon-	or	semicolon-	separated	list	of	root	directories	in	which	MXML	
sources	are	located.	Normally,	these	contain	asubfolder	named	com	.	
For	example,	if	the	Flex	source	root	specified	is	foo/bar/src	,then	
foo/bar/src/com/fortify/manager/util/Foo.mxml	is	
transformed	into	an	object	named	com.fortify.manager.util.Foo	
(an	object	named	Foo	in	the	package	com.fortify.manager.util	).	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	97	of	216 

Flex	and	ActionScript	
Option	Description
Equivalent	Property	Name:	
com.fortify.sca.FlexSourceRoots	
Note:	-flex-	sdk-	root	and	âflex-	source-	roots	are	primarily	for	MXML	translation,	and	are	
optional	if	you	are	scanning	pure	ActionScript.	Use	âflex-	libraries	for	resolving	all	
ActionScript.
Fortify	Static	Code	Analyzer	translates	MXML	files	into	ActionScript	and	then	runs	them	through	an	
ActionScript	parser.	The	generated	ActionScript	is	simple	to	analyze;	not	rigorously	correct	like	the	Flex	
runtime	model.	As	aconsequence,	you	might	get	parse	errors	with	MXML	files.	For	instance,	the	XML	
parsing	could	fail,	translation	to	ActionScript	could	fail,	and	the	parsing	of	the	resulting	ActionScript	
could	also	fail.	If	you	see	any	errors	that	do	not	have	aclear	connection	to	the	original	source	code,	
notify	Micro	Focus	Fortify	Customer	Support	.	
ActionScript	Command-	Line	Examples	
The	following	examples	illustrate	command-	line	syntax	for	typical	scenarios	for	translating	ActionScript.	
Example	1	
The	following	example	is	for	asimple	application	that	contains	only	one	MXML	file	and	asingle	SWF	
library	(MyLib.swf	):	
sourceanalyzer	-b	MyFlexApp	-flex-	libraries	lib/MyLib.swf	-flex-	sdk-	root	
/home/myself/flex-	sdk/	-flex-	source-	roots	.	my/app/FlexApp.mxml	
This	identifies	the	location	of	the	libraries	to	include	and	the	Flex	SDK	and	the	Flex	source	root	
locations.	The	single	MXML	file,	located	in	/my/app/FlexApp.mxml	,results	in	translating	the	MXML	
application	as	asingle	ActionScript	class	called	FlexApp	and	located	in	the	my.app	package.	
Example	2	
The	following	example	is	for	an	application	in	which	the	source	files	are	relative	to	the	src	directory.	It	
uses	asingle	SWF	library,	MyLib.swf	,and	the	Flex	and	framework	libraries	from	the	Flex	SDK:	
sourceanalyzer	-b	MyFlexProject	-flex-	sdk-	root	/home/myself/flex-	sdk/	
-flex-	source-	roots	src/	-flex-	libraries	lib/MyLib.swf	""src/**/*.mxml""	
""src/**/*.as""
This	example	locates	the	Flex	SDK	and	uses	Fortify	Static	Code	Analyzer	file	specifiers	to	include	the	
.as	and	.mxml	files	in	the	src	folder.	It	is	not	necessary	to	explicitly	specify	the	.SWC	files	located	in	
the	âflex-	sdk-	root	,although	this	example	does	so	for	the	purposes	of	illustration.	Fortify	Static	
Code	Analyzer	automatically	locates	all	.SWC	files	in	the	specified	Flex	SDK	root,	and	it	assumes	that	
these	are	libraries	intended	for	use	in	translating	ActionScript	or	MXML	Â files.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	98	of	216 

Example	3	
In	this	example,	the	Flex	SDK	root	and	Flex	libraries	are	specified	in	aproperties	file	because	typing	in	
the	data	is	time	consuming	and	the	data	is	generally	constant.	Divide	the	application	into	two	sections	
and	store	them	in	folders:	amain	section	folder	and	amodules	folder.	Each	folder	contains	asrc	folder	
where	the	paths	start.	File	specifiers	contain	wild	cards	to	pick	up	all	the	.mxml	and	.as	files	in	both	src	
folders.	An	MXML	file	in	main/src/com/foo/util/Foo.mxml	is	translated	as	an	ActionScript	class	
named	Foo	in	the	package	com.foo.util	,for	example,	with	the	source	roots	specified	here:	
sourceanalyzer	-b	MyFlexProject	-flex-	source-	roots	main/src:modules/src	
""./main/src/**/*.mxml""	""./main/src/**/*.as""	""./modules/src/**/*.mxml""	
""./modules/src/**/*.as""
Handling	Resolution	Warnings	
To	see	all	warnings	that	were	generated	during	translation,	type	the	following	command	before	you	
start	the	scan	phase:	
sourceanalyzer	-b	<build_	id>	-show-	build-	warnings	
ActionScript	Warnings	
You	might	receive	amessage	similar	to	the	following:	
The	ActionScript	front	end	was	unable	to	resolve	the	following	imports:	
a.b	at	y.as:2.	foo.bar	at	somewhere.as:5.	a.b	at	foo.mxml:8.	
This	error	occurs	when	Fortify	Static	Code	Analyzer	cannot	find	all	of	the	required	libraries.	You	might	
need	to	specify	additional	SWC	or	SWF	Flex	libraries	(-flex-	libraries	option	or	
com.fortify.sca.FlexLibraries	property)	so	that	Fortify	Static	Code	Analyzer	can	complete	the	
analysis.
Translating	ColdFusion	Code	
To	treat	undefined	variables	in	aCFML	page	as	tainted,	uncomment	the	following	line	in	<sca_	
install_	dir>	/Core/config/fortify-	sca.properties	:	
#com.fortify.sca.CfmlUndefinedVariablesAreTainted=true
This	instructs	the	Dataflow	Analyzer	to	watch	out	for	register-	globals-	style	vulnerabilities.	However,	
enabling	this	property	interferes	with	Dataflow	Analyzer	findings	in	which	avariable	in	an	included	page	
is	initialized	to	atainted	value	in	an	earlier-	occurring	included	page.	
ColdFusion	Command-	Line	Syntax	
Type	the	following	to	translate	ColdFusion	source	code:	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	99	of	216 

sourceanalyzer	-b	<build_	id>	-source-	base-	dir	<dir>	<files>	|	<file_	
specifiers>
where:
l	<build_	id>	specifies	the	build	ID	for	the	project	
l	<dir>	specifies	the	root	directory	of	the	web	application	
l	<files>	|<file_	specifiers>	specifies	the	CFML	source	code	files	
For	adescription	of	how	to	use	<file_	specifiers>	,see	""Specifying	Files	and	Directories""	on	
page	Â 123	.	
Note:	Fortify	Static	Code	Analyzer	calculates	the	relative	path	to	each	CFML	source	file	with	the	
-source-	base-	dir	directory	as	the	starting	point.	Fortify	Static	Code	Analyzer	uses	these	
relative	paths	when	it	generates	instance	IDs.	If	you	move	the	entire	application	source	tree	to	a	
different	directory,	the	Fortify	Static	Code	Analyzer	-generated	instance	IDs	remain	the	same	if	you	
specify	an	appropriate	parameter	for	the	-source-	base-	dir	option.	
ColdFusion	Command-	Line	Options	
The	following	table	describes	the	ColdFusion	options.	
ColdFusion	Option	Description	
-source-	base-	dir	<web_	app_	root_	dir>	<files>	|	
<file_	specifiers>	
The	web	application	root	directory.	
Equivalent	Property	Name:	
com.fortify.sca.SourceBaseDir	
Translating	SQL	
On	Windows	platforms,	Fortify	Static	Code	Analyzer	assumes	that	files	with	the	.sql	extension	are	T-	
SQL	rather	than	PL/SQL.	If	you	have	PL/SQL	files	with	the	.sql	extension	on	Windows,	you	must	
configure	Fortify	Static	Code	Analyzer	to	treat	them	as	PL/SQL.	
To	specify	the	SQL	type	for	translation	on	Windows	platforms,	type	one	of	the	following	translation	
commands:
sourceanalyzer	-b	<build_	id>	-sql-	language	TSQL	<files>	
or
sourceanalyzer	-b	<build_	id>	-sql-	language	PL/SQL	<files>	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	100	of	216 

Alternatively,	you	can	change	the	default	behavior	for	files	with	the	.sql	extension.	In	the	fortify-	
sca.properties	file,	set	the	com.fortify.sca.fileextensions.sql	property	to	TSQL	or	
PLSQL	.	
PL/SQL	Command-	Line	Example	
The	following	example	shows	the	syntax	to	translate	two	PL/SQL	files:	
sourceanalyzer	-b	MyProject	x.pks	y.pks	
The	following	example	shows	how	to	translate	all	PL/SQL	files	in	the	sources	directory:	
sourceanalyzer	-b	MyProject	""sources/**/*.pks""	
T-	SQL	Command-	Line	Example	
The	following	example	shows	the	command	to	translate	two	T-	SQL	files:	
sourceanalyzer	-b	MyProject	x.sql	y.sql	
The	following	example	shows	how	to	translate	all	T-	SQL	files	in	the	sources	directory:	
sourceanalyzer	-b	MyProject	""sources\**\*.sql""	
Note:	This	example	assumes	the	com.fortify.sca.fileextensions.sql	property	in	
fortify-	sca.properties	is	set	to	TSQL	.	
Translating	Scala	Code	
To	translate	Scala	code,	you	must	have	astandard	Lightbend	Enterprise	Suite	license.	Download	the	
Scala	translation	plugin	from	Lightbend.	For	instructions	on	how	to	download	and	translate	Scala	code,	
see	the	Lightbend	documentation	at	https://developer.lightbend.com/guides/fortify	.	
Important!	If	your	project	contains	source	code	other	than	Scala,	you	must	translate	the	Scala	
code	using	the	Lightbend's	Scala	translation	plugin,	and	then	translate	the	other	source	code	with	
sourceanalyzer	using	the	same	build	ID	before	you	run	the	scan	phase.	
Translating	ASP/VBScript	Virtual	Roots	
Fortify	Static	Code	Analyzer	allows	you	to	handle	ASP	virtual	roots.	For	web	servers	that	use	virtual	
directories	as	aliases	that	map	to	physical	directories,	Fortify	Static	Code	Analyzer	enables	you	to	use	an	
alias.
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	101	of	216 

For	example,	you	can	have	virtual	directories	named	Include	and	Library	that	refer	to	the	physical	
directories	C:\WebServer\CustomerOne\inc	and	C:\WebServer\CustomerTwo\Stuff	,	
respectively.
The	following	example	shows	the	ASP/VBScript	code	for	an	application	that	uses	virtual	includes:	
<!-	-#include	virtual=""Include/Task1/foo.inc""-	->	
For	this	example,	the	previous	ASP	code	refers	to	the	file	in	the	following	physical	location:	
C:\Webserver\CustomerOne\inc\Task1\foo.inc
The	real	directory	replaces	the	virtual	directory	name	Include	in	this	example.	
Accommodating	Virtual	Roots	
To	provide	the	mapping	of	each	virtual	directory	to	Fortify	Static	Code	Analyzer	,you	must	set	the	
com.fortify.sca.ASPVirtualRoots.name_	of_	virtual_	directory	property	in	your	Fortify	
Static	Code	Analyzer	command-	line	invocation	as	shown	in	the	following	example:	
sourceanalyzer	-Dcom.fortify.sca.ASPVirtualRoots.	<virtual_	
directory>	=<full_	path_	to_	corresponding_	physical_	directory>	
Note:	On	Windows,	if	the	physical	path	includes	spaces,	you	must	enclose	the	property	setting	in	
quotes:
sourceanalyzer	""-	Dcom.fortify.sca.ASPVirtualRoots.	<virtual_	
directory>	=<full_	path_	to_	corresponding_	physical_	directory>	""	
To	expand	on	the	example	in	the	previous	section,	pass	the	following	property	value	to	Fortify	Static	
Code	Analyzer	:	
-Dcom.fortify.sca.ASPVirtualRoots.Include=""C:\WebServer\CustomerOne\inc""
-Dcom.fortify.sca.ASPVirtualRoots.Library=""C:\WebServer\CustomerTwo\Stuff""	
This	maps	Include	to	C:\WebServer\CustomerOne\inc	and	Library	to	
C:\WebServer\CustomerTwo\Stuff	.	
When	Fortify	Static	Code	Analyzer	encounters	the	#include	directive:	
<!-	-	#include	virtual=""Include/Task1/foo.inc""	-->	
Fortify	Static	Code	Analyzer	determines	if	the	project	contains	aphysical	directory	named	Include	.If	
there	is	no	such	physical	directory,	Fortify	Static	Code	Analyzer	looks	through	its	runtime	properties	
and	finds	the	-Dcom.fortify.sca.ASPVirtualRoots.Include=	
""C:\WebServer\CustomerOne\inc""	setting.	Fortify	Static	Code	Analyzer	then	looks	for	this	file:	
C:\WebServer\CustomerOne\inc\Task1\foo.inc	.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	102	of	216 

Alternatively,	you	can	set	this	property	in	the	fortify-	sca.properties	file	located	in	<sca_	
install_	dir>	\Core\config	.You	must	escape	the	backslash	character	(\)	in	the	path	of	the	physical	
directory	as	shown	in	the	following	example:	
com.fortify.sca.ASPVirtualRoots.Library=C:\\WebServer\\CustomerTwo\\Stuff
com.fortify.sca.ASPVirtualRoots.Include=C:\\WebServer\\CustomerOne\\inc
Note:	The	previous	version	of	the	ASPVirtualRoot	property	is	still	valid.	You	can	use	it	on	the	
Fortify	Static	Code	Analyzer	command	line	as	follows:	
-Dcom.fortify.sca.ASPVirtualRoots=C:\WebServer\CustomerTwo\Stuff;
C:\WebServer\CustomerOne\inc
This	prompts	Fortify	Static	Code	Analyzer	to	search	through	the	listed	directories	in	the	order	specified	
when	it	resolves	avirtual	include	directive.	
Using	Virtual	Roots	Example	
You	have	afile	as	follows:	
C:\files\foo\bar.asp
To	specify	this	file,	use	the	following	include:	
<!-	-	#include	virtual=""/foo/bar.asp"">	
Then	set	the	virtual	root	in	the	sourceanalyzer	command	as	follows:	
-Dcom.fortify.sca.ASPVirtualRoots=C:\files\foo	
This	strips	the	/foo	from	the	front	of	the	virtual	root.	If	you	do	not	specify	foo	in	the	
com.fortify.sca.ASPVirtualRoots	property,	then	Fortify	Static	Code	Analyzer	looks	for	
C:\files\bar.asp	and	fails.	
The	sequence	to	specify	virtual	roots	is	as	follows:	
1.	Remove	the	first	part	of	the	path	in	the	source.	
2.	Replace	the	first	part	of	the	path	with	the	virtual	root	as	specified	on	the	command	line.	
Translating	Dockerfiles	
By	default,	Fortify	Static	Code	Analyzer	translates	the	following	files	as	Dockerfiles:	Dockerfile	,	
dockerfile	,*.Dockerfile	,and	*.dockerfile	.	
Note:	You	can	modify	the	file	extension	used	to	detect	Dockerfiles	using	the	
com.fortify.sca.fileextensions	property.	See	""fortify-	sca.properties""	on	page	Â 182	.	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	103	of	216 

Fortify	Static	Code	Analyzer	accepts	the	following	escape	characters	in	Dockerfiles:	Â backslash	(\)and	
backquote	(`).	If	the	escape	character	is	not	set	in	the	Dockerfile,	then	Fortify	Static	Code	Analyzer	
assumes	that	the	backslash	is	the	escape	character.	
The	syntax	to	translate	adirectory	that	contains	Dockerfiles	is	shown	in	the	following	example:	
sourceanalyzer	-b	<build_	id>	<dir>	
If	the	Dockerfile	is	malformed	and	Fortify	Static	Code	Analyzer	cannot	parse	the	file,	an	error	is	written	
to	the	log	and	analysis	of	the	Dockerfile	is	skipped.	The	following	is	an	example	of	the	error	written	to	
the	log:	
Dockerfile	parser	error	1:20	:	mismatched	input	'\n'	expecting	{LINE_	
EXTEND,	WHITESPACE}	
Unable	to	parse	config	file	
C:/Users/jsmith/MyProj/docker/dockerfile/ProjA.Dockerfile
Classic	ASP	Command-	Line	Example	
To	translate	asingle	file	classic	ASP	Â written	in	VBScript	named	MyASP.asp	,type:	
sourceanalyzer	-b	mybuild	""MyASP.asp""	
VBScript	Command-	Line	Example	
To	translate	aVBScript	file	named	myApp.vb	,type:	
sourceanalyzer	-b	mybuild	""myApp.vb""	
User	Guide	
Chapter	15:	Translating	Other	Languages	and	Configurations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	104	of	216 

Chapter	16:	Integrating	into	a	Build	
You	can	integrate	the	analysis	into	supported	build	tools.	
This	section	contains	the	following	topics:	
Build	Integration	105	
Modifying	aBuild	Script	to	Invoke	Fortify	Static	Code	Analyzer	106	
Touchless	Build	Integration	107	
Ant	Integration	107	
Gradle	Integration	107	
Maven	Integration	109	
Build	Integration	
You	can	translate	entire	projects	in	asingle	operation.	Prefix	your	original	build	operation	with	the	
sourceanalyzer	command	followed	by	the	Fortify	Static	Code	Analyzer	options.	
The	basic	command-	line	syntax	to	translate	acomplete	project	is:	
sourceanalyzer	-b	<build_	id>	[<sca_	options>	]	<build_	tool>	[<build_	tool_	
options>	]	
where	<build_	tool>	is	the	name	of	your	build	tool,	such	as	make,	gmake,	msbuild,	devenv,	or	
xcodebuild.	See	the	Micro	Focus	Fortify	Software	System	Requirements	document	for	alist	of	
supported	build	tools.	Fortify	Static	Code	Analyzer	executes	your	build	tool	and	intercepts	all	compiler	
operations	to	collect	the	specific	command	line	used	for	each	input.	
Note:	Fortify	Static	Code	Analyzer	only	processes	the	compiler	commands	that	the	build	tool	
executes.	If	you	do	not	clean	your	project	before	you	execute	the	build,	then	Fortify	Static	Code	
Analyzer	only	processes	those	files	that	the	build	tool	re-	compiles.	
For	information	about	integrating	with	Xcodebuild,	see	""iOS	Â Code	Analysis	Command-	Line	Syntax""	on	
page	Â 75	.For	information	about	integration	with	MSBuild,	see	""Translating	Visual	Studio	and	MSBuild	
Projects""	on	page	Â 58	.	
Successful	build	integration	requires	that	the	build	tool:	
l	Executes	aFortify	Static	Code	Analyzer	-supported	compiler	
l	Executes	the	compiler	on	the	operating	system	path	search,	not	with	ahardcoded	path	(This	
requirement	does	not	apply	to	xcodebuild	integration.)	
l	Executes	the	compiler,	rather	than	executing	asub-	process	that	then	executes	the	compiler	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	105	of	216 

If	you	cannot	meet	these	requirements	in	your	environment,	see	""Modifying	aBuild	Script	to	Invoke	
Fortify	Static	Code	Analyzer""	below	.	
Make	Example	
If	you	build	your	project	with	the	following	build	commands:	
make	clean	
make
make	install	
then	you	can	simultaneously	translate	and	compile	the	entire	project	with	the	following	commands:	
make	clean	
sourceanalyzer	-b	<build_	id>	make	
make	install	
Modifying	a	Build	Script	to	Invoke	Fortify	Static	Code	
Analyzer
As	an	alternative	to	build	integration,	you	can	modify	your	build	script	to	prefix	each	compiler,	linker,	
and	archiver	operation	with	the	sourceanalyzer	command.	For	example,	amakefile	often	defines	
variables	for	the	names	of	these	tools:	
CC=gcc
CXX=g++
LD=ld
AR=ar
You	can	prepend	the	tool	references	in	the	makefile	with	the	sourceanalyzer	command	and	the	
appropriate	Fortify	Static	Code	Analyzer	options.	
CC=sourceanalyzer	-b	mybuild	gcc	
CXX=sourceanalyzer	-b	mybuild	g++	
LD=sourceanalyzer	-b	mybuild	ld	
AR=sourceanalyzer	-b	mybuild	ar	
When	you	use	the	same	build	ID	for	each	operation,	Fortify	Static	Code	Analyzer	automatically	
combines	each	of	the	separately-	translated	files	into	asingle	translated	project.	
User	Guide	
Chapter	16:	Integrating	into	aBuild	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	106	of	216 

Touchless	Build	Integration	
Fortify	Static	Code	Analyzer	includes	ageneric	build	tool	called	touchless	that	enables	translation	of	
projects	using	build	systems	that	Fortify	Static	Code	Analyzer	Â does	not	directly	support.	The	command-	
line	syntax	for	touchless	build	integration	is:	
sourceanalyzer	-b	<build_	id>	touchless	<build_	command>	
For	example,	you	might	use	apython	script	called	build.py	to	compute	dependencies	and	execute	
appropriately-	ordered	C	compiler	operations.	Then	to	execute	your	build,	run	the	following	command:	
python	build.py	
Fortify	Static	Code	Analyzer	does	not	have	native	support	for	such	abuild	design.	However,	you	can	
use	the	touchless	build	tool	to	translate	and	build	the	entire	project	with	the	single	command:	
sourceanalyzer	-b	<build_	id>	touchless	python	build.py	
The	same	requirements	for	successful	build	integration	with	supported	build	systems	described	earlier	
in	this	chapter	(see	""Build	Integration""	on	page	Â 105	)apply	to	touchless	integration	with	unsupported	
build	systems.	
Ant	Integration	
Fortify	Static	Code	Analyzer	provides	an	easy	way	to	translate	Java	source	files	for	projects	that	use	an	
Ant	build	file.	You	can	apply	this	integration	on	the	command	line	without	modifying	the	Ant	
build.xml	file.	When	the	build	runs,	Fortify	Static	Code	Analyzer	intercepts	all	javac	task	invocations	
and	translates	the	Java	source	files	as	they	are	compiled.	
Note:	You	must	translate	any	JSP	files,	configuration	files,	or	any	other	non-	Java	source	files	that	
are	part	of	the	application	in	aseparate	step.	
To	use	the	Ant	integration,	make	sure	that	the	sourceanalyzer	executable	is	on	the	system	PATH	.	
Prepend	your	Ant	command-	line	with	the	sourceanalyzer	command	as	follows:	
sourceanalyzer	-b	<build_	id>	ant	[<ant_	options>	]	
Gradle	Integration	
You	can	translate	projects	that	are	built	with	Gradle	without	any	modification	of	the	build.gradle	
file.	When	the	build	runs,	Fortify	Static	Code	Analyzer	translates	the	source	files	as	they	are	compiled.	
See	the	Micro	Focus	Fortify	Software	System	Requirements	document	Â for	platforms	and	languages	
User	Guide	
Chapter	16:	Integrating	into	aBuild	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	107	of	216 

supported	specifically	for	Gradle	integration.	Any	files	in	the	project	in	unsupported	languages	for	
Gradle	integration	are	not	translated	(with	no	error	reporting).	These	files	are	therefore	not	analyzed,	
and	any	existing	potential	vulnerabilities	can	go	undetected.	
To	integrate	Fortify	Static	Code	Analyzer	into	your	Gradle	build,	make	sure	that	the	sourceanalyzer	
executable	is	on	the	system	PATH.	Prepend	the	Gradle	command	line	with	the	sourceanalyzer	
command	as	follows:	
sourceanalyzer	-b	<build_	id>	<sca_	options>	gradle	[<gradle_	options>	]	
<gradle	tasks>	
For	example:	
sourceanalyzer	-b	buildxyz	gradle	clean	build	
sourceanalyzer	-b	buildxyz	gradle	--info	assemble	
If	your	build	file	name	is	different	than	build.gradle	,then	include	the	build	file	name	with	the	--	
build-	file	option	as	shown	in	the	following	example:	
sourceanalyzer	-b	buildxyz	gradle	--build-	file	sample.gradle	clean	
assemble
You	can	also	use	the	Gradle	Wrapper	(gradlew	)as	shown	in	the	following	example:	
sourceanalyzer	-b	<build_	id>	gradlew	[<gradle_	options>	]	
If	your	application	uses	XML	Â or	property	configuration	files,	translate	these	files	with	aseparate	
sourceanalyzer	command.	Use	the	same	build	ID	that	you	used	for	the	project	files.	The	following	
are	examples:	
sourceanalyzer	-b	<build_	id>	<path_	to_	xml_	files>	
sourceanalyzer	-b	<build_	id>	<path_	to_	properties_	files>	
After	translating	the	project	with	gradle	or	gradlew,	you	can	then	perform	the	analysis	phase	as	shown	
in	the	following	example:	
sourceanalyzer	-b	<build_	id>	-scan	-f	myresults.fpr	
Including	Verbose	and	Debug	Options	
If	you	use	the	Fortify	Static	Code	Analyzer	-verbose	option,	then	you	must	also	include	the	-gradle	
option.	Use	of	this	option	applies	to	both	Gradle	and	the	Gradle	Wrapper.	For	example:	
sourceanalyzer	-b	buildxyz	-gradle	-verbose	gradle	assemble	
As	part	of	the	gradle	integration,	Fortify	Static	Code	Analyzer	temporarily	updates	the	original	build	file	
build.gradle	.If	you	Â include	the	-debug	option,	Fortify	Static	Code	Analyzer	Â saves	acopy	of	the	
original	build	file	as	build.gradle.orig	).	After	the	analysis	with	the	-debug	option	is	complete,	
User	Guide	
Chapter	16:	Integrating	into	aBuild	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	108	of	216 

rename	the	build.gradle.orig	file	back	to	build.gradle	and	run	sourceanalyzer	again	without	
the	-debug	option.	
Maven	Integration	
Fortify	Static	Code	Analyzer	includes	aMaven	plugin	that	provides	away	to	add	the	following	
capabilities	to	your	Maven	project	builds:	
l	Fortify	Static	Code	Analyzer	clean,	translate,	scan	
l	Fortify	Static	Code	Analyzer	export	mobile	build	session	(MBS)	for	aFortify	Static	Code	Analyzer	
translated	project	
l	Send	translated	code	to	Micro	Focus	Fortify	ScanCentral	SAST	
l	Upload	results	to	Micro	Focus	Fortify	Software	Security	Center	
You	can	use	the	plugin	directly	or	integrate	its	functionality	into	your	build	process.	
Installing	and	Updating	the	Fortify	Maven	Plugin	
The	Fortify	Maven	Plugin	is	located	in	<sca_	install_	dir>	/plugins/maven	.This	directory	contains	
abinary	and	asource	version	of	the	plugin	in	both	zip	and	tarball	archives.	To	install	the	plugin,	extract	
the	version	(binary	or	source)	that	you	want	to	use,	and	then	follow	the	instructions	in	the	included	
README.TXT	file.	Perform	the	installation	in	the	directory	where	you	extracted	the	archive.	
For	information	about	supported	versions	of	Maven,	see	the	Micro	Focus	Fortify	Software	System	
Requirements	document.	
If	you	have	aprevious	version	of	the	Fortify	Maven	Plugin	installed,	and	then	install	the	latest	version.	
Uninstalling	the	Fortify	Maven	Plugin	
To	uninstall	the	Fortify	Maven	Plugin	,manually	delete	all	files	from	the	<maven_	local_	
repo>	/repository/com/fortify/ps/maven/plugin	directory.	
Testing	the	Fortify	Maven	Plugin	Installation	
After	you	install	the	Fortify	Maven	Plugin	,use	one	of	the	included	sample	files	to	be	sure	your	
installation	works	properly.	
To	test	the	Fortify	Maven	Plugin	using	the	Eightball	sample	file:	
1.	Add	the	directory	that	contains	the	sourceanalyzer	executable	to	the	path	environment	
variable.
For	example:	
export	set	PATH=$PATH:/	<sca_	install_	dir>	/bin	
or	
User	Guide	
Chapter	16:	Integrating	into	aBuild	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	109	of	216 

set	PATH=%PATH%;	<sca_	install_	dir>	/bin	
2.	Type	sourceanalyzer	-version	to	test	the	path	setting.	
Fortify	Static	Code	Analyzer	version	information	is	displayed	if	the	path	setting	is	correct.	
3.	Navigate	to	the	sample	Eightball	directory:	<root_	dir>	/samples/EightBall	.	
4.	Type	the	following	command:	
mvn	com.fortify.sca.plugins.maven:sca-	maven-	plugin:	<ver>	:clean	
where	<ver>	is	the	version	of	the	Fortify	Maven	Plugin	you	are	using.	If	the	version	is	not	
specified,	Maven	uses	the	latest	version	of	the	Fortify	Maven	Plugin	that	is	installed	in	the	local	
repository.
Note:	To	see	the	version	of	the	Fortify	Maven	Plugin	,open	the	pom.xml	file	that	you	
extracted	in	<root_	dir>	in	atext	editor.	The	Fortify	Maven	Plugin	version	is	specified	in	the	
<version>	Â element.	
5.	If	the	command	in	step	4	completed	successfully,	then	the	Fortify	Maven	Plugin	is	installed	
correctly.	The	Fortify	Maven	Plugin	is	not	installed	correctly	if	you	get	the	following	error	message:	
[ERROR]	Error	resolving	version	for	plugin	
'com.fortify.sca.plugins.maven:sca-	maven-	plugin'	from	the	repositories	
Check	the	Maven	local	repository	and	try	to	install	the	Fortify	Maven	Plugin	again.	
Using	the	Fortify	Maven	Plugin	
There	are	two	ways	to	perform	aFortify	analysis	on	amaven	project:	
l	As	aMaven	Plugin	
In	this	method,	you	perform	the	Fortify	analysis	tasks	as	goals	with	the	mvn	command.	For	example,	
the	following	command	is	used	to	translate	source	code:	
mvn	com.fortify.sca.plugins.maven:sca-	maven-	plugin:	<ver>	:translate	
To	analyze	your	code	this	way,	see	the	documentation	included	with	the	Fortify	Maven	Plugin	.The	
following	table	describes	where	to	find	the	documentation	after	the	Fortify	Maven	Plugin	is	properly	
installed.
Package	Type	Documentation	Location	
Binary	<root_	dir>	/docs/index.html	
Source	<root_	dir>	/sca-	maven-	plugin/target/site/index.html	
l	In	aFortify	Static	Code	Analyzer	build	integration	
User	Guide	
Chapter	16:	Integrating	into	aBuild	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	110	of	216 

In	this	method,	prepend	the	maven	command	used	to	build	your	project	with	the	sourceanalyzer	
command	and	any	Fortify	Static	Code	Analyzer	options.	To	analyze	your	files	as	part	of	aFortify	
Static	Code	Analyzer	build	integration:	
a.	Clean	out	the	previous	build:	
sourceanalyzer	-b	<build_	id>	-clean	
b.	Translate	the	code:	
sourceanalyzer	-b	<build_	id>	[<sca_	options>	]	[<mvn_	command_	with_	
options>	]	
For	example:	
sourceanalyzer	-b	buildxyz	mvn	package	
The	following	additional	example	includes	the	Fortify	Static	Code	Analyzer	option	to	exclude	
selected	files	from	the	analysis.	To	specify	the	files	you	want	to	exclude,	add	the	-exclude	
option	to	the	translate	step	as	shown	in	the	following	example:	
sourceanalyzer	-b	buildxyz	-exclude	""fileA;fileB;fileC;""	mvn	package	
Note:	On	Windows,	separate	the	file	names	with	asemicolon;	and	on	all	other	platforms	use	
acolon.	
See	""Command-	Line	Interface""	on	page	Â 112	for	descriptions	of	available	Fortify	Static	Code	
Analyzer	options.	
c.	Complete	the	analysis	by	running	the	scan	as	shown	in	the	following	example:	
sourceanalyzer	-b	<build_	id>	[<sca_	scan_	options>	]	-scan	-f	
myresults.fpr	
User	Guide	
Chapter	16:	Integrating	into	aBuild	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	111	of	216 

Chapter	17:	Command-	Line	Interface	
This	chapter	describes	general	Fortify	Static	Code	Analyzer	command-	line	options	and	how	to	specify	
source	files	for	analysis.	Command-	line	options	that	are	specific	to	alanguage	are	described	in	the	
chapter	for	that	language.	
This	section	contains	the	following	topics:	
Translation	Options	112	
Analysis	Options	114	
Output	Options	117	
Other	Options	120	
Directives	122	
Specifying	Files	and	Directories	123	
Translation	Options	
The	following	table	describes	the	translation	options.	
Translation	Option	Description	
-b	<build_	id>	Specifies	the	build	ID.	Fortify	Static	Code	Analyzer	uses	the	build	ID	to	
track	which	files	are	compiled	and	combined	as	part	of	abuild,	and	
later,	to	scan	those	files.	
Equivalent	Property	Name:	
com.fortify.sca.BuildID	
-disable-	language	Specifies	acolon-	separated	list	of	languages	to	exclude	from	the	
translation	phase.	The	valid	language	values	are	abap	,	
actionscript	,apex	,cfml	,cobol	,cpp	,csharp	,golang	,java	,	
javascript	,jsp	,kotlin	,objc	,php	,plsql	,python	,ruby	,scala	,	
sql	,swift	,tsql	,typescript	,and	vb	.	
Equivalent	Property	Name:	
com.fortify.sca.DISabledLanguages	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	112	of	216 

Translation	Option	Description	
-enable-	language	Specifies	acolon-	separated	list	of	languages	to	translate.	The	valid	
language	values	are	abap	,actionscript	,apex	,cfml	,cobol	,cpp	,	
csharp	,golang	,java	,javascript	,jsp	,kotlin	,objc	,php	,	
plsql	,python	,ruby	,scala	,sql	,swift	,tsql	,typescript	,and	
vb	.	
Equivalent	Property	Name:	
com.fortify.sca.EnabledLanguages	
-exclude
<file_	specifiers>	
Removes	files	from	the	list	of	files	to	translate.	Separate	multiple	file	
paths	with	semicolons	(Windows)	or	colons	(non-	Windows	systems).	
See	""Specifying	Files	and	Directories""	on	page	Â 123	for	more	
information	on	how	to	use	file	specifiers.	
For	example:	
sourceanalyzer	âcp	""**/*.jar""	""**/*""	
-excludeÂ ""**/Test/*.java""	
This	example	excludes	all	Java	files	in	any	Test	subdirectory.	
Note:	When	you	integrate	the	translation	with	acompiler	or	a	
build	tool,	Fortify	Static	Code	Analyzer	translates	all	source	files	
that	the	compiler	or	build	tool	processes	even	if	they	are	specified	
with	this	option.	
Equivalent	Property	Name:	
com.fortify.sca.exclude	
-encoding	<encoding_	
name>	
Specifies	the	source	file	encoding	type.	Fortify	Static	Code	Analyzer	
enables	you	to	scan	aproject	that	contains	differently	encoded	source	
files.	To	work	with	amulti-	encoded	project,	you	must	specify	the	-	
encoding	option	in	the	translation	phase,	when	Fortify	Static	Code	
Analyzer	first	reads	the	source	code	file.	Fortify	Static	Code	Analyzer	
remembers	this	encoding	in	the	build	session	and	propagates	it	into	
the	FVDL	file.	
Valid	encoding	names	are	from	the	java.nio.charset.Charset	.	
Typically,	if	you	do	not	specify	the	encoding	type,	Fortify	Static	Code	
Analyzer	uses	file.encoding	from	the	
java.io.InputStreamReader	constructor	with	no	encoding	
parameter.	In	afew	cases	(for	example	with	the	ActionScript	parser),	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	113	of	216 

Translation	Option	Description
Fortify	Static	Code	Analyzer	defaults	to	UTF-	8.	
Equivalent	Property	Name:	
com.fortify.sca.InputFileEncoding	
-nc	When	specified	before	acompiler	command	line,	Fortify	Static	Code	
Analyzer	translates	the	source	file	but	does	not	run	the	compiler.	
-noextension-	type	
<file_	type>	
Specifies	the	file	type	for	source	files	that	have	no	file	extension.	The	
possible	values	are:	ABAP,	ACTIONSCRIPT,	APEX,	APEX_	TRIGGER,	
ARCHIVE,	ASPNET,	ASP,	ASPX,	BITCODE,	BYTECODE,	CFML,	
COBOL,	CSHARP,	DOCKERFILE,	GO,	HTML,	JAVA,	JAVA_	
PROPERTIES,	JAVASCRIPT,	JSP,	JSPX,	KOTLIN,	MSIL,	MXML,	PHP,	
PLSQL,	PYTHON,	RUBY,	RUBY_	ERB,	SCALA,	SWIFT,	TLD,	SQL,	
TSQL,	TYPESCRIPT,	VB,	VB6,	VBSCRIPT,	VISUAL_	FORCE,	and	XML.	
Analysis	Options	
The	following	table	describes	the	analysis	options.	
Analysis	Option	Description	
-scan	Causes	Fortify	Static	Code	Analyzer	to	perform	analysis	for	the	specified	
build	ID.	
Note:	Do	not	use	this	option	together	with	the	-scan-	module	
option	in	the	same	sourceanalyzer	command.	
-scan-	module	Causes	Fortify	Static	Code	Analyzer	to	Â perform	analysis	for	the	specified	
build	ID	as	aseparate	module.	
Note:	Do	not	use	this	option	together	with	the	-scan	option	in	the	
same	sourceanalyzer	command.	
Equivalent	Property	Name:	
com.fortify.sca.ScanScaModule	
-include-	modules	Specifies	the	libraries	previously	scanned	as	separate	modules	in	acomma-	
or	colon-	separated	list	of	build	IDs	to	include	in	the	project	scan.	
Equivalent	Property	Name:	
com.fortify.sca.IncludeScaModules	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	114	of	216 

Analysis	Option	Description	
-analyzers	Specifies	the	analyzers	you	want	to	enable	with	acolon-	or	comma-	
separated	list	of	analyzers.	The	valid	analyzer	names	are	buffer	,	
content	,configuration	,controlflow	,dataflow	,findbugs	,	
nullptr	,semantic	,and	structural	.You	can	use	this	option	to	
disable	analyzers	that	are	not	required	for	your	security	requirements.	
Equivalent	Property	Name:	
com.fortify.sca.DefaultAnalyzers	
-b	<build_	id>	Specifies	the	build	ID.	
Equivalent	Property	Name:	
com.fortify.sca.BuildID	
-p	<level>	|Â 	
-scan-	precision	
<level>	
Scans	the	project	with	ascan	precision	level.	Scans	with	alower	precision	
level	are	performed	faster.	The	valid	values	are	1	and	2.For	more	
information,	see	""Configuring	Scan	Speed	with	Speed	Dial""	on	page	Â 146	.	
Equivalent	Property	Name:	
com.fortify.sca.PrecisionLevel	
-quick	Scans	the	project	in	quick	scan	mode	using	the	fortify-	sca-	
quickscan.properties	file,	providing	aless	in-	depth	analysis.	By	
default,	it	disables	the	Buffer	Analyzer	and	the	Control	Flow	Analyzer.	In	
addition,	it	applies	the	Quick	View	filter	set.	
Equivalent	Property	Name:	
com.fortify.sca.QuickScanMode	
-bin	<binary>	|	
-binary-	name	
<binary>	
Specifies	asubset	of	source	files	to	scan.	Only	the	source	files	that	were	
linked	in	the	named	binary	at	build	time	are	included	in	the	scan.	You	can	
use	this	option	multiple	times	to	specify	the	inclusion	of	multiple	binaries	
in	the	scan.	
Equivalent	Property	Name:	
com.fortify.sca.BinaryName	
-disable-	default-	
rule-	type	
<type>	
Disables	all	rules	of	the	specified	type	in	the	default	Rulepacks.	You	can	
use	this	option	multiple	times	to	specify	multiple	rule	types.	
The	<type>	parameter	is	the	XML	tag	minus	the	suffix	Rule	.For	
example,	use	DataflowSource	for	DataflowSourceRule	elements.	You	
can	also	specify	specific	sections	of	characterization	rules,	such	as	
Characterization:Control	flow	,Characterization:Issue	,and	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	115	of	216 

Analysis	Option	Description
Characterization:Generic	.	
The	<type>	parameter	is	case-	insensitive.	
-filter	<file>	Specifies	aresults	filter	file.	See	""Filtering	the	Analysis""	on	page	Â 165	for	
more	information	about	this	option.	
Equivalent	Property	Name:	
com.fortify.sca.FilterFile	
-findbugs	Enables	FindBugs	analysis	for	Java	code.	You	must	specify	the	Java	class	
directories	with	the	-java-	build-	dir	option,	which	is	described	in	""Java	
Command-	Line	Options""	on	page	Â 48	.	
Equivalent	Property	Name:	
com.fortify.sca.EnableFindbugs	
-no-	default-	issue-	
rules	
Disables	rules	in	default	Rulepacks	that	lead	directly	to	issues.	Still	loads	
rules	that	characterize	the	behavior	of	functions.	
Note:	This	is	equivalent	to	disabling	the	following	rule	types:	
DataflowSink,	Semantic,	Controlflow,	Structural,	Configuration,	
Content,	Statistical,	Internal,	and	Characterization:Issue.	
Equivalent	Property	Name:	
com.fortify.sca.NoDefaultIssueRules	
-no-	default-	rules	Specifies	not	to	load	rules	from	the	default	Rulepacks.	Fortify	Static	Code	
Analyzer	processes	the	Rulepacks	for	description	elements	and	language	
libraries,	but	processes	no	rules.	
Equivalent	Property	Name:	
com.fortify.sca.NoDefaultRules	
-no-	default-	
source-	rules	
Disables	source	rules	in	the	default	Rulepacks.	
Note:	Characterization	source	rules	are	not	disabled.	
Equivalent	Property	Name:	
com.fortify.sca.NoDefaultSourceRules	
-no-	default-	sink-	
rules	
Disables	sink	rules	in	the	default	Rulepacks.	
Note:	Characterization	sink	rules	are	not	disabled.	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	116	of	216 

Analysis	Option	Description
Equivalent	Property	Name:	
com.fortify.sca.NoDefaultSinkRules	
-project-	template	Specifies	the	issue	template	file	to	use	for	the	scan.	This	only	affects	scans	
on	the	local	machine.	If	you	upload	the	FPR	Â to	Micro	Focus	Fortify	
Software	Security	Center	server,	it	uses	the	issue	template	assigned	to	the	
application	version.	
Equivalent	Property	Name:	
com.fortify.sca.ProjectTemplate	
-rules	<file>	|	
<dir>	
Specifies	acustom	Rulepack	or	directory.	You	can	use	this	option	multiple	
times	to	specify	multiple	Rulepack	files.	If	you	specify	adirectory,	includes	
all	of	the	files	in	the	directory	with	the	.bin	and	.xml	extensions.	
Equivalent	Property	Name:	
com.fortify.sca.RulesFile	
Output	Options	
The	following	table	describes	the	output	options.	
Output	Option	Description	
-f	<file>	|Â 	
-output-	file	
<file>	
Specifies	the	file	to	which	results	are	written.	If	you	do	not	specify	an	
output	file,	Fortify	Static	Code	Analyzer	writes	the	output	to	the	terminal.	
Equivalent	Property	Name:	
com.fortify.sca.ResultsFile	
-format	<format>	Controls	the	output	format.	Valid	options	are	fpr	,fvdl	,fvdl.zip	,	
text	,and	auto	.The	default	is	auto	,which	selects	the	output	format	
based	on	the	file	extension	of	the	file	provided	with	the	-f	option.	
The	FVDL	is	an	XML	file	that	contains	the	detailed	Fortify	Static	Code	
Analyzer	analysis	results.	This	includes	vulnerability	details,	rule	
descriptions,	code	snippets,	command-	line	options	used	in	the	scan,	and	
any	scan	errors	or	warnings.	
The	FPR	is	apackage	of	the	analysis	results	that	includes	the	FVDL	file	as	
well	as	additional	information	such	as	acopy	of	the	source	code	used	in	
the	scan,	the	external	metadata,	and	custom	rules	(if	applicable).	Micro	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	117	of	216 

Output	Option	Description
Focus	Fortify	Audit	Workbench	is	automatically	associated	with	the	.fpr	
file	extension.	
Note:	If	you	use	result	certification,	you	must	specify	the	fpr	format.	
See	the	Micro	Focus	Fortify	Audit	Workbench	User	Guide	for	
information	about	result	certification.	
You	can	prevent	some	of	the	information	from	being	included	in	the	FPR	
or	FVDL	Â file	to	improve	scan	time	or	output	file	size.	See	other	options	in	
this	table	and	see	the	""fortify-	sca.properties""	on	page	Â 182	.	
Equivalent	Property	Name:	
com.fortify.sca.Renderer	
-append	Appends	results	to	the	file	specified	with	the	-f	option.	The	resulting	FPR	
contains	the	issues	from	the	earlier	scan	as	well	as	issues	from	the	current	
scan.	The	build	information	and	program	data	(lists	of	sources	and	sinks)	
sections	are	also	merged.	To	use	this	option,	the	output	file	format	must	
be	fpr	or	fvdl	.For	information	on	the	-format	output	option,	see	the	
description	in	this	table.	
The	engine	data,	which	includes	Fortify	security	content	Â information,	
command-	line	options,	system	properties,	warnings,	errors,	and	other	
information	about	the	execution	of	Fortify	Static	Code	Analyzer	(as	
opposed	to	information	about	the	program	being	analyzed),	is	not	
merged.	Because	engine	data	is	not	merged	with	the	-append	option,	
Fortify	does	not	certify	results	generated	with	-append	.	
If	this	option	is	not	specified,	Fortify	Static	Code	Analyzer	adds	any	new	
findings	to	the	FPR	file,	and	labels	the	older	result	as	previous	findings.	
In	general,	only	use	the	-append	option	when	it	is	not	possible	to	analyze	
an	entire	application	at	once.	
Equivalent	Property	Name:	
com.fortify.sca.OutputAppend	
-build-	label	
<label>	
Specifies	the	label	of	the	project	being	scanned.	Fortify	Static	Code	
Analyzer	does	not	use	this	label	but	includes	it	in	the	analysis	results.	
Equivalent	Property	Name:	
com.fortify.sca.BuildLabel	
-build-	project	Specifies	the	name	of	the	project	being	scanned.	Fortify	Static	Code	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	118	of	216 

Output	Option	Description	
<project>	Analyzer	does	not	use	the	name	but	includes	it	in	the	analysis	results.	
Equivalent	Property	Name:	
com.fortify.sca.BuildProject	
-build-	version	
<version>	
Specifies	the	version	of	the	project	being	scanned.	Fortify	Static	Code	
Analyzer	does	not	use	the	version	but	includes	it	in	the	analysis	results.	
Equivalent	Property	Name:	
com.fortify.sca.BuildVersion	
-disable-	source-	
bundling	
Excludes	source	files	from	the	FPR	file.	
Equivalent	Property	Name:	
com.fortify.sca.FPRDisableSourceBundling	
-fvdl-	no-	
descriptions	
Excludes	the	Fortify	security	content	descriptions	from	the	analysis	results	
file.
Equivalent	Property	Name:	
com.fortify.sca.FVDLDisableDescriptions	
-fvdl-	no-	
enginedata	
Excludes	the	engine	data	from	the	analysis	results	file.	The	engine	data	
includes	Fortify	security	content	Â information,	command-	line	options,	
system	properties,	warnings,	errors,	and	other	information	about	the	
execution	of	Fortify	Static	Code	Analyzer	.	
Equivalent	Property	Name:	
com.fortify.sca.FVDLDisableEngineData	
-fvdl-	no-	progdata	Excludes	program	data	from	the	analysis	results	file.	This	removes	the	
taint	source	information	from	the	Functions	view	in	Fortify	Audit	
Workbench	.	
Equivalent	Property	Name:	
com.fortify.sca.FVDLDisableProgramData	
-fvdl-	no-	snippets	Excludes	the	code	snippets	from	the	analysis	results	file.	
Equivalent	Property	Name:	
com.fortify.sca.FVDLDisableSnippets	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	119	of	216 

Other	Options	
The	following	table	describes	other	options.	
Other	Option	Description	
@<file>	Reads	command-	line	options	from	the	specified	file.	
Note:	By	default,	this	file	uses	the	JVM	Â system	encoding.	You	can	
change	the	encoding	by	using	the	
com.fortify.sca.CmdlineOptionsFileEncoding	property	
specified	in	the	fortify-	sca.properties	file.	For	more	
information	about	this	property,	see	""fortify-	sca.properties""	on	
page	Â 182	.	
-h	|	
-?	|	
-help	
Prints	asummary	of	command-	line	options.	
-debug	Includes	debug	information	in	the	Fortify	Support	log	file,	which	is	only	
useful	for	Micro	Focus	Fortify	Customer	Support	to	help	troubleshoot.	
Equivalent	Property	Name:	
com.fortify.sca.Debug	
-debug-	verbose	This	is	the	same	as	the	-debug	option,	but	it	includes	more	details,	
specifically	for	parse	errors.	
Equivalent	Property	Name:	
com.fortify.sca.DebugVerbose	
-verbose	Sends	verbose	status	messages	to	the	console	and	to	the	Fortify	Support	
log	file.	
Equivalent	Property	Name:	
com.fortify.sca.Verbose	
-logfileÂ 	<file>	Specifies	the	log	file	that	Fortify	Static	Code	Analyzer	creates.	
Equivalent	Property	Name:	
com.fortify.sca.LogFile	
-clobber-	log	Directs	Fortify	Static	Code	Analyzer	to	overwrite	the	log	file	for	each	run	
of	sourceanalyzer.	Without	this	option,	Fortify	Static	Code	Analyzer	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	120	of	216 

Other	Option	Description
appends	information	to	the	log	file.	
Equivalent	Property	Name:	
com.fortify.sca.ClobberLogFile	
-quiet	Disables	the	command-	line	progress	information.	
Equivalent	Property	Name:	
com.fortify.sca.Quiet	
-version	|	
-v	
Displays	the	Fortify	Static	Code	Analyzer	version	number.	
-autoheap	Enables	automatic	allocation	of	memory	based	on	the	physical	memory	
available	on	the	system.	This	is	the	default	memory	allocation	setting.	
-Xmx	<size>	M	|G	Specifies	the	maximum	amount	of	memory	Fortify	Static	Code	Analyzer	
uses.
Heap	sizes	between	32	Â GB	and	48	Â GB	are	not	advised	due	to	internal	JVM	
implementations.	Heap	sizes	in	this	range	perform	worse	than	at	32	GB.	
Heap	sizes	smaller	than	32	Â GB	are	optimized	by	the	JVM.	If	your	scan	
requires	more	than	32	Â GB,	then	you	probably	need	64	Â GB	or	more.	As	a	
guideline,	assuming	no	other	memory	intensive	processes	are	running,	do	
not	allocate	more	than	2/3	of	the	available	memory.	
When	you	specify	this	option,	make	sure	that	you	do	not	allocate	more	
memory	than	is	physically	available,	because	this	degrades	performance.	
As	aguideline,	and	the	assumption	that	no	other	memory	intensive	
processes	are	running,	do	not	allocate	more	than	2/3	of	the	available	
memory.
Note:	Specifying	this	option	overrides	the	default	memory	allocation	
you	would	get	with	the	-autoheap	option.	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	121	of	216 

Directives
Use	the	following	directives	to	list	information	about	previous	translation	commands.	Use	only	one	
directive	at	atime	and	do	not	use	any	directive	in	conjunction	with	normal	translation	or	analysis	
commands.
Directive	Description	
-clean	Deletes	all	Fortify	Static	Code	Analyzer	intermediate	files	and	build	
records.	If	abuild	ID	is	specified,	only	files	and	build	records	relating	to	
that	build	ID	are	deleted.	
-show-	binaries	Displays	all	objects	that	were	created	but	not	used	in	the	production	of	
any	other	binaries.	If	fully	integrated	into	the	build,	it	lists	all	of	the	
binaries	produced.	
-show-	build-	ids	Displays	alist	of	all	known	build	IDs.	
-show-	build-	tree	When	you	scan	with	the	-bin	option,	displays	all	files	used	to	create	the	
binary	and	all	files	used	to	create	those	files	in	atree	layout.	If	the	-bin	
option	is	not	present,	the	tree	is	displayed	for	each	binary.	
Note:	This	option	can	generate	an	extensive	amount	of	information.	
-show-	build-	
warnings	
Use	with	-b	<build_	id>	to	show	any	errors	and	warnings	that	occurred	
in	the	translation	phase	on	the	console.	
Note:	Fortify	Audit	Workbench	also	displays	these	errors	and	
warnings	in	the	results	certification	panel.	
-show-	files	Lists	the	files	in	the	specified	build	ID.	When	the	-bin	option	is	present,	
displays	only	the	source	files	that	went	into	the	binary.	
-show-	loc	Displays	the	number	of	lines	in	the	code	being	translated.	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	122	of	216 

Specifying	Files	and	Directories	
File	specifiers	are	expressions	that	allow	you	to	pass	along	list	of	files	or	adirectory	to	Fortify	Static	
Code	Analyzer	using	wildcard	characters.	Fortify	Static	Code	Analyzer	recognizes	two	types	of	wildcard	
characters:	asingle	asterisk	character	(*)	matches	part	of	afile	name,	and	double	asterisk	characters	(**)	
recursively	matches	directories.	You	can	specify	one	or	more	files,	one	or	more	file	specifiers,	or	a	
combination	of	files	and	file	specifiers.	
<files>	|	<file_	dir_	specifiers>	
Note:	File	specifiers	do	not	apply	to	C,	C++,	or	Objective-	C++.	
The	following	table	describes	examples	of	file	and	directory	specifiers.	
File	/Directory	Specifier	Description	
<dir>
<dir>	/**/*	
Matches	all	files	in	the	named	directory	and	any	subdirectories	
or	the	named	directory	when	used	for	adirectory	parameter.	
<dir>	/**/Example.java	Matches	any	file	named	Example.java	found	in	the	named	
directory	or	any	subdirectories.	
<dir>	/*.java	
<dir>	/*.jar	
Matches	any	file	with	the	specified	extension	found	in	the	
named	directory.	
<dir>	/**/*.kt	
<dir>	/**/*.jar	
Matches	any	file	with	the	specified	extension	found	in	the	
named	directory	or	any	subdirectories.	
<dir>	/**/beta/**	Matches	all	directories	and	files	found	in	the	named	directory	
that	have	beta	in	the	path,	including	beta	as	afile	name.	
<dir>	/**/classes/	Matches	all	directories	and	files	with	the	name	classes	found	
in	the	named	directory	and	any	subdirectories.	
**/test/**	Matches	all	files	in	the	current	directory	tree	that	have	atest	
element	in	the	path,	including	test	as	afile	name.	
**/webgoat/*	Matches	all	files	in	any	webgoat	directory	in	the	current	
directory	tree.	
Matches:
l	/src/main/java/org/owasp/webgoat	
l	/test/java/org/owasp/webgoat	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	123	of	216 

File	/Directory	Specifier	Description
Does	not	match	(assignments	directory	does	not	match)	
l	/test/java/org/owasp/webgoat/assignments	
Note:	Windows	and	many	Linux	shells	automatically	expand	parameters	that	contain	the	asterisk	
character	(*),	so	you	must	enclose	file-	specifier	expressions	in	quotes.	Also,	on	Windows,	you	can	
use	the	backslash	character	(\)	as	the	directory	separator	instead	of	the	forward	slash	(/).	
User	Guide	
Chapter	17:	Command-	Line	Interface	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	124	of	216 

Chapter	18:	Command-	Line	Utilities	
This	section	contains	the	following	topics:	
Fortify	Static	Code	Analyzer	Utilities	125	
About	Updating	Security	Content	126	
Working	with	FPR	Files	from	the	Command	Line	128	
Generating	Reports	from	the	Command	Line	135	
Checking	the	Fortify	Static	Code	Analyzer	Scan	Status	139	
Fortify	Static	Code	Analyzer	Utilities	
Fortify	Static	Code	Analyzer	command-	line	utilities	enable	you	to	manage	Fortify	Security	Content	and	
FPR	files,	run	reports,	perform	post-	installation	configuration,	and	monitor	scans.	These	utilities	are	
located	in	<sca_	install_	dir>	/bin	.The	utilities	for	Windows	are	provided	as	.bat	or	.cmd	files.	The	
following	table	describes	the	utilities.	
Note:	By	default,	log	files	for	most	of	the	utilities	are	written	to	the	following	directory:	
l	On	Windows:	C:\Users\	<username>	\AppData\Local\Fortify\	<utility_	name>	-	
<version>	\log	
l	On	Linux	and	macOS	:<userhome>	/.fortify/	<utility_	name>	-<version>	\log	
Utility	Description	
More
Information	
fortifyupdate	Compares	installed	security	content	to	the	current	version	
and	makes	any	required	updates	
""About
Updating
Security
Content""	on	
the	next	page	
FPRUtility	With	this	utility	you	can:	
l	Merge	audited	projects	
l	Verify	FPR	signatures	
l	Display	mappings	for	amigrated	project	
l	Display	any	errors	associated	with	an	FPR	
l	Display	the	number	of	issues	in	an	FPR	
""Working	with	
FPR	Files	
from	the	
Command
Line""	on	
page	Â 128	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	125	of	216 

Utility	Description	
More
Information	
l	Display	filtered	lists	of	issues	in	different	formats	
l	Display	table	of	analyzed	functions	
l	Combine	or	split	source	code	files	and	audit	projects	into	
FPR	files	
BIRTReportGenerator
ReportGenerator	
Generates	BIRT	Â reports	and	legacy	reports	from	FPR	files	""Generating
Reports	from	
the	Command	
Line""	on	
page	Â 135	
scapostinstall	After	you	install	Fortify	Static	Code	Analyzer	,this	utility	
enables	you	to	migrate	properties	files	from	aprevious	
version	of	Fortify	Static	Code	Analyzer	,specify	alocale,	
and	specify	aproxy	server	for	security	content	updates	and	
for	Fortify	Software	Security	Center	.	
""Running	the	
Post-	Install	
Tool""	on	
page	Â 38	
SCAState	Provides	state	analysis	information	on	the	JVM	during	the	
scan	phase	
""Checking	the	
Fortify	Static	
Code
Analyzer	Scan	
Status""	on	
page	Â 139	
About	Updating	Security	Content	
You	can	use	the	fortifyupdate	utility	to	download	the	latest	Fortify	Secure	Â Coding	Rulepacks	and	
metadata	from	the	Fortify	Customer	Portal	for	your	installation.	
The	fortifyupdate	utility	gathers	information	about	the	existing	security	content	in	your	Fortify	
installation	and	contacts	the	update	server	with	this	information.	The	server	returns	new	or	updated	
security	content,	and	removes	any	obsolete	security	content	from	your	Fortify	Static	Code	Analyzer	
installation.	If	your	installation	is	current,	amessage	is	displayed	to	that	effect.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	126	of	216 

Updating	Security	Content	
Use	the	fortifyupdate	utility	to	either	download	security	content	or	import	alocal	copy	of	the	
security	content.	This	utility	is	located	in	the	<sca_	install_	dir>	/bin	directory.	
To	update	your	Fortify	Static	Code	Analyzer	installation	with	the	latest	Fortify	Secure	Â Coding	
Rulepacks	and	external	metadata	from	the	Fortify	Customer	Portal	,type	the	following	command:	
fortifyupdate	[<options>	]	
fortifyupdate	Command-	Line	Options	
The	following	table	lists	the	fortifyupdate	options.	
fortifyUpdate	Option	Description	
-import	<file>	.zip	Imports	the	ZIP	Â file	that	contains	archived	security	
content.	Rulepacks	are	extracted	to	the	<sca_	
install_	dir>	/Core/config/rules	directory.	
-coreDir	<dir>	Specifies	acore	directory	where	the	update	is	stored.	If	
this	is	not	specified,	the	update	is	made	in	the	<sca_	
install_	dir>	.	
Important!	Make	sure	that	you	copy	the	contents	
of	the	<sca_	install_	dir>	/config/keys	folder	
and	paste	it	to	aconfig/keys	folder	in	this	
directory	before	you	run	fortifyupdate.	
-includeMetadata	Specifies	to	only	update	external	metadata.	
-includeRules	Specifies	to	only	update	Rulepacks.	
-locale	<locale>	Specifies	alocale.	The	default	is	the	value	set	for	the	
locale	property	in	the	fortify.properties	
configuration	file.	
For	more	information	about	the	fortify.properties	
configuration	file,	see	the	Micro	Focus	Fortify	Static	
Code	Analyzer	Tools	Properties	Reference	Guide	.	
-proxyhost	<host>	Specifies	aproxy	server	network	name	or	IP	Â address.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	127	of	216 

fortifyUpdate	Option	Description	
-proxyport	<port>	Specifies	aproxy	server	port	number.	
-proxyUsername
<username>	
If	the	proxy	server	requires	authentication,	specifies	the	
user	name.	
-proxyPassword
<password>	
If	the	proxy	server	requires	authentication,	specifies	the	
password.	
-showInstalledRules	Displays	the	currently	installed	Rulepacks	including	any	
custom	rules	or	metadata.	
-showInstalledExternalMetadata	Displays	the	currently	installed	external	metadata.	
-url	<url>	Specifies	aÂ URL	from	which	to	download	the	security	
content.	The	default	URL	Â is	
https://update.fortify.com	or	the	value	set	for	
the	rulepackupdate.server	property	in	the	
server.properties	configuration	file.	
For	more	information	about	the	server.properties	
configuration	file,	see	the	Micro	Focus	Fortify	Static	
Code	Analyzer	Tools	Properties	Reference	Guide	.	
-acceptKey	Accept	the	public	key.	When	this	is	specified,	you	are	not	
prompted	to	provide	apublic	key.	Use	this	option	to	
accept	the	public	key	if	you	are	updating	from	anon-	
standard	location	(that	you	specify	with	the	-url	
option).	
-acceptSSLCertificate	Use	the	SSL	Â certificate	provided	by	the	server.	
Working	with	FPR	Files	from	the	Command	Line	
Use	the	FPRUtility	that	is	located	in	the	bin	directory	of	your	Fortify	Static	Code	Analyzer	installation	to	
perform	the	following	tasks:	
l	""Merging	FPR	Files""	on	the	next	page	
l	""Displaying	Analysis	Results	Information	from	an	FPR	File""	on	page	Â 130	
l	""Extracting	aSource	Archive	from	an	FPR	File""	on	page	Â 134	
l	""Allocating	More	Memory	for	FPRUtility""	on	page	Â 135	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	128	of	216 

Merging	FPR	Files	
The	FPRUtility	-merge	option	combines	the	analysis	information	from	two	FPR	files	into	asingle	FPR	
file	using	the	values	of	the	primary	project	to	resolve	conflicts.	
To	merge	FPR	files:	
FPRUtility	-merge	-project	<primary>	.fpr	-source	<secondary>	.fpr	\	
-f	<output>	.fpr	
To	merge	FPR	files	and	set	instance	ID	Â migrator	options:	
FPRUtility	-merge	-project	<primary>	.fpr	-source	<secondary>	.fpr	\	
-f	<output>	.fpr	-iidmigratorOptionsÂ ""	<iidmigrator_	options>	""	
FPRUtility	Data	Merge	Options	
The	following	table	lists	the	FPRUtility	options	that	apply	to	merging	data.	
FPRUtility	Option	Description	
-merge	Merges	the	specified	project	and	source	FPR	files.	
-project	<primary>	.fpr	Specifies	the	primary	FPR	Â file	to	merge.	Conflicts	are	resolved	using	
the	values	in	this	file.	
-source	<secondary>	.fpr	Specifies	the	secondary	FPR	Â file	to	merge.	The	primary	project	
overrides	values	if	conflicts	exist.	
-f	<output>	.fpr	Specifies	the	name	of	the	merged	output	file.	This	file	is	the	result	
of	the	merged	files.	
Note:	When	you	specify	this	option,	neither	of	the	original	FPR	
files	are	modified.	If	you	do	not	use	this	option,	the	primary	
FPR	is	overwritten	with	the	merged	results.	
-forceMigration	Forces	the	migration,	even	if	the	engine	and	the	Rulepack	versions	
of	the	two	projects	are	the	same.	
-useMigrationFile
<mapping_	file>	
Specifies	an	instance	ID	mapping	file.	This	enables	you	to	modify	
mappings	manually	rather	than	using	the	migration	results.	Supply	
your	own	instance	ID	mapping	file.	
-useSourceIssueTemplate	Specifies	to	use	the	filter	sets	and	folders	from	the	issue	template	in	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	129	of	216 

FPRUtility	Option	Description
the	secondary	FPR.	By	default,	Fortify	Static	Code	Analyzer	uses	
the	filter	sets	and	folders	from	the	issue	template	in	the	primary	
FPR.	
-iidmigratorOptions
<iidmigrator_	options>	
Specifies	instance	ID	Â migrator	options.	Separate	included	options	
with	spaces	and	enclosed	them	in	quotes.	Some	valid	options	are:	
l	-i	provides	acase-	sensitive	file	name	comparison	of	the	merged	
files	
l	-u	<scheme_	file>	tells	iidmigrator	to	read	the	matching	
scheme	from	<scheme_	file>	for	instance	ID	Â migration	
Note:	Wrap	-iidmigrator	options	in	single	quotes	('-	u	
<scheme_	file>	')when	working	from	aCygwin	command	
prompt.
Windows	example:	
FPRUtility	-merge	-project	<primary>	.fpr	
-source	<secondary>	.fpr	-f	<output>	.fpr	
-iidmigratorOptions	""-	u	scheme_	file""	
-debug	Displays	debug	information	that	can	be	helpful	to	troubleshoot	
issues	with	Â FPRUtility.	
FPRUtility	Data	Merge	Exit	Codes	
Upon	completion	of	the	-merge	command,	FPRUtility	provides	one	of	the	exit	codes	described	in	the	
following	table.	
Exit	Code	Description	
0	The	merge	completed	successfully.	
5	The	merge	failed.	
Displaying	Analysis	Results	Information	from	an	FPR	File	
The	FPRUtility	-information	option	displays	information	about	the	analysis	results.	You	can	obtain	
information	to:	
l	Validate	signatures	
l	Examine	any	errors	associated	with	the	FPR	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	130	of	216 

l	Obtain	the	number	of	issues	for	each	analyzer,	vulnerability	category,	or	custom	grouping	
l	Obtain	lists	of	issues	(including	some	basic	information).	You	can	filter	these	lists.	
To	display	project	signature	information:	
FPRUtility	-information	-signature	-project	<project>	.fpr	-f	<output>	.txt	
To	display	afull	analysis	error	report	for	the	FPR:	
FPRUtility	-information	-errors	-project	<project.fpr>	-f	<output>	.txt	
To	display	the	number	of	issues	per	vulnerability	category	or	analyzer:	
FPRUtility	-information	-categoryIssueCounts	-project	<project>	.fpr	
FPRUtility	-information	-analyzerIssueCounts	-project	<project>	.fpr	
To	display	the	number	of	issues	for	acustom	grouping	based	on	asearch:	
FPRUtility	-information	-search	-queryÂ ""search	expression""	\	
[-	categoryIssueCounts]	[-	analyzerIssueCounts]	\	
[-	includeSuppressed]	[-	includeRemoved]	\	
-project	<project>	.fpr	-f	<output>	.txt	
Note:	By	default,	the	result	does	not	include	suppressed	and	removed	issues.	To	include	
suppressed	or	removed	issues,	use	the	-includeSuppressed	or	-includeRemoved	options.	
To	display	information	for	issues	in	CSV	Â format:	
FPRUtility	-information	-listIssues	\	
-search	[-	queryAll	|	-query	""search	expression""]	\	
[-	categoryIssueCounts]	[-	analyzerIssueCouts]	\	
[-	includeSuppressed]	[-	includeRemoved]	\	
-project	<project>	.fpr	-f	<output>	.csv	-outputFormat	CSV	
FPRUtility	Information	Options	
The	following	table	lists	the	FPRUtility	options	that	apply	to	project	information.	
FPRUtility	Option	Description	
-information	Displays	information	for	the	project.	
One	of:	
-signature
-mappings
-errors	
The	-signature	option	displays	the	signature.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	131	of	216 

FPRUtility	Option	Description	
-versions
-functionsMeta
-categoryIssueCounts
-analyzerIssueCounts
-search	-query	<search_	expression>	
-search	-queryAll	
The	-mappings	option	displays	the	migration	
mappings	report.	
The	-errors	option	displays	afull	error	report	for	the	
FPR.
The	-versions	option	displays	the	engine	version	
and	the	Rulepack	version	used	in	the	static	scan.	
The	-functionsMeta	option	displays	all	functions	
that	the	static	analyzer	encountered	in	CSV	format.	To	
filter	which	functions	are	displayed,	include	-	
excludeCoveredByRules	,and	-	
excludeFunctionsWithoutSource	.	
The	-categoryIssueCounts	option	displays	the	
number	of	issues	for	each	vulnerability	category.	
The	-analyzerIssueCounts	option	displays	the	
number	of	issues	for	each	analyzer.	
The	-search	-query	option	displays	the	number	of	
issues	in	the	result	of	your	specified	search	expression.	
To	display	the	number	of	issues	per	vulnerability	
category	or	analyzer,	add	the	optional	-	
categoryIssueCounts	and	-	
analyzerIssueCounts	options	to	the	search	option.	
Use	the	-includeSuppressed	and	-	
includeRemoved	options	to	include	suppressed	or	
removed	issues.	
The	-search	-queryAll	searches	all	the	issues	in	
the	FPR,	including	suppressed	and	removed	issues.	
-project	<project>	.fpr	Specifies	the	FPR	from	which	to	extract	the	results	
information.	
-f	<output>	Specifies	the	output	file.	The	default	is	System.out	.	
-outputformat	<format>	Specifies	the	output	format.	The	valid	values	are	TEXT	
and	CSV	.The	default	value	is	TEXT	.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	132	of	216 

FPRUtility	Option	Description	
-listIssues	Displays	the	location	for	each	issue	in	one	of	the	
following	formats:	
<sink_	filename>	:<line_	num>	or	
<sink_	filename>	:<line_	num>	(<category>	
|Â <analyzer>	)	
You	can	also	use	the	-listIssues	option	with	-	
search	and	with	both	issueCounts	grouping	options.	
If	you	group	by	-categoryIssueCounts	,then	the	
output	includes	(<analyzer>	)and	if	you	group	by	-	
analyzerIssueCounts	,then	the	output	includes	
(<category>	).	
If	you	specify	the	-outputFormat	CSV	,then	each	
issue	is	displayed	as	aline	in	the	format:	
""<instanceid>	"",	""<category>	"",	
""<sink_	filename>	:<line_	num>	"",	
""<analyzer>	""	
-debug	Displays	debug	information	that	can	be	helpful	to	
troubleshoot	issues	with	Â FPRUtility.	
FPRUtility	Signature	Exit	Codes	
Upon	completion	of	the	-information	-signature	command,	FPRUtility	provides	one	of	the	exit	
codes	described	in	the	following	table.	
Exit	Code	Description	
0	The	project	is	signed,	and	all	the	signatures	are	valid.	
1	The	project	is	signed,	and	some,	but	not	all,	of	the	signatures	passed	the	validity	test.	
2	The	project	is	signed	but	none	of	the	signatures	are	valid.	
3	The	project	had	no	signatures	to	validate.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	133	of	216 

Extracting	a	Source	Archive	from	an	FPR	File	
The	FPRUtility	-sourceArchive	option	creates	asource	archive	(FSA)	file	from	aspecified	FPR	file	
and	removes	the	source	code	from	the	FPR	file.	You	can	extract	the	source	code	from	an	FPR	file,	
merge	an	existing	source	archive	(FSA)	Â back	into	an	FPR	file,	or	recover	source	files	from	asource	
archive.
To	archive	data:	
FPRUtility	-sourceArchive	-extract	-project	<project>	.fpr	-f	
<outputArchive>	.fsa	
To	archive	data	to	afolder:	
FPRUtility	-sourceArchive	-extract	-project	<project>	.fpr	\	
-recoverSourceDirectory	-f	<output_	folder>	
To	add	an	archive	to	an	FPR	file:	
FPRUtility	-sourceArchive	-mergeArchive	-project	<project>	.fpr	\	
-source	<old_	source_	archive>	.fsa	-f	<project_	with_	archive>	.fpr	
To	recover	files	that	are	missing	from	an	FPR	Â file:	
FPRUtility	-sourceArchive	-fixSecondaryFileSources	\	
-payload	<source_	archive>	.zip	-project	<project>	.fpr	-f	<output>	.fpr	
FPRUtility	Source	Archive	Options	
The	following	table	lists	the	FPRUtility	options	that	apply	to	working	with	the	source	archive.	
FPRUtility	Option	Description	
-sourceArchive	Creates	an	FSA	file	so	that	you	can	extract	asource	
archive.	
One	of:	
-extract
-mergeArchive
-fixSecondaryFileSources	
Use	the	-extract	option	to	extract	the	contents	of	
the	FPR	file.	
Use	the	-mergeArchive	option	to	merge	the	contents	
of	the	FPR	file	with	an	existing	archived	file	(-source	
option).	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	134	of	216 

FPRUtility	Option	Description
Use	the	-fixSecondaryFileSources	option	to	
recover	source	files	from	asource	archive	(-payload	
option)	Â missing	from	an	FPR	file.	
-project	<project>	.fpr	Specifies	the	FPR	to	archive.	
-recoverSourceDirectory	Use	with	the	-extract	option	to	extract	the	source	as	
afolder	with	restored	source	files.	
-source	<old_	source_	archive>	.fsa	Specifies	the	name	of	the	existing	archive.	Use	only	if	
you	are	merging	an	FPR	file	with	an	existing	archive	(-	
mergeArchive	option).	
-payload	<source_	archive>	.zip	Use	with	the	-fixSecondaryFileSources	option	to	
specify	the	source	archive	from	which	to	recover	source	
files.	
-f	<project_	with_	archive>	.fpr	|	
<output_	archive>	.fsa	|	
<output_	folder>	
Specifies	the	output	file.	You	can	generate	an	FPR,	a	
folder,	or	an	FSA	file.	
-debug	Displays	debug	information	that	can	be	helpful	to	
troubleshoot	issues	with	Â FPRUtility.	
Allocating	More	Memory	for	FPRUtility	
Performing	tasks	with	large	and	complex	FPR	files	could	trigger	out-	of-	memory	errors.	By	default,	
1000	Â MB	is	allocated	for	FPRUtility.	To	increase	the	memory,	add	the	-Xmx	option	to	the	command	line.	
For	example,	to	allocate	2Â GB	for	FPRUtility,	use	the	following	command:	
FPRUtility	-Xmx2G	-merge	-project	<primary>	.fpr	-source	<secondary>	.fpr	\	
-f	<output>	.fpr	
Generating	Reports	from	the	Command	Line	
There	are	two	command-	line	utilities	to	generate	reports:	
l	BIRTReportGeneratorâ	Produces	reports	that	are	based	on	the	Business	Intelligence	and	Reporting	
Technology	(BIRT)	system	from	FPR	Â files.	
Note:	If	you	are	using	atext-	based	Linux	system	running	OpenJDK,	you	must	install	DejaVu	
Sans	and	DejaVu	Serif	fonts	to	successfully	generate	BIRT	reports.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	135	of	216 

l	ReportGeneratorâ	Generates	legacy	reports	from	FPR	files.	You	can	specify	areport	template,	
otherwise	adefault	report	template	is	used.	See	the	Micro	Focus	Fortify	Audit	Workbench	User	
Guide	for	adescription	of	the	available	report	templates.	
Generating	a	BIRT	Report	
The	basic	command-	line	syntax	to	generate	aBIRT	report	is:	
BIRTReportGenerator	-template	<template_	name>	
-source	<audited_	proj>	.fpr	-format	<format>	
-output	<report_	file>	
The	following	is	an	example	of	how	to	generate	an	OWASP	Â Top	10	2017	report	with	additional	options:	
BIRTReportGenerator	-template	""owaspÂ top	10""	-source	auditedProj.fpr	
-format	pdf-	showSuppressed	--Version	""owaspÂ top	10	2017""	
--UseFortifyPriorityOrder	-output	MyOWASP_	Top10_	Rpt.pdf	
BIRTReportGenerator	Command-	Line	Options	
The	following	table	lists	the	BIRTReportGenerator	options.	
BIRTReportGenerator	Option	Description	
-template	<template_	name>	(Required)	Â Specifies	the	report	template	name.	The	valid	
values	for	<template_	name>	Â are	Â ""CWE	Top	25	2019""	,	
""CWE/SANS	Top	25""	,""Developer	Workbook""	,""DISA	
CCIÂ 2""	,""DISAÂ STIG""	,""FISMAÂ Compliance""	,GDPR	,	
MISRA	,""OWASP	ASVS	4.0""	,""OWASPÂ Mobile	Top	10""	,	
""OWASP	Top	10""	,""PCIÂ DSSÂ Compliance""	and	
""PCIÂ SSFÂ Compliance""	.	
Note:	You	only	need	to	enclose	the	report	template	
name	in	quotes	if	aspace	exists	in	the	<template	
name>	.The	template	name	values	are	not	case-	
sensitive.	
-source	<audited_	proj>	.fpr	(Required)	Â Specifies	the	audited	project	on	which	to	base	
the	report.	
-format	pdf	|Â doc	Â |html	|xls	(Required)	Â Specifies	the	generated	report	format.	
Note:	The	format	values	are	not	case-	sensitive.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	136	of	216 

BIRTReportGenerator	Option	Description	
-output	<report_	file.***>	(Required)	Â Specifies	the	file	to	which	the	report	is	written.	
-searchQuery	<query>	Specifies	aÂ search	query	to	filter	issues	before	generating	
the	report.	For	example:	
-searchQuery	audited:false	
For	adescription	of	the	search	query	syntax,	see	the	Micro	
Focus	Fortify	Audit	Workbench	User	Guide	.	
-showSuppressed	Include	issues	that	are	marked	as	suppressed.	
-showRemoved	Include	issues	that	are	marked	as	removed.	
-showHidden	Include	issues	that	are	marked	as	hidden.	
-filterSet	<filterset_	name>	Specifies	afilter	set	to	use	to	generate	the	report	(for	
example,	-filterSet	""Quick	View""	).	
--Version	<version>	Specifies	the	version	for	the	template.	The	valid	values	for	
the	template	versions	are	listed	below.	
Note:	Templates	that	are	not	listed	here	have	only	
one	version	available.	
If	you	do	not	specify	aversion	when	multiple	
versions	are	available,	the	most	recent	version	
based	on	the	external	metadata	used	when	the	
FPR	Â was	created	is	used	by	default.	The	template	
version	values	are	not	case-	sensitive.	
l	For	the	""CWE/SANS	Top	25""	template,	the	version	is	
""<year>	CWE/SANSÂ Top	25""	(for	example,	""2011	
CWE/SANS	Top	25""	)	
l	For	the	""DISA	STIG""	template,	the	version	
isÂ ""DISAÂ STIGÂ 	<version>	""	(for	example,	
""DISAÂ STIGÂ 4.10""	)	
l	For	the	MISRA	template,	the	available	versions	are	
""MISRAÂ CÂ 2012""	or	""MISRAÂ C++Â 2008""	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	137	of	216 

BIRTReportGenerator	Option	Description
l	For	the	""OWASP	Top	10""	template,	the	version	is	
""OWASPÂ TopÂ 10	<year>	""	(for	example,	""OWASP	Top	
10	2017""	)	
l	For	the	""PCI	DSS	Compliance""	template,	the	version	
isÂ ""<version>	Compliance""	(for	example,	""3.2	
Compliance""	)	
--IncludeDescOfKeyTerminology	Include	the	Description	of	Key	Terminology	section	in	the	
report.	
--IncludeAboutFortify	Include	the	About	Fortify	Solutions	section	in	the	report.	
--SecurityIssueDetails	Provide	detailed	descriptions	of	reported	issues.	This	
option	is	not	available	for	the	Developer	Workbook	
template.	
--UseFortifyPriorityOrder	Use	Fortify	Priority	Order	instead	of	folder	names	to	
categorize	issues.	This	option	is	not	available	for	the	
Developer	Workbook	and	PCI	Compliance	templates.	
-h	|-help	Displays	detailed	information	about	the	options.	
-debug	Displays	debug	information	that	can	be	helpful	to	
troubleshoot	issues	with	Â BIRTReportGenerator.	
Generating	a	Legacy	Report	
To	generate	aPDF	report,	type	the	following	command:	
ReportGenerator	-format	pdf	-f	<myreport>	.pdf	-source	<myresults>	.fpr	
To	generate	an	XML	report,	type	the	following	command:	
ReportGenerator	-format	xml	-f	<myreport>	.xml	-source	<myresults>	.fpr	
ReportGenerator	Command-	Line	Options	
The	following	table	lists	the	ReportGenerator	options.	
ReportGenerator	Option	Description	
-format	pdf	|xml	(Required)	Specifies	the	generated	report	format.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	138	of	216 

ReportGenerator	Option	Description	
-source	<audited_	proj>	.fpr	(Required)	Â Specifies	the	audited	project	on	which	to	base	the	
report.	
-f	<report_	file.***>	(Required)	Â Specifies	the	file	to	which	the	report	is	written.	
-template	<template_	name>	Specifies	the	report	template.	If	not	specified,	ReportGenerator	
uses	the	default	template.	The	default	template	is	located	in	
<sca_	install_	dir>	
/Core/config/reports/DefaultReportDefinition.xm
l.
Note:	Enclose	the	<template_	name>	in	quotes	if	it	contains	
any	spaces.	
-user	<username>	Specifies	auser	name	to	add	to	the	report.	
-showSuppressed	Include	issues	marked	as	suppressed.	
-showRemoved	Include	issues	marked	as	removed.	
-showHidden	Include	issues	marked	as	hidden.	
-filterSet	<filterset_	
name>	
Specifies	afilter	set	to	use	to	generate	the	report	(for	example,	
-filterset	""Quick	View""	).	
-verbose	Displays	status	messages	to	the	console.	
-debug	Displays	debug	information	that	can	be	helpful	to	troubleshoot	
issues	with	Â ReportGenerator.	
-h	Displays	detailed	information	about	the	options.	
Checking	the	Fortify	Static	Code	Analyzer	Scan	
Status
Use	the	SCAState	utility	to	see	up-	to-	date	state	analysis	information	during	the	scan	phase.	
To	check	Fortify	Static	Code	Analyzer	state:	
1.	Run	aFortify	Static	Code	Analyzer	scan.	
2.	Open	another	command	window.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	139	of	216 

3.	Type	the	following	at	the	command	prompt:	
SCAState	[<options>	]	
SCAState	Utility	Command-	Line	Options	
The	following	table	lists	the	SCAState	utility	options.	
SCAState	Option	Description	
-a	|Â 	
--all	
Displays	all	available	information.	
-debug	Displays	information	that	is	useful	to	debug	SCAState	behavior.	
-ftd	|Â 	
--full-	thread-	dump	
Prints	athread	dump	for	every	thread.	
-h	|Â --help	Displays	the	help	information	for	the	SCAState	utility.	
-hd	<filename>	|Â 	
--heap-	dump	<filename>	
Specifies	the	file	to	which	the	heap	dump	is	written.	The	file	is	
interpreted	relative	to	the	remote	scanâs	working	directory;	this	is	not	
necessarily	the	same	directory	where	you	are	running	SCAState.	
-liveprogress	Displays	the	ongoing	status	of	arunning	scan.	This	is	the	default.	If	
possible,	this	information	is	displayed	in	aseparate	terminal	window.	
-nogui	Causes	the	Fortify	Static	Code	Analyzer	Â state	information	to	display	
in	the	current	terminal	window	instead	of	in	aseparate	window.	
-pi	|	
--program-	info	
Displays	information	about	the	source	code	being	scanned,	including	
how	many	source	files	and	functions	it	contains.	
-pid	<process_	id>	Specifies	the	currently	running	Fortify	Static	Code	Analyzer	Â process	
ID.	Use	this	option	if	there	are	multiple	Fortify	Static	Code	Analyzer	
processes	running	simultaneously.	
To	obtain	the	process	ID	on	Windows	systems:	
1.	Open	acommand	window.	
2.	Type	tasklist	at	the	command	prompt.	
A	list	of	processes	is	displayed.	
3.	Find	the	java.exe	process	in	the	list	and	note	its	PID.	
To	find	the	process	ID	on	Linux	systems:	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	140	of	216 

SCAState	Option	Description
l	Type	ps	aux	|	grep	sourceanalyzer	at	the	command	
prompt.	
-progress	Displays	scan	information	up	to	the	point	at	which	the	command	is	
issued.	This	includes	the	elapsed	time,	the	current	phase	of	the	
analysis,	and	the	number	of	results	already	obtained.	
-properties	Displays	configuration	settings	(this	does	not	include	sensitive	
information	such	as	passwords).	
-scaversion	Displays	the	Fortify	Static	Code	Analyzer	version	number	for	the	
sourceanalyzer	that	is	currently	running.	
-td	|Â 	
--thread-	dump	
Prints	athread	dump	for	the	main	scanning	thread.	
-timers	Displays	information	from	the	timers	and	counters	that	are	
instrumented	in	Fortify	Static	Code	Analyzer	.	
-version	Displays	the	SCAState	version.	
-vminfo	Displays	the	following	statistics	that	JVM	standard	MXBeans	
provides:	ClassLoadingMXBean,	CompilationMXBean,	
GarbageCollectorMXBeans,	MemoryMXBean,	
OperatingSystemMXBean,	RuntimeMXBean,	and	ThreadMXBean.	
<none>	Displays	scan	progress	information	(this	is	the	same	as	-progress	).	
Note:	Fortify	Static	Code	Analyzer	writes	Java	process	information	to	the	location	of	the	TMP	
system	environment	variable.	On	Windows	systems,	the	TMP	system	environment	variable	location	
is	C:\Users\	<userID>	\AppData\Local\Temp	.If	you	change	this	TMP	system	environment	
variable	to	point	to	adifferent	location,	SCAState	cannot	locate	the	sourceanalyzer	Java	
process	and	does	not	return	the	expected	results.	To	resolve	this	issue,	change	the	TMP	system	
environment	variable	to	match	the	new	TMP	location.	Fortify	recommends	that	you	run	SCAState	
as	an	administrator	on	Windows.	
User	Guide	
Chapter	18:	Command-	Line	Utilities	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	141	of	216 

Chapter	19:	Improving	Performance	
This	chapter	provides	guidelines	and	tips	to	optimize	memory	usage	and	performance	when	analyzing	
different	types	of	codebases	with	Fortify	Static	Code	Analyzer	.	
This	section	contains	the	following	topics:	
Hardware	Considerations	142	
Sample	Scans	143	
Tuning	Options	144	
Quick	Scan	145	
Configuring	Scan	Speed	with	Speed	Dial	146	
Breaking	Down	Codebases	147	
Limiting	Analyzers	and	Languages	148	
Optimizing	FPR	Â Files	149	
Monitoring	Long	Running	Scans	153	
Hardware	Considerations	
The	variety	of	source	code	makes	accurate	predictions	of	memory	usage	and	scan	times	impossible.	The	
factors	that	affect	memory	usage	and	performance	consists	of	many	different	factors	including:	
l	Code	type	
l	Codebase	size	and	complexity	
l	Ancillary	languages	used	(such	as	JSP,	JavaScript,	and	HTML)	
l	Number	of	vulnerabilities	
l	Type	of	vulnerabilities	(analyzer	used)	
Fortify	developed	the	following	set	of	""best	guess""	hardware	recommendations	based	on	real-	world	
application	scan	results.	The	following	table	lists	these	recommendations	based	on	the	complexity	of	the	
application.	In	general,	increasing	the	number	of	available	cores	might	improve	scan	times.	
Application
Complexity	CPU	Â Cores	
RAM	Â 	
(GB)	Description	
Simple	4	16	A	standalone	system	that	runs	on	aserver	or	desktop	such	as	
abatch	job	or	acommand-	line	utility.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	142	of	216 

Application
Complexity	CPU	Â Cores	
RAM	Â 	
(GB)	Description	
Medium	8	32	A	standalone	system	that	works	with	complex	computer	
models	such	as	atax	calculation	system	or	ascheduling	
system.	
Complex	16	128	A	three-	tiered	business	system	with	transactional	data	
processing	such	as	afinancial	system	or	acommercial	website.	
Very	Complex	32	256	A	system	that	delivers	content	such	as	an	application	server,	
database	server,	or	content	management	system.	
Note:	TypeScript	and	JavaScript	scans	increase	the	analysis	time	significantly.	If	the	total	lines	of	
code	in	an	application	consist	of	more	than	20%	TypeScript	or	JavaScript,	use	the	next	highest	
recommendation.
The	Micro	Focus	Fortify	Software	System	Requirements	document	describes	the	system	requirements.	
However,	for	large	and	complex	applications,	Fortify	Static	Code	Analyzer	requires	more	capable	
hardware.	This	includes:	
l	Disk	I/O	â	Fortify	Static	Code	Analyzer	is	I/O	intensive	and	therefore	the	faster	the	hard	drive,	the	
more	savings	on	the	I/O	transactions.	Fortify	recommends	a7,200	RPM	drive,	although	a10,000	
RPM	drive	(such	as	the	WD	Raptor)	or	an	SSD	drive	is	better.	
l	Memory	â	See	""Memory	Tuning""	on	page	Â 157	for	more	information	about	how	to	determine	the	
amount	of	memory	required	for	optimal	performance.	
l	CPU	â	Fortify	recommends	a2.1	GHz	or	faster	processor.	
Sample	Scans	
These	sample	scans	were	performed	using	Fortify	Static	Code	Analyzer	version	20.2.0	on	dedicated	
virtual	machines.	These	scans	were	run	using	Micro	Focus	Fortify	Software	Security	Content	2020	
Update	3.The	following	table	shows	the	scan	times	you	can	expect	for	several	common	open-	source	
projects.
Project	Name	Â 	Language	
Translation
Time	(mm:ss)	
Analysis	(Scan)	
Time	(mm:ss)	
Total
Issues	LOC	System	Configuration	
nasm	0.98.38	C/C++	00:27	06:01	1,254	12,074	Linux	VM	with	4Â CPUs	and	
32	Â GB	of	RAM	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	143	of	216 

Project	Name	Â 	Language	
Translation
Time	(mm:ss)	
Analysis	(Scan)	
Time	(mm:ss)	
Total
Issues	LOC	System	Configuration	
WebGoat	7.0.1	Java	00:16	00:51	421	3,628	Linux	VM	with	8Â CPUs	and	
32	Â GB	of	RAM	WordPress	Java	00:17	01:29	665	10,055	
CakePHP	PHP	00:25	01:56	2,354	54,564	
phpBB	3	PHP	00:29	02:11	1,273	39,581	
SharpZipLib	.NET	01:01	02:28	1,439	12,200	Windows	VM	with	8Â CPUs	
and	32	Â GB	of	RAM	
Hackademic-next	JavaScript	01:29	04:07	453	43,838	Linux	VM	with	8CPUs	and	
32	Â GB	of	RAM	prisma	TypeScript	00:57	02:39	52	22,911	
numpy-	1.13.3	Python	3	01:51	08:32	247	92,816	
MediaBrowser	Swift	01:24	01:37	23	6,768	macOS	VM	with	2Â CPUs	
and	8Â GB	of	RAM	
Tuning	Options	
Fortify	Static	Code	Analyzer	can	take	along	time	to	process	complex	projects.	The	time	is	spent	in	
different	phases:	
l	Translation	
l	Analysis	
Fortify	Static	Code	Analyzer	can	produce	large	analysis	result	files	(FPRs),	which	can	cause	along	time	
to	audit	and	upload	to	Micro	Focus	Fortify	Software	Security	Center	.This	is	referred	to	as	the	following	
phase:
l	Audit/Upload	
The	following	table	lists	tips	on	how	to	improve	performance	in	the	different	time-	consuming	phases.	
Phase	Option	Description	More	Information	
Translation	-export-	build-	
session
-import-	build-	
session	
Translate	and	scan	Â on	
different	machines	
""Mobile	Build	Sessions""	on	
page	Â 43	
Analysis	-quick	Run	aquick	scan	""Quick	Scan""	on	the	next	page	
Analysis	-scan-
precision	
Set	the	scan	precision	""Configuring	Scan	Speed	with	
Speed	Dial""	on	page	Â 146	
Analysis	-bin	Scan	the	files	related	to	a	""Breaking	Down	Codebases""	on	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	144	of	216 

Phase	Option	Description	More	Information	
binary	page	Â 147	
Analysis	-Xmx	<size>	M	|G	Set	maximum	heap	size	""Memory	Tuning""	on	page	Â 157	
Analysis	-Xss	<size>	M	|Â G	Set	stack	size	for	each	
thread	
""Memory	Tuning""	on	page	Â 157	
Analysis
Audit/Upload	
-filter	<file>	Apply	afilter	using	a	
filter	file	
""Filter	Files""	on	page	Â 149	
Analysis
Audit/Upload	
-disable-
source-
bundling	
Exclude	source	files	from	
the	FPR	file	
""Excluding	Source	Code	from	the	
FPR""	on	page	Â 150	
Quick	Scan	
Quick	scan	mode	provides	away	to	quickly	scan	your	projects	for	critical-	and	high-	priority	issues.	
Fortify	Static	Code	Analyzer	performs	the	scan	faster	by	reducing	the	depth	of	the	and	applying	the	
Quick	View	filter	set.	Quick	scan	settings	are	configurable.	For	more	details	about	the	configuration	of	
quick	scan	mode,	see	""fortify-	sca-	quickscan.properties""	on	page	Â 211	.	
Quick	scans	are	agreat	way	to	get	many	applications	through	an	assessment	so	that	you	can	quickly	
find	issues	and	begin	remediation.	The	performance	improvement	you	get	depends	on	the	complexity	
and	size	of	the	application.	Although	the	scan	is	faster	than	afull	scan,	it	does	not	provide	as	robust	a	
result	set.	Fortify	recommends	that	you	run	full	scans	whenever	possible.	
Limiters
The	depth	of	the	Fortify	Static	Code	Analyzer	analysis	sometimes	depends	on	the	available	resources.	
Fortify	Static	Code	Analyzer	uses	acomplexity	metric	to	trade	off	these	resources	with	the	number	of	
vulnerabilities	that	it	can	find.	Sometimes,	this	means	giving	up	on	aparticular	function	when	it	does	not	
look	like	Fortify	Static	Code	Analyzer	has	enough	resources	available.	
Fortify	Static	Code	Analyzer	enables	the	user	to	control	the	âcutoffâ	point	by	using	Fortify	Static	Code	
Analyzer	limiter	properties.	The	different	analyzers	have	different	limiters.	You	can	run	apredefined	set	
of	these	limiters	using	aquick	scan.	See	the	""fortify-	sca-	quickscan.properties""	on	page	Â 211	for	
descriptions	of	the	limiters.	
To	enable	quick	scan	mode,	use	the	-quick	option	with	-scan	option.	With	quick	scan	mode	enabled,	
Fortify	Static	Code	Analyzer	applies	the	properties	from	the	<sca_	install_	
dir>	/Core/config/fortify-	sca-	quickscan.properties	file,	in	addition	to	the	standard	
<sca_	install_	dir>	/Core/config/fortify-	sca.properties	file.	You	can	adjust	the	limiters	
that	Fortify	Static	Code	Analyzer	uses	by	editing	the	fortify-	sca-	quickscan.properties	file.	If	
you	modify	fortify-	sca.properties	,it	also	affects	quick	scan	behavior.	Fortify	recommends	that	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	145	of	216 

you	do	performance	tuning	in	quick	scan	mode,	and	leave	the	full	scan	in	the	default	settings	to	
produce	ahighly	accurate	scan.	For	description	of	the	quick	scan	mode	properties,	see	""Fortify	Static	
Code	Analyzer	Properties	Files""	on	page	Â 180	.	
Using	Quick	Scan	and	Full	Scan	
l	Run	full	scans	periodically	â	A	periodic	full	scan	is	important	as	it	might	find	issues	that	quick	scan	
mode	does	not	detect.	Run	afull	scan	at	least	once	per	software	iteration.	If	possible,	run	afull	scan	
periodically	when	it	will	not	interrupt	the	development	workflow,	such	as	on	aweekend.	
l	Compare	quick	scan	with	a	full	scan	â	To	evaluate	the	accuracy	impact	of	aquick	scan,	perform	a	
quick	scan	and	afull	scan	on	the	same	codebase.	Open	the	quick	scan	results	in	Micro	Focus	Fortify	
Audit	Workbench	and	merge	it	into	the	full	scan.	Group	the	issues	by	New	Issue	to	produce	alist	of	
issues	detected	in	the	full	scan	but	not	in	the	quick	scan.	
l	Quick	scans	and	Micro	Focus	Fortify	Software	Security	Center	server	â	To	avoid	overwriting	
the	results	of	afull	scan,	by	default	Fortify	Software	Security	Center	ignores	uploaded	FPR	files	
scanned	in	quick	scan	mode.	However,	you	can	configure	aFortify	Software	Security	Center	
application	version	so	that	FPR	Â files	scanned	in	quick	scan	are	processed.	For	more	information,	see	
analysis	results	processing	rules	in	the	Micro	Focus	Fortify	Software	Security	Center	User	Guide	.	
Configuring	Scan	Speed	with	Speed	Dial	
The	speed	dial	feature	is	available	in	this	release	as	atechnology	preview.	You	can	configure	the	speed	
and	depth	of	the	scan	by	specifying	aprecision	level	for	the	scan	phase.	You	can	use	these	precision	
levels	adjust	the	scan	time	to	fit	for	example,	into	apipeline	and	quickly	find	aset	of	vulnerabilities	while	
the	developer	is	still	working	on	the	code.	Although	scans	with	the	speed	dial	settings	are	faster	than	a	
full	scan,	it	does	not	provide	as	robust	aresult	set.	Fortify	recommends	that	you	run	full	scans	whenever	
possible.
The	precision	level	controls	the	depth	and	precision	of	the	scan	by	associating	configuration	properties	
with	each	level.	The	configuration	properties	files	for	each	level	are	located	in	the	<sca_	install_	
dir>	/Core/config/scales	directory.	There	is	one	file	for	each	level:	(level-	<precision_	
level>	.properties	).	You	can	modify	the	settings	in	these	files	to	create	your	own	specific	precision	
levels.
Important!	As	this	feature	is	atechnology	preview,	be	aware	that	if	you	modify	the	configuration	
files	they	might	be	overwritten	with	an	upgrade	of	Fortify	Static	Code	Analyzer	.	
Notes:
l	By	default,	Micro	Focus	Fortify	Software	Security	Center	blocks	uploaded	scans	performed	with	
aprecision	level.	However,	you	can	configure	your	Fortify	Software	Security	Center	application	
version	so	that	uploaded	audit	projects	scanned	with	these	precision	levels	are	processed.	
l	If	you	merge	aspeed	dial	scan	with	afull	scan,	this	might	remove	issues	from	previous	scans	that	
still	exist	in	your	application	(and	would	be	detected	again	with	afull	scan).	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	146	of	216 

To	specify	the	speed	dial	setting	for	ascan,	include	the	-scan-	precision	(or	-p)Â option	in	the	scan	
phase	as	shown	in	the	following	example:	
sourceanalyzer	-b	<build_	id>	-scan	-scan-	precision	<level>	-f	
myresults.fpr
Note:	You	cannot	use	the	speed	dial	setting	and	the	-quick	option	in	the	same	scan	command.	
The	following	table	describes	the	two	precision	levels.	
Precision
Level	Description	
1	This	is	the	quickest	scan	and	is	recommended	if	you	are	scanning	afew	files.	By	default,	a	
scan	with	this	precision	level	disables	the	Buffer	Analyzer,	Control	Flow	Analyzer,	
Dataflow	Analyzer,	and	Null	Pointer	Analyzer.	
2	By	default,	ascan	with	this	precision	level	enables	all	analyzers.	The	scan	runs	quicker	by	
performing	with	reduced	limiters.	This	results	in	fewer	issues	detected.	
You	can	also	specify	the	scan	precision	level	with	the	com.fortify.sca.PrecisionLevel	property	
in	the	fortify-	sca.properties	file.	For	example:	
com.fortify.sca.PrecisionLevel=1
Breaking	Down	Codebases	
It	is	more	efficient	to	break	down	large	projects	into	independent	modules.	For	example,	if	you	have	a	
portal	application	that	consists	of	several	modules	that	are	independent	of	each	other	or	have	very	little	
interactions,	you	can	translate	and	scan	the	modules	separately.	The	caveat	to	this	is	that	you	might	
lose	dataflow	issue	detection	if	some	interactions	exist.	
For	C/C++,	you	might	reduce	the	scan	time	by	using	the	âbin	option	with	the	âscan	option.	You	need	
to	pass	the	binary	file	as	the	parameter	(such	as	-bin	<filename>	.exe	-scan	or	-bin	
<filename>	.dll	-scan	).	Fortify	Static	Code	Analyzer	finds	the	related	files	associated	with	the	
binary	and	scans	them.	This	is	useful	if	you	have	several	binaries	in	amakefile.	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	147	of	216 

The	following	table	lists	some	useful	Fortify	Static	Code	Analyzer	command-	line	options	to	break	down	
codebases.
Option	Description	
-bin	<binary>	Specifies	asubset	of	source	files	to	scan.	Only	the	source	files	that	were	
linked	in	the	named	binary	at	build	time	are	included	in	the	scan.	You	can	
use	this	option	multiple	times	to	specify	the	inclusion	of	multiple	binaries	
in	the	scan.	
-show-	binaries	Displays	all	objects	that	were	created	but	not	used	in	the	production	of	
any	other	binaries.	If	fully	integrated	into	the	build,	it	lists	all	of	the	
binaries	produced.	
-show-	build-	tree	When	used	with	the	-bin	option,	displays	all	files	used	to	create	the	
binary	and	all	files	used	to	create	those	files	in	atree	layout.	If	the	-bin	
option	is	not	present,	Fortify	Static	Code	Analyzer	displays	the	tree	for	
each	binary.	
Limiting	Analyzers	and	Languages	
Occasionally,	you	might	find	that	asignificant	amount	of	the	scan	time	is	spent	either	running	one	
particular	analyzer	or	analyzing	aparticular	language.	It	is	possible	that	this	particular	analyzer	or	
language	is	not	important	to	your	security	requirements.	You	can	limit	the	specific	analyzers	that	run	
and	the	specific	languages	that	Fortify	Static	Code	Analyzer	translates.	
Disabling	Analyzers	
To	disable	specific	analyzers,	include	the	-analyzers	option	to	Fortify	Static	Code	Analyzer	at	scan	
time	with	acolon-	or	comma-	separated	list	of	analyzers	you	want	to	enable.	The	full	list	of	analyzers	is:	
buffer	,content	,configuration	,controlflow	,dataflow	,findbugs	,nullptr	,semantic	,and	
structural	.	
For	example,	to	run	ascan	that	only	includes	the	Dataflow,	Control	Flow,	and	Buffer	analyzers,	use	the	
following	scan	command:	
sourceanalyzer	-b	<build_	id>	-analyzers	dataflow:controlflow:buffer	-scan	
-f	myresults.fpr	
You	can	also	do	the	same	thing	by	setting	com.fortify.sca.DefaultAnalyzers	in	the	Fortify	
Static	Code	Analyzer	property	file	<sca_	install_	dir>	/Core/config/fortify-	
sca.properties	.For	example,	to	achieve	the	equivalent	of	the	previous	scan	command,	set	the	
following	in	the	properties	file:	
com.fortify.sca.DefaultAnalyzers=dataflow:controlflow:buffer
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	148	of	216 

Disabling	Languages	
To	disable	specific	languages,	include	the	-disable-	language	option	in	the	translation	phase,	which	
specifies	alist	of	languages	that	you	want	to	exclude.	The	full	list	of	valid	language	parameters	is:	
abap,	actionscript,	apex,	cfml,	cpp,	cobol,	configuration,	dotnet,	java,	
javascript,	jsp,	objc,	php,	plsql,	python,	ruby,	scala,	sql,	swift,	tsql,	
typescript,	vb	
For	example,	to	perform	atranslation	that	excludes	SQL	and	PHP	files,	use	the	following	command:	
sourceanalyzer	-b	<build_	id>	<src_	files>	-disable-	language	sql:php	
You	can	also	disable	languages	by	setting	the	com.fortify.sca.DISabledLanguages	property	in	
the	Fortify	Static	Code	Analyzer	properties	file	<sca_	install_	dir>	/Core/config/fortify-	
sca.properties	.For	example,	to	achieve	the	equivalent	of	the	previous	translation	command,	set	the	
following	in	the	properties	file:	
com.fortify.sca.DISabledLanguages=sql:php
Optimizing	FPR	Â Files	
This	chapter	describes	how	to	handle	performance	issues	related	to	the	audit	results	(FPR)	file.	This	
includes	reducing	the	scan	time,	reducing	FPR	Â file	size,	and	tips	for	opening	large	FPR	files.	
Filter	Files	
Filter	files	are	flat	files	that	you	can	specify	with	ascan	using	the	-filter	option.	Use	afilter	file	to	filter	
out	particular	vulnerability	instances,	rules,	and	vulnerability	categories.	If	you	determine	that	acertain	
issue	category	or	rule	is	not	relevant	for	aparticular	scan,	you	can	stop	Fortify	Static	Code	Analyzer	
from	flagging	these	types	of	issues	and	adding	them	to	the	FPR.	Using	afilter	file	can	reduce	both	the	
scan	time	and	results	file	size.	
For	example,	if	you	are	scanning	asimple	program	that	just	reads	aspecified	file,	you	might	not	want	to	
see	path	manipulation	issues,	because	these	are	likely	planned	as	part	of	the	functionality.	To	filter	out	
path	manipulation	issues,	create	afile	that	contains	asingle	line:	
Path	Manipulation	
Save	this	file	as	filter.txt	.Use	the	-filter	option	for	the	scan	as	shown	in	the	following	example:	
sourceanalyzer	-b	<build_	id>	-scan	-f	myresults.fpr	-filter	filter.txt	
The	myresults.fpr	does	not	include	any	issues	with	the	category	Path	Manipulation.	
For	more	information	about	filter	files,	see	""Filtering	the	Analysis""	on	page	Â 165	.	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	149	of	216 

Excluding	Issues	from	the	FPR	Â with	Filter	Sets	
Filters	in	an	issue	template	determine	how	the	results	from	Fortify	Static	Code	Analyzer	are	shown.	For	
example,	you	can	have	afilter	to	put	any	detected	SQL	Injection	issues	into	aseparate	folder	called	SQL	
Injections	,or	you	might	have	afilter	that	hides	issues	with	aconfidence	below	acertain	threshold.	In	
addition	to	filters,	filter	sets	enable	you	to	have	aselection	of	filters	used	at	any	one	time.	Each	FPR	has	
an	issue	template	associated	with	it.	You	can	use	filter	sets	to	reduce	the	number	of	issues	based	on	
conditions	you	specify	with	filters	in	an	issue	template.	This	can	dramatically	reduce	the	size	of	an	FPR.	
To	do	this,	use	Micro	Focus	Fortify	Audit	Workbench	to	create	afilter	and	afilter	set	and	then	run	the	
Fortify	Static	Code	Analyzer	scan	with	the	filter	set.	For	more	detailed	instructions	about	how	to	create	
filters	and	filter	sets	in	Fortify	Audit	Workbench	,see	the	Micro	Focus	Fortify	Audit	Workbench	User	
Guide	.The	following	example	describes	the	basic	steps	for	how	to	create	and	use	ascan-	time	filter:	
1.	In	this	example,	suppose	you	use	OWASP	Top	10	2017	and	you	only	want	to	see	issues	
categorized	within	this	standard.	Create	afilter	in	Fortify	Audit	Workbench	such	as:	
If	[OWASP	Top	10	2017]	does	not	contain	A	Then	hide	issue	
This	filter	looks	through	the	issues	and	if	an	issue	does	not	map	to	an	OWASP	Top	10	2017	
category	with	âAâ	in	the	name,	then	it	hides	it.	Because	all	OWASP	Top	10	2017	categories	start	
with	âAâ	(A1,	A2,	â¦,	A10),	then	any	category	without	the	letter	âAâ	is	not	in	the	OWASP	Top	10	
2017.	The	filter	hides	the	issues	from	view	in	Fortify	Audit	Workbench	,but	they	are	still	in	the	FPR.	
2.	In	Fortify	Audit	Workbench	,create	anew	filter	set	called	OWASP_	Filter_	Set	that	contains	the	
previous	filter,	and	then	export	the	issue	template	to	afile	called	IssueTemplate.xml	.	
3.	You	can	then	specify	this	filter	at	scan-time	with	the	following	command:	
sourceanalyzer	-b	<build_	id>	-scan	-f	myFilteredResults.fpr	
-project-	template	IssueTemplate.xml	-Dcom.fortify.sca.FilterSet=OWASP_	
Filter_	set	
In	the	previous	example,	the	inclusion	of	the	-Dcom.fortify.sca.FilterSet	property	tells	Fortify	
Static	Code	Analyzer	to	use	the	OWASP_	Filter_	Set	filter	set	from	the	issue	template	
IssueTemplate.xml	.Any	filters	that	hide	issues	from	view	are	removed	and	are	not	written	to	the	
FPR.	Therefore,	you	can	reduce	the	visible	number	of	issues,	make	the	scan	very	targeted,	and	reduce	
the	size	of	the	resulting	FPR	file.	
Note:	Although	filtering	issues	with	afilter	set	can	reduce	the	size	of	the	FPR,	they	do	not	usually	
reduce	the	scan	time.	Fortify	Static	Code	Analyzer	examines	Â the	filter	set	after	it	calculates	the	
issues	to	determine	whether	to	write	them	to	the	FPR	file.	The	filters	in	afilter	set	determine	the	
rule	types	that	Fortify	Static	Code	Analyzer	loads.	
Excluding	Source	Code	from	the	FPR	
You	can	reduce	the	scan	time	and	the	size	of	the	FPR	Â file	by	excluding	the	source	code	information	
from	the	FPR.	This	is	especially	valuable	for	large	source	files	or	codebases.	You	do	not	generally	get	a	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	150	of	216 

scan	time	reduction	for	small	source	files.	
There	are	two	ways	to	prevent	Fortify	Static	Code	Analyzer	from	including	source	code	in	the	FPR.	You	
can	set	the	property	in	the	<sca_	install_	dir>	/Core/config/fortify-	sca.properties	file	or	
specify	an	option	on	the	command	line.	The	following	table	describes	these	settings.	
Property	Name	Description	
com.fortify.sca.
FPRDisableSourceBundling=true
Command-	Line	Option:	
-disable-	source-	bundling	
This	excludes	source	code	from	the	FPR.	
com.fortify.sca.
FVDLDisableSnippets=true
Command-	Line	Option:	
âfvdl-	no-	snippets	
This	excludes	code	snippets	from	the	FPR.	
The	following	command-	line	example	uses	both	options:	
sourceanalyzer	-b	<build_	id>	-disable-	source-	bundling	
-fvdl-	no-	snippets	-scan	-f	mySourcelessResults.fpr	
Reducing	the	FPR	Â File	Size	
There	are	afew	ways	to	reduce	the	size	of	FPR	files.	The	quickest	way	to	do	this	without	affecting	
results	is	to	exclude	the	source	code	from	the	FPR	as	described	in	""Excluding	Source	Code	from	the	
FPR""	on	the	previous	page	.	
There	are	afew	other	options	and	properties	that	you	can	use	to	select	what	is	excluded	from	the	FPR.	
You	can	set	these	properties	in	the	Fortify	Static	Code	Analyzer	properties	file:	<sca_	install_	
dir>	/Core/config/fortify-	sca.properties	or	specify	them	during	the	scan	phase	with	-	
D<property_	name>	=true	.Most	of	these	options	have	an	equivalent	command-	line	option.	
Property	Name	Description	
com.fortify.sca.
FPRDisableMetatable
=true
Command-	Line	Option:	Â 	
-disable-	metatable	
This	excludes	the	metatable	from	the	FPR.	Micro	
Focus	Fortify	Audit	Workbench	uses	the	metatable	to	
map	information	in	Functions	view.	
com.fortify.sca.
FVDLDisableDescriptions
=true	
This	excludes	rule	descriptions	from	the	Â FPR.	If	you	
do	not	use	custom	descriptions,	the	descriptions	in	
the	Fortify	Taxonomy	(https://vulncat.fortify.com	)	
are	used.	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	151	of	216 

Property	Name	Description	
Command-	Line	Option:	Â 	
-fvdl-	no-	descriptions	
com.fortify.sca.
FVDLDisableEngineData
=true
Command-	Line	Option:	Â 	
-fvdl-	no-	enginedata	
This	excludes	engine	data	from	the	FPR.	This	is	
useful	if	your	FPR	contains	alarge	number	of	
warnings	when	you	open	the	file	in	Fortify	Audit	
Workbench	.	
Note:	If	you	exclude	engine	data	from	the	FPR,	
you	must	merge	the	FPR	with	the	current	audit	
project	locally	before	you	upload	it	to	Micro	
Focus	Fortify	Software	Security	Center	.Fortify	
Software	Security	Center	cannot	merge	it	on	the	
server	because	the	FPR	does	not	contain	the	
Fortify	Static	Code	Analyzer	version.	
com.fortify.sca.
FVDLDisableProgramData
=true
Command-	Line	Option:	Â 	
-fvdl-	no-	progdata	
This	excludes	the	program	data	from	the	FPR.	This	
removes	the	Taint	Sources	information	from	the	
Functions	view	in	Fortify	Audit	Workbench	.This	
property	typically	only	has	aminimal	effect	on	the	
overall	size	of	the	FPR	file.	
Opening	Large	FPR	Â Files	
To	reduce	the	time	required	to	open	alarge	FPR	file,	there	are	some	properties	that	you	can	set	in	the	
<sca_	install_	dir>	/Core/config/fortify.properties	configuration	file.	For	more	
information	about	these	properties,	see	the	Micro	Focus	Fortify	Static	Code	Analyzer	Tools	Properties	
Reference	Guide	.The	following	table	describes	these	properties.	
Property	Name	Description	
com.fortify.
model.DisableProgramInfo=true	
This	setting	disables	use	of	the	code	navigation	
features	in	Micro	Focus	Fortify	Audit	
Workbench	.	
com.fortify.
model.IssueCutOffStartIndex
=<num>	(inclusive)	
com.fortify.
model.IssueCutOffEndIndex
=<num>	(exclusive)	
The	IssueCutOffStartIndex	property	is	
inclusive	and	IssueCutOffEndIndex	is	
exclusive	so	that	you	can	specify	asubset	of	
issues	you	want	to	see.	For	example,	to	see	the	
first	100	issues,	specify	the	following:	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	152	of	216 

Property	Name	Description
com.fortify.model.
IssueCutOffStartIndex=0
com.fortify.model.
IssueCutOffEndIndex=101
Because	the	IssueCutOffStartIndex	is	0	by	
default,	you	do	not	need	to	specify	this	
property.	
com.fortify.
model.IssueCutOffByCategoryStartIndex=
<num>	(inclusive)	
com.fortify.
model.IssueCutOffByCategoryEndIndex=
<num>	(exclusive)	
These	two	properties	are	similar	to	the	previous	
cutoff	properties	except	these	are	specified	for	
each	category.	For	example,	to	see	the	first	five	
issues	for	every	category,	specify	the	following:	
com.fortify.model.
IssueCutOffByCategoryEndIndex=6	
com.fortify.
model.MinimalLoad=true	
This	minimizes	the	data	loaded	in	the	FPR.	This	
also	restricts	usage	of	the	Functions	view	and	
might	prevent	Fortify	Audit	Workbench	from	
loading	the	source	from	the	FPR.	
com.fortify.
model.MaxEngineErrorCount=
<num>	
This	property	specifies	the	number	of	Fortify	
Static	Code	Analyzer	reported	warnings	that	
are	loaded	with	the	FPR.	For	projects	with	a	
large	number	of	scan	warnings,	this	can	reduce	
both	load	time	in	Fortify	Audit	Workbench	and	
the	amount	of	memory	required	to	open	the	
FPR.	
com.fortify.
model.ExecMemorySetting	
Specifies	the	JVM	heap	memory	size	for	Audit	
Workbench	to	launch	external	utilities	such	as	
iidmigrator	and	fortifyupdate.	
Monitoring	Long	Running	Scans	
When	you	run	Fortify	Static	Code	Analyzer	,large	and	complex	scans	can	often	take	along	time	to	
complete.	During	the	scan	it	is	not	always	clear	what	is	happening.	While	Fortify	recommends	that	you	
provide	your	debug	logs	to	the	Micro	Focus	Fortify	Customer	Support	team,	there	are	acouple	of	ways	
to	see	what	Fortify	Static	Code	Analyzer	is	doing	and	how	it	is	performing	in	real-	time.	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	153	of	216 

Using	the	SCAState	Utility	
The	SCAState	command-	line	utility	enables	you	to	see	up-	to-	date	state	analysis	information	during	the	
analysis	phase.	The	SCAState	utility	is	located	in	the	<sca_	install_	dir>	/bin	directory.	In	addition	
to	alive	view	of	the	analysis,	it	also	provides	aset	of	timers	and	counters	that	show	where	Fortify	Static	
Code	Analyzer	spends	its	time	during	the	scan.	For	more	information	about	how	to	use	the	SCAState	
utility,	see	the	""Checking	the	Fortify	Static	Code	Analyzer	Scan	Status""	on	pageÂ 139	.	
Using	JMX	Tools	
You	can	use	tools	to	monitor	Fortify	Static	Code	Analyzer	with	JMX	technology.	These	tools	can	
provide	away	to	track	Fortify	Static	Code	Analyzer	performance	over	time.	For	more	information	about	
these	tools,	see	the	full	Oracle	documentation	available	at:	Â http://docs.oracle.com	.	
Note:	These	are	third-	party	tools	and	Micro	Focus	does	not	provide	or	support	them.	
Using	JConsole	
JConsole	is	an	interactive	monitoring	tool	that	complies	with	the	JMX	specification.	The	disadvantage	of	
JConsole	is	that	you	cannot	save	the	output.	
To	use	JConsole,	you	must	first	set	some	additional	JVM	parameters.	Set	the	following	environment	
variable:
export	SCA_	VM_	OPTS=""-	Dcom.sun.management.jmxremote	
-Dcom.sun.management.jmxremote.port=9090
-Dcom.sun.management.jmxremote.ssl=false
-Dcom.sun.management.jmxremote.authenticate=false""	
After	the	JMX	parameters	are	set,	start	aFortify	Static	Code	Analyzer	scan.	During	the	scan,	start	
JConsole	to	monitor	Fortify	Static	Code	Analyzer	locally	or	remotely	with	the	following	command:	
jconsole	<host_	name>	:9090	
Using	Java	VisualVM	
Java	VisualVM	offers	the	same	capabilities	as	JConsole.	It	also	provides	more	detailed	information	on	
the	JVM	and	enables	you	to	save	the	monitor	information	to	an	application	snapshot	file.	You	can	store	
these	files	and	open	them	later	with	Java	VisualVM.	
Similar	to	JConsole,	before	you	can	use	Java	VisualVM,	you	must	set	the	same	JVM	parameters	
described	in	""Using	JConsole""	above	.	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	154	of	216 

After	the	JVM	parameters	are	set,	start	the	scan.	You	can	then	start	Java	VisualVM	to	monitor	the	scan	
either	locally	or	remotely	with	the	following	command:	
jvisualvm	<host_	name>	:9090	
User	Guide	
Chapter	19:	Improving	Performance	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	155	of	216 

Chapter	20:	Troubleshooting	
This	section	contains	the	following	topics:	
Exit	Codes	156	
Memory	Tuning	157	
Scanning	Complex	Functions	159	
Issue	Non-	Determinism	161	
Accessing	Log	Files	162	
Configuring	Log	Files	162	
Reporting	Issues	and	Requesting	Enhancements	164	
Exit	Codes	
The	following	table	describes	the	possible	Fortify	Static	Code	Analyzer	exit	codes.	
Exit
Code	Description	
0	Success	
1	Generic	failure	
2	Invalid	input	files	
(this	could	indicate	that	an	attempt	was	made	to	translate	afile	that	has	afile	extension	that	
Fortify	Static	Code	Analyzer	does	not	support)	
3	Process	timed	out	
4	Analysis	completed	with	numbered	warning	messages	written	to	the	console	and/or	to	the	
log	file	
5	Analysis	completed	with	numbered	error	messages	written	to	the	console	and/or	to	the	log	
file	
6	Scan	phase	was	unable	to	generate	issue	results	
By	default,	Fortify	Static	Code	Analyzer	only	returns	exit	codes	0,	1,	2,	or	3.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	156	of	216 

You	can	extend	the	default	exit	code	options	by	setting	the	com.fortify.sca.ExitCodeLevel	
property	in	the	<sca_	install_	dir>	/Core/Config/fortify-	sca.properties	file.	
The	valid	values	are:	
l	nothing	â	Returns	exit	codes	0,	1,	2,	or	3.	This	is	the	default	setting.	
l	warnings	â	Returns	exit	codes	0,	1,	2,	3,	4,	or	5.	
l	errors	â	Returns	exit	codes	0,	1,	2,	3,	or	5.	
l	no_	output_	file	â	Returns	exit	codes	0,	1,	2,	3,	or	6.	
Memory	Tuning	
The	amount	of	physical	RAM	required	for	ascan	depends	on	the	complexity	of	the	code.	By	default,	
Fortify	Static	Code	Analyzer	automatically	allocates	the	memory	it	uses	based	on	the	physical	memory	
available	on	the	system.	This	is	generally	sufficient.	As	described	in	""Output	Options""	on	page	Â 117	,you	
can	adjust	the	Java	heap	size	with	the	-Xmx	command-	line	option.	
This	section	describes	suggestions	for	what	you	can	do	if	you	encounter	OutOfMemory	errors	during	
the	analysis.	
Note:	You	can	set	the	memory	allocation	options	discussed	in	this	section	to	run	for	all	scans	by	
setting	the	SCA_	VM_	OPTS	environment	variable.	
Java	Heap	Exhaustion	
Java	heap	exhaustion	is	the	most	common	memory	problem	that	might	occur	during	Fortify	Static	Code	
Analyzer	scans.	It	is	caused	by	allocating	too	little	heap	space	to	the	Java	virtual	machine	that	Fortify	
Static	Code	Analyzer	uses	to	scan	the	code.	You	can	identify	Java	heap	exhaustion	from	the	following	
symptom.
Symptom
One	or	more	of	these	messages	appears	in	the	Fortify	Static	Code	Analyzer	log	file	and	in	the	
command-	line	output:	
There	is	not	enough	memory	available	to	complete	analysis.	For	details	on	
making	more	memory	available,	please	consult	the	user	manual.	
java.lang.OutOfMemoryError:	Java	heap	space	
java.lang.OutOfMemoryError:	GC	overhead	limit	exceeded	
Resolution
To	resolve	aJava	heap	exhaustion	problem,	allocate	more	heap	space	to	the	Fortify	Static	Code	
Analyzer	Java	virtual	machine	when	you	start	the	scan.	To	increase	the	heap	size,	use	the	-Xmx	
command-	line	option	when	you	run	the	Fortify	Static	Code	Analyzer	scan.	For	example,	-Xmx1G	makes	
1	GB	available.	Before	you	use	this	parameter,	determine	the	maximum	allowable	value	for	Java	heap	
space.	The	maximum	value	depends	on	the	available	physical	memory.	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	157	of	216 

Heap	sizes	between	32	Â GB	and	48	Â GB	are	not	advised	due	to	internal	JVM	implementations.	Heap	sizes	
in	this	range	perform	worse	than	at	32	Â GB.	Heap	sizes	smaller	than	32	Â GB	are	optimized	by	the	JVM.	If	
your	scan	requires	more	than	32	Â GB,	then	you	probably	need	64	Â GB	or	more.	As	aguideline,	assuming	
no	other	memory	intensive	processes	are	running,	do	not	allocate	more	than	2/3	of	the	available	
memory.
If	the	system	is	dedicated	to	running	Fortify	Static	Code	Analyzer	,you	do	not	need	to	change	it.	
However,	if	the	system	resources	are	shared	with	other	memory-intensive	processes,	subtract	an	
allowance	for	those	other	processes.	
Note:	You	do	not	need	to	account	for	other	resident	but	not	active	processes	(while	Fortify	Static	
Code	Analyzer	is	running)	Â that	the	operating	system	might	swap	to	disk.	Allocating	more	physical	
memory	to	Fortify	Static	Code	Analyzer	than	is	available	in	the	environment	might	cause	
âthrashing,â	which	typically	slows	down	the	scan	along	with	everything	else	on	the	system.	
Native	Heap	Exhaustion	
Native	heap	exhaustion	is	arare	scenario	where	the	Java	virtual	machine	can	allocate	the	Java	memory	
regions	on	startup,	but	is	left	with	so	few	resources	for	its	native	operations	(such	as	garbage	collection)	
that	it	eventually	encounters	afatal	memory	allocation	failure	that	immediately	terminates	the	process.	
Symptom
You	can	identify	native	heap	exhaustion	by	abnormal	termination	of	the	Fortify	Static	Code	Analyzer	
process	and	the	following	output	on	the	command	line:	
#	A	fatal	error	has	been	detected	by	the	Java	Runtime	Environment:	
#
#	java.lang.OutOfMemoryError:	requested	...	bytes	for	GrET	...	
Because	this	is	afatal	Java	virtual	machine	error,	it	is	usually	accompanied	by	an	error	log	created	in	the	
working	directory	with	the	file	name	Â hs_	err_	pidNNN.log	.	
Resolution
Because	the	problem	is	aresult	of	overcrowding	within	the	process,	the	resolution	is	to	reduce	the	
amount	of	memory	used	for	the	Java	memory	regions	(Java	heap).	Reducing	this	value	should	reduce	
the	crowding	problem	and	allow	the	scan	to	complete	successfully.	
Stack	Overflow	
Each	thread	in	aJava	application	has	its	own	stack.	The	stack	holds	return	addresses,	function/method	
call	arguments,	and	so	on.	If	athread	tends	to	process	large	structures	with	recursive	algorithms,	it	
might	need	alarge	stack	for	all	those	return	addresses.	With	the	JVM,	you	can	set	that	size	with	the	-	
Xss	option.	
Symptoms
This	message	typically	appears	in	the	Fortify	Static	Code	Analyzer	log	file,	but	might	also	appear	in	the	
command-	line	output:	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	158	of	216 

java.lang.StackOverflowError
Resolution
The	default	stack	size	is	16	MB.	To	increase	the	stack	size,	pass	the	-Xss	option	to	the	
sourceanalyzer	command.	For	example,	-Xss32M	increases	the	stack	to	32	MB.	
Scanning	Complex	Functions	
During	aFortify	Static	Code	Analyzer	scan,	the	Dataflow	Analyzer	might	encounter	afunction	for	
which	it	cannot	complete	the	analysis	and	reports	the	following	message:	
Function	<name>	is	too	complex	for	<analyzer>	analysis	and	will	be	skipped	
(<identifier>	)	
where:
l	<name>	is	the	name	of	the	source	code	function	
l	<analyzer>	is	the	name	of	the	analyzer	
l	<identifier>	is	the	type	of	complexity,	which	is	one	of	the	following:	
l	l:Too	many	distinct	locations	
l	m:Out	of	memory	
l	s:Stack	size	too	small	
l	t:Analysis	taking	too	much	time	
l	v:Function	visits	exceed	the	limit	
The	depth	of	analysis	Fortify	Static	Code	Analyzer	performs	sometimes	depends	on	the	available	
resources.	Fortify	Static	Code	Analyzer	uses	acomplexity	metric	to	tradeoff	these	resources	against	the	
number	of	vulnerabilities	that	it	can	find.	Sometimes,	this	means	giving	up	on	aparticular	function	when	
Fortify	Static	Code	Analyzer	does	not	have	enough	resources	available.	This	is	normally	when	you	see	
the	""Function	too	complex""	messages.	
When	you	see	this	message,	it	does	not	necessarily	mean	that	Fortify	Static	Code	Analyzer	completely	
ignored	the	function	in	the	program.	For	example,	the	Dataflow	Analyzer	typically	visits	afunction	
many	times	before	completing	the	analysis,	and	might	not	have	run	into	this	complexity	limit	in	the	
previous	visits.	In	this	case,	the	results	include	anything	learned	from	the	previous	visits.	
You	can	control	the	""give	up""	point	using	Fortify	Static	Code	Analyzer	properties	called	limiters.	
Different	analyzers	have	different	limiters.	
The	following	sections	provide	adiscussion	of	aresolution	for	this	issue.	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	159	of	216 

Dataflow	Analyzer	Limiters	
There	are	three	types	of	complexity	identifiers	for	the	Dataflow	Analyzer:	
l	l:Too	many	distinct	locations	
l	m:Out	of	memory	
l	s:Stack	size	too	small	
l	v:Function	visits	exceed	the	limit	
To	resolve	the	issue	identified	by	s,increase	the	stack	size	for	by	setting	-Xss	Â to	avalue	greater	than	
16	Â MB.	
To	resolve	the	complexity	identifier	of	m,increase	the	physical	memory	for	Fortify	Static	Code	Analyzer	.	
To	resolve	the	complexity	identifier	of	l,you	can	adjust	the	following	limiters	in	the	Fortify	Static	Code	
Analyzer	property	file	<sca_	install_	dir>	/Core/config/fortify-	sca.properties	or	on	the	
command	line.	
Property	Name	Default	Value	
com.fortify.sca.
limiters.MaxTaintDefForVar	
1000	
com.fortify.sca.
limiters.MaxTaintDefForVarAbort	
4000	
com.fortify.sca.
limiters.MaxFieldDepth	
4	
The	MaxTaintDefForVar	limiter	is	adimensionless	value	expressing	the	complexity	of	afunction,	
while	MaxTaintDefForVarAbort	is	the	upper	bound	for	it.	Use	the	MaxFieldDepth	limiter	to	
measure	the	precision	when	the	Dataflow	Analyzer	analyzes	any	given	object.	Fortify	Static	Code	
Analyzer	always	tries	to	analyze	objects	at	the	highest	precision	possible.	
If	agiven	function	exceeds	the	Â MaxTaintDefForVar	limit	at	agiven	precision,	the	Dataflow	Analyzer	
analyzes	that	function	with	lower	precision	(by	reducing	the	Â MaxFieldDepth	limiter).	Â When	you	reduce	
the	precision,	it	reduces	the	complexity	of	the	analysis.	When	the	precision	cannot	be	reduced	any	
further,	Fortify	Static	Code	Analyzer	then	proceeds	with	analysis	at	the	lowest	precision	until	either	it	
finishes,	or	the	complexity	exceeds	the	Â MaxTaintDefForVarAbort	limiter.	In	other	words,	Fortify	
Static	Code	Analyzer	tries	harder	at	the	lowest	precision	to	get	at	least	some	results	from	the	function.	
If	Fortify	Static	Code	Analyzer	reaches	the	Â MaxTaintDefForVarAbort	limiter,	it	gives	up	on	the	
function	entirely	and	you	get	the	""Function	too	complex""	warning.	
To	resolve	the	complexity	identifier	of	v,you	can	adjust	the	property	
com.fortify.sca.limiters.MaxFunctionVisits	.This	property	sets	the	maximum	number	of	
times	the	taint	propagation	analyzer	visits	functions.	The	default	is	50	.	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	160	of	216 

Control	Flow	and	Null	Pointer	Analyzer	Limiters	
There	are	two	types	of	complexity	identifiers	for	both	Control	Flow	and	Null	Pointer	analyzers:	
l	m:Out	of	memory	
l	t:Analysis	taking	too	much	time	
Due	to	the	way	that	the	Dataflow	Analyzer	handles	function	complexity,	it	does	not	take	an	indefinite	
amount	of	time.	Control	Flow	and	Null	Pointer	analyzers,	however,	can	take	avery	long	time	when	
analyzing	very	complex	functions.	Therefore,	Fortify	Static	Code	Analyzer	provides	away	to	abort	the	
analysis	when	this	happens,	and	then	you	get	the	""Function	too	complex""	message	with	acomplexity	
identifier	of	t.	
To	change	the	maximum	amount	of	time	these	analyzers	spend	to	analyze	functions,	you	can	adjust	the	
following	property	values	in	the	Fortify	Static	Code	Analyzer	property	file	<sca_	install_	
dir>	/Core/config/fortify-	sca.properties	or	on	the	command	line.	
Property	Name	Description	
Default
Value	
com.fortify.sca.
CtrlflowMaxFunctionTime	
Sets	the	time	limit	(in	milliseconds)	for	Control	Flow	
analysis	on	asingle	function.	
600000
(10	Â minutes)	
com.fortify.sca.
NullPtrMaxFunctionTime	
Sets	the	time	limit	(in	milliseconds)	for	Null	Pointer	
analysis	on	asingle	function.	
300000
(5	Â minutes)	
To	resolve	the	complexity	identifier	of	m,increase	the	physical	memory	for	Fortify	Static	Code	Analyzer	.	
Note:	If	you	increase	these	limiters	or	time	settings,	it	makes	the	analysis	of	complex	functions	take	
longer.	It	is	difficult	to	characterize	the	exact	performance	implications	of	aparticular	value	for	the	
limiters/time,	because	it	depends	on	the	specific	function	in	question.	If	you	never	want	to	see	the	
""Function	too	complex""	warning,	you	can	set	the	limiters/time	to	an	extremely	high	value,	however	it	
can	cause	unacceptable	scan	time.	
Issue	Non-	Determinism	
Running	in	parallel	analysis	mode	might	introduce	issue	non-	determinism.	If	you	experience	any	
problems,	contact	Micro	Focus	Fortify	Customer	Support	and	disable	parallel	analysis	mode.	Disabling	
parallel	analysis	mode	results	in	sequential	analysis,	which	can	be	substantially	slower	but	provides	
deterministic	results	across	multiple	scans.	
To	disable	parallel	analysis	mode:	
1.	Open	the	fortify-	sca.properties	file	located	in	the	<sca_	install_	dir>	/core/config	
directory	in	atext	editor.	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	161	of	216 

2.	Change	the	value	for	the	com.fortify.sca.MultithreadedAnalysis	property	to	false	.	
com.fortify.sca.MultithreadedAnalysis=false	
Accessing	Log	Files	
By	default,	Fortify	Static	Code	Analyzer	creates	two	log	files	in	the	following	location:	
l	On	Windows:	C:\Users\	<user>	\AppData\Local\Fortify\sca	<version>	\log	
l	On	other	platforms:	$HOME/.fortify/sca	<version>	/log	
where	<version>	is	the	version	of	Fortify	Static	Code	Analyzer	that	you	are	using.	
The	following	table	describes	the	two	log	files.	
Default	File	Name	Description	
sca.log	The	standard	log	provides	alog	of	informational	messages,	
warnings,	and	errors	that	occurred	in	the	run	of	
sourceanalyzer.	
sca_	FortifySupport.log	The	Fortify	Support	log	provides:	
l	The	same	log	messages	as	the	standard	log	file,	but	with	
additional	details	
l	Additional	detailed	messages	that	are	not	included	in	the	
standard	log	file	
This	log	file	is	only	helpful	to	Micro	Focus	Fortify	Customer	
Support	or	the	development	team	to	troubleshoot	any	
possible	issues.	
If	you	encounter	warnings	or	errors	that	you	cannot	resolve,	provide	the	Fortify	Support	log	file	to	
Micro	Focus	Fortify	Customer	Support	.	
Configuring	Log	Files	
You	can	configure	the	information	that	Fortify	Static	Code	Analyzer	writes	to	the	log	files	by	setting	
logging	properties	(see	""fortify-	sca.properties""	on	page	Â 182	).	You	can	configure	the	following	log	file	
settings:
l	The	location	and	name	of	the	log	file	
Property:	com.fortify.sca.LogFile	
l	Log	level	(see	""Understanding	Log	Levels""	on	the	next	page	)	
Property:	com.fortify.sca.LogLevel	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	162	of	216 

l	Whether	to	overwrite	the	log	files	for	each	run	of	sourceanalyzer	
Property:	com.fortify.sca.ClobberLogFile	
Command-	line	option:	-clobber-	log	
Understanding	Log	Levels	
The	log	level	you	select	gives	you	all	log	messages	equal	to	and	greater	than	it.	The	log	levels	in	the	
following	table	are	listed	in	order	from	least	to	greatest.	For	example,	the	default	log	level	of	
INFO	Â includes	log	messages	with	the	following	levels:	Â INFO,	WARN,	ERROR,	and	FATAL.	You	can	set	
the	log	level	with	the	com.fortify.sca.LogLevel	property	in	the	<sca_	install_	
dir>	/Core/config/fortify.sca.properties	file	or	on	the	command-	line	using	the	-D	option.	
Log
Level	Description	
DEBUG	Includes	information	that	could	be	used	by	Micro	Focus	Fortify	Customer	Support	or	the	
development	team	to	Â troubleshoot	an	issue	
INFO	Basic	information	about	the	translation	or	scan	process	
WARN	Information	about	issues	where	the	translation	or	scan	did	not	stop,	but	might	require	
your	attention	for	accurate	results	
ERROR	Information	about	an	issue	that	might	require	attention	
FATAL	Information	about	an	error	that	caused	the	translation	or	scan	to	abort	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	163	of	216 

Reporting	Issues	and	Requesting	Enhancements	
Feedback	is	critical	to	the	success	of	this	product.	To	request	enhancements	or	patches,	or	to	report	
issues,	visit	Micro	Focus	Fortify	Customer	Support	at	https://www.microfocus.com/support	.	
Include	the	following	information	when	you	contact	customer	support:	
l	Product:	Fortify	Static	Code	Analyzer	
l	Version	number:	To	determine	the	version	number,	run	the	following:	
sourceanalyzer	-version	
l	Platform:	(for	example,	Red	Hat	Enterprise	Linux	<version>	)	
l	Operating	system:	(such	as	Linux)	
To	request	an	enhancement,	include	adescription	of	the	feature	enhancement.	
To	report	an	issue,	provide	enough	detail	so	that	support	can	duplicate	the	issue.	The	more	descriptive	
you	are,	the	faster	support	can	analyze	and	resolve	the	issue.	Also	include	the	log	files,	or	the	relevant	
portions	of	them,	from	when	the	issue	occurred.	
User	Guide	
Chapter	20:	Troubleshooting	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	164	of	216 

Appendix	A:	Filtering	the	Analysis	
This	section	contains	the	following	topics:	
Filter	Files	165	
Filter	File	Example	165	
Filter	Files	
You	can	create	afile	to	filter	out	particular	vulnerability	instances,	rules,	and	vulnerability	categories	
when	you	run	the	sourceanalyzer	command.	You	specify	the	file	with	the	-filter	analysis	option.	
Note:	Fortify	recommends	that	you	only	use	filter	files	if	you	are	an	advanced	user.	Do	not	use	
filter	files	for	standard	audits,	because	auditors	typically	want	to	see	and	evaluate	all	issues	that	
Fortify	Static	Code	Analyzer	finds.	
A	filter	file	is	atext	file	that	you	can	create	with	any	text	editor.	You	specify	only	the	filter	items	that	you	
do	not	want	in	this	file.	Each	filter	item	is	on	aseparate	line	in	the	filter	file.	You	can	specify	the	following	
filter	types:	
l	Category	
l	Instance	ID	
l	Rule	ID	
The	filters	are	applied	at	different	times	in	the	analysis	process,	based	on	the	type	of	filter.	Fortify	Static	
Code	Analyzer	applies	category	and	rule	ID	filters	in	the	initialization	phase	before	any	analysis	has	
taken	place,	whereas	an	instance	ID	filter	is	applied	after	the	analysis	phase.	
Filter	File	Example	
As	an	example,	the	following	output	is	from	ascan	of	the	EightBall.java	,located	in	the	<sca_	
install_	dir>	/Samples/basic/eightball	directory.	
The	following	commands	are	executed	to	produce	the	analysis	results:	
sourceanalyzer	-b	eightball	EightBall.java	
sourceanalyzer	-b	eightball	-scan	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	165	of	216 

The	following	results	show	five	detected	issues:	
[F7A138CDE5235351F6A4405BA4AD7C53	:	low	:	Unchecked	Return	Value	:	
semantic	]	
EightBall.java	(12)	:	Reader.read	()	
[6291C6A33303ED270C269917AA8A1005	:	high	:	Path	Manipulation	:	dataflow	]	
EightBall.java	(12)	:	->new	FileReader	(0)	
EightBall.java	(8)	:	<=>	(filename)	
EightBall.java	(8)	:	<-	>Integer.parseInt	(0-	>return)	
EightBall.java	(6)	:	<=>	(filename)	
EightBall.java	(4)	:	->EightBall.main	(0)	
[176CC0B182267DD538992E87EF41815F	:	critical	:	Path	Manipulation	:	
dataflow	]	
EightBall.java	(12)	:	->new	FileReader	(0)	
EightBall.java	(6)	:	<=>	(filename)	
EightBall.java	(4)	:	->EightBall.main	(0)	
[E4B3ACF92911ED6D98AAC15876739EC7	:	high	:	Unreleased	Resource	:	Streams	:	
controlflow	]	
EightBall.java	(12)	:	start	->	loaded	:	new	FileReader	(...)	
EightBall.java	(14)	:	loaded	->	end_	of_	scope	:	end	scope	:	Resource	
leaked	
EightBall.java	(12)	:	start	->	loaded	:	new	FileReader	(...)	
EightBall.java	(12)	:	java.io.IOException	thrown	
EightBall.java	(12)	:	loaded	->	loaded	:	throw	
EightBall.java	(12)	:	loaded	->	end_	of_	scope	:	end	scope	:	Resource	
leaked	:	java.io.IOException	thrown	
[BB9F74FFA0FF75C9921D0093A0665BEB	:	low	:	J2EE	Bad	Practices	:	Leftover	
Debug	Code	:	structural	]	
EightBall.java	(4)	
The	following	is	example	filter	file	content	that	performs	the	following:	
l	Remove	all	results	related	to	the	J2EE	Â Bad	Practice	category	
l	Remove	the	Path	Manipulation	based	on	its	instance	ID	
l	Remove	any	dataflow	issues	that	were	generated	from	aspecific	rule	ID	
#This	is	a	category	to	filter	from	scan	output	
J2EE	Bad	Practices	
User	Guide	
Appendix	A:	Filtering	the	Analysis	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	166	of	216 

#This	is	an	instance	ID	of	a	specific	issue	to	be	filtered	
#from	scan	output	
6291C6A33303ED270C269917AA8A1005
#This	is	a	specific	Rule	ID	that	leads	to	the	reporting	of	a	
#specific	issue	in	the	scan	output:	in	this	case	the	
#dataflow	sink	for	a	Path	Manipulation	issue.	
823FE039-	A7FE-	4AAD-	B976-	9EC53FFE4A59	
To	test	the	filtered	output,	copy	the	above	text	and	paste	it	into	afile	with	the	name	test_	
filter.txt	.	
To	apply	the	filtering	in	the	test_	filter.txt	file,	execute	the	following	command:	
sourceanalyzer	-b	eightball	-scan	-filter	test_	filter.txt	
The	filtered	analysis	produces	the	following	results:	
[176CC0B182267DD538992E87EF41815F	:	critical	:	Path	Manipulation	:	
dataflow	]	
EightBall.java	(12)	:	->new	FileReader	(0)	
EightBall.java	(6)	:	<=>	(filename)	
EightBall.java	(4)	:	->EightBall.main	(0)	
[E4B3ACF92911ED6D98AAC15876739EC7	:	high	:	Unreleased	Resource	:	Streams	:	
controlflow	]	
EightBall.java	(12)	:	start	->	loaded	:	new	FileReader	(...)	
EightBall.java	(14)	:	loaded	->	end_	of_	scope	:	end	scope	:	Resource	
leaked	
EightBall.java	(12)	:	start	->	loaded	:	new	FileReader	(...)	
EightBall.java	(12)	:	java.io.IOException	thrown	
EightBall.java	(12)	:	loaded	->	loaded	:	throw	
EightBall.java	(12)	:	loaded	->	end_	of_	scope	:	end	scope	:	Resource	
leaked	:	java.io.IOException	thrown	
User	Guide	
Appendix	A:	Filtering	the	Analysis	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	167	of	216 

Appendix	B:	Fortify	Scan	Wizard	
This	section	contains	the	following	topics:	
Preparing	to	use	the	Fortify	Scan	Wizard	168	
Starting	the	Fortify	Scan	Wizard	169	
Preparing	to	use	the	Fortify	Scan	Wizard	
Fortify	Scan	Wizard	Â uses	the	information	you	provide	to	create	ascript	with	the	commands	for	Fortify	
Static	Code	Analyzer	to	translate	and	scan	project	code	and	optionally	upload	the	results	directly	to	
Micro	Focus	Fortify	Software	Security	Center	.You	can	use	Fortify	Scan	Wizard	to	run	your	scans	locally	
or	upload	them	to	aMicro	Focus	Fortify	ScanCentral	SAST	server.	
Note:	If	you	generate	ascript	on	aWindows	system,	you	cannot	run	that	script	on	anon-	Windows	
system.	Likewise,	if	you	generate	ascript	on	anon-	Windows	system,	you	cannot	run	it	on	a	
Windows	system.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	168	of	216 

To	use	the	Fortify	Scan	Wizard	,you	need	the	following:	
l	Location	of	the	build	directory	or	directories	of	the	project	to	be	scanned	
l	Access	to	the	build	directory	or	directories	of	the	project	to	be	scanned	
l	To	scan	Java	code,	the	version	of	the	Java	JDK	used	to	develop	the	code	
l	To	use	Fortify	ScanCentral	SAST	to	scan	your	code,	the	ScanCentral	Controller	URL	
l	(Optional)	Location	of	custom	rule	files	
To	upload	your	scan	results	to	Fortify	Software	Security	Center	,you	also	need:	
l	The	Fortify	Software	Security	Center	server	URL	
l	Your	Fortify	Software	Security	Center	login	credentials	
l	An	upload	authentication	token	
Note:	If	you	do	not	have	an	upload	token,	you	can	use	the	Fortify	Scan	Wizard	to	generate	one.	
To	do	this,	you	must	have	Fortify	Software	Security	Center	login	credentials.	
If	you	do	not	have	Fortify	Software	Security	Center	login	credentials,	you	must	have	the	following:	
l	Application	name	
l	Application	version	name	
Note:	Fortify	Scan	Wizard	uses	adefault	scan	memory	setting	of	90%	of	the	total	available	memory	
if	it	is	greater	than	4Â GB,	otherwise	the	default	memory	setting	is	2/3	the	total	available	memory.	
Adjust	the	scan	memory	as	necessary	in	the	Translation	and	Scan	step.	
Starting	the	Fortify	Scan	Wizard	
To	start	the	Fortify	Scan	Wizard	with	Fortify	SCA	and	Applications	installed	locally,	do	one	of	the	
following,	based	on	your	operating	system:	
l	On	Windows,	select	Start	>	All	Programs	>	Fortify	SCA	and	Applications	<version>	>Â Scan	
Wizard	.	
l	On	Linux,	navigate	to	the	<sca_	install_	dir>	/bin	directory,	and	then	run	the	ScanWizard	file	
from	the	command	line.	
l	On	macOS	,navigate	to	the	<sca_	install_	dir>	/bin	directory,	and	then	double-	click	
ScanWizard	.	
User	Guide	
Appendix	B:	Fortify	Scan	Wizard	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	169	of	216 

Appendix	C:	Sample	Projects	
The	Fortify	SCA	and	Applications	installation	might	include	several	code	samples	that	you	can	use	to	
when	learning	to	use	Fortify	Static	Code	Analyzer	.If	you	installed	the	sample	files,	they	are	in	the	
following	directory:	
<sca_	install_	dir>	/Samples	
The	Samples	directory	contains	two	subdirectories:	basic	and	advanced	.Each	code	sample	includes	a	
README.txt	file	that	provides	instructions	on	how	to	scan	the	code	with	Fortify	Static	Code	Analyzer	
and	view	the	results	in	Micro	Focus	Fortify	Audit	Workbench	.	
The	basic	subdirectory	includes	an	assortment	of	simple	language-	specific	code	samples.	The	
advanced	subdirectory	includes	more	advanced	samples	including	source	code	to	help	you	integrate	
Fortify	Static	Code	Analyzer	with	your	bug	tracker	application.	For	information	on	integrating	bug	
tracker	applications	with	Fortify	Audit	Workbench	,see	Micro	Focus	Fortify	Audit	Workbench	User	
Guide	.	
This	section	contains	the	following	topics:	
Basic	Samples	170	
Advanced	Samples	172	
Basic	Samples	
The	following	table	describes	the	sample	files	in	the	<sca_	install_	dir>	/Samples/basic	directory	
and	provides	alist	of	the	vulnerabilities	that	the	samples	demonstrate.	Many	of	the	samples	includes	a	
README.txt	file	that	provides	details	and	instructions	on	its	use.	
Folder	Name	Description	Vulnerabilities	
cpp	A	C++	sample	file	and	instructions	to	analyze	code	
that	has	asimple	dataflow	vulnerability.	Fortify	
analysis	requires	agcc	or	cl	compiler.	
Command	Injection	
Memory	Leak	
database	A	database.pks	sample	file.	This	SQL	sample	
includes	issues	in	SQL	code.	
Access	Control:	Database	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	170	of	216 

Folder	Name	Description	Vulnerabilities	
eightball	A	Java	application	(EightBall.java	)that	exhibits	
bad	error	handling.	It	requires	an	integer	argument.	If	
you	supply	afile	name	instead	of	an	integer	as	the	
argument,	it	displays	the	file	contents.	
Path	Manipulation	
Unreleased	Resource:	
Streams
Unchecked	Return	Value	
J2EE	Bad	Practices:	
Leftover	Debug	Code	
formatstring	The	formatstring.c	file.	Fortify	analysis	requires	a	
gcc	or	cl	compiler.	
Format	String	
java13	The	Sample.java	file.	Privacy	Violation	
Insecure	Randomness:	
Hardcoded	Seed	
J2EE	Bad	Practices:	
Leftover	Debug	Code	
Poor	Logging	Practice:	Use	
of	aSystem	Output	System	
javascript	The	sample.js	JavaScript	file.	Cross-	Site	Scripting	
Open	Redirect	
Privacy	Violation	
nullpointer	The	NullPointerSample.java	file.	Null	Dereference	
php	Two	PHP	Â files:	Â sink.php	and	source.php	.	
Analyzing	source.php	reveals	simple	dataflow	
vulnerabilities	and	adangerous	function.	
Cross-	Site	Scripting	
SQL	Injection	
sampleOutput	A	sample	output	file	(WebGoat5.0.fpr	)from	the	
WebGoat	project	located	in	the	
Samples/advanced/webgoat	directory.	
Various	
stackbuffer	The	stackbuffer.c	file.	Fortify	analysis	requires	a	
gcc	or	cl	compiler.	
Buffer	Overflow	
toctou	The	toctou.c	file.	Fortify	analysis	requires	agcc	or	cl	
compiler.	
Time-	of-	Check/Time-	of-	
Use	(Race	Condition)	
User	Guide	
Appendix	C:	Sample	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	171	of	216 

Folder	Name	Description	Vulnerabilities	
vb6	The	command-	injection.bas	file.	Command	Injection	
SQL	Injection	
vbscript	The	source.asp	and	sink.asp	files.	SQL	Injection	
Advanced	Samples	
The	following	table	describes	the	samples	in	the	<sca_	install_	dir>	/Samples/advanced	directory.	
Many	of	the	samples	include	aREADME.txt	file	that	provides	additional	details	and	instructions	on	how	
to	analyze	the	sample.	
Folder	Name	Description	
BugTrackerPlugin
<bugtracker>	
Includes	source	code	for	the	supported	bug	tracker	plugin.	
c++	A	sample	solution	for	each	supported	version	of	Visual	Studio.	
To	use	this	sample,	you	must	have	the	following	installed:	
l	A	supported	version	of	Visual	Studio	Visual	C/C++	
l	Fortify	Static	Code	Analyzer	
l	To	analyze	the	sample	from	Visual	Studio	as	described	in	the	README.txt	
file,	you	must	have	the	Micro	Focus	Fortify	Extension	for	Visual	Studio	
installed	for	your	Visual	Studio	version	
The	code	includes	aCommand	Injection	issue	and	an	Unchecked	Return	Value	
issue.	
configuration	A	sample	Java	EE	application	that	has	vulnerabilities	in	its	web	module	
deployment	descriptor	web.xml	.	
crosstier	A	sample	that	has	vulnerabilities	that	span	multiple	application	technologies	
(Java,	PL/SQL,	JSP,	and	struts).	
The	output	contains	several	issues	of	different	types,	including	two	Access	
Control	vulnerabilities.	One	of	these	is	across-	tier	result.	It	has	adataflow	trace	
from	user	input	in	Java	code	that	can	affect	aSELECT	statement	in	PL/SQL.	
User	Guide	
Appendix	C:	Sample	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	172	of	216 

Folder	Name	Description	
csharp	A	simple	C#	program	that	has	SQL	injection	vulnerabilities.	Versions	are	
included	for	each	supported	version	of	Visual	Studio.	Analysis	of	this	sample	
reveals	SQL	Injection	,Unreleased	Resource,	and	Path	Manipulation	
vulnerabilities.	Other	categories	might	also	be	present,	depending	on	the	
Rulepack	version	used	in	the	scan.	
customrules	Several	simple	source	code	samples	and	Rulepack	files	that	illustrate	how	four	
different	analyzers:	Semantic,	Dataflow,	Control	Flow,	and	Configuration	
interpret	rules.	This	folder	also	includes	several	miscellaneous	samples	of	real-	
world	rules	that	you	can	use	to	scan	real	applications.	
ejb	A	sample	Java	EE	cross-	tier	application	with	Servlets	and	EJBs.	
filters	A	sample	that	uses	the	Fortify	Static	Code	Analyzer	-filter	option.	
findbugs	A	sample	that	demonstrates	how	to	run	the	FindBugs	static	analysis	tool	
together	with	Fortify	Static	Code	Analyzer	and	filter	out	results	that	overlap.	
java1.5	A	sample	Java	file:	ResourceInjection.java	.The	result	file	includes	aPath	
Manipulation,	aJ2EE	Bad	Practices,	and	aPoor	Style	vulnerability.	
javaAnnotations	A	sample	application	that	illustrates	problems	that	might	arise	from	its	use	and	
how	to	fix	the	problems	with	the	Fortify	Java	Annotations.	
This	sample	illustrates	how	the	use	of	Fortify	Annotations	can	result	in	
increased	accuracy	in	the	reported	vulnerabilities.	The	README.txt	file	
describes	the	potential	problems	and	solutions	associated	with	the	sample	
application.	
JavaDoc	JavaDoc	directory	for	the	public-	api	and	WSClient	.	
riches.java	A	Java	EE	1.4	sample	web	application	with	various	known	security	vulnerabilities	
including	Cross-	Site	Scripting,	SQL	Injection,	and	Command	Injection.	
riches.net	A	.NET	4.0	sample	web	application	with	various	known	security	vulnerabilities	
including	Cross-	Site	Scripting,	SQL	Injection,	and	Command	Injection.	
swift	The	iGoat-	Swift	folder	contains	the	iGoat-	Swift	source	provided	by	the	Open	
Web	Application	Security	Project	(OWASP).	To	analyze	this	project,	you	must	
have	asupported	xcodebuild	version	installed.	The	README.txt	file	describes	
the	vulnerabilities	that	are	revealed	in	the	analysis	of	this	application.	
User	Guide	
Appendix	C:	Sample	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	173	of	216 

Folder	Name	Description	
webgoat	The	WebGoat	test	Java	EE	web	application	provided	by	the	Open	Web	
Application	Security	Project	(OWASP).	This	directory	contains	the	WebGoat	5.0	
source	code.	
User	Guide	
Appendix	C:	Sample	Projects	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	174	of	216 

Appendix	D:	Fortify	Java	Annotations	
Fortify	provides	two	versions	of	the	Java	Fortify	annotations	library.	
l	Annotations	with	the	retention	policy	set	to	CLASS	(FortifyAnnotations-	CLASS.jar	).	
With	this	version	of	the	library,	Fortify	annotations	are	propagated	to	the	bytecode	during	
compilation.	
l	Annotations	with	the	retention	policy	set	to	SOURCE	(FortifyAnnotations-	SOURCE.jar	).	
With	this	version	of	the	library,	Fortify	annotations	are	not	propagated	to	the	bytecode	after	the	
code	that	uses	them	is	compiled.	
If	you	use	Fortify	products	to	analyze	bytecode	of	your	applications	(for	example,	with	Fortify	on	
Demand	assessments),	then	use	the	version	with	the	annotation	retention	policy	set	to	CLASS.	If	you	
use	Fortify	products	to	analyze	the	source	code	of	your	applications,	you	can	use	either	version	of	the	
library,	however	Fortify	strongly	recommends	that	you	use	the	library	with	retention	policy	set	to	
SOURCE.
Important!	Leaving	Fortify	annotations	in	production	code	is	asecurity	risk	because	they	can	leak	
information	about	potential	security	problems	in	the	code.	Fortify	recommends	that	you	use	
annotations	with	the	retention	policy	set	to	CLASS	only	for	internal	Fortify	analysis,	and	never	use	
them	in	your	application	production	builds.	
This	section	outlines	the	annotations	available.	A	sample	application	is	included	with	the	Fortify	SCA	
and	Applications	samples	installation	Â in	the	<sca_	install_	
dir>	/Samples/advanced/javaAnnotations	directory.	A	README.txt	file	included	in	the	directory	
describes	the	sample	application,	problems	that	might	arise	from	it,	and	how	to	fix	these	problems	using	
the	Fortify	Java	Annotations.	
There	are	two	limitations	with	Fortify	Java	annotations:	
l	Each	annotation	can	specify	only	one	input	and/or	one	output.	
l	You	can	apply	only	one	annotation	of	each	type	to	the	same	target.	
Fortify	provides	three	main	types	of	annotations:	
l	""Dataflow	Annotations""	below	
l	""Field	and	Variable	Annotations""	on	page	Â 178	
l	""Other	Annotations""	on	page	Â 179	
You	also	can	write	rules	to	support	your	own	custom	annotations.	Contact	Micro	Focus	Fortify	
Customer	Support	for	more	information.	
Dataflow	Annotations	
There	are	four	types	of	Dataflow	annotations,	similar	to	Dataflow	rules:	Source,	Sink,	Passthrough,	and	
Validate.	All	are	applied	to	methods	and	specify	the	inputs	and/or	outputs	by	parameter	name	or	the	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	175	of	216 

strings	this	and	return	.Additionally,	you	can	apply	the	Dataflow	Source	and	Sink	annotations	to	the	
function	arguments.	
Source	Annotations	
The	acceptable	values	for	the	annotation	parameter	are	this	,return	,or	afunction	parameter	name.	
For	example,	you	can	assign	taint	to	an	output	of	the	target	method.	
@FortifyDatabaseSource	(""return""	)	
String	[]	loadUserProfile	(String	userID	)	{	
...
}	
For	example,	you	can	assign	taint	to	an	argument	of	the	target	method.	
void	retrieveAuthCode	(@FortifyPrivateSource	String	authCode	)	{	
...
}	
In	addition	to	specific	source	annotations,	Fortify	provides	ageneric	untrusted	taint	source	called	
FortifySource	.	
The	following	is	acomplete	list	of	source	annotations:	
l	FortifySource	
l	FortifyDatabaseSource	
l	FortifyFileSystemSource	
l	FortifyNetworkSource	
l	FortifyPCISource	
l	FortifyPrivateSource	
l	FortifyWebSource	
Passthrough	Annotations	
Passthrough	annotations	transfer	any	taint	from	an	input	to	an	output	of	the	target	method.	It	can	also	
assign	or	remove	taint	from	the	output,	in	the	case	of	FortifyNumberPassthrough	and	
FortifyNotNumberPassthrough	.The	acceptable	values	for	the	in	annotation	parameter	are	""this""	
or	afunction	parameter	name.	The	acceptable	values	for	the	out	annotation	parameter	are	this	,	
return	,or	afunction	parameter	name.	
@FortifyPassthrough	(in=	""a""	,out=	""return""	)	
String	toLowerCase	(String	a)	{	
...
}	
User	Guide	
Appendix	D:	Fortify	Java	Annotations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	176	of	216 

Use	FortifyNumberPassthrough	to	indicate	that	the	data	is	purely	numeric.	Numeric	data	cannot	
cause	certain	types	of	issues,	such	as	cross-	site	scripting,	regardless	of	the	source.	Using	
FortifyNumberPassthrough	can	reduce	false	positives	of	this	type.	If	aprogram	decomposes	
character	data	into	anumeric	type	(int,	int	[],	and	so	on),	you	can	use	FortifyNumberPassthrough	.If	
aprogram	concatenates	numeric	data	into	character	or	string	data,	then	use	
FortifyNotNumberPassthrough	.	
The	following	is	acomplete	list	of	passthrough	annotations:	
l	FortifyPassthrough	
l	FortifyNumberPassthrough	
l	FortifyNotNumberPassthrough	
Sink	Annotations	
Sink	annotations	report	an	issue	when	taint	of	the	appropriate	type	reaches	an	input	of	the	target	
method.	Acceptable	values	for	the	annotation	parameter	are	this	or	afunction	parameter	name.	
@FortifyXSSSink	(""a""	)	
void	printToWebpage	(int	a)	{	
...
}	
You	can	also	apply	the	annotation	to	the	function	argument	or	the	return	parameter.	In	the	following	
example,	an	issue	is	reported	when	taint	reaches	the	argument	a.	
void	printToWebpage	(int	b,	@FortifyXSSSink	String	a)	{	
...
}	
The	following	is	acomplete	list	of	the	sink	annotations:	
l	FortifySink	
l	FortifyCommandInjectionSink	
l	FortifyPCISink	
l	FortifyPrivacySink	
l	FortifySQLSink	
l	FortifySystemInfoSink	
l	FortifyXSSSink	
User	Guide	
Appendix	D:	Fortify	Java	Annotations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	177	of	216 

Validate	Annotations	
Validate	annotations	remove	taint	from	an	output	of	the	target	method.	Acceptable	values	for	the	
annotation	parameter	are	this	,return	,or	afunction	parameter	name.	
@FortifyXSSValidate	(""return""	)	
String	xssCleanse	(String	a)	{	
...
}	
The	following	is	acomplete	list	of	validate	sink	annotations:	
l	FortifyValidate	
l	FortifyCommandInjectionValidate	
l	FortifyPCIValidate	
l	FortifyPrivacyValidate	
l	FortifySQLValidate	
l	FortifySystemInfoValidate	
l	FortifyXSSValidate	
Field	and	Variable	Annotations	
You	can	apply	these	annotations	to	fields	and	(in	most	cases)	variables.	
Password	and	Private	Annotations	
Use	password	and	private	annotations	to	indicate	whether	the	target	field	or	variable	is	apassword	or	
private	data.	
@FortifyPassword	String	x;	
@FortifyNotPassword	String	pass;	
@FortifyPrivate	String	y;	
@FortifyNotPrivate	String	cc;	
In	the	previous	example,	string	xwill	be	identified	as	apassword	and	checked	for	privacy	violations	and	
hardcoded	passwords.	The	string	pass	will	not	be	identified	as	apassword.	Without	the	annotation,	it	
might	cause	false	positives.	The	FortifyPrivate	and	FortifyNotPrivate	annotations	work	
similarly,	only	they	do	not	cause	privacy	violation	issues.	
User	Guide	
Appendix	D:	Fortify	Java	Annotations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	178	of	216 

Non-	Negative	and	Non-	Zero	Annotations	
Use	these	annotations	to	indicate	disallowed	values	for	the	target	field	or	variable.	
@FortifyNonNegative	int	index;	
@FortifyNonZero	double	divisor;	
In	the	previous	example,	an	issue	is	reported	if	anegative	value	is	assigned	to	index	or	zero	is	assigned	
to	divisor	.	
Other	Annotations	
Check	Return	Value	Annotation	
Use	the	FortifyCheckReturnValue	annotation	to	add	atarget	method	to	the	list	of	functions	that	
require	acheck	of	the	return	values.	
@FortifyCheckReturnValue
int	openFile	(String	filename	){	
...
}	
Dangerous	Annotations	
With	the	FortifyDangerous	annotation,	any	use	of	the	target	function,	field,	variable,	or	class	is	
reported.	Acceptable	values	for	the	annotation	parameter	are	CRITICAL	,HIGH	,MEDIUM	,or	LOW	.These	
values	indicat	how	to	categorize	the	issue	based	on	the	Fortify	Priority	Order	values).	
@FortifyDangerous	{""CRITICAL""	}	
public	class	DangerousClass	{	
@FortifyDangerous	{""HIGH""	}	
String	dangerousField;	
@FortifyDangerous	{""LOW""	}	
int	dangerousMethod	()	{	
...
}
}	
User	Guide	
Appendix	D:	Fortify	Java	Annotations	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	179	of	216 

Appendix	E:	Configuration	Options	
The	Fortify	SCA	and	Applications	installer	places	aset	of	properties	files	on	your	system.	Properties	files	
contain	configurable	settings	for	Micro	Focus	Fortify	Static	Code	Analyzer	runtime	analysis,	output,	and	
performance.
This	section	contains	the	following	topics:	
Fortify	Static	Code	Analyzer	Properties	Files	180	
fortify-	sca.properties	182	
fortify-	sca-	quickscan.properties	211	
Fortify	Static	Code	Analyzer	Properties	Files	
The	properties	files	are	located	in	the	<sca_	install_	dir>	/Core/config	directory.	
The	installed	properties	files	contain	default	values.	Fortify	recommends	that	you	consult	with	your	
project	leads	before	you	make	changes	to	the	properties	in	the	properties	files.	You	can	modify	any	of	
the	properties	in	the	configuration	file	with	any	text	editor.	You	can	also	specify	the	property	on	the	
command	line	with	the	-D	option.	
The	following	table	describes	the	primary	properties	files.	Additional	properties	files	are	described	in	
Micro	Focus	Fortify	Static	Code	Analyzer	Tools	Properties	Reference	Guide	.	
Properties	File	Name	Description	
fortify-	sca.properties	Defines	the	Fortify	Static	Code	Analyzer	configuration	properties.	
fortify-	sca-	
quickscan.properties	
Defines	the	configuration	properties	applicable	for	aFortify	Static	
Code	Analyzer	quick	scan.	
Properties	File	Format	
In	the	properties	file,	each	property	consists	of	apair	of	strings:	the	first	string	is	the	property	name	and	
the	second	string	is	the	property	value.	
com.fortify.sca.fileextensions.htm=HTML
As	shown	above,	the	property	sets	the	translation	to	use	for	.htm	files.	The	property	name	is	
com.fortify.sca.fileextension.htm	and	the	value	is	set	to	HTML	.	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	180	of	216 

Note:	When	you	specify	apath	for	Windows	systems	as	the	property	value,	you	must	escape	any	
backslash	character	(\)	with	abackslash	(for	example:	
com.fortify.sca.ASPVirtualRoots.Library=C:\\WebServer\\CustomerA\\inc	).	
Disabled	properties	are	commented	out	of	the	properties	file.	To	enable	these	properties,	remove	the	
comment	symbol	(#)	and	save	the	properties	file.	In	the	following	example,	the	
com.fortify.sca.LogFile	property	is	disabled	in	the	properties	file	and	is	not	part	of	the	
configuration:
#	default	location	for	the	log	file	
#com.fortify.sca.LogFile=${com.fortify.sca.ProjectRoot}/sca/log/sca.log
Precedence	of	Setting	Properties	
Fortify	Static	Code	Analyzer	uses	properties	settings	in	aspecific	order.	You	can	override	any	previously	
set	properties	with	the	values	that	you	specify.	Keep	this	order	in	mind	when	making	changes	to	the	
properties	files.	
The	following	table	lists	the	order	of	precedence	for	Fortify	Static	Code	Analyzer	properties.	
Order	Property	Â Specification	Description	
1	Command	line	with	the	
-D	option	
Properties	specified	on	the	command	line	have	the	highest	
priority	and	you	can	specify	them	in	any	scan.	
2	Fortify	Static	Code	
Analyzer	Â quick	scan	
configuration	file	
Note:	You	can	specify	either	quick	scan	or	ascan	precision	
level.	Therefore,	these	property	settings	both	have	second	
priority.
Properties	specified	in	the	quick	scan	configuration	file	
(fortify-	sca-	quickscan.properties	)have	the	second	
priority,	but	only	if	you	include	the	-quick	option	to	enable	quick	
scan	mode.	
Fortify	Static	Code	
Analyzer	scan	precision	
property	files	
Properties	specified	in	the	scan	precision	property	files	have	the	
second	priority,	but	only	if	you	include	the	-scan-	precision	
option	to	enable	scan	precision.	
3	Fortify	Static	Code	
Analyzer	configuration	
file	
Properties	specified	in	the	Fortify	Static	Code	Analyzer	
configuration	file	(fortify-	sca.properties	)have	the	lowest	
priority.	Edit	this	file	to	change	the	property	values	on	amore	
permanent	basis	for	all	scans.	
Fortify	Static	Code	Analyzer	also	relies	on	some	properties	that	have	internally	defined	default	values.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	181	of	216 

fortify-	sca.properties	
The	following	table	summarizes	the	properties	available	for	use	in	the	fortify-	sca.properties	file.	
See	""fortify-	sca-	quickscan.properties""	on	page	Â 211	for	additional	properties	that	you	can	use	in	this	
properties	file.	The	description	for	each	property	includes	the	value	type,	the	default	value,	the	
equivalent	command-	line	option	(if	applicable),	and	an	example.	
Property	Name	Description	
com.fortify.sca.
BuildID	
Specifies	the	build	ID	of	the	build.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-b	
com.fortify.sca.
ProjectRoot	
Specifies	the	folder	to	store	intermediate	files	generated	in	
the	translation	and	scan	phases.	Fortify	Static	Code	
Analyzer	makes	extensive	use	of	intermediate	files	located	
in	this	project	root	directory.	In	some	cases,	you	achieve	
better	performance	for	analysis	by	making	sure	this	
directory	is	on	local	storage	rather	than	on	anetwork	
drive.
Value	Type:	String	(path)	
Default	(Windows):	
${win32.LocalAppdata}\Fortify
Note:	${win32.LocalAppdata}	is	aspecial	variable	
that	points	to	the	windows	Local	Application	Data	
shell	folder.	
Default	(Non-	Windows):	$home/.fortify	
Command-	Line	Option:	-project-	root	
Example:	com.fortify.sca.ProjectRoot=	
C:\Users\	<user>	\AppData\Local\	
com.fortify.sca.
DisableDeadCode
Elimination	
Dead	code	is	code	that	can	never	be	executed,	such	as	
code	inside	the	body	of	an	if	statement	that	always	
evaluates	to	false.	If	this	property	is	set	to	true,	then	
Fortify	Static	Code	Analyzer	does	not	identify	dead	code,	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	182	of	216 

Property	Name	Description
does	not	report	dead	code	issues,	and	reports	other	
vulnerabilities	in	the	dead	code,	even	though	they	are	
unreachable	during	execution.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
DeadCodeFilter	
If	set	to	true,	Fortify	Static	Code	Analyzer	removes	dead	
code	issues,	for	example	because	the	compiler	generated	
dead	code	and	it	does	not	appear	in	the	source	code.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
fileextensions.java
com.fortify.sca.
fileextensions.cs
com.fortify.sca.
fileextensions.js
com.fortify.sca.
fileextensions.py
com.fortify.sca.
fileextensions.rb
com.fortify.sca.
fileextensions.aspx
com.fortify.sca.
fileextensions.php
Note:	This	is	apartial	list.	For	the	
complete	list,	see	the	properties	file.	
Specifies	how	to	translate	specific	file	extensions	for	
languages	that	do	not	require	build	integration.	The	valid	
types	are:	ABAP,	ACTIONSCRIPT,	APEX,	APEX_	
TRIGGER,	ARCHIVE,	ASPNET,	ASP,	ASPX,	BITCODE,	
BYTECODE,	CFML,	COBOL,	CSHARP,	DOCKERFILE,	GO,	
HTML,	JAVA,	JAVA_	PROPERTIES,	JAVASCRIPT,	JSP,	
JSPX,	KOTLIN,	MSIL,	MXML,	PHP,	PLSQL,	PYTHON,	
RUBY,	RUBY_	ERB,	SCALA,	SWIFT,	TLD,	SQL,	TSQL,	
TYPESCRIPT,	VB,	VB6,	VBSCRIPT,	VISUAL_	FORCE,	and	
XML.
Value	Type:	String	(valid	language	type)	
Default:	See	the	fortify-	sca.properties	file	for	the	
complete	list.	
Examples:
com.fortify.sca.fileextensions.java=JAVA
com.fortify.sca.fileextensions.cs=CSHARP
com.fortify.sca.fileextensions.js=TYPESCRIP
T
com.fortify.sca.fileextensions.py=PYTHON
com.fortify.sca.fileextensions.rb=RUBY
com.fortify.sca.fileextensions.aspx=ASPNET
com.fortify.sca.fileextensions.php=PHP	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	183	of	216 

Property	Name	Description
You	can	also	specify	avalue	of	oracle:	<path_	to_	
script>	to	programmatically	supply	alanguage	type.	
Provide	ascript	that	accepts	one	command-	line	parameter	
of	afile	name	that	matches	the	specified	file	extension.	
The	script	must	write	the	valid	Fortify	Static	Code	
Analyzer	file	type	(see	previous	list)	Â to	stdout	and	exit	
with	areturn	value	of	zero.	If	the	script	returns	anon-	zero	
return	code	or	the	script	does	not	exist,	the	file	is	not	
translated	and	Fortify	Static	Code	Analyzer	writes	a	
warning	to	the	log	file.	
Example:
com.fortify.sca.fileextensions.jsp=
oracle:	<path_	to_	script>	
com.fortify.sca.
compilers.javac=
com.fortify.sca.
util.compilers.
JavacCompiler
com.fortify.sca.
compilers.c++=
com.fortify.sca.
util.compilers.
GppCompiler
com.fortify.sca.
compilers.make=
com.fortify.sca.
util.compilers.
TouchlessCompiler
com.fortify.sca.
compilers.mvn=
com.fortify.sca.
util.compilers.
MavenAdapter
Note:	This	is	apartial	list.	For	the	
complete	list,	
see	the	properties	file.	
Specifies	custom-	named	compilers.	
Value	Type:	String	(compiler)	
Default:	See	the	Compilers	section	in	the	fortify-	
sca.properties	file	for	the	complete	list.	
Example:
To	tell	Fortify	Static	Code	Analyzer	that	âmy-	gccâ	is	agcc	
compiler:
com.fortify.sca.
compilers.my-	gcc=	
com.fortify.sca.util.compilers.
GccCompiler
Notes:
l	Compiler	names	can	begin	or	end	with	an	asterisk	
(*),	which	matches	zero	or	more	characters.	
l	Execution	of	Apple	LLVM	Â clang/clang++	is	not	
supported	with	the	gcc/g++	command	names.	You	
can	specify	the	following:	
com.fortify.sca.compilers.g++=
com.fortify.sca.util.compilers.
GppCompiler	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	184	of	216 

Property	Name	Description	
com.fortify.sca.
UseAntListener	
If	set	to	true,	Fortify	Static	Code	Analyzer	includes	
com.fortify.dev.ant.SCAListener	in	the	compiler	
options.
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
exclude	
Specifies	afile	or	alist	of	files	to	exclude	from	translation.	
Separate	the	file	list	with	semicolons	(Windows)	or	colons	
(non-	Windows	systems).	
Note:	Fortify	Static	Code	Analyzer	only	uses	this	
property	during	translation	without	build	integration.	
When	you	integrate	with	acompiler	or	build	tool,	
Fortify	Static	Code	Analyzer	translates	all	source	files	
that	the	compiler	or	build	tool	processes	even	if	they	
are	specified	with	this	property.	
Value	Type:	String	(list	of	file	names)	
Default:	Not	enabled	
Command-	Line	Option:	-exclude	
Example:	com.fortify.sca.exclude=	
file1.x;file2.x	
com.fortify.sca.
CmdlineOptionsFileEncoding	
Specifies	the	encoding	of	the	command-	line	options	file	
provided	with	@<filename>	(see	""Other	Options""	on	
page	Â 120	).	You	can	use	this	property,	for	example,	to	
specify	Unicode	file	paths	in	the	options	file.	Valid	
encoding	names	are	from	the	
java.nio.charset.Charset
Note:	This	property	is	only	valid	in	the	fortify-	
sca.properties	file	and	does	not	work	in	the	
fortify-	sca-	quickscan.properites	file	or	with	
the	-D	option.	
Value	Type:	String	
Default:	JVM	system	default	encoding	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	185	of	216 

Property	Name	Description
Example:
com.fortify.sca.CmdLineOptionsFileEncoding=
UTF-	8	
com.fortify.sca.
InputFileEncoding	
Specifies	the	source	file	encoding	type.	Fortify	Static	Code	
Analyzer	allows	you	to	scan	aproject	that	contains	
differently	encoded	source	files.	To	work	with	amulti-	
encoded	project,	you	must	specify	the	-encoding	option	
in	the	translation	phase,	when	Fortify	Static	Code	
Analyzer	first	reads	the	source	code	file.	Fortify	Static	
Code	Analyzer	remembers	this	encoding	in	the	build	
session	and	propagates	it	into	the	FVDL	file.	
Typically,	if	you	do	not	specify	the	encoding	type,	Fortify	
Static	Code	Analyzer	uses	file.encoding	from	the	
java.io.InputStreamReader	constructor	with	no	
encoding	parameter.	In	afew	cases	(for	example	with	the	
ActionScript	parser),	Fortify	Static	Code	Analyzer	defaults	
to	UTF-	8.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-encoding	
Example:
com.fortify.sca.InputFileEncoding=UTF-	16	
com.fortify.sca.
xcode.TranslateAfterError	
Specifies	whether	the	xcodebuild	touchless	adapter	
continues	translation	if	the	xcodebuild	subprocess	exited	
with	anon-	zero	exit	code.	If	set	to	false,	translation	stops	
after	encountering	anon-	zero	xcodebuild	exit	code	and	
the	Fortify	Static	Code	Analyzer	touchless	build	halts	with	
the	same	exit	code.	If	set	to	true,	the	Fortify	Static	Code	
Analyzer	touchless	build	executes	translation	of	the	build	
file	identified	prior	to	the	xcodebuild	exit,	and	Fortify	
Static	Code	Analyzer	exits	with	an	exit	code	of	zero	
(unless	some	other	error	also	occurs).	
Regardless	of	this	setting,	if	xcodebuild	exits	with	anon-	
zero	code,	then	the	xcodebuild	exit	code,	stdout,	and	
stderr	are	written	to	the	log	file.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	186	of	216 

Property	Name	Description
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
Apex	
If	set	to	true,	Fortify	Static	Code	Analyzer	uses	Apex	
translation	for	files	with	the	.cls	extension	and	
Visualforce	translation	for	files	with	the	.component	
extension.
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-apex	
com.fortify.sca.
ApexObjectPath	
Specifies	the	absolute	path	of	the	custom	sObject	JSON	
file	sobjects.json	.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-apex-	sobject-	path	
com.fortify.sca.
AddImpliedMethods	
If	set	to	true,	Fortify	Static	Code	Analyzer	generates	
implied	methods	when	it	encounters	implementation	by	
inheritance.
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
DefaultAnalyzers	
Specifies	acomma-	or	colon-	separated	list	of	the	types	of	
analysis	to	perform.	The	valid	values	for	this	property	are	
buffer	,content	,configuration	,controlflow	,	
dataflow	,findbugs	,nullptr	,semantic	,and	
structural	.	
Value	Type:	String	
Default:	This	property	is	commented	out	and	all	analysis	
types	are	used	in	scans.	
Command-	Line	Option:	-analyzers	
com.fortify.sca.
EnableAnalyzer	
Specifies	acomma-	or	colon-	separated	list	of	analyzers	to	
use	for	ascan	in	addition	to	the	default	analyzers.	The	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	187	of	216 

Property	Name	Description
valid	values	for	this	property	are	buffer	,content	,	
configuration	,controlflow	,dataflow	,findbugs	,	
nullptr	,semantic	,and	structural	.	
Value	Type:	String	
Default:	(none)	
com.fortify.sca.
ExitCodeLevel	
Extends	the	default	exit	code	options.	See	""Exit	Codes""	on	
page	Â 156	for	adescription	of	the	exit	codes.	The	valid	
values	are:	
The	valid	values	are:	
l	nothing	â	Returns	exit	codes	0,	1,	2,	or	3.	This	is	the	
default	setting.	
l	warnings	â	Returns	exit	codes	0,	1,	2,	3,	4,	or	5.	
l	errors	â	Returns	exit	codes	0,	1,	2,	3,	or	5.	
l	no_	output_	file	â	Returns	exit	codes	0,	1,	2,	3,	or	6.	
com.fortify.sca.
hoa.Enable	
If	set	to	true,	higher-	order	analysis	is	enabled.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
Phase0HigherOrder.
Languages	
The	languages	for	which	to	run	higher-	order	analysis.	
Valid	values	are	python	,swift	,ruby	,javascript	,and	
typescript	.	
Value	Type:	String	(comma-	separated	list	of	languages)	
Default:
python,ruby,swift,javascript,typescript	
com.fortify.sca.
Phase0HigherOrder.Timeout.
Hard	
Specifies	the	total	time	(in	seconds)	for	higher-	order	
analysis.	When	the	analyzer	reaches	the	hard	timeout	limit,	
it	exits	immediately.	
Fortify	recommends	this	timeout	limit	in	case	some	issue	
causes	the	analysis	to	run	too	long.	Fortify	recommends	
that	you	set	the	hard	timeout	to	about	50%	longer	than	
the	soft	timeout,	so	that	either	the	fixpoint	pass	limiter	or	
the	soft	timeout	occurs	first.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	188	of	216 

Property	Name	Description
Value	Type:	Number	
Default:	2700	
com.fortify.sca.
PrecisionLevel	
Specifies	the	scan	precision.	Scans	with	alower	precision	
level	are	performed	faster.	The	valid	values	are	1	and	2.	
Value	Type:	Number	
Default:	(none)	
Command-	Line	Option:	-scan-	precision	|-p	
com.fortify.sca.
MaxPassthroughChainDepth	
Specifies	the	length	of	ataint	path	between	input	Â and	
output	parameters	in	afunction	call.	
Value	Type:	Integer	
Default:	4	
com.fortify.sca.
TypeInferenceLanguages	
Comma-	or	colon-	separated	list	of	languages	that	use	type	
inference.	This	setting	improves	the	precision	of	the	
analysis	for	dynamically-	typed	languages.	
Value	Type:	String	
Default:	javascript,python,ruby,typescript	
com.fortify.sca.
TypeInferencePhase0
Timeout	
The	total	amount	of	time	(in	seconds)	Â that	type	inference	
can	spend	in	phase	0	(the	interprocedural	analysis).	
Unlimited	if	set	to	zero	or	is	not	specified.	
Value	Type:	Long	
Default:	300	
com.fortify.sca.
TypeInferenceFunctionTimeout	
The	amount	of	time	(in	seconds)	that	type	inference	can	
spend	to	analyze	asingle	function.	Unlimited	if	set	to	zero	
or	is	not	specified.	
Value	Type:	Long	
Default:	60	
com.fortify.sca.
DisableFunctionPointers	
If	set	to	true,	disables	function	pointers	during	the	scan.	
Value	Type:	Boolean	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	189	of	216 

Property	Name	Description
Default:	false	
com.fortify.sca.
RulesFileExtensions	
Specifies	alist	of	file	extensions	for	rules	files.	Any	files	in	
<sca_	install_	dir>	/Core/config/rules	(or	a	
directory	specified	with	the	-rules	option)	whose	
extension	is	in	this	list	is	included.	The	.bin	extension	is	
always	included,	regardless	of	the	value	of	this	property.	
The	delimiter	for	this	property	is	the	system	path	
separator.
Value	Type:	String	
Default:	.xml	
com.fortify.sca.
RulesFile	
Specifies	acustom	Rulepack	or	directory.	If	you	specify	a	
directory,	all	of	the	files	in	the	directory	with	the	.bin	and	
.xml	extensions	are	included.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-rules	
com.fortify.sca.
NoDefaultRules	
If	set	to	true,	rules	from	the	default	Rulepacks	are	not	
loaded.	Fortify	Static	Code	Analyzer	processes	the	
Rulepacks	for	description	elements	and	language	libraries,	
but	no	rules	are	processed.	
Value	Type:	Â Boolean	
Default:	(none)	
Command-	Line	Option:	-no-	default-	rules	
com.fortify.sca.
NoDefaultIssueRules	
If	set	to	true,	disables	rules	in	default	Rulepacks	that	lead	
directly	to	issues.	Still	loads	rules	that	characterize	the	
behavior	of	functions.	This	can	be	helpful	when	creating	
custom	issue	rules.	
Value	Type:	Â Boolean	
Default:	(none)	
Command-	Line	Option:	-no-	default-	issue-	rules	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	190	of	216 

Property	Name	Description	
com.fortify.sca.
NoDefaultSourceRules	
If	set	to	true,	disables	source	rules	in	the	default	
Rulepacks.	This	can	be	helpful	when	creating	custom	
source	rules.	
Note:	Characterization	source	rules	are	not	disabled.	
Value	Type:	Â Boolean	
Default:	(none)	
Command-	Line	Option:	-no-	default-	source-	rules	
com.fortity.sca.
NoDefaultSinkRules	
If	set	to	true,	disables	sink	rules	in	the	default	Rulepacks.	
This	can	be	helpful	when	creating	custom	sink	rules.	
Note:	Characterization	sink	rules	are	not	disabled.	
Value	Type:	Â Boolean	
Default:	(none)	
Command-	Line	Option:	-no-	default-	sink-	rules	
com.fortify.sca.
DefaultRulesDir	
Sets	the	directory	used	to	search	for	the	Fortify	provided	
encrypted	rules	files.	
Value	Type:	String	(path)	
Default:
${com.fortify.Core}/config/rules	
com.fortify.sca.
CustomRulesDir	
Sets	the	directory	used	to	search	for	custom	rules.	
Value	Type:	String	(path)	
Default:
${com.fortify.Core}/config/customrules	
com.fortify.sca.
EnableFindbugs	
If	set	to	true,	FindBugs	is	enabled	as	part	of	the	scan.	
Value	Type:	Boolean	
Default:	true	
Command-	Line	Option:	-findbugs	
com.fortify.sca.
findbugs.maxheap	
Sets	the	maximum	heap	size	for	findbugs.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	191	of	216 

Property	Name	Description
Value	Type:	String	
Default:	Maximum	heap	size	for	Fortify	Static	Code	
Analyzer
Example:
com.fortify.sca.findbugs.maxheap=500m	
com.fortify.sca.
SuppressLowSeverity	
If	set	to	true,	Fortify	Static	Code	Analyzer	ignores	low	
severity	issues	found	in	ascan.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
LowSeverityCutoff	
Specifies	the	cutoff	level	for	severity	suppression.	Fortify	
Static	Code	Analyzer	ignores	any	issues	found	with	a	
lower	severity	value	than	the	one	specified	for	this	
property.
Value	Type:	Number	
Default:	1.0	
com.fortify.sca.
analyzer.controlflow.
EnableTimeOut	
Specifies	whether	to	enable	Control	Flow	Analyzer	
timeouts.
Value	Type:	Â Boolean	
Default:	true	
com.fortify.sca.
RegExecutable	
On	Windows	platforms,	specifies	the	path	to	the	reg.exe	
system	utility.	Specify	the	paths	in	Windows	syntax,	not	
Cygwin	syntax,	even	when	you	run	Fortify	Static	Code	
Analyzer	from	within	Cygwin.	Escape	backslashes	with	an	
additional	backslash.	
Value	Type:	String	(path)	
Default:	reg	
Example:
com.fortify.sca.RegExecutable=
C:\\Windows\\System32\\reg.exe	
com.fortify.sca.
FilterFile	
Specifies	the	path	to	afilter	file	for	the	scan.	See	""Filter	
Files""	on	page	Â 165	for	more	information.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	192	of	216 

Property	Name	Description
Value	Type:	Â String	(path)	
Default:	(none)	
Command-	Line	Option:	-filter	
com.fortify.sca.
FilteredInstanceIDs	
Specifies	acomma-	separated	list	of	IIDs	to	be	filtered	out	
using	afilter	file.	
Value	Type:	String	
Default:	(none)	
com.fortify.sca.
BinaryName	
Specifies	asubset	of	source	files	to	scan.	Only	the	source	
files	that	were	linked	in	the	named	binary	at	build	time	are	
included	in	the	scan.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-bin	or	Â -binary-	name	
com.fortify.sca.
QuickScanMode	
If	set	to	true,	Fortify	Static	Code	Analyzer	performs	a	
quick	scan.	Fortify	Static	Code	Analyzer	uses	the	settings	
from	fortify-	sca-	quickscan.properties	,instead	
of	the	fortify-	sca.properties	configuration	file.	
Value	Type:	Boolean	
Default:	(not	enabled)	
Command-	Line	Option:	-quick	
com.fortify.sca.
ProjectTemplate	
Specifies	the	issue	template	file	to	use	for	the	scan.	This	
only	affects	scans	on	the	local	machine.	If	you	upload	the	
FPR	Â to	Micro	Focus	Fortify	Software	Security	Center	
server,	it	uses	the	issue	template	assigned	to	the	
application	version.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-project-	template	
Example:
com.fortify.sca.ProjectTemplate=	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	193	of	216 

Property	Name	Description
test_	issuetemplate.xml	
com.fortify.sca.
ScanScaModule	
If	set	to	true,	Fortify	Static	Code	Analyzer	performs	
modular	scan	of	this	project,	which	enables	use	of	this	
library's	build	ID	with	the	include-	modules	option	(or	
the	com.fortify.sca.IncludeScaModules	property)	
in	subsequent	scans.	
This	property	is	ignored	if	the	-scan	command-	line	option	
is	specified.	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-scan-	module	
com.fortify.sca.
IncludeScaModules	
Specifies	acomma-	or	colon-	separated	list	of	build	IDs	for	
libraries	pre-	scanned	as	separate	modules	to	use	in	the	
project	scan.	Each	build	ID	must	denote	an	existing	
scanned	library.	
Value	Type:	String	(build	IDs)	
Default:	(none)	
Command-	Line	Option:	-include-	modules	
Example:
com.fortify.sca.IncludeScaModules=LibA,LibB	
com.fortify.sca.
alias.Enable	
If	set	to	true,	enables	alias	analysis.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
UniversalBlacklist	
Specifies	alist	of	functions	to	hide	from	all	analyzers.	
Value	Type:	String	(colon-	separated	list)	
Default:	.*yyparse.*	
com.fortify.sca.
MultithreadedAnalysis	
Specifies	whether	or	not	Fortify	Static	Code	Analyzer	runs	
in	parallel	analysis	mode.	
Value	Type:	Boolean	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	194	of	216 

Property	Name	Description
Default:	true	
com.fortify.sca.
ThreadCount	
Specifies	the	number	of	threads	for	parallel	analysis	mode.	
Add	this	property	only	if	you	need	to	reduce	the	number	
of	threads	used	because	of	aresource	constraint.	If	you	
experience	an	increase	in	scan	time	or	problems	with	your	
scan,	areduction	in	the	number	of	threads	used	might	
solve	the	problem.	
Value	type:	Integer	
Default:	(number	of	available	processor	cores)	
com.fortify.sca.
DISabledLanguages	
Add	acolon-	separated	list	of	languages	to	exclude	from	
the	translation	phase.	The	valid	language	values	are	abap	,	
actionscript	,apex	,cfml	,cobol	,cpp	,csharp	,	
golang	,java	,javascript	,jsp	,kotlin	,objc	,php	,	
plsql	,python	,ruby	,scala	,sql	,swift	,tsql	,	
typescript	,and	vb	.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-disable-	language	
com.fortify.sca.
EnabledLanguages	
Specifies	acolon-	separated	list	of	languages	to	translate.	
The	valid	language	values	are	abap	,actionscript	,	
apex	,cfml	,cobol	,cpp	,csharp	,golang	,java	,	
javascript	,jsp	,kotlin	,objc	,php	,plsql	,python	,	
ruby	,scala	,sql	,swift	,tsql	,typescript	,and	vb	.	
Value	Type:	String	
Default:	All	languages	in	the	specified	source	are	
translated	unless	explicitly	excluded	with	the	
com.fortify.sca.DISabledLanguages	property.	
Command-	Line	Option:	-enable-	language	
com.fortify.sca.
JdkVersion	
Specifies	the	Java	source	code	version	to	the	Java	
translator.
Value	Type:	String	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	195	of	216 

Property	Name	Description
Default:	1.8	
Command-	Line	Option:	-jdk	
com.fortify.sca.
JavaClasspath	
Specifies	the	class	path	used	to	analyze	Java	source	code.	
Specify	the	paths	as	asemicolon-	separated	list	(Windows)	
or	acolon-	separated	list	(non-	Windows	systems).	
Value	Type:	String	(paths)	
Default:	(none)	
Command-	Line	Option:	-cp	or	-classpath	
com.fortify.sca.
Appserver	
Specifies	the	application	server	to	process	JSP	Â files.	The	
valid	values	are	weblogic	or	websphere	.	
Value	Type:	Â String	
Default:	(none)	
Command-	Line	Option:	-appserver	
com.fortify.sca.
AppserverHome	
Specifies	the	application	server's	home	directory.	For	
WebLogic,	this	is	the	path	to	the	directory	that	contains	
server/lib	.For	WebSphere,	this	is	the	path	to	the	
directory	that	contains	the	JspBatchCompiler	script.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-appserver-	home	
com.fortify.sca.
AppserverVersion	
Specifies	the	version	of	the	application	server.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-appserver-	version	
com.fortify.sca.
JavaExtdirs	
Specifies	directories	to	include	implicitly	on	the	class	path	
for	WebLogic	and	WebSphere	application	servers.	
Value	Type:	String	
Default:	(none)	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	196	of	216 

Property	Name	Description
Command-	Line	Option:	-extdirs	
com.fortify.sca.
JavaSourcepath	
Specifies	acolon-	or	semicolon-	separated	list	of	source	file	
directories	that	are	not	included	in	the	scan	but	are	used	
for	name	resolution.	The	source	path	is	similar	to	
classpath,	except	it	uses	source	files	rather	than	class	files	
for	resolution.	
Value	Type:	String	(paths)	
Default:	(none)	
Command-	Line	Option:	-sourcepath	
com.fortify.sca.
JavaSourcepathSearch	
If	set	to	true,	Fortify	Static	Code	Analyzer	only	translates	
source	files	that	are	referenced	by	the	target	file	list.	
Otherwise,	Fortify	Static	Code	Analyzer	translates	all	files	
included	in	the	source	path.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
DefaultJarsDirs	
Specifies	semicolon-	or	colon-	separated	list	of	directories	
of	commonly	used	JAR	files.	The	JAR	files	located	in	these	
directories	are	appended	to	the	end	of	the	class	path	
option	(-cp	).	
Value	Type:	String	
Default:	(none)	
com.fortify.sca
jsp.UseSecurityManager	
If	set	to	true,	the	JSP	parser	uses	JSP	security	manager.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
jsp.DefaultEncoding	
Specifies	the	encoding	for	JSPs.	
Value	Type:	String	(encoding)	
Default:	ISO-	8859-	1	
com.fortify.sca.
ExcludeDisabledProjects	
If	set	to	true,	excludes	any	disabled	projects	in	a	
.NET	Â solution	file	from	the	set	of	projects	to	translate.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	197	of	216 

Property	Name	Description
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-exclude-	disabled-	
projects	
WinForms.
TransformDataBindings
WinForms.
TransformMessageLoops
WinForms.
TransformChange
NotificationPattern
WinForms.
CollectionMutation
Monitor.Label
WinForms.
ExtractEventHandlers	
Set	various	.NET	options.	
Value	Type:	Boolean	and	String	
Defaults	and	Examples:	
WinForms.TransformDataBindings=true
WinForms.TransformMessageLoops=true
WinForms.TransformChangeNotificationPattern
=
true
WinForms.CollectionMutationMonitor.Label=
WinFormsDataSource
WinForms.ExtractEventHandlers=true	
com.fortify.sca.
EnableDOMModeling	
If	set	to	true,	Fortify	Static	Code	Analyzer	generates	
JavaScript	code	to	model	the	DOM	Â tree	that	an	HTML	Â file	
generated	during	the	translation	phase	and	identifies	
DOM-	related	issues	(such	as	cross-	site	scripting	issues).	
Enable	this	property	if	the	code	you	are	
translatingincludes	HTML	files	that	have	embedded	or	
referenced	JavaScript	code.	
Note:	Enabling	this	property	can	increase	the	
translation	time.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca
DOMModeling.tags	
If	you	set	the	com.fortify.sca.EnableDOMModeling	
property	to	true,	you	can	specify	additional	HTML	tags	for	
Fortify	Static	Code	Analyzer	to	include	in	the	DOM	
modeling.
Value	Type:	String	(comma-	separated	HTML	Â tag	names)	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	198	of	216 

Property	Name	Description
Default:	body	,button	,div	,form	,iframe	,input	,head	,	
html	,and	p.	
Example:
com.fortify.sca.DOMModeling.tags=ul,li	
com.fortify.sca.
JavaScript.src.domain.
whitelist	
Specifies	trusted	domain	names	where	Fortify	Static	Code	
Analyzer	can	download	referenced	JavaScript	files	for	the	
scan.	Delimit	the	URLs	with	vertical	bars.	
Value	Type:	String	
Default:	(none)	
Example:	com.fortify.sca.JavaScript.	
src.domain.whitelist=
http://www.xyz.com|http://www.123.org	
com.fortify.sca.
DisableJavascript
Extraction	
If	set	to	true,	JavaScript	code	embedded	in	JSP,	JSPX,	
PHP,	and	HTML	Â files	is	not	extracted	and	not	scanned.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
skip.libraries.ES6
com.fortify.sca.
skip.libraries.jQuery
com.fortify.sca.
skip.libraries.javascript
com.fortify.sca.
skip.libraries.typescript	
Specifies	alist	of	comma-	or	colon-	separated	JavaScript	
technology	library	files	that	are	not	translated.	You	can	
use	regular	expressions	in	the	file	names.	Note	that	the	
regular	expression	'(-	\d\.\d\.\d)?'	is	automatically	
inserted	before	.min.js	or	.js	for	each	file	name	
included	in	the	
com.fortify.sca.skip.libraries.jQuery
property	value.	
Value	Type:	String	
Defaults:
l	ES6:	es6-	shim.min.js,system-	polyfills.js,	
shims_	for_	IE.js	
l	jQuery:	jquery.js,jquery.min.js,	
jquery-	migrate.js,jquery-	migrate.min.js,	
jquery-	ui.js,jquery-	ui.min.js,	
jquery.mobile.js,jquery.mobile.min.js,
jquery.color.js,jquery.color.min.js,	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	199	of	216 

Property	Name	Description	
jquery.color.svg-	names.js,	
jquery.color.svg-	names.min.js,	
jquery.color.plus-	names.js,	
jquery.color.plus-	names.min.js,	
jquery.tools.min.js	
l	javascript:	bootstrap.js,	
bootstrap.min.js,
typescript.js,
typescriptServices.js	
l	typescript:	typescript.d.ts,	
typescriptServices.d.ts	
com.fortify.sca.
follow.imports	
If	set	to	true,	files	included	with	an	import	statement	are	
included	in	the	JavaScript	translation.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
exclude.unimported.node.modul
es	
If	set	to	true,	only	imported	node_	modules	are	included	in	
the	JavaScript	translation.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
PHPVersion	
Specifies	the	PHP	Â version.	For	alist	of	valid	versions,	see	
the	Micro	Focus	Fortify	Software	System	Requirements	.	
Value	Type:	String	
Default:	7.0	
Command-	Line	Option:	-php-	version	
com.fortify.sca.
PHPSourceRoot	
Specifies	the	PHP	source	root.	
Value	Type:	Boolean	
Default:	(none)	
Command-	Line	Option:	-php-	source-	root	
com.fortify.sca.
PythonPath	
Specifies	acolon-	or	semicolon-	separated	list	of	additional	
import	directories.	Fortify	Static	Code	Analyzer	does	not	
respect	PYTHONPATH	environment	variable	that	the	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	200	of	216 

Property	Name	Description
Python	runtime	system	uses	to	find	import	files.	Use	this	
property	to	specify	the	additional	import	directories.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-python-	path	
com.fortify.sca.
PythonVersion	
Specifies	the	Python	source	code	version	you	want	to	
scan.	The	valid	values	are	2	and	3.	
Value	Type:	Number	
Default:	2	
Command-	Line	Option:	-python-	version	
com.fortify.sca.
DjangoTemplateDirs	
Specifies	path	to	Django	templates.	Fortify	Static	Code	
Analyzer	does	not	use	the	TEMPLATE_	DIRS	setting	from	
the	Django	settings.py	file.	
Value	Type:	String	(paths)	
Default:	(none)	
Command-	Line	Option:	-django-	template-	dirs	
com.fortify.sca.
DjangoDisableAutodiscover	
Specifies	that	Fortify	Static	Code	Analyzer	does	not	
automatically	discover	Django	templates.	
Value	Type:	Boolean	
Default:	(none)	
Command-	Line	Option:	-django-	disable-	
autodiscover	
com.fortify.sca.
RubyLibraryPaths	
Specifies	one	or	more	paths	to	directories	that	contain	
Ruby	libraries.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-ruby-	path	
com.fortify.sca.
RubyGemPaths	
Specifies	the	path	(s)	to	aRubyGems	location.	Set	this	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	201	of	216 

Property	Name	Description
value	if	the	project	has	associated	gems	to	scan.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-rubygem-	path	
com.fortify.sca.
FlexLibraries	
Specifies	asemicolon-	separated	list	(Windows)	or	acolon-	
separated	list	(non-	Windows	systems)	of	libraries	to	""link""	
to.	This	list	must	include	flex.swc	,framework.swc	,and	
playerglobal.swc	(which	are	usually	located	in	the	
frameworks/libs	directory	in	your	Flex	SDK	root).	Use	
this	property	primarily	to	resolve	ActionScript.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-flex-	libraries	
com.fortify.sca.
FlexSdkRoot	
Specifies	the	root	location	of	avalid	Flex	SDK.	The	folder	
must	contain	aframeworks	folder	that	contains	aflex-	
config.xml	file.	It	must	also	contain	abin	folder	that	
contains	an	mxmlc	executable.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-flex-	sdk-	root	
com.fortify.sca.
FlexSourceRoots	
Specifies	any	additional	source	directories	for	aFlex	
project.	Separate	the	list	of	directories	with	semicolons	
(Windows)	or	colons	(non-	Windows	systems).	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-flex-	source-	root	
com.fortify.sca.
AbapDebug	
If	set	to	true,	Fortify	Static	Code	Analyzer	adds	ABAP	
statements	to	debug	messages.	
Value	Type:	String	(statement)	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	202	of	216 

Property	Name	Description
Default:	(none)	
com.fortify.sca.
AbapIncludes	
When	Fortify	Static	Code	Analyzer	encounters	an	ABAP	
'INCLUDE'	directive,	it	looks	in	the	named	directory.	
Value	Type:	String	(path)	
Default:	(none)	
com.fortify.sca.
CobolFixedFormat	
If	set	to	true,	specifies	fixed-	format	COBOL	to	direct	
Fortify	Static	Code	Analyzer	to	only	look	for	source	code	
between	columns	8-	72	in	all	lines	of	code.	
Value	Type:	Â Boolean	
Default:	false	
Command-	Line	Option:	-fixed-	format	
com.fortify.sca.
SqlLanguage	
Sets	the	SQL	language	variant.	The	valid	values	are	PLSQL	
(for	Oracle	PL/SQL)	Â and	TSQL	(for	Microsoft	T-	SQL).	
Value	Type:	String	(SQL	language	type)	
Default:	TSQL	
Command-	Line	Option:	-sql-	language	
com.fortify.sca.
CfmlUndefinedVariablesAreTain
ted	
If	set	to	true,	Fortify	Static	Code	Analyzer	treats	
undefined	variables	in	CFML	pages	as	tainted.	This	serves	
as	ahint	to	the	Dataflow	Analyzer	to	watch	out	for	
register-	globals-	style	vulnerabilities.	However,	enabling	
this	property	interferes	with	dataflow	findings	where	a	
variable	in	an	included	page	is	initialized	to	atainted	value	
in	an	earlier-	occurring	included	page.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
CaseInsensitiveFiles	
If	set	to	true,	make	CFML	files	case-	insensitive	for	
applications	developed	using	acase-	insensitive	file	system	
and	scanned	on	case-	sensitive	file	systems.	
Value	Type:	Boolean	
Default:	(not	enabled)	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	203	of	216 

Property	Name	Description	
com.fortify.sca.
SourceBaseDir	
Specifies	the	base	directory	for	ColdFusion	projects.	
Value	Type:	String	(path)	
Default:	(none)	
Command-	Line	Option:	-source-	base-	dir	
com.fortify.sca.
FVDLDisableDescriptions	
If	set	to	true,	excludes	Fortify	security	content	
descriptions	from	the	analysis	results	file	(FVDL).	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-fvdl-	no-	descriptions	
com.fortify.sca.
FVDLDisableProgramData	
If	set	to	true,	excludes	the	ProgramData	section	from	the	
analysis	results	file	(FVDL).	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-fvdl-	no-	progdata	
com.fortify.sca.
FVDLDisableEngineData	
If	set	to	true,	excludes	the	engine	data	from	the	analysis	
results	file	(FVDL).	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-fvdl-	no-	enginedata	
com.fortify.sca.
FVDLDisableSnippets	
If	set	to	true,	excludes	code	snippets	from	the	analysis	
results	file	(FVDL).	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-fvdl-	no-	snippets	
com.fortify.sca.
FVDLDisableLabelEvidence	
If	set	to	true,	excludes	the	label	evidence	from	the	analysis	
results	file	(FVDL).	
Value	Type:	Boolean	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	204	of	216 

Property	Name	Description
Default:	false	
com.fortify.sca.
FVDLStylesheet	
Specifies	location	of	the	style	sheet	for	the	analysis	results.	
Value	Type:	String	(path)	
Default:
${com.fortify.Core}/resources/sca/fvdl2html
.xsl	
com.fortify.sca.
ResultsFile	
The	file	to	which	results	are	written.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-f	
Example:
com.fortify.sca.ResultsFile=myresults.fpr	
com.fortify.sca.
OutputAppend	
If	set	to	true,	Fortify	Static	Code	Analyzer	appends	results	
to	an	existing	results	file.	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-append	
com.fortify.sca.
Renderer	
Controls	the	output	format.	The	valid	values	are	fpr	,	
fvdl	,text	,and	auto	.The	default	of	auto	selects	the	
output	format	based	on	the	file	extension	of	the	file	
provided	with	the	-f	option.	
Value	Type:	String	
Default:	auto	
Command-	Line	Option:	-format	
com.fortify.sca.
ResultsAsAvailable	
If	set	to	true,	Fortify	Static	Code	Analyzer	prints	results	as	
they	become	available.	This	is	helpful	if	you	do	not	specify	
the	-f	option	(to	specify	an	output	file)	Â and	print	to	
stdout.
Value	Type:	Boolean	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	205	of	216 

Property	Name	Description
Default:	false	
com.fortify.sca.
BuildProject	
Specifies	aname	for	the	scanned	project.	Fortify	Static	
Code	Analyzer	does	not	use	this	name	but	includes	it	in	
the	results.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-build-	project	
com.fortify.sca.
BuildLabel	
Specifies	alabel	for	the	scanned	project.	Fortify	Static	
Code	Analyzer	does	not	use	this	label	but	includes	it	in	the	
results.
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-build-	label	
com.fortify.sca.
BuildVersion	
Specifies	aversion	number	for	the	scanned	project.	Fortify	
Static	Code	Analyzer	does	not	use	this	version	number	
but	it	is	included	in	the	results.	
Value	Type:	String	
Default:	(none)	
Command-	Line	Option:	-build-	version	
com.fortify.sca.
MachineOutputMode	
Output	information	in	aformat	that	scripts	or	Fortify	
Static	Code	Analyzer	tools	can	use	rather	than	printing	
output	interactively.	Instead	of	asingle	line	to	display	scan	
progress,	anew	line	is	printed	below	the	previous	one	on	
the	console	to	display	updated	progress.	
Value	Type:	Boolean	
Default:	(not	enabled)	
com.fortify.sca.
SnippetContextLines	
Sets	the	number	of	lines	of	code	to	display	surrounding	an	
issue.	The	two	lines	of	code	on	each	side	of	the	line	where	
the	error	occurs	are	always	included.	By	default,	five	lines	
are	displayed.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	206	of	216 

Property	Name	Description
Value	Type:	Number	
Default:	2	
com.fortify.sca.
MobileBuildSessions	
If	set	to	true,	Fortify	Static	Code	Analyzer	copies	source	
files	into	the	build	session	directory.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
ExtractMobileInfo	
If	set	to	true,	Fortify	Static	Code	Analyzer	extracts	the	
build	ID	and	the	Fortify	Static	Code	Analyzer	version	
number	from	the	mobile	build	session.	
Note:	Fortify	Static	Code	Analyzer	does	not	extract	
the	mobile	build	with	this	property.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
ClobberLogFile	
If	set	to	true,	Fortify	Static	Code	Analyzer	overwrites	the	
log	file	for	each	run	of	sourceanalyzer.	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-clobber-	log	
com.fortify.sca.
LogFile	
Specifies	the	default	log	file	name	and	location.	
Value	Type:	String	(path)	
Default:
${com.fortify.sca.ProjectRoot}/log/sca.log
and	Â ${com.fortify.sca.ProjectRoot}/log/sca_	
FortifySupport.log
Command-	Line	Option:	-logfile	
com.fortify.sca.
LogLevel	
Specifies	the	minimum	log	level	for	both	log	files.	The	valid	
values	are:	DEBUG	,INFO	,WARN	,ERROR	,and	FATAL	.For	
more	information,	see	""Accessing	Log	Files""	on	page	Â 162	
and	""Configuring	Log	Files""	on	page	Â 162	.	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	207	of	216 

Property	Name	Description
Value	Type:	String	
Default:	INFO	
com.fortify.sca.
PrintPerformanceDataAfterScan	
If	set	to	true,	Fortify	Static	Code	Analyzer	writes	
performance-	related	data	to	the	Fortify	Support	log	file	
after	the	scan	is	complete.	This	value	is	automatically	set	to	
true	when	in	debug	mode.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
Debug	
Includes	debug	information	in	the	Fortify	Support	log	file,	
which	is	only	useful	for	Micro	Focus	Fortify	Customer	
Support	to	help	troubleshoot.	
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-debug	
com.fortify.sca.
DebugVerbose	
This	is	the	same	as	the	com.fortify.sca.Debug	
property,	but	it	includes	more	details,	specifically	for	parse	
errors.
Value	Type:	Boolean	
Default:	(not	enabled)	
Command-	Line	Option:	-debug-	verbose	
com.fortify.sca.
Verbose	
If	set	to	true,	includes	verbose	messages	in	the	Fortify	
Support	log	file.	
Value	Type:	Boolean	
Default:	(not	enabled)	
Command-	Line	Option:	-verbose	
com.fortify.sca.
DebugTrackMem	
If	set	to	true,	enables	additional	debugging	for	
performance	information	to	be	written	to	the	Fortify	
Support	log	.	
Value	Type:	Boolean	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	208	of	216 

Property	Name	Description
Default:	(not	enabled)	
Command-	Line	Option:	-debug-	mem	
com.fortify.sca.
CollectPerformanceData	
If	set	to	true,	enables	additional	timers	to	track	
performance.
Value	Type:	Boolean	
Default:	(not	enabled)	
com.fortify.sca.
Quiet	
If	set	to	true,	disables	the	command-	line	progress	
information.
Value	Type:	Boolean	
Default:	false	
Command-	Line	Option:	-quiet	
com.fortify.sca.
MonitorSca	
If	set	to	true,	Fortify	Static	Code	Analyzer	monitors	its	
memory	use	and	warns	when	JVM	garbage	collection	
becomes	excessive.	
Value	Type:	Boolean	
Default:	true	
com.fortify.sca.
cpfe.command	
Sets	the	location	of	the	CPFE	Â binary	to	use	in	the	
translation	phase.	
Value	Type:	String	(path)	
Default:
${com.fortify.Core}/private-	bin/sca/cpfe48	
com.fortify.sca.
cpfe.441	
If	set	to	true,	Fortify	Static	Code	Analyzer	uses	CPFE	
version	4.4.1.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
cpfe.441.command	
Sets	the	location	of	the	CPFE	binary	(version	4.4.1)	to	use	
in	the	translation	phase.	
Value	Type:	String	(path)	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	209	of	216 

Property	Name	Description
Default:
${com.fortify.Core}/private-
bin/sca/cpfe441.rfct	
com.fortify.sca.
cpfe.options	
Adds	options	to	the	CPFE	command	line	to	use	when	
translating	C/C++	Â code.	
Value	Type:	String	
Default:
--remove_	unneeded_	entities	--suppress_	vtbl	
-tused	
com.fortify.sca.
cpfe.file.option	
Sets	the	name	of	CPFE	option	that	specifies	the	output	
(for	example	NST)	file	name.	
Value	Type:	String	
Default:	--gen_	c_	file_	name	
Example:
com.fortify.sca.cpfe.file.option=
--gen_	c_	file_	name	
com.fortify.sca.
cpfe.multibyte	
If	set	to	true,	CPFE	handles	multibyte	characters	in	the	
source	code.	This	enables	Fortify	Static	Code	Analyzer	to	
handle	code	with	multibyte	encoding,	such	as	SJIS	
(Japanese).
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
cpfe.CaptureWarnings	
If	set	to	true,	any	CPFE	warnings	are	included	in	the	
Fortify	Static	Code	Analyzer	log.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
cpfe.FailOnError	
If	set	to	true,	CPFE	fails	if	there	is	an	error.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
cpfe.IgnoreFileOpen	
If	set	to	true,	any	failure	to	open	asource	file	(including	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	210	of	216 

Property	Name	Description	
Failures	headers)	is	considered	awarning	instead	of	an	error.	
Value	Type:	Boolean	
Default:	false	
com.fortify.sca.
ASPVirtualRoots.
<virtual_	path>	
Specifies	asemicolon	delimited	list	of	full	paths	to	virtual	
roots	used.	
Value	Type:	String	
Default:	(none)	
Example:
com.fortify.sca.ASPVirtualRoots.Library=
c:\\WebServer\\CustomerTwo\\Stuff
com.fortify.sca.ASPVirtualRoots.Include=
c:\\WebServer\\CustomerOne\\inc	
com.fortify.sca.
DisableASPExternalEntries	
If	set	to	true,	disables	ASP	external	entries	in	the	analysis.	
Value	Type:	Boolean	
Default:	false	
fortify-	sca-	quickscan.properties	
Fortify	Static	Code	Analyzer	offers	aless	in-	depth	scan	known	as	aquick	scan.	This	option	scans	the	
project	in	quick	scan	mode,	using	the	property	values	in	the	fortify-	sca-	quickscan.properties	
file.	By	default,	aquick	scan	reduces	the	depth	of	the	analysis	and	applies	the	Quick	View	filter	set.	The	
Quick	View	filter	set	provides	only	critical	and	high	priority	issues.	
Note:	Properties	in	this	file	are	only	used	if	you	specify	the	-quick	option	on	the	command	line	for	
your	scan.	
The	table	provides	two	sets	of	default	values:	the	default	value	for	quick	scans	and	the	default	value	for	
normal	scans.	If	only	one	default	value	is	shown,	the	value	is	the	same	for	both	normal	scans	and	quick	
scans.
Property	Name	Description	
com.fortify.sca.
CtrlflowMaxFunctionTime	
Sets	the	time	limit	(in	milliseconds)	for	Control	Flow	
analysis	on	asingle	function.	
Value	Type:	Integer	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	211	of	216 

Property	Name	Description
Quick	Scan	Default:	30000	
Default:	600000	
com.fortify.sca.
DisableAnalyzers	
Specifies	acomma-	or	colon-	separated	list	of	analyzers	
to	disable	during	ascan.	The	valid	values	for	this	
property	are:	buffer	,content	,configuration	,	
controlflow	,dataflow	,findbugs	,nullptr	,	
semantic	,and	structural	.	
Value	Type:	String	
Quick	Scan	Default:	controlflow:buffer	
Default:	(none)	
com.fortify.sca.
FilterSet	
Specifies	the	filter	set	to	use.	You	can	use	this	property	
with	an	issue	template	to	filter	at	scan-	time	instead	of	
post-	scan.	See	
com.fortify.sca.ProjectTemplate	described	in	
""fortify-	sca.properties""	on	page	Â 182	to	specify	an	issue	
template	that	contains	the	filter	set	to	use.	
When	set	to	Quick	View	,this	property	runs	rules	that	
have	apotentially	high	impact	and	ahigh	likelihood	of	
occurring	and	rules	that	have	apotentially	high	impact	
and	alow	likelihood	of	occurring.	Filtered	issues	are	not	
written	to	the	FPR	and	therefore	this	can	reduce	the	
size	of	an	FPR.	For	more	information	about	filter	sets,	
see	the	Micro	Focus	Fortify	Audit	Workbench	User	
Guide	.	
Value	Type:	String	
Quick	Scan	Default:	QuickÂ View	
Default:	(none)	
com.fortify.sca.
FPRDisableMetatable	
Disables	the	creation	of	the	metatable,	which	includes	
information	for	the	Function	view	in	Micro	Focus	
Fortify	Audit	Workbench	.This	metatable	enables	right-	
click	on	avariable	in	the	source	window	to	show	the	
declaration.	If	C/C++	scans	take	an	extremely	long	time,	
setting	this	property	to	true	can	potentially	reduce	the	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	212	of	216 

Property	Name	Description
scan	time	by	hours.	
Value	Type:	Boolean	
Quick	Scan	Default:	true	
Default:	false	
Command-	Line	Option:	-disable-	metatable	
com.fortify.sca.
FPRDisableSourceBundling	
Disables	source	code	inclusion	in	the	FPR	file.	Prevents	
Fortify	Static	Code	Analyzer	from	generating	marked-	
up	source	code	files	during	ascan.	If	you	plan	to	upload	
FPR	files	that	are	generated	as	aresult	of	aquick	scan	
to	Fortify	Software	Security	Center	,you	must	set	this	
property	to	false	.	
Value	Type:	Boolean	
Quick	Scan	Default:	true	
Default:	false	
Command-	Line	Option:	-disable-	source-	
bundling	
com.fortify.sca.
NullPtrMaxFunctionTime	
Sets	the	time	limit	(in	milliseconds)	for	Null	Pointer	
analysis	for	asingle	function.	The	standard	default	is	
five	minutes.	If	this	value	is	set	to	ashorter	limit,	the	
overall	scan	time	decreases.	
Value	Type:	Integer	
Quick	Scan	Default:	10000	
Default:	300000	
com.fortify.sca.
TrackPaths	
Disables	path	tracking	for	Control	Flow	analysis.	Path	
tracking	provides	more	detailed	reporting	for	issues,	
but	requires	more	scan	time.	To	disable	this	for	JSP	
only,	set	it	to	NoJSP	.Specify	None	to	disable	all	
functions.
Value	Type:	String	
Quick	Scan	Default:	(none)	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	213	of	216 

Property	Name	Description
Default:	NoJSP	
com.fortify.sca.
limiters.ConstraintPredicateSize	
Specifies	the	size	limit	for	complex	calculations	in	the	
Buffer	Analyzer.	Skips	calculations	that	are	larger	than	
the	specified	size	value	in	the	Buffer	Analyzer	to	
improve	scan	time.	
Value	Type:	Integer	
Quick	Scan	Default:	10000	
Default:	500000	
com.fortify.sca.
limiters.MaxChainDepth	
Controls	the	maximum	call	depth	through	which	the	
Dataflow	Analyzer	tracks	tainted	data.	Increase	this	
value	to	increase	the	coverage	of	dataflow	analysis,	
which	results	in	longer	scan	times.	
Note:	Call	depth	refers	to	the	maximum	call	depth	
on	adataflow	path	between	ataint	source	and	sink,	
rather	than	call	depth	from	the	program	entry	
point,	such	as	main	()	.	
Value	Type:	Integer	
Quick	Scan	Default:	3	
Default:	5	
com.fortify.sca.
limiters.MaxFunctionVisits	
Sets	the	number	of	times	taint	propagation	analyzer	
visits	functions.	
Value	Type:	Integer	
Quick	Scan	Default:	5	
Default:	50	
com.fortify.sca.
limiters.MaxPaths	
Controls	the	maximum	number	of	paths	to	report	for	a	
single	dataflow	vulnerability.	Changing	this	value	does	
not	change	the	results	that	are	found,	only	the	number	
of	dataflow	paths	displayed	for	an	individual	result.	
Note:	Fortify	does	not	recommend	setting	this	
property	to	avalue	larger	than	5	because	it	might	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	214	of	216 

Property	Name	Description
increase	the	scan	time.	
Value	Type:	Integer	
Quick	Scan	Default:	1	
Default:	5	
com.fortify.sca.
limiters.MaxTaintDefForVar	
Sets	acomplexity	limit	for	the	Dataflow	Analyzer.	
Dataflow	incrementally	decreases	precision	of	analysis	
on	functions	that	exceed	this	complexity	metric	for	a	
given	precision	level.	
Value	Type:	Integer	
Quick	Scan	Default:	250	
Default:	1000	
com.fortify.sca.
limiters.MaxTaintDefForVarAbort	
Sets	ahard	limit	for	function	complexity.	If	complexity	
of	afunction	exceeds	this	limit	at	the	lowest	precision	
level,	the	analyzer	skips	analysis	of	the	function.	
Value	Type:	Integer	
Quick	Scan	Default:	500	
Default:	4000	
User	Guide	
Appendix	E:	Configuration	Options	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	215	of	216 

Send	Documentation	Feedback	
If	you	have	comments	about	this	document,	you	can	contact	the	documentation	team	by	email.	
Note:	If	you	are	experiencing	atechnical	issue	with	our	product,	do	not	email	the	documentation	
team.	Instead,	contact	Micro	Focus	Fortify	Customer	Support	at	
https://www.microfocus.com/support	so	they	can	assist	you.	
If	an	email	client	is	configured	on	this	computer,	click	the	link	above	to	contact	the	documentation	team	
and	an	email	window	opens	with	the	following	information	in	the	subject	line:	
Feedback	on	User	Guide	(Fortify	Static	Code	Analyzer	20.2.0	)	
Just	add	your	feedback	to	the	email	and	click	send.	
If	no	email	client	is	available,	copy	the	information	above	to	anew	message	in	aweb	mail	client,	and	send	
your	feedback	to	FortifyDocTeam@microfocus.com	.	
We	appreciate	your	feedback!	
Micro	Focus	Fortify	Static	Code	Analyzer	(20.2.0	)	Page	216	of	216 

"
Non,"Solution guide 
Transform your data with 
Cloudera on Azure   

Today, technology is crucial business strategy 	â	
from finding new ways to reach customers, to 
building new business models, to enhancing 
the productivity of your team and operations. 
Digital transformation shapes how businesses 
navigate their futures. 
Technology is transforming 
business innovation 
91%
68%
85%
64%	
94%	
Technology shapes how businesses innovate and grow
Digital transformation happens in the cloud 
This disruption occurs because technology is changing faster than ever before. 
Breakthrough innovations in sensors, social media, apps and data are causing the volume 
of data to explode. Fortunately, the scale and economics of cloud computing make it 
possible to capture and harness data, creating new opportunities to innovate, grow faster, 
and achieve more than ever before. 
Digital transformation and the cloud are intertwined. Studies show that companies that 
embrace the cloud grow faster than those who do not. Itâs no wonder that in the near 
future almost all workloads will be based in the cloud. 
Technology is a business disruptor
Digital transformation encourages the disruption of 
markets, steadily decreasing the average lifespan of 
companies on the S&P 500 over time. 
Companies that embrace digital transformation can 
manage increasing volatility regardless of their 
industry. 	
Average company lifespan on S&P 500 Index (in 
years) and select leading businesses by era
1950	1960	1970	1980	1990	2000	2010	2020	
10
20
30
40
50
60	
Engage your 	
customers	
Empower your 	
employees	
Optimize your 
operations
Transform 
your products	
The components 	
of digital 	
transformation	
1.Research Report, ISACA, Information Systems Audit and Control Association, 20182.Survey, DXC Technology and the Economist Intelligence  Unit, 2019	3.Survey, Couchbase, August 20184.Report, Cisco Global Cloud Index, 2018   

Azure is the leading 
cloud destination
Unique partner ecosystem 
for accelerated growth	
Scale
Limitless	
Hybrid
Operational 
freedom 	
Security	
Always a step ahead	
Azure has built a portfolio of partner	-driven 	
solutions to help customers achieve more. 
By integrating popular partner tools and 
frameworks, the Azure platforms enables 
customers to easily bring their existing 
workloads and tools to Azure and build the 
way that best serves their business. 	
Azure is the best cloud 
for Linux 
Azure enables a seamless migration of Linux 
workloads through a platform of services.
Choose from a variety of Linux distributions, 
workloads, and fully 
managed open source databases on Azure. 
Compatibility with the latest open source 
extensions and tools helps streamline 
development.	
Azure open source momentum
Leading global businesses run Azure on Linux
Azure is the cloud 	that helps your 	
customers turn ideas into solutions. 
Build, deploy, and manage applications 
in the cloud, on	-premises, and at the 	
edge with heterogenous environments 
(Linux and Windows workloads). 
>50%
of VM cores 
are Linux	
>60%
of Azure marketplace 
images are Linux	-	
based	
>47K
customers using 
Azure open source 
databases	
>100K
Azure Data Studio 
instances with Azure 
open source database 
extensions	
A.I.	
Intelligent by default   

Azure and Cloudera together	
Innovation anywhere with Azure for your hybrid architecture 
60+ Azure regions around the world and 700+ VM size options
Fully managed databases services for MySQL, PostgreSQL & MariaDB
Linux infrastructure for SQL and open source databases
Seamless migration to Linux and container offerings
Built	-in-tools and support for existing software and hardware	
Best	-in	-class Linux infrastructure and tools	
Choice of open source databases
Hybrid anywhere	
Azure offers leading infrastructure and open	-source databases that are built for 	
where you are today and where your modernization takes you.  
CLOUDERA DATA PLATFORM (CDP)
Cloudera Data Platform (CDP), powered by Microsoft Azure, is a secure, global enterprise data cloud that 
delivers high	-performance and faster insights from your data, with a single pane of glass across private, 	
hybrid and public clouds.	CDP offers a competitive edge by optimizing speed, flexibility and cost with a 	
modern architecture built on Microsoft Azure offering a secure and compliant hybrid Cloud solution with 
deep technical data processing and visualization, integration and manageability at global	-scale.	
CDP accelerates the customer journey to Azure
Adaptive scaling
Stop guessing at the 
performance requirements 
needed to handle a workload 
that canât yet be predicted. 
With CDP Public Cloud, 
capacity can be changed 
quickly, better matching the 
performance requirements of 
new workloads, speeding up 
deployment while effectively 
managing costs.	
Intelligent migration
Improve the utilization of 
clusters suffering from highly 
variable jobs. CDP Public 
Cloud simplifies migrating 
these workloads, so they get 
the capacity they require 
when they need it. This frees 
up on	-premises resources to 	
handle steady, more 
predictable workloads, 
improving cost efficiency. 	
Burst to cloud
Quiet ânoisy neighborsâ 
affecting the SLAs of important 
applications efficiently and 
cost	-effectively. CDP Public 	
Cloud makes it easy to move a 
workload (the data, metadata, 
policies, etc.) to public could 
and provide the ârightâ amount 
of capacity, so SLAs for key 
workloads are quickly met.  

Cloudera Data Platform 
reference architecture
Cloudera Data Platform
Data
Infrastructure	
Linux	Azure	Kubernetes
Service	
Azure DBfor PostgreSQL	
Azure Private Link
Azure Sentinel
Azure Monitoring
Azure Policy	
Azure Blueprints
Azure Backup 
Azure Site Recovery	
SQL Server on Azure VMs	
Streaming data 	sources	
Clickstream	Sensors	
Machine logs	Social	
Enterprise data 	sources	
CRM
Inventory	
Network
Ordering	
Billing
Usage	
Ingest	Store	Prep	Model & serve	
Cloudera 
Dataflow & 
Streaming	
Cloudera 
Data 	
Engineering	
Cloudera 
Data Lake
Cloudera 	
Operational DB	
Cloudera 	
Data Warehouse	
Cloudera 	
Machine Learning	
METADATA / SCHEMA / MIGRATION / SECURITY / GOVERNANCE	
MANAGEMENT
CONSOLE	DATA CATALOG  /  REPLICATION MANAGER  /  WORKLOAD MANAGER	
Azure Data Lake Storage Gen2	Power BI	
Azure VMs	Azure	Active 	
Directory	
Deploy new Cloudera workloads to Azure	
Burst on	-premises 	workloads to Azure for additional capacity	
Migrate data to Azure including security policies & governance	
Deliver cloud	-native analytics on Azure in a secure, cost	-efficient, and scalable manner 	
with Clouder	a Data Platform. 

Letâs get started
Ready to rapidly and securely migrate your Linux workloads on Azure? 
Take the next steps today! 	
1.	Discover how 	Cloudera Data Platform	can manage 	
and secure your data	
3.	Get hands	-on with 	Cloudera Data Platform Public Cloud 	
with a 	free test 	drive	
3.	Already 	familiar with CDP public cloud? Start your 	free 	
60	-day trial 	
Maersk streamlined its  
operations and 
improved the value of 
its IT resources with 
Cloudera on Azure	
Komatsu Mining uses 
Cloudera on Azure to 
optimizes its IoT 
analytics platform	
Banco de CrÃ©dito del 
PerÃº (BCP) reduced 
development time by 80 
percent using Cloudera 
Enterprise
Customer story	Customer story	
Customer story	
See how companies of all sizes use Cloudera on Azure
More r	esources	
Cloudera Data Platform on Microsoft Azure Marketplace
Ebook: Powering cloud possibilities: How to ensure current actions support future innovations
Lufthansa Technik Case Study 
Zurich Customer Case Study
Cloudera Data Warehouse on Azure Provides Fast, Cost	-Effective and Highly Scalable Analytics  

Thank you.  

"
Non,"Curriculum Vitae
Ra jeev R. Ra jeAssociate Dean
School of Science
Professor
Department of Computer and Information Science
Indiana University Purdue University Indianapolis
(317) 274 5174
rra je@cs.iupui.edu/rra je@iupui.edu http://www.cs.iupui.edu/ rra je
September 11, 2022
1 

Name
Ra jeev R. Ra je
Education
Ph.D. in Computer Engineering, Syracuse University, Syracuse, NY, 1994
M.S. in Computer Engineering, Syracuse University, Syracuse, NY, 1994
B.E. in Electrical Engineering, VJTI, University of Bombay, Bombay, India, 1984
Summary ÂSenior Researcher, Educator, and Administrator with 28 years of higher education experience
and a successful track record of implementing many academic initiatives; Author/Co-author
of 150+ peer-reviewed publications; PI/Co-PI for grants worth more than $6.5M; Senior
Member of ACM and IEEE; Recipient of awards including the Trustees Teaching Award,
School of Science Teaching and Service Awards, J. N. Tata Endowment Scholarship, and
other research, teaching, and service honors.
Professional Appointments ÂJanuary 2019 { Present
Associate Dean for Planning, Finance, and Faculty Aairs
School of Science
Indiana University Purdue University Indianapolis (IUPUI), Indianapolis, IN
Responsibilities
Plan for and advise the Dean on scal, faculty, personnel, IT, international collabora-
tions, and research needs of the school.
 Construct, oversee, and manage the school's budget ( 70M $) including scal oversight
of the new research and education building.
 Act as a liaison for all faculty-related matters including the tenure and promotion guide-
lines and the associated process.
 Initiate educational and research collaborations with international universities.
 Directly supervise the HR, IT, and scal personnel of the School.
 Create, update, streamline, and maintain all the operating policies of the school.
 Represent the school on the Council of Associate Deans for Faculty Aairs and on the
India Recruitment Steering Committee.
Accomplishments Successfully implemented university mandated scal cuts, to oset the eects of the
pandemic, to the school's budget without eliminating any occupied positions for 2 con-
secutive years.
 Ensured a smooth transition of faculty-, HR-, IT-, and scal-related functions of the
school during two deanship changes and the ongoing pandemic.
 Eciently managed nancial aspects of a new teaching and research building.
2 


Assisted the Dean in appointing 3 new Chairs and reappointing 2 Chairs in last two
years. Provided personalized information sessions to the 2 new Chairs.
 Formulated, in conjunction with the Deans and Chairs, comprehensive packages for
successful hiring of 9 new tenure track and 7 non-tenure track colleagues in last three
and half years. Also, assisted the Deans in designing retention packages for many faculty
and sta colleagues.
 Currently leading the faculty and sta groups tasked with the creation of the develop-
ment guidelines for the school's new strategic plan.
 Created and enhanced various policies such as the Administrative Supplement, Research
Faculty Hiring, Graduate Fee Remission, Research Instrument Purchase and Repair,
Distance Education Fee, and IT policies.
 Initiated and oversaw multiple academic partnerships with Indian Universities for joint
(e.g., 2+2 and 3.15+1.5) academic programs. Many MoU (Memorandum of Understand-
ing) and LoI (Letter of Intent), with Indian Universities, were signed during past three
years.
 Participated in hiring, and oversaw on-boarding, of a Recruitment Manager in India
{ Applications and admits, for Indian students, for fall'22 are up by 123% and 241%,
respectively, when compared with the pre-pandemic levels.
 Contributed to the enhancement of the Promotion and Tenure guidelines for incorpo-
rating DEI-related specics and the creation of the faculty grievances policy.
 Represented the school on a campus-wide task force that developed strategies for dealing
with campus' internationalization eorts during the pandemic.
Â January 2012 { August 2019
Chair of the Promotion and Tenure (P&T) Committee
School of Science, IUPUI, Indianapolis, IN
Responsibilities
Coordinate and enforce all aspects of the P&T process of the school and ensure its
integrity.
Accomplishments Oversaw and co-wrote, multiple times, enhancements to the school's P&T guidelines.
 Signicantly streamlined school's entire P&T process.
 Conceptualized and co-organized annual P&T workshops (since 2018) for tenure and
non-tenure faculty colleagues { these workshops are highly appreciated by colleagues.
Achieved 95+% P&T success rate for the school.
 Formally and informally mentored many, including URM and women, faculty colleagues
in their P&T journey.
Â September 2007 { December 2018
Associate Chair (Acting Chair in Fall'15)
Department of Computer and Information Science, IUPUI, Indianapolis, IN
Responsibilities
Advise and assist the Chair on all academic and scal aspects of the department.
3 


Act as Chair's proxy during his absence and represent the department at the school and
the campus level.
Accomplishments Led the creation of the self-assessment report for the external review of the department
and the proposal for autonomous Ph.D. program.
 Received the approval for an autonomous Ph.D. program, by the Indiana Commission
for Higher Education, during my tenure as the Acting Chair.
 Made signicant contributions to the creation and nurturing of the CIS Ph.D. Program.
 Co-created the CIS Industrial Partnership Program.
 Chaired the Faculty Search Committees many times, including the years when URM
and women faculty colleagues were hired.
Â September 2006 { July 2015
Graduate Program Director
Department of Computer and Information Science (CIS), IUPUI, Indianapolis, IN
Responsibilities
Oversee, grow, and ensure the quality of the Graduate Program in the department.
Accomplishments Signicantly (by many folds) expanded the graduate programs (both M.S. and Ph.D.).
 Coordinated the entire Ph.D. qualication process, for many years, with the Computer
Science Department and the Graduate School at Purdue University.
 Initiated and nurtured multiple eorts to create joint academic programs with interna-
tional universities.
 Received an approval for 6 new Graduate Certicates.
Â September 2006 { August 2007
Interim Associate Chair
Department of Computer and Information Science, IUPUI, Indianapolis, IN
Â July 2009 { Present
Professor
Department of Computer and Information Science, IUPUI, Indianapolis, IN
Â Spring 2004 { Spring 2006
Adjunct Associate Professor (Courtesy Appointment)
Department of Computer and Information Sciences, University of Alabama at Birmingham,
Birmingham, AL
Â August 2002 { June 2009
Associate Professor
Department of Computer and Information Science, IUPUI, Indianapolis, IN
Â August 1996 { July 2002
Assistant Professor
Department of Computer and Information Science, IUPUI, Indianapolis, IN
4 

Â
August 1994 { July 1996
Visiting Assistant Professor
Department of Computer and Information Science, IUPUI, Indianapolis, IN
Â September 1988 { July 1994
Graduate Research Assistant
NY CASE Center & Department of Electrical and Computer Engineering, Syracuse Univer-
sity, Syracuse, NY
Â May 1988 { August 1988
Graduate Research Assistant
Department of Electrical Engineering, University of Rhode Island, Kingston, RI
Â September 1987 { August 1988
Assistant and Graduate Assistant
Administrative Computer Center, University of Rhode Island, Kingston, RI
Â June 1984 { August 1987
Pro ject Engineer
Controls and Automation Systems Engineering Division
Siemens Limited, Bombay, India
Research Interests Distributed and Component-based Systems, Software Engineering, Service-oriented Comput-
ing, Faculty Aairs, and Higher-ed Administration
Honors and Awards
First-place winner , as a member of IUPUI's team, in the NSF's COVID Chal lenge { Faculty
category , 2022
 IUPUI School of Science Service Award , 2020
 Keynote Talk , 3rd
International Virtual Conference On Recent Trends in Advanced Comput-
ing, 2020
 Plenary Talk , International Virtual Conference on Industry 4.0, 2020
 Letter of Appreciation from the US NIST, 2019
 Best Paper Presentation Award (Session - joint with my Ph.D. student), 20th IEEE ICCIT,
Dhaka, Bangladesh, 2017
 Selected for a biographic entry in the Garje Marathibook by Granthali Prakashan, 2017
 Research Selected for an entry in the 2016 Compendium Industry-Nominated Technological
Breakthroughs for NSF I/UCRCs (joint with my Ph.D. student), 2016
 ACM Senior Member , 2014
 IEEE Senior Member , 2014
 Letter of Appreciation from the IEEE/CSI International Conference on High Performance
Computing and Applications (ICHPCA), 2014
5 


Keynote Address , 2nd
International Conference On Network Infrastructure Management Sys-
tems, Mumbai, India, 2013
 IUPUI Trustees Teaching Award , 2010 and 2001
 Favorite Faculty Award , Computer Science Club, IUPUI, 2007-08
 Co-author, Top-10 Downloaded Paper, ACM Digital Library , 2006
 Nominated for the SoS Research Award , 2006
 Co-author, One of the Top Bioinformatics paper, BRIC Bioinformatic Online Newsletter ,
2005
 Recognition Plaque , 12th
IEEE International Conference on High-performance Computing,
2005
 Certicate of Appreciation , Department of the Information Systems and Operations Manage-
ment, Ball State University, 2005
 Nominated to Who's Who Among America's Teachers , 2004
 Certicate of Appreciation , ACM SAC, 2003
 Nominated for the Forty Under 40 Program of the Indianapolis Business Journal, 2003-04
 Certicate of Appreciation , School of Science, IUPUI, 2002
 The IUPUI School of Science Teaching Award , 2000
 Commendation for Technical Excel lence in Reviewing , Recognized by IEEE Computer, 1998-
99
 Selected as Level-3, Level-2, and Level-1 Mentor by the Graduating Students at IUPUI School
of Science, 1996-2002 and 2004-22
 The Teaching Excel lence Recognition Award (TERA) , Department of Computer and Infor-
mation Science and the School of Science at IUPUI, 1998-99 and 1996-97
 Excel lence in Teaching , Recognized by the IUPUI Intercollegiate Athletics Department, 1998-
99
 The Professor of the Year 1996-97 , Computer Science Club and the Department of Computer
and Information Science at IUPUI
 The Overal l Professor of the Year 1995-96 , Computer Science Club and the Department of
Computer and Information Science, IUPUI
 Graduate Research Assistantship , Syracuse University, NY CASE Center and University of
Rhode Island, 1988-94
 J. N. Tata Scholarship , J. N. Tata Endowment, 1986
 Anandrao Ganapat Pandit Scholarship for achieving the 1st
Rank, University of Bombay,
1982-83
 Shekhar N. Naik Silver Medal for Science , Indian Education Society, 1978
6 

Grants (External and Internal - as PI and Co-PI)
1. Real-Time Algorithms and Software Systems for Heterogeneous Data Driven Policing of Social Harm, NSF, $791,513, 2017-22 (Co-PI)
2. RealCV: Making the Invisible Visible, Indiana University, $48,311, 2020-23 (Co-PI)
3. Static Code Analysis Tool Modernization Pro ject, Kestrel Technology and DHS, $49,340,
2016-18 (Co-PI)
4. Reducing False Positive Reported By Static Code Analysis Tools, Department of Homeland Security, $580,458, 2015-20 (Co-PI)
5. Testing-as-a-Service: Static Code Analysis (SCA) Tool Study { Phases I-IV, S2
E RC , Lock-
heed Martin, Northrop Grumman and Department of Homeland Security, $111,541, 2013-16
(Co-PI)
6. Grant for hosting 2015 S2
E RC Fall Showcase, IUPUI, $3,500, 2015 (PI)
7. A Pervasive Computing Infrastructure for Supporting CS Graduate Courses, IUPUI SoS Teaching Support, $27,657, 2014-15 (Joint PI)
8. Modeling, Specifying, Discovering, and Integrating Trust into Distributed Real-time and Em- bedded (DRE) Systems, S2
E RC and Air Force Research Lab, $79,000, 2011-13 (PI)
9. A Distributed Framework for Indoor Location Tracking, Purdue Research Foundation Re- search Grant, $17,608, 2013-14 (Joint PI)
10. International Travel Grant, Purdue Research Foundation, $1,000, 2011
11. Developing a Fast and Accurate Parallel Solver for Multi-scale Biochemical Reacting Systems, IUPUI, $2000, 2009-10 (Co-PI)
12. Developing Fast and Accurate Embarrassingly Parallel Solver for Multi-Scale Chemically Reacting Systems, IUPUI, $1500, 2008-09 (Co-PI)
13. Signature Center for Bio-Computing, IUPUI, $285,000, 2007-2009 (Co-PI)
14. An Environment for Distributed Component-based Software Development, Indigo Founda- tion, $26,400, 2005-2006 (PI)
15. A Framework for Seamless Interoperation of Heterogeneous Distributed Software Compo- nents, US Oce of Naval Research, $1,558,803, 2001-2005 (PI)
16. Indiana University SUR Proposal: Information Technology Applications for the Life Sciences, IBM Corporation, $1.2 Million (hardware), 2001 (Co-PI)
17. Test-assessment and Minimization for the Microsoft Windows NT Operating System, Mi- crosoft Corporation, $93,742, 2000-2002 (PI)
18. An Active, Personalized, Adaptive, Multi-format Biological Information Delivery System, National Science Foundation, $494,297, 2000-2004 (Co-PI)
19. Indiana University SUR Proposal: Tightly integrated distributed supercomputing - the Indi- ana TeraCloud, IBM Corporation, $1 Million (hardware), 2000-2001 (Co-PI)
7 

20. A Distributed Information Filtering System for Digital Libraries, National Science Founda-
tion, $315,386, 1999-2003 (Co-PI)
21. Intelligent Software System for Bimolecular Database Access and Analysis, Eli Lilly & Com- pany, $22,200, 1999-2000 (Co-PI)
22. Biomedical Tele-Visualization, Indiana University High Performance Network Applications Program, $20,000, 1999-2000 (Co-PI)
23. Grants-in-Aid (Teaching), Faculty Development Oce at IUPUI, $3,000, 1996-1997 (PI)
Publications Books
1. Yuanshun Dai, Yi Pan, Ra jeev R. Ra je (Eds.), Advanced Parallel And Distributed Comput- ing: Evaluation, Improvement And Practice, Nova Science Publishers, Inc., ISBN: 1-60021-
202-6, 2007.
2. Ra jeev R. Ra je, Farookh Hussain, R. Jagadeesh Kannan (Eds.), Articial Intelligence and Technologies, Select Proceedings of ICRTAC-AIT 2020, Lecture Notes in Electrical Engineer-
ing, Springer Verlag, Singapore, ISBN: 9789811664472, 2021.
Refereed Book Chapters
3. Ra jeev R. Ra je, Zhiqing Liu, Sivakumar Chinnasamy, Ming Zhong, Wilfred Mascarenhas,Joseph Williams, \Distributed-Ob ject Computing"", in High-performance Cluster Computing,
Vol.2, Pages: 249-273, 1999.
4. Ra jeev R. Ra je, Anantharaman Kalyanraman, Nanditha Nayani, \Distributed-Ob ject Com- puting Tools"", Tools and Environments for Paral lel and Distributed Computing"" , pp. 79-147,
2004.
5. Andrew M. Olson, Ra jeev R. Ra je, Barrett R. Bryant, Mikhail Auguston, Carol Burt, \UniFrame { A Unied Framework for Developing Service-oriented, Component-based Dis-
tributed Software Systems"", in Service-Oriented Software System Engineering: Chal lenges
and Practices , pp. 68-87, 2005.
6. Andrew M. Olson, Ra jeev R. Ra je, Barrett R. Bryant, Mikhail Auguston, Carol Burt, \UniFrame - Automating the Construction of Large-Scale Distributed Systems"", in Advanced
Paral lel And Distributed Computing: Evaluation, Improvement And Practice , pp. 197-212,
2007.
7. Ra jeev R. Ra je, Jayasree Gandhamaneni, Andrew M. Olson, Barrett R. Bryant, \MURDS: A Mobile-Agent-based Distributed Discovery System"", Encyclopedia of Mobile Computing and
Commerce { Volume 1 , pp. 436-441, 2007.
8. Mihran Tuceryan, Ra jeev R. Ra je,\Distributed Heterogeneous Tracking for Augmented Real- ity Using Component-based Software Technologies"", Encyclopedia of Mobile Computing and
Commerce { Volume 2 , pp. 207-212, 2007.
9. Ra jeev R. Ra je, Sivakumar Chinnasamy, Andrew Olson, William Higdon, \The Application and Enhancement of LePUS for Specifying Design Patterns"" in Design Patterns Formalization
Techniques , pp. 236-257, 2007.
8 

10. Ra jeev R. Ra je, Alex Crespi, Omkar J.Tilak, Andrew Olson, \An Access Control Model for
the Components in a Distributed System"" in Handbook of Research on Information Assurance
and Security , pp. 254-265, 2008.
11. Omkar Tilak, Ra jeev R. Ra je, Andrew Olson, \Assurance for Temporal Compatibility Using Contracts"" in Handbook of Research on Information Assurance and Security , pp. 360-371,
2008.
12. Preeti Mulay, Rahul Joshi, Ayushi Misra, Ra jeev R. Ra je, \Detection of Personality Traits of Sarcastic People (PTSP): a Social-IoT based approach"", in IoT and Big Data Analytics
for Smart Generation , Intelligent Systems Reference Library, vol 154, pp. 237-261, Springer,
2018/2019.
Journal Articles
13. Ra jeev R. Ra je, \SIMATIC S5-110S DIMOS - Diagnostics and Monitoring System"", Siemens
Circuit , Vol. XXII, 1-2/87 (January/April 1987), pp. 34-37.
14. Ra jeev R. Ra je, Joseph Williams, Michael Boyles, \An Asynchronous Remote Method Invo- cation (ARMI) Mechanism for Java"", Concurrency: Practice and Experience , Vol. 9(11), pp.
1207-1211 (1997).
15. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Michael Boyles, Artur Papiez, Nila Patel, Mathew Palakal, Javed Mostafa, \A Bidding Mechanism for Web-Based Agents Involved in Informa-
tion Classication"", World Wide Web, 1(1998), pp. 155-165.
16. Ra jeev Ra je, Michael Boyles, Shiaofen Fang, \CEV: Collaborative Environment for Visualiza- tion Using Java-RMI"", Concurrency: Practice and Experience , Vol. 10(11-13), pp. 1079-1085,
(1998).
17. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Michael Boyles, Nila Patel, Javed Mostafa, \On Designing and Implementing a Collaborative System Using the Distributed-Ob ject Model of
Java-RMI"", Paral lel and Distributed Computing Practices , Vol. 1, No. 4, December 1998, pp.
3-13.
18. Ra jeev R. Ra je, Sivakumar Chinnasamy, \Designing a Distributed-ob ject Computing Envi- ronment for Global-scale Systems { Challenges and Issues"", (Invited) ACM SIGAPP Applied
Computing Review , Vol. 7, No.1, pp. 25-30, Spring 1999.
19. Ra jeev R. Ra je, \Experiences in building Distributed-ob ject Systems"", (Invited) Journal of
Alabama Academy of Sciences, Vol. 71, No. 2, pp. 57, April 2000.
20. Ra jeev R. Ra je, Ming Zhong, Tongyu Wang, Joseph Williams, \A Case Study: A Distributed Concurrent System with AspectJ"", ACM SIGAPP Applied Computing Review , Vol. 9, No.
2, pp. 17-23, Summer 2001.
21. Ra jeev R. Ra je, Barrett Bryant, Mikhail Auguston, Andrew Olson, Carol Burt, \A QoS-based Framework for Creating Distributed and Heterogeneous Software Components"", Concurrency
and Computation: Practice and Experience :2002, 14, pp. 1009-1034.
22. Ra jeev R. Ra je, Mingyong Qiao, Snehasis Mukhopadhyay, Shengquan Peng, Mathew Palakal, Javed Mostafa, \Homogeneous Agent-based Distributed Information Filtering"", Cluster Com-
puting , Vol. 5 (2002), No. 4, pp. 377-388.
9 

23. Mathew Palakal, Snehasis Mukhopadhyay, Javed Mostafa, Ra jeev Ra je, Mathias N'Cho, San-
tosh K. Mishra, \An Intelligent Biological Information Management System"", Bioinformatics
Journal , Vol. 18, No. 10, pp. 1283-1288, 2002.
24. Snehasis Mukhopadhyay, Shengquan Peng, Ra jeev R. Ra je, Mathew Palakal, Javed Mostafa, \Large-scale Multi-agent Information Classication Using Dynamic Acquaintance Lists"", Jour-
nal of the American Society for Information Science and Technology , Vol. 54, No. 10, pp.
966-975, 2003.
25. Mathew Palakal, Matthew Stephens, Snehasis Mukhopadhyay, Ra jeev Ra je, Simon Rhodes, \Identication of Biological Relationships from text documents using ecient computational
Methods"", Journal of Bioinformatics and Computational Biology (JBCB) , Vol. 1, No. 2
(2003), pp. 1-34.
26. Ra jeev R. Ra je, Daocheng Zhu, Snehasis Mukhopadhyay, Liying Tang, Mathew Palakal, Javed Mostafa, \COBioSIFTER - A CORBA-based Distributed Multi Agent Biological In-
formation Management System"", Cluster Computing, Vol. 7, 373-389, 2004.
27. Chunmin Yang, Barrett R. Bryant, Carol Burt, Ra jeev R. Ra je, Andrew M. Olson, Mikhail Auguston, \Formal Methods for Quality of Service Analysis in Component-based Distributed
Computing"", Journal of Design & Process Science: Transactions of the Society for Design
and Process Science , 8,2, pp. 137-149, 2004.
28. Fei Cao, Barrett Bryant, Ra jeev R. Ra je, Andrew Olson, Mikhail Auguston, Wei Zhao, Carol Burt, \A Component Assembly Approach Based on Aspect-oriented Generative Domain
Modeling"", Electronic Notes in Theoretical Computer Science (ENTCS) , Elsevier Science,
Vol. 114, pp. 119-136, 2005.
29. Snehasis Mukhopadhyay, Shengquan Peng, Ra jeev R. Ra je, Mathew Palakal, Javed Mostafa, \Distributed Multi-Agent Information Filtering { A Comparative Study"", Journal of the
American Society for Information Science and Technology , Vol. 56, No. 8, pp. 834-842,
2005.
30. Fei Cao, Barrett Bryant, Ra jeev R. Ra je, Andrew Olson, Mikhail Auguston, Wei Zhao, Carol Burt, \Dynamic Composition Patterns for Distributed Components"", Journal of Universal
Computer Science , Vol. 11, No. 10, pp. 1645-1675, 2005.
31. Changlin Sun, Ra jeev R. Ra je, Barrett Bryant, Omkar Tilak, \Compositional Reasoning of Performance in Component-Based Distributed Systems"", Cluster Computing, 11(4), pp.
331-340, 2008.
32. Ketaki A. Pradhan, Lahiru Gallege, Alfredo Moreno, Ra jeev R. Ra je, \MDE-URDS { A Mobile Device Enabled Service Discovery System"" (extended version of the conference paper),
International Journal on Advanced Computing and Communication Networks , Vol. 3, No. 1
(2011), pp. 33-37, 2011.
33. Anjali Kumari, Ketaki A. Pradhan, Lahiru S. Gallege, Ra jeev R. Ra je, \Synchronization Level Specication and Matching of Software Components"", Software Engineering: An In-
ternational Journal , Vol. 2, No. 1, pp. 7-19, March 2012.
34. Dimuthu U. Gamage, Ryan Rybarczyk, Ra jeev R. Ra je, \A Practical Approach to Adaptive Service Composition"", Software Engineering: An International Journal , Vol. 3, No.1, pp.
7-20, April 2013.
10 

35. Andrew Olson, Ra jeev Ra je, Barun Devara ju, Lahiru Gallege, \Learning Improves Service
Discovery"", Concurrency and Computation: Practice and Experience , 27(7): 1679-1694 27(7):
1679-1694, 2015.
36. Lahiru Gallege, Dimuthu Gamage, James Hill, Ra jeev R. Ra je, \Understanding the Trust of Software-intensive Distributed Systems"", Concurrency and Computation: Practice and
Experience , 28(1): 114-143, 2016.
37. Dimuthu Gamage, Lahiru Gallege, Ra jeev R. Ra je, \Composing Context-Aware Distributed Systems using QoS and Trust Principles"", International Journal of Services Computing , 4(2),
pp. 32-48, 2016.
38. Ruchika Malhotra, Megha Khanna, Ra jeev R. Ra je, \On the Application of Search-based Techniques for Software Engineering Predictive Modeling: A Systematic Review and Future
Directions"", Swarm and Evolutionary Computation , 32:85-109, 2017.
39. Boakye Dankwa, Raghavendran Vijayan, Darsh Sanghavi, Mihran Tuceryan, Ra jeev R. Ra je, \Trust in Vehicle-to-Vehicle Communication"", International Journal of Scientic & Engineer-
ing Research (extended version of the conference paper), Volume 8, Issue 6, pp. 1-6, June
2017.
40. George Mohler, Jeremy Carter, Ra jeev Ra je, \Improving Social Harm Indices with a Modu- lated Hawkes Process"", International Journal of Forecasting, Volume 34, Issue 3, pp. 431-439,
July { September 2018.
41. Jeremy Carter, George Mohler, Ra jeev Ra je, Nahida Chowdhury, Saurabh Pandey, \The Indianapolis Harmspot Policing Experiment"", Journal of Criminal Justice, Volume 74, May
-{ June 2021, 101814.
42. Amrita Mangaonkar, Rohit Pawar, Nahida Chowdhury, Ra jeev R. Ra je, \Enhancing Collab- orative Detection of Cyberbullying Behavior in Twitter Data"", Cluster Computing, Volume
25, Issue 2, pp. 1263-1277, April 2022 (Online January 2022).
43. Ayush Maharjan, Nahida Sultana Chowdhury, Ra jeev R. Ra je, \Evaluation of Static Analysis Tools for Mobile App Security"", Acta Scientic Computer Sciences , Volume 4 Issue 2: 37-43,
January 2022.
Journal Articles Under Review
44. Dimuthu Gamage, Ra jeev R. Ra je, \Distributed Computing Applications of Context-Aware QoS and Trust Prediction Framework"", IEEE Transactions on Services Computing , 2022.
45. Nahida Sultana Chowdhury, Ayush Maharjan, Ra jeev R. Ra je, \E-SERS: An Enhanced Ap- proach to Trust-based Ranking of Apps"", IEEE Transactions on Technology and Society ,
2022.
Refereed Conference/Workshop Proceedings (Note: Due to the unique and dynamic nature of Computer Science, it is an accepted practice
to consider peer reviewed journal and peer reviewed conference publications to be equiva-
lent. This policy is also consistent with the Best Practices Memo article that appeared in the
Computing Research News (https://archive.cra.org/reports/tenure review.pdf ). The papers
published in such peer-reviewed proceedings are ful l papers and not abstracts and are revised
based on reviewers' feedback before appearing in their nal publication forms.)
11 

46. Daniel J. Pease, Ra jeev R. Ra je, \SHARADA - A Sharable HierARchical Architectural
Database"", Proceedings of International Conference on Advances in Data Management, CO-
MAD'91 , Bombay, India, 1991.
47. Ra jeev R. Ra je, Daniel J. Pease, Zaide Liu, Neng T. Lin, \A Graphical Tool for Shared Data Analysis of Parallel Fortran Programs"", Proceedings of European Simulation Symposium,
ESS'93 , Delft, The Netherlands, 1993.
48. Ra jeev R. Ra je, Daniel J. Pease, Songqing Cai, Zaide Liu, Neng T. Lin, \MVSW - A Batch Job Submission Environment under AIX"", Proceedings of USING'93, Atlanta, Georgia, 1993.
49. Ra jeev R. Ra je, Daniel J. Pease, Sanjay D. Jejurikar, Neng T. Lin, \A Graphical Hierarchi- cal Flowchart Generator for Parallel Fortran Programs"", Proceedings of Computer Graphics
International '93 , Lausanne, Switzerland, 1993.
50. Ra jeev R. Ra je, Daniel J. Pease, Edward T. Guy III, \Class Partitions - A New Approach to Sequential Ob ject Oriented Programs"", Proceedings of 29th Hawaii International Conference
on System Sciences (Organized by IEEE), HICSS-29 , Hawaii, 1996.
51. Ra jeev R. Ra je, Daniel J. Pease, Edward T. Guy III, \OFFERS { A Tool for Hierarchical Implicit Analysis of Sequential Ob ject Oriented Programs"", Proceedings of ACM's Symposium
on Applied Computing, SAC'96 , Philadelphia, Pennsylvania, 1996.
52. Ra jeev R. Ra je, Ramana Pidaparti, C. Yokomoto, \An Active Collaborative Environment for Engineering Applications/Education Using Java-RMI"", Proceedings of ASEE Il linois/Indiana
Sectional Conference , Indianapolis, Indiana, 1997.
53. Chihfeng Lin, Daniel Pease, Ra jeev R. Ra je, \An Optimal-Joint-Coordinate Block Matching Algorithm for Motion-Compensated Coding"" Proceedings of IEEE 1997 Data Compression
Conference DCC '97 , Snowbird, Utah, 1997.
54. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Michael Boyles, Nila Patel, Javed Mostafa, \D- SIFTER: A Collaborative Information Classier"", Proceedings of the International Confer-
ence on Information, Communication and Signal Processing (organized by IEEE Singapore
Section), ICICS'97 , Singapore, 1997.
55. Ra jeev R. Ra je, Dennis Gannon, \A Timing Experiment with Java-RMI, CORBA and C- RPC"", Proceedings of the 5th International Conference on Advanced Computing (organized
in-cooperation with IEEE Computer Society), ADCOMP'97 , Chennai, India, 1997.
56. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Michael Boyles, Artur Papiez, \An Economic Framework for a Web-based Collaborative Information Classier"", Proceedings of the In-
ternational Association of Science and Technology for Development, SE'97 Conference , San
Francisco, California, 1997.
57. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Michael Boyles, Nila Patel, Javed Mostafa, \On Designing and Implementing a Collaborative System Using Java-RMI"", Proceedings of the
5th International Conference on Advanced Computing (organized in-cooperation with IEEE
Computer Society), ADCOMP'97 , Chennai, India, 1997.
58. Ra jeev R. Ra je, Antony Teal, Joseph Coulson, Sihai Yao, William Winn, Edward Guy III, \CCASEE { A Collaborative Computer Assisted Software Engineering Environment"",
Proceedings of the International Association of Science and Technology for Development
(IASTED), SE'97 Conference , San Francisco, California, 1997.
12 

59. Chihfeng Lin, Daniel Pease, Ra jeev R. Ra je, \An Ecient Block Matching Algorithm Based
on a Valid Assumption of the Convex Distortion"", Proceedings of IEEE International Con-
ference on Image Processing, ICIP'97 , Washington, DC, 1997.
60. Sivakumar Chinnasamy, Ra jeev R. Ra je, Zhiqing Liu, \Specication of Design Patterns: An Analysis"", Proceedings of the 7th International Conference on Advanced Computing and
Communications (Co-sponsored by IEEE Computer Society), ADCOM'99 , Roorkee, India,
1999.
61. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Mingyong Qiao, Mathew Palakal, Javed Mostafa, \Experiments with a Distributed Information Filtering System"", Proceedings of the 4th World
Multiconference on Systems, Cybernetics and Informatics, SCI-2000 , Orlando, Florida, 2000.
62. Ra jeev R. Ra je, \UMM: Unied Meta-ob ject Model"" { Proceedings of4th
IEEE International
Conference on Algorithms and Architecture for Paral lel Processing, ICA3PP'2000 , Hongkong,
2000.
63. Mathew Stephens, Mathew Palakal, Snehasis Mukhopadhyay, Ra jeev R. Ra je, Javed Mostafa, \Detecting Gene Relationships From Medline Abstracts"", Proceedings of Pacic Symposium
on Biocomputing 2001 , Honolulu, Hawaii, 2001.
64. Shengquan Peng, Snehasis Mukhopadhyay, Ra jeev R. Ra je, Mathew Palakal, Javed Mostafa, \A Comparison of Single-Agent and Multi-Agent Information Classication"" { CD-ROM
Proceedings of 10th IEEE Heterogeneous Computing Workshop, HCW 2001 , San Francisco,
California, 2001.
65. Ra jeev R. Ra je, Mingyong Qiao, Snehasis Mukhopadhyay, Mathew Palakal, Javed Mostafa, \SIFTER-II: A Heterogeneous Agent Society for Information Filtering"" { Proceedings of ACM
Symposium on Applied Computing, SAC'01 , Las Vegas, Nevada, 2001.
66. Ra jeev R. Ra je, Sivakumar Chinnasamy, \eLePUS { A Language for Specication of Software Design Patterns"" { Proceedings of the ACM Symposium on Applied Computing, SAC'01 , Las
Vegas, Nevada, 2001.
67. Ra jeev R. Ra je, Barrett Bryant, Mikhail Auguston, Andrew Olson, Carol Burt, \A Unied Approach for the Integration of Distributed Heterogeneous Software Components"" { Pro-
ceedings of the 2001 Monterey Workshop (Sponsored by DARPA, ONR, ARO and AFOSR) ,
Monterey, California, 2001.
68. Mathew Palakal, Snehasis Mukhopadhyay, Javed Mostafa, Ra jeev Ra je, Mathias N'Cho, Santosh K. Mishra, \An Intelligent Biological Information Management System"", Proceedings
of the ACM Symposium on Applied Computing, SAC'02 , Madrid, Spain, 2002.
69. Girish Brahnmath, Ra jeev R. Ra je, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, \A Quality of Service Catalog for Software Components"", Proceedings of the Southeast-
ern Software Engineering Conference , Huntsville, Alabama, 2002.
70. Carol Burt, Ra jeev R. Ra je, Mikhail Auguston, Barrett Bryant, Andrew Olson, \Quality of Service (QoS) Standards for Model Driven Architecture"", Proceedings of the Southeastern
Software Engineering Conference , Huntsville, Alabama, 2002.
71. Chunmin Yang, Barrett Bryant, Ra jeev Ra je, Mikhail Auguston, Andrew Olson, Carol Burt, \Formal Specication in Heterogeneous Distributed Software Integration"", Proceedings of the
40th Annual ACM Southeast Conference , Raleigh, North Carolina, 2002.
13 

72. Fei Cao, Barrett Bryant, Ra jeev Ra je, Mikhail Auguston, Andrew Olson, Carol Burt, \Spec-
ifying Heterogeneous Distributed Components"", Proceedings of the 40th Annual ACM South-
east Conference , Raleigh, North Carolina, 2002.
73. Wei Zhao, Barrett Bryant, Ra jeev Ra je, Mikhail Auguston, Andrew Olson, Carol Burt, \A Unied Approach to Component Assembly Based on Generative Programming"", Proceedings
of the 2002 Workshop on Generative Programming , Austin, Texas, 2002.
74. Wei Zhao, Barrett Bryant, Ra jeev Ra je, Mikhail Auguston, Andrew Olson, Carol Burt, \Gen- erative Composition of Distributed and Heterogeneous Components"", Proceedings of the 40th
Annual ACM Southeast Conference , Raleigh, North Carolina, 2002.
75. Nanditha Siram, Ra jeev Ra je, Barrett Bryant, Andrew Olson, Mikhail Auguston, Carol Burt, \An Architecture for the UniFrame Resource Discovery Service"", Proceedings of the 3rd In-
ternational Workshop on Software Engineering and Midd leware , Orlando, Florida, 2002.
76. Mathew Palakal, Mathew Stephens, Snehasis Mukhopadhyay, Ra jeev Ra je, Simon Rhodes, \A Multi-level Text Mining Method to Extract Biological Relationships"", Proceedings of the
IEEE Computer Society Bioinformatics Conference, CSB2002 , Palo Alto, California, 2002.
77. Barrett Bryant, Mikhail Auguston, Ra jeev Ra je, Andrew Olson, Carol Burt, \Formal Speci- cation of Generative Component Assembly Using Two-Level Grammar"", Proceedings of the
SEKE 2002, 14th International Conference on Software Engineering and Know ledge Engi-
neering , Ischia, Italy, 2002.
78. Carol C. Burt, Barrett R. Bryant, Ra jeev R. Ra je, Andrew Olson. Mikhail Auguston, \Qual- ity of Service Issues Related to Transforming Platform Independent Models to Platform
Specic Models"", Proceedings of the 6th IEEE International Enterprise Distributed Object
Computing Conference , Lausanne, Switzerland, 2002.
79. Changlin Sun, Ra jeev Ra je, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, Zhisheng Huang, \Composition and Decomposition of Quality of Service Parameters in Dis-
tributed Component-Based Systems"", Proceedings of the IEEE 5th International Conference
on Algorithms and Architectures for Paral lel Processing , Beijing, China, 2002.
80. Zhisheng Huang, Ra jeev Ra je, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, Changlin Sun, \System-Level Generative Programming of Unied Approach Based on
UMM for the Integration of Distributed Software Components"", Proceedings of the IEEE 5th
International Conference on Algorithms and Architectures for Paral lel Processing , Beijing,
China, 2002.
81. Fei Cao, Barrett Bryant, Ra jeev Ra je, Mikhail Auguston, Andrew Olson, Carol Burt, \Com- ponent Specication and Wrapper/Glue Code Generation with Two-Level Grammar using
Domain Specic Knowledge"", Proceedings of ICFEM 2002, 4th International Conference on
Formal Engineering Methods , Shanghai, China, 2002.
82. Chunmin Yang, Beum-Seuk Lee, Barrett Bryant, Carol Burt, Ra jeev Ra je, Andrew Olson, \Formal Specication of Non-Functional Aspects in Two-Level Grammar"", Proceedings of
the UML 2002 Workshop on Component-Based Software Engineering and Modeling Non-
Functional Aspects , Dresden, Germany, 2002.
83. Wei Zhao, Barrett R. Bryant, Ra jeev R. Ra je, Mikhail Auguston, Andrew M. Olson, Carol C. Burt, \A Component Assembly Architecture with Two-Level Grammar Infrastructure"",
14 

Proceedings of the OOPSLA'2002 Workshop on Generative Techniques in the context of MDA
,
Seattle, Washington, 2002.
84. Purvi Shah, Barrett R. Bryant, Ra jeev R. Ra je, Carol Burt, Andrew Olson, Mikhail Au- guston, \Interoperability between Mobile Distributed Components using the UniFrame Ap-
proach"", Proceedings of the 41st Annual ACM South East Conference , Savannah, GA, 2003.
85. Natasha Gupta, Ra jeev R. Ra je, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, \Analyzing the Web Services and UniFrame Paradigms"", Proceedings of the Southeast-
ern Software Engineering Conference , Huntsville, Alabama, 2003.
86. Carol C. Burt, Ra jeev R. Ra je, Barrett R. Bryant, Andrew Olson, Mikhail Auguston, \Model Driven Security: Unication of Authorization Models for Fine-Grain Access Control"", Pro-
ceedings of the 7th IEEE International Enterprise Distributed Object Computing Conference
EDOC 2003 , Brisbane, Australia, 2003.
87. Wei Zhao, Barrett R. Bryant, Je Gray, Carol C. Burt, Ra jeev R. Ra je, Mikhail Auguston, Andrew M. Olson, \A Generative and Model Driven Framework for Automated Software
Product Generation"", Proceedings of the 6th Workshop on Component-Based Software Engi-
neering: Automated Reasoning and Prediction , Portland, Oregon, 2003.
88. Snehasis Mukhopadhyay, Mathew Palakal, Daocheng Zhu, Ra jeev Ra je, \Intelligent Infor- mation Management in Bioinformatics"", Proceedings of the 12th Yale Workshop on Adaptive
and Learning Systems , New Heaven, Connecticut, 2003.
89. Fei Cao, Barrett Bryant, Carol Burt, Jerey Gray, Ra jeev Ra je, Andrew Olson, Mikhail Au- guston, \Modeling Web Services: Towards System Integration in UniFrame"", Proceedings of
the 7th World Conference on Integrated Design and Process technology (IDPT 2003) , Austin,
Texas, 2003.
90. Chunmin Yang, Barrett Bryant, Carol Burt, Ra jeev R. Ra je, Andrew Olson, Mikhail Au- guston, \Formal Methods for Quality of Service Analysis in Component-Based Distributed
Computing"", Proceedings of the 7th World Conference on Integrated Design and Process tech-
nology (IDPT 2003) , Austin, Texas, 2003.
91. Fei Cao, Barrett R. Bryant, Carol C. Burt, Zhisheng Huang, Ra jeev R. Ra je, Andrew M. Olson, Mikhail Auguston, \Automating Feature-Oriented Domain Analysis"", Proceedings of
the International Conference on Software Engineering Research and Practice (SERP'03) , Las
Vegas, Nevada, 2003.
92. Barrett Bryant, Beum-Seuk Lee, Fei Cao, Wei Zhao, Carol Burt, Ra jeev Ra je, Andrew Olson, Mikhail Auguston, \From Natural Language Requirements to Executable Models of Software
Components"", Proceedings of the 2003 Monterey Workshop , Chicago, Illinois, 2003.
93. Fei Cao, Barrett Bryant, Ra jeev Ra je, Mikhail Auguston, Andrew Olson, Carol Burt, \As- sembling Components with Aspect-oriented Modeling/Specication"", Proceedings of UML
Workshop Workshop in Software Model Engineering WiSE@UML'2003) , San Francisco, Cal-
ifornia, 2003.
94. Snehasis Mukhopadhyay, Mathew Palakal, Vijay Kodiripaka, Ra jeev R. Ra je, Javed Mostafa, \Managing Information Flow for Complex, Dynamic Tasks Using Multi-agent Collaboration"",
Proceedings of the 2003 IEEE International Symposium on Intel ligent Control, ISIC'03 , Hous-
ton, TX, 2003.
15 

95. Fei Cao, Barrett Bryant, Ra jeev R. Ra je, Mikhail Auguston, Andrew Olson, Carol Burt,
\A Component Assembly Approach Based on Aspect-Oriented Generative Domain Model-
ing"", Proceedings of SC'04, Software Composition Workshop aliated with ETAPS 2004 ,
Barcelona, Spain, 2004.
96. Wei Zhao, Barrett R. Bryant, Ra jeev R. Ra je, Mikhail Auguston, Carol C. Burt, Andrew M. Olson, \Grammatically Interpreting Feature Compositions"", Proceedings of the 16th Inter-
national Conference on Software Engineering and Know ledge Engineering (SEKE'04) , Ban,
Canada, 2004.
97. Fei Cao, Barrett Bryant, Carol Burt, Ra jeev R. Ra je, Andrew Olson, Mikhail Auguston, \A Meta-modeling Approach to Web Services"", Proceedings of ICWS04, IEEE International
Conference on Web Services , San Diego, California, 2004.
98. Wei Zhao, Barrett R. Bryant, Ra jeev R. Ra je, Mikhail Auguston, Carol C. Burt, Andrew M. Olson, \Automated Glue/Wrapper Code Generation in Integration of Distributed and
Heterogeneous Software Components"", Proceedings of the 8th IEEE Enterprise Distributed
Computing Systems Conference (EDOC'04) , Monterey, California, 2004.
99. Pradeep Mysore, Ra jeev R. Ra je, Purushotham Bangalore, Barrett Bryant, \GridFrame { A Framework for Building Component-based Grid Systems"", 12th International Conference on
Advanced Computing & Communication (ADCOM '04) , Ahmedabad, India, 2004.
100. Fei Cao, Barrett Bryant, Wei Zhao, Carol Burt, Ra jeev R. Ra je, Andrew Olson, Mikhail Auguston, \Marshaling and Unmarshaling Models Using Entity-Relationship Model"", Pro-
ceedings of the ACM Symposium on Applied Computing , Santa Fe, New Mexico, 2005.
101. Shih-hsi Liu, Barrett Bryant, Jerey Gray, Ra jeev R. Ra je, Andrew Olson, Mikhail Augus- ton, \Two-level Assurance of QoS Requirements for Distributed Real-time and Embedded
Systems"", Proceedings of the ACM Symposium on Applied Computing, SAC'05 , Santa Fe,
New Mexico, 2005.
102. Shih-hsi Liu, Barrett Bryant, Jerey Gray, Ra jeev R. Ra je, Andrew Olson, Mikhail Auguston, \QoS-UniFrame: A Petri Net-based Approach to Assure QoS Requirements of Distributed
Real-time and Embedded Systems"", 12th IEEE International Conference and Workshop on
the Engineering of Computer-based Systems (ECBS'05) , Greenbelt, Maryland, 2005.
103. Shih-hsi Liu, Fei Cao, Barrett Bryant, Jerey Gray, Ra jeev R. Ra je, Andrew Olson, Mikhail Auguston, \Quality of Service Requirement Analyses for Component Composition: A Two-
level Grammar Approach"", Proceedings of the 17th International Conference on Software
Engineering and Know ledge Engineering (SEKE'05) , Taipei, Taiwan, 2005.
104. Shih-hsi Liu, Barrett Bryant, Jerey Gray, Ra jeev R. Ra je, Mihran Tuceryan, Andrew Olson, Mikhail Auguston, \QoSPL: A QoS-driven Software Product Line Engineering Framework
for Distributed Real-time ad Embedded Systems"", Proceedings of the 18th International
Conference on Software Engineering and Knowledge Engineering (SEKE'06), San Francisco,
California, 2006.
105. Omkar J. Tilak, Ra jeev R. Ra je, Xukai Zou, \Composing Access Control Policies of Dis- tributed Components"", Proceedings of the 2nd IEEE International Symposium on Dependable,
Autonomic and Secure Computing , Indianapolis, Indiana, 2006.
16 

106. Shih-Hsi Liu, Barrett R. Bryant, Mikhail Auguston, Je Gray, Ra jeev Ra je and Mihran
Tuceryan, \A Component-Based Approach for Constructing High-Condence Distributed
Embedded Systems"", Proceedings of the Monterey Workshop Series, 2005 Theme: Work-
shop on Networked Systems: Realization of Reliable Systems on Top of Unreliable Networked
Platforms , Laguna Beach, California, 2006.
107. Omkar Tilak, Ra jeev R. Ra je, \Temporal Interaction Contracts for Components in a Dis- tributed System"", Proceedings of the 11th IEEE International Enterprise Distributed Object
Computing Conference EDOC 2007 , Annapolis, Maryland, 2007.
108. Girish Joshi, Ra jeev Ra je, Mihran Tuceryan, \Designing and Experimenting with a Dis- tributed Tracking System"", Proceedings of the 14th IEEE International Conference on Par-
al lel and Distributed Systems , Melbourne, Australia, 2008.
109. Ra jeev R. Ra je, Pratibha Katuri, Anjali Kumari, Omkar Tilak, \Multi-level Matching of Dis- tributed Software Components"", Proceedings of the International Conference on Computer,
Communication, and Instrumentation , Mumbai, India, 2009.
110. Snehasis Mukhopadhyay, Shengquan Peng, Ra jeev R. Ra je, Mathew Palakal, Javed Mostafa, \Performance and Processing Time of Some Information Filtering Systems on a Benchmark
Text Data Set"", Proceedings of the 21st International Conference on Software Engineering
and Know ledge Engineering , Boston, MA, 2009.
111. Ketaki A. Pradhan, Lahiru Gallege, Alfredo Moreno, Ra jeev R. Ra je, \MDE-URDS: A Mobile Device Enabled Service Discovery System"", Proceedings of the International Conference on
Recent trends in Computing and Communications , Chennai, India, 2009.
112. Omkar J. Tilak, Snehasis Mukhopadhyay, Ra jeev R. Ra je, Mihran Tuceryan, \A Novel Re- inforcement Learning Framework for Sensor Subset Selection"", Proceedings of 2010 IEEE
International Conference on Networking, Sensing, and Control , Chicago, IL, 2010.
113. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Sucheta Phatak, Rashmi Shastri, Lahiru Gallege, \Software Service Selection by Multi-Level Matching and Reinforcement Learning"", Proceed-
ings of the 5th International ICST Conference on Bio-Inspired Models of Network, Informa-
tion, and Computing Systems (in Cooperation with ACM SIGSIM) , Boston, MA, 2010.
114. Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \A Next-generation of a Distributed Tracking System"", Proceedings of the International Conference On Demand Computing , Ban-
guluru, India, 2010.
115. Lahiru Gallege, Ketaki Pradhan, Ra jeev R. Ra je, \Experiments with a Multi-level Discovery System"", Proceedings of the 1st International Conference on Computing , New Delhi, India,
2010.
116. Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \Enhancing a Distributed Tracking System"", Proceedings of the 3rd International Joint Conference on Information and Commu-
nication Technology , Mumbai, India, 2011.
117. Lahiru S. Gallege, Dimuthu U. Gamage, James H. Hill, Ra jeev R. Ra je, \Towards a Com- prehensive Method for Integrating Trust into Enterprise DRE Systems"", Proceedings of the
17th IEEE International Conference on Embedded and Real-Time Computing Systems and
Applications, Work-in-Progress Session , Toyoma, Japan, 2011.
17 

118. Lahiru S. Gallege, Dimuthu U. Gamage, James H. Hill, Ra jeev R. Ra je, \A Case Study
in Composing a Trusted Distributed Real-time and Embedded (DRE) System"", Proceedings
of the International Conference On Network Infrastructure Management Systems , Mumbai,
India, 2011.
119. Dimuthu U. Gamage, Zhisheng Huang, Andrew Olson, Ra jeev R. Ra je, \Creating QoS- aware Distributed Computing Systems Using UniFrame Approach"", Proceedings of the 2nd
International Conference on Computing , New Delhi, India, 2011.
120. Lahiru S. Gallege, Aboli J. Phadke, Ra jeev R. Ra je, Meghna Babbar-Sebens, \Cloud Service Selection from Earth Science Domain"", Proceedings of the 2nd International Conference on
Recent Trends in Information Technology and Computer Science , Mumbai, India, 2012.
121. Dimuthu U. Gamage, Lahiru S. Gallege, James H. Hill, Ra jeev R. Ra je, \A Compositional Trust Model for Predicting the Trust Value of Software System QoS Properties"", Proceedings
of the 10th IEEE/IFIP International Conference on Embedded and Ubiquitous Computing ,
Paphos, Cyprus, 2012.
122. Lahiru S. Gallege, Dimuthu U. Gamage, James H. Hill, Ra jeev R. Ra je, \Trust Contract of a Service and its role in Service Selection for Distributed Software Systems"", Proceedings of the
8th Cyber Security and Information Intel ligence Research Workshop , Oak Ridge, TN, 2013.
123. Lahiru S. Gallege, Dimuthu U. Gamage, James H. Hill, Ra jeev R. Ra je, \Experimental Eval- uation of Trustworthiness of Compositional Systems"", Proceedings of the 2nd International
Conference On Network Infrastructure Management Systems , Mumbai, India, 2013.
124. Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \eDOTS 2.0: A Pervasive Indoor Track- ing System"" Proceedings of the International Conference on Software Engineering and Know l-
edge Engineering (SEKE'13) , Boston, MA, 2013.
125. Lahiru S. Gallege, Dimuthu Gamage, James H. Hill, Ra jeev R. Ra je, \Trustworthy Service Selection using Long-term Monitoring of Trust Contracts"", Proceedings of the 17th IEEE
International EDOC Conference , Vancouver, Canada, 2013.
126. Lahiru S. Gallege, Dimuthu Gamage, James H. Hill, Ra jeev R. Ra je, \Towards Trust-Based Recommender Systems for Online Software Services"", Proceedings of the 9th Cyber Security
and Information Intel ligence Research Workshop , Oak Ridge, TN, 2014.
127. Lakshmi Manohar Rao Velicheti, Dennis Feiock, T. Manjula Peiris, Ra jeev R. Ra je, James H. Hill, \ Towards Modeling the Behavior of Static Code Analysis Tools"", Proceedings of the
9th Cyber Security and Information Intel ligence Research Workshop , Oak Ridge, TN, 2014.
128. Aboli Phadke, Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \Incorporating Mobile Devices in Indoor Tracking "", Proceedings of the 3rd International Conference On Network
Infrastructure Management Systems , Mumbai, India, 2014.
129. Ruchika Malhotra, Ra jeev R. Ra je, \An Empirical Comparison of Machine Learning Tech- niques for Software Defect Prediction "", Proceedings of the 8th International Conference on
Bio-inspired Information and Communications Technologies , Boston, MA, 2014.
130. Dimuthu Gamage, Lahiru S. Gallege, Ra jeev R. Ra je, \A QoS and Trust Prediction Frame- work for Context-Aware Composed Distributed Systems"", Proceedings of the 22nd IEEE
International Conference on Web Services , New York, NY, 2015.
18 

131. Amrita Mangaonkar, Allenoush Hayrapetian, Ra jeev R. Ra je, \Collaborative Detection of Cy-
berbullying Behavior in Twitter Data"", Proceedings of IEEE International Electro/Information
Technology Conference , Naperville, IL, 2015.
132. Ruchika Malhotra, Anuradha Chug, Allenoush Hayrapetian, Ra jeev R. Ra je, \Analyzing and Evaluating Security Features in Software Requirements"", Proceedings of the International
conference on Innovation and Chal lenges in Cyber Security (co-sponsored by IEEE) , Noida,
India, 2016.
133. Lahiru Gallege, Ra jeev R. Ra je, \Selecting and Recommending Online Software Services by Evaluating External Attributes"", Proceedings of 11th Annual Cyber and Information Security
Research Conference , Oak Ridge, TN, 2016.
134. Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \Infusing Trust in Indoor Tracking"", Proceedings of the 10th ACM International Conference on Distributed and Event-Based Sys-
tems , Irvine, CA, 2016.
135. Dimuthu Gamage, Lahiru S. Gallege, Ra jeev R. Ra je, \A QoS and Trust Adaptation Frame- work for Composed Distributed Systems"", Proceedings of the 13th IEEE International Con-
ference on Services Computing , San Francisco, CA, 2016.
136. Sonali Sharma, Ra jeev R. Ra je, Ruchika Malhotra, \Towards Formalizing Adaptive Software Services"", Proceedings of the IEEE India International Conference on Information Processing ,
New Delhi, India, 2016.
137. Lahiru Gallege, Ra jeev R. Ra je, \Parallel Methods for Evidence and Trust based Selection and Recommendation of Software Apps from Online Marketplaces"", Proceedings of the 12th
Annual Cyber and Information Security Research (CISR) Conference , Oak Ridge, TN, 2017.
138. Boakye Dankwa, Raghavendran Vijayan, Darsh Sanghavi, Mihran Tuceryan, Ra jeev R. Ra je, \Trust in Vehicle-to-Vehicle Communication"", Proceedings of the International Conference
on Futuristic Trends in Computational Analysis and Know ledge Management (Technical Co-
sponsor IEEE) , Noida, India, 2017.
139. Zachary P. Reynolds, Abhinandan B. Jayanth, Ugur Koc, Adam Porter, Ra jeev R. Ra je, James H. Hill, \Identifying and Documenting False Positive Patterns Generated by Static
Code Analysis Tools"", Proceedings of the SER&IP Workshop in conjunction with ICSE ,
Buenes Aires, Argentina, 2017.
140. Nahida Chowdhury, Ra jeev R. Ra je, \Disparity between the Programmatic Views and the User Perceptions of Mobile Apps"", Proceedings of the 20th IEEE ICCIT , Dhaka, Bangladesh,
2017.
141. Allenoush Hayrapetian, Ra jeev R. Ra je, \Empirically Analyzing and Evaluating Security Fea- tures in Software Requirements"", Proceedings of the 11th Innovations in Software Engineering
Conference , Hyderabad, India, 2018.
142. Enas Alikhashashneh, Ra jeev Ra je, James Hill, \Using Software Engineering Metrics to Eval- uate the Quality of Static Code Analysis Tools"", Proceedings of the 1st International Confer-
ence on Data Intel ligence and Security , South Padre Island, USA, 2018.
143. Yash Agarwal, Rohit Pawar, Akshay Joshi, Ranadheer Gorrepati, Ra jeev Ra je, \Distributed Cyberbulling Detection System with Multiple Server Congurations"", Proceedings of the 17th
IEEE International Conference on Electro Information Technology , Rochester, MI, 2018.
19 

144. George Mohler, Ra jeev Ra je, Jeremy Carter, Mathew Valasik, P. Jerey Brantingham, \A
penalized likelihood method for balancing accuracy and fairness in predictive policing"", Pro-
ceedings of the IEEE SMC Conference , Miayzaki, Japan, 2018.
145. Enas Alikhashashneh, Ra jeev R. Ra je, James Hill, \Using Machine Learning Techniques to Classify and Predict Static Code Analysis Tool Warnings"", Proceedings of the 15th ACS/IEEE
International Conference on Computer Systems and Applications (AICCSA) , Aqaba, Jordan,
2018.
146. Saurabh Pandey, Nahida Sultana Chowdhury, Milan Patil, Ra jeev R. Ra je, Shreyas C S, George Mohler, Jeremy Carter, \CDASH: Community Data Analytics for Social Harm Pre-
vention"", Proceedings of the IEEE International Smart Cities Conference (ISC2) , Kansas
City, MO, 2018.
147. Rohit Pawar, Apeksha Jangam, Vishwesh Janardhana, Ra jeev R. Ra je, Meeta Pradhan, Preeti Mulay, Any Chacko, \Diabetes Readmission Prediction using Distributed and Collab-
orative Paradigms"", Proceedings of the IEEE International conference on Data Science and
Analytics , Pune, India, 2018.
148. Nahida Chowdhury, Ra jeev R. Ra je, \A Holistic Ranking Scheme for Apps"", Proceedings of
the 21th IEEE ICCIT , Dhaka, Bangladesh, 2018.
149. Rohit Pawar, Ra jeev R. Ra je, \Multilingual Cyberbullying Detection System"", Proceedings
of the IEEE International Conference on Electro/Information Technology (EIT) , Brookings,
SD, 2019.
150. Keyur Mehta, Anushka Patil, Snehal Vyawahare, Ra jeev R. Ra je, \A System to Compute Safest Path for Commuters"", Proceedings of the 2nd World Summit on Advances in Science,
Engineering and Technology , Indianapolis, IN, 2019.
151. Saurabh Pandey, Nahida Chowdhury, Ra jeev R. Ra je, George Mohler, Jeremy Carter, \Trust Estimation of Historical Social Harm Events in Indianapolis Metro Area"", Proceedings of the
IEEE International Smart Cities Conference (ISC2) , Casablanca, Morocco, 2019.
152. Nahida Chowdhury, Ra jeev R. Ra je, \SERS: A Security-related and Evidence-based Ranking Scheme for Mobile Apps"", Proceedings of the First IEEE International Conference on Trust,
Privacy and Security in Intel ligent Systems, and Applications , Los Angles, CA, 2019.
153. Preeti Mulay, Neha Divekar, Swati Kadlag, Ra jeev R. Ra je, Sushama Purandare \Simple way to achieve inner-health with Manache Shlok: A Machine Learning Way "", Proceedings
of the 8th International Conference on Bhagavad Gita and Ramayan as Perennial Sources of
Leadership , Varanasi, India, 2020.
154. Nahida Sultana Chowdhury, Ra jeev R. Ra je, Saurabh Pandey, George Mohler, Jeremy Carter, \Enhancing Trust-based Data Analytics for Forecasting Social Harm"", Proceedings of the
IEEE International Smart Cities Conference , 2020.
155. Umesh Ra ja, Nahida Sultana Chowdhury, Ra jeev R. Ra je, Rachel Wheeler, Jane Williams, Aaron Ganci, \COVID CV: A System for Creating Holistic Academic CVs during a Global
Pandemic"", Proceedings of the IEEE International Conference on Electro/Information Tech-
nology (EIT) , Mount Pleasant, MI, 2021.
20 

Editorial Comments
156. Ra jeev R. Ra je, \Distributed Computing: A Choice of the Present and the Future!"", Guest Editor's Introduction, ACM SIGAPP Applied Computing Review, Vol. 7, No.1, Page: 4,
Spring 1999 .
157. Chang-Hyun Jo, Ra jeev R. Ra je, \Editorial Message: Technical Track on Programming Lan- guages and Ob ject Technologies"", ACM Symposium on Applied Computing, SAC'02 , 2002.
158. Ra jeev R. Ra je, Barrett R. Bryant, \Ob ject-based Cluster Computing"", Guest Editor's In- troduction, ACM SIGAPP Applied Computing Review , 2002.
159. Barrett R. Bryant, Ra jeev R. Ra je, \Distributed Ob ject and Component-based Software Systems"", Mini-track Introduction, Thirty-sixth Annual Hawaii International Conference on
System Sciences, HICSS-36 , 2003.
160. Chang-Hyun Jo, Ra jeev R. Ra je, \Editorial Message: Technical Track on Programming Lan- guages and Ob ject Technologies"", ACM Symposium on Applied Computing, SAC'03 , 2003.
161. Barrett R. Bryant, Ra jeev R. Ra je, \Distributed Ob ject and Component-based Software Systems"", Mini-track Introduction, Thirty-seventh Annual Hawaii International Conference
on System Sciences, HICSS-37 , 2004.
162. Davide Ancona, Ra jeev R. Ra je, Mirko Viroli, \Editorial Message: Technical Track on Ob ject- oriented Programming Languages and Systems"", ACM Symposium on Applied Computing,
SAC'04, and SAC'05 , 2004 and 2005.
163. Barrett R. Bryant, Ra jeev R. Ra je, Vana Kalogeraki, \Distributed Ob ject and Component- based Software Systems"", Mini-track Introduction, Thirty-seventh Annual Hawaii Interna-
tional Conference on System Sciences, HICSS-37 , 2004.
164. Barrett R. Bryant, Ra jeev Ra je, Marjan Mernik, \Editorial Message: Technical Track on Programming Languages"", ACM Symposium on Applied Computing, SAC'18 , 2018.
165. Barrett R. Bryant, Ra jeev Ra je, \Editorial Message: Technical Track on Programming Lan- guages"", ACM Symposium on Applied Computing, SAC'19-22 , 2019-22.
Posters
166. Sivakumar Chinnasamy, Ra jeev R. Ra je, \Formal Specication of Software Design Patterns"", Spring SERC Showcase , 2000.
167. Ra jeev R. Ra je, Snehasis Mukhopadhyay, Mathew Palakal, Javed Mostafa, \SIFTER"", Su-
percomputing , 2000.
168. Ra jeev R. Ra je, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, \UniFrame"", Fal l SERC showcase , 2001.
169. Ra jeev R. Ra je, Natasha Gupta, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, \Encompassing .Net into UniFrame"", SERC Spring showcase, 2002.
170. Ra jeev R. Ra je, Girish Brahnmath, Natasha Gupta, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, \QoS Framework of UniFrame"", SERC Fal l showcase, 2002.
171. Ra jeev R. Ra je, Andrew Olson, Barrett Bryant, Mikhail Auguston, Carol Burt, \UniFrame"", SERC Spring and Fal l showcases , 2003.
21 

172. Pradeep Mysore, Ra jeev R. Ra je, Barrett Bryant, Purushotham Bangalore, \GridFrame"",
SIGSOFT'04 , 2004.
173. Ra jeev R. Ra je, Amruta Jejurikar, Saurabh Agrawal, Pratibha Katuri, Andrew Olson, \Ex- perimenting with Multi-level Matching of Software Components"", SERC Fal l showcase, 2006.
174. Subir K. Chakrabarti, Ra jeev R. Ra je, \Cloud Computing, Information Flow and Markets"", NSF PI Meeting: The Science of Cloud, Arlington, VA, 2011.
175. Lahiru Gallege, Dimuthu Gamage, James Hill, Ra jeev Ra je, \Modeling, Specifying, Discover- ing, and Integrating Trust into Distributed Real-time and Embedded (DRE) Systems"" S 2
ERC
Showcase, Ames, IA, November 2011; Arlington, VA, May 2012; Muncie, IN; November 2012
and Chicago, IL, May 2013.
176. Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \e-DOT: An Indoor Distributed Track- ing System"", S 2
ERC Showcase, Ames, IA, November 2011, and Arlington, VA, May 2012.
177. Arjan Durresi, Ra jeev R. Ra je, \Trustworthy Cloud Computing using an Adaptive Architec- ture"", S 2
ERC Showcase, Muncie, IN, November 2012.
178. James Hill, Ra jeev R. Ra je, \A Congurable Programming and Execution Model for Multi- core Architectures"", S 2
ERC Showcase, Muncie, IN, November 2012.
179. James Hill, Ra jeev R. Ra je, \Testing-as-a- Service: Static Code Analysis (SCA) Tool Study"", S2
ERC Showcase, Muncie, IN, November 2012.
180. Lahiru S. Gallege, Dimuthu U. Gamage, James H. Hill, Ra jeev R. Ra je, \Trusted Service Representation and Selection for Generating Distributed Real-time and Embedded (DRE)
Systems"", IUPUI Research Day, 2012.
181. Sonali Sharma, Ra jeev R. Ra je, \A Framework for Creating Adaptive Software Services and Distributed Systems"", S 2
ERC Showcase, Muncie, IN, November 2012.
182. Dimuthu U. Gamage, Lahiru S. Gallege, James H. Hill, Ra jeev R. Ra je, \Trusted Service Composition for Distributed Real-Time and Embedded (DRE) Systems"", IUPUI Research
Day, 2012.
183. Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \e-DOTS: An Indoor Tracking Solution"", IUPUI Research Day, 2012 and 2013.
184. Lakshmi Manohar Rao Velicheti, Dennis C. Feiock, Ra jeev R. Ra je, James H. Hill, \Qual- itative and Quantitative Evaluation of Static Code Analysis Tools"", IUPUI Research Day,
2013.
185. Ryan Rybarczyk, Ra jeev R. Ra je, Mihran Tuceryan, \Opportunistic Indoor Tracking"", IUPUI Innovation to Enterprise Showcase & Forum, 2013.
186. Lakshmi Manohar Rao Velicheti, Dennis C. Feiock, Ra jeev R. Ra je, James H. Hill, \Testing- as-a- Service: Static Code Analysis (SCA) Tool Study"", S 2
ERC Showcase, Chicago, IL, May
2013 Pensacola, FL, November 2013, Washington DC, May 2014, and Muncie, IN November
2014.
187. Lahiru Gallege, Dimuthu Gamage, James Hill, Ra jeev Ra je, \A Holistic Approach to Software Reuse"" S 2
ERC Showcase, Pensacola, FL, November 2013.
22 

188. Allenoush Hayrapetian, Lahiru Gallege, James Hill, Ra jeev Ra je, \A Narrative-based Ap-
proach to Requirement Analysis"", S 2
ERC Showcase, Washington DC, May 2014.
189. Ruchika Malhotra, James Hill, Ra jeev Ra je, \Development of a Defect Prediction System using Machine Learning Techniques"" S 2
ERC Showcase, Muncie, IN, November 2014.
190. Amrita Mangaonkar, Allenoush Hayrapetian, Ra jeev Ra je, \Detection of Cyberbullying in Twitter Data using Collaborative Approaches"" S 2
ERC Showcase, Muncie, IN, November
2014.
191. Allenoush Hayrapetian, Ruchika Malhotra, Ra jeev Ra je, \Analyzing and Evaluating Security Features in Software Requirements"", S 2
ERC Showcase, Roanoke,VA, May 2015.
192. Ra jeev Ra je, Dimuthu Gamage, \Quantifying, Assessing and Composing Trust in IoT"", S2
ERC Showcase, Indianapolis, IN, November 2015.
193. Xia Ning, Ra jeev Ra je, \Ranking and Recommending Secure Services"", S 2
ERC Showcase,
Indianapolis, IN, November 2015.
194. Ra jeev Ra je, Lahiru Gallege, Ryan Rybarczyk, \Summarizing and Interpreting Semi-structured Data Over Long-term Using Evidences"", S 2
ERC Showcase, Muncie, IN, May 2016.
195. Ra jeev Ra je, Ryan Rybarczyk, Xia Ning, Nahida Chowdhury, \Collecting and Analyzing Internal Evidences of Software Services via Model Checking"", S 2
ERC Showcase, Washington
DC, May 2017.
196. Nahida Chowdhury, Ra jeev Ra je, \TRR: Trust-based Mobile apps selection and ordering over traditional feedback mechanism"", Research Day, IUPUI, Indianapolis, IN, April 2018.
197. Saurabh Pandey, Nahida Chowdhury, Milan Patil, Ra jeev Ra je, George Mohler, Jeremy Carter, \W-CDASH: A Service-based Decentralized Prediction System of Social Harm Events"",
Research Day, IUPUI, Indianapolis, IN, April 2018.
198. Nahida Chowdhury, Ra jeev R. Ra je, \A Comprehensive Ranking Scheme for Apps"", Grad Cohort for Women 2019, CRA-W, Chicago, IL, April 2019.
Abstracts
199. Michael Boyles, Nila Patel, Snehasis Mukhopadhyay, Ra jeev Ra je, \A Collaborative Approach to Information Filtering"", Proceedings of Argonne Symposium for Undergraduates , pp: 33,
1996.
200. Michael Boyles, Nila Patel, Snehasis Mukhopadhyay, Ra jeev Ra je, \A Distributed Envi- ronment For Collaborative Information Filtering"", Proceedings of Second Indiana University
Undergraduate Research Conference , 1996.
201. Michael Boyles, Nila Patel, Snehasis Mukhopadhyay, Ra jeev Ra je, \The Eects of a \Time- out"" Period for a Distributed Information Filtering System"", Proceedings of Butler University
Undergraduate Research Conference , pp: 38, 1997.
Technical Reports
202. Ra jeev R. Ra je, Daniel J. Pease, \A Class Algebra - A New Approach to Class Analysis"", NY CASE Center, No. 9309, pp: 1-45, 1993.
23 

203. Sivakumar Chinnasamy, Yi Dai, Ra jeev R. Ra je, \Formalization of Design Pattern Speci-
cations: A Survey"", Department of Computer and Information Science Technical Report,
TR-CIS-0799-14, 1999.
204. Girish Brahnmath, Ra jeev R. Ra je, Andrew Olson, Changlin Sun, \Quality of Service Catalog for Software Components"", Technical Report (TR-CIS-0219-01), Department of Computer
and Information Science, Indiana University-Purdue University Indianapolis, 2001.
205. Carol Burt, Barrett R. Bryant, Ra jeev R. Ra je, Andrew Olson, Mikhail Auguston, \An analysis of Distributed Component Architectures Standards and the QOS standards Needed
to Progress Unication"", Technical Report (CIS-TR-2001-11), Department of Computer and
Information Sciences, University of Alabama Birmingham, 2001.
Faculty Mentor for Stand-alone Student Publications
206. Joseph Williams, \An Implicit Ob ject Oriented Programming Environment Based on OF- FERS"" { Proceedings of First Indiana University Undergraduate Research Conference, 1995.
207. Sivakumar Chinnasamy, \Analysis of Design Patterns"", IUPUI School of Science Graduate
Research Symposium , 1999.
208. Lahiru Gallege, \TruSSCom: Proposal for Trustworthy Service Representation, Selection and Negotiation for Integrating Software Systems"", ACM SPLASH Doctoral Symposium, 2013.
209. Ryan Rybarczyk, \Proposal for Managing Sensor Selection Through The Integration of Trust for Indoor Tracking Systems"", IEEE PerCom Doctoral Symposium , 2015 {Won the IEEE
TCPP Best Poster and Presentation Award ( $1000) .
Presentations
Panels1. \Preparing Applications for M.S. in Computer Science"", Panelist in the webinar organized by Yashna Trust-EducationUSA, July 2022.
2. \STEM Around the World"", Panelist in the webinar organized by WiSH, Purdue House, and I-House, IUPUI, February 2022.
3. \Best Practices in Industry Advisory Board Development"", Panelist at the 2019 Annual Meeting of the Council of Colleges of Arts & Sciences, November 2019.
Tutorials
1. \Distributed-Ob ject Computing { An Overview of Java-RMI and CORBA"" { At the Fifth International Conference on Advanced Computing, organized in-cooperation with IEEE Com-
puter Society, ADCOMP'97, December 1997.
2. \Multi-agent Systems"" { In the Spring 2000 Meeting of the Institute for Operation Research and the Management Sciences (INFORMS) Computing Society, May 2000.
Invited Talks (This list does not include presentations delivered at conferences and workshops.)
1. \The applications of VISRAM/DIMOS to Industrial Automation"", Industrial Workshop in Siemens Limited, Bombay, India, June 1987.
24 

2. \Ob ject-Flow - A Symbiosis of Ob ject Oriented Design and Data-Flow Analysis"", Minnow-
brook Workshop on Software Engineering for Paral lel Computing , New York, August 1992.
3. \Can Ob ject-Oriented Design and Data-Flow Computation Complement Each Other Well?"", At Victoria Jubilee Technical Institute, Bombay, India, September 1992.
4. \Ob ject-Oriented Program Partitions"", United Technologies Research Center, Hartford, Con- necticut, January 1994.
5. \A Boolean Algebra of Classes"", Tektronix Laboratories, Hillsboro, Oregon, February 1994.
6. \Ob ject-Flow - A Symbiosis of Ob ject Oriented Design and Data-Flow Analysis"", Department of Computer and Information Science, Indiana University-Purdue University Indianapolis,
Indianapolis, IN, May 1994.
7. \HPJava - A Solution to Meta-Challenges"", Department of Computer and Information Sci- ence, Indiana University-Purdue University Indianapolis , Indianapolis, IN, April 1996.
8. \Computer Science { What is it Exactly?"", To Ben Davis High School Students, Indianapolis, IN, May 1996.
9. \Collaborative Visualization"", Partners of IU's Virtual Reality Initiative, Indianapolis, IN, May 1997.
10. \Principles of Distributed-Ob ject Computing and their relation to PLC Programming"", Con- cept Engineers Incorporated, Mumbai, India, December 1997.
11. \Experiences with the Java-RMI Paradigm for Distributed Computing"" { Software Tech- nology Committee of the Indiana Business Modernization & Technology Corporation, Indi-
anapolis, IN, March 1998.
12. \Experiments with Distributed-Ob ject Systems"", At Delco Electronics Incorporated, Kokomo, IN, April 1998.
13. \Knowledge Sharing and Collaboration: Applications in Bioinformatics Research, (joint), Eli Lilly & Company, Indianapolis, IN, September 1998.
14. \Distributed-ob ject Computing using Java-RMI"", 1998 Indianapolis Client/Server and Inter- net Developer's Conference, Indianapolis, IN, October 1998.
15. \Distributed Computing with Java-RMI"", Indianapolis Java User's Group, Indianapolis, IN, May 1999.
16. \Distributed Software Systems { Development, Testing and Reliability"" (joint), at Microsoft Corporation, Redmond, WA, June 1999.
17. \Knowledge Sharing and Collaboration: Applications in Bioinformatics Research"", Software Technology Committee of the Indiana Business Modernization & Technology Corporation,
Indianapolis, IN, September 1999.
18. \An Intelligent Information Filtering System for Digital Libraries"", (joint), NSF Digital Li- braries Initiative All-Pro jects Meeting, Ithaca, NY, October 1999.
19. \On Designing a Multi-Agent System for Distributed Information Filtering"", Fall 1999 Meet- ing of the Institute for Operation Research and the Management Sciences Computing Society,
Philadelphia, PA, November 1999.
25 

20. \UMM: Unied Meta-ob ject Model and Global Systems"", Department of Computer and
Information Sciences, University of Alabama, Birmingham, AL, January 2000.
21. \Experiences in Building Distributed-ob ject Systems"" { Plenary Speaker{ Symposium on
Ob ject-oriented Distributed Computing at the 77 th
Annual Meeting of the Alabama Academy
of Science, Birmingham, AL, March 2000.
22. \D-SIFTER: A Case Study in Designing a Multi-Agent System"", Spring 2000 Meeting of the Institute for Operation Research and the Management Sciences Computing Society, Salt Lake
City, UT, May 2000.
23. \Distributed-ob ject Systems"", IBM Watson Research Center, Hawthorne, NY, May 2000.
24. \SIFTER: A Content-based Information Filtering System"" (joint), NSF Digital Libraries Initiative All-Pro jects Meeting, Stratford-upon-Avon, England, June 2000.
25. \D-SIFTER: A Distributed Information Filtering System"", NSF Digital Libraries Initiative All-Pro jects Meeting, Stratford-upon-Avon, England, June 2000.
26. \Distributed-ob ject Systems"", NiSys Corporation, Indianapolis, IN, July 2000.
27. \Heterogeneous Distributed-ob ject Computing"", Indiana SERC Showcase, Indianapolis, IN, June 2001.
28. \A Framework for Seamless Integration of Distributed Heterogeneous Software Components"", ONR CIP/SW Meeting, Arlington, VA, July 2001.
29. \UniFrame"" (joint), OMG Technical Meeting, Toronto, Canada, September 2001.
30. \UniFrame"" (joint), Indiana SERC Fall Showcase, Ball State University, Muncie, IN, Decem- ber 2001.
31. \UniFrame"" (joint), OMG Technical Meeting, Anaheim, CA, February 2002.
32. \Encompassing .Net into UniFrame"" (joint), SERC Spring Showcase, University of West Virginia, Morgantown, VW, May 2002.
33. \UniFrame"", Oce of Naval Research, London, England, May 2002.
34. \UniFrame"", (joint), University of Edinburgh, Edinburgh, Scotland, May 2002.
35. \UniFrame"", (joint), Lancaster University, Lancaster, England, May 2002.
36. \UniFrame"", NSF US-EU Workshop, Landsdowne, VA, September 2002.
37. \QoS Framework of UniFrame"" (joint), SERC Fall Showcase, Ball State University, Muncie, IN, December 2002.
38. \UniFrame"", VJTI, University of Mumbai, Mumbai, India, December 2002.
39. \UniFrame"", ONR CIP/SW Review, Harpers Ferry, VA, May 2003.
40. \UniFrame"", ONR CIP/SW Review, Annapolis Junction, MD, November 2003.
41. \Model-driven Security: Unication of the Authorization Models for Fine-grain Access Con- trol"" (joint), SERC Fall Showcase, Ball State University, Muncie, IN, December 2003.
26 

42. \Modeling Component Quality of Service"" (joint), SERC Spring Showcase, West Virginia
University, Morgantown, WV, May 2004.
43. \Experiments with the UniFrame Resource Discovery System"", CS Department, University of Alabama at Birmingham and IEEE Computer Society Chapter, Birmingham, AL, July
2005.
44. \UniFrame: An Exercise in Developing Distributed Software Systems"", Department of Infor- mation Systems and Operations Management, Ball State University, Muncie, IN, November
2005.
45. \Computer Science at IUPUI"", Department of Computer Science, Ball State University, Muncie, IN, October 2006.
46. \Experimenting with Multi-level Matching of Software Components"" (joint), SERC Fal l show-
case , Muncie, IN December 2006.
47. \Computer Science at IUPUI"", Ruparel College, SIES College, Vivekanand College, Xavier Institute of Engineering, Kelkar College, Somaiya College, Sathye College, and VPM Poly-
technic College, India, January 2008.
48. \Experiments with Multi-level Contracts and Matching in Distributed Systems { The UniFrame Way"", Department of Electrical and Computer Engineering, IUPUI, Indianapolis, IN, Febru-
ary 2008.
49. \UniFrame"", Research Day, IUPUI, Indianapolis, IN, March 2009.
50. \S 2
ERC Research at IUPUI"", SERC Showcase, Indianapolis, IN, September 2009.
51. \MDE-URDS"", VJTI, Mumbai, India, April 2011.
52. \MDE-URDS"", VESIT, Mumbai, India, April 2011.
53. \A Distributed Framework for Location Tracking"" (joint), S 2
ERC Showcase, Arlington, VA,
May 2012.
54. \Modeling, Specifying, Discovering, and Integrating Trust into Distributed Real-time and Embedded (DRE) Systems"" (joint), S 2
ERC Showcase, Ames, IA, November 2011; Arlington,
VA, May 2012; Muncie, IN, November 2012; Chicago, IL, May 2013.
55. \Testing-as-a-Service: Static Code Analysis (SCA) Tool Study"", (joint), S 2
ERC Showcase,
Muncie, IN, November 2012; Chicago, IL, May 2013, Indianapolis, IN, November 2015.
56. \Trustworthy Cloud Computing using an Adaptive Architecture"", (joint), S 2
ERC Showcase,
Muncie, IN, November 2012.
57. \Using Multi-core Architectures to Improve Real-time Instrumentation Capabilities of Soft- ware Systems"", (joint), S 2
ERC Showcase, Muncie, IN, November 2012.
58. \Trust in Distributed Software Systems"", Keynote Address, 2 nd
International Conference On
Network Infrastructure Management Systems, Mumbai, India, June 2013.
59. \A Holistic Approach to Software Reuse"" S 2
ERC Showcase, Pensacola, FL, November 2013.
60. \A Narrative-based Approach to Requirement Analysis"" (joint), S 2
ERC Showcase, Washing-
ton DC, May 2014.
27 

61. \Development of a Defect Prediction System using Machine Learning Techniques "" (joint),S
2
ERC
Showcase, Muncie, IN, November 2014.
62. \Analyzing and Evaluating Security Features in Software Requirements"", (joint) S 2
ERC
Showcase, Roanoke,VA, May 2015.
63. \Ranking and Recommending Secure Services"", (joint) S 2
ERC Showcase, Indianapolis, IN,
November 2015.
64. \Quantifying, Assessing and Composing Trust in IoT"", S 2
ERC Showcase, Indianapolis, IN,
November 2015.
65. \Summarizing and Interpreting Semi-structured Data Over Long-term Using Evidences"", (joint) S 2
ERC Showcase, Muncie, IN, May 2016.
66. \Collecting and Analyzing Internal Evidences of Software Services via Model Checking"", (joint) S 2
ERC Showcase, Washington DC, May 2017.
67. \High-assurance, Quality-aware, and Veriable Distributed Software Systems -{ Still the Final Frontier?"", National Institute of Technology { Calicut, India, January 2018.
68. \High-assurance, Quality-aware, and Veriable Distributed Software Systems -{ Still the Final Frontier?"", Plenary Talk, International Virtual Conference on Industry 4.0, Organized by VIT
and Manchester Metropolitan University, July 2020.
69. \Multi-lingual Cyberbullying detection using collaborative approaches"", Keynote Talk, 3 rd
International Virtual Conference on Recent Trends in Advanced Computing - Articial In-
telligence and Technologies, December 2020.
Professional Service Editorial Boards
Â Associate Editor { International Journal of E-adoption { 2008-Present
Â Member of the Editorial Board { Services Transactions on Internet of Things { 2016-Present
Â Member of the Editorial Board { CSI Transactions on ICT{ 2011-2015
Â Member of the Editorial Board { Software Engineering: An International Journal { 2011-
Present
Â Member of the Editorial Board { International Journal of Information Technology, Commu-
nications and Convergence { 2009-2017
Â Guest Co-Editor { ACM SIGAPP Computing Review { A Special Issue on Object and Com-
ponent Technologies for Cluster Computing { 2001/2002
Â Guest Editor { ACM SIGAPP Computing Review { A Special Issue on Distributed Computing
{ Spring 1999
Conference Organization
Â Track Co-chair { Programming Languages Track { ACM Symposium on Applied Computing
(SAC) { 2018-22
28 

Â
General Conference Chair { International Conference on Recent Trends in Advanced Com-
puting { 2020
Â Local Host { Security and Software Engineering Fal l Showcase (NSF Industry University
Cooperative Research Center) { 2015
Â Local Arrangements Chair { ACM SPLASH Conference{ 2013
Â Program Vice-Chair { The IEEE International Conference on High Performance Computing
and Communications (HPCC) { 2008 and 2010
Â Program Vice-Chair { The 14th IEEE International Conference on Paral lel and Distributed
Systems (ICPADS) { 2008
Â Track Chair { Networked Based Information Systems (NBiS) { 2009
Â Member of the Advisory Board { The 1st International Conference On Computer, Commu-
nication And Instrumentation (ICCCI) { 2009
Â Track Co-Organizer { Model-Based Software Engineering Track { Twentieth International
Conference on Software Engineering and Know ledge Engineering (SEKE) { 2008
Â Publicity Chair of Euro-PADS{ 1998
Â Publicity Chair { HiPC{ 2005-07
Â Track Co-chair { Object Oriented Programming and Systems { ACM Symposium on Applied
Computing (SAC) { 2004 and 2005
Â Mini-Track Co-Chair { Distributed Object and Component-based Software Systems { Annual
Hawaii International Conferences on System Sciences (HICSS) { 2003, 2004 and 2005
Â Track Co-chair { Programming Languages and Object Technology { ACM Symposium on
Applied Computing (SAC) { 2002 and 2003
Â Mini-track Chair { Distributed Objects and Components { ACM Symposium on Applied Com-
puting (SAC) { 2001
Â Workshop Co-chair { IEEE International Workshop on Object and Component Technologies
for Cluster Computing { IEEE International Symposium on Cluster Computing and the Grid
(CCGRID) { 2001
Â Session Chair { ADCOMP{ 1997;INFORMS { Spring 2000; HICSS{ 2003; ICICI{ 2009;
NBiS { 2009; INTERFACE { 2011 and 2013
Conference Program Committee Membership
Â International Conference on Software Engineering and Knowledge Engineering (SEKE) (2008-
Present)
Â IEEE Enterprise Distributed Ob ject Computing Conference (EDOC)(2003-Present)
Â IEEE International Conference on Services Computing (2014-2019)
Â IEEE International Conference on High Performance Computing and Communications (2013-
14)
29 

Â
ACM SAC, Cloud Computing Track (2016-17)
Â 10th EAI International Conference on Bio-inspired Information and Communications Tech-
nologies (2015-17)
Â HiPC Student Symposium (2009-10)
Â IEEE International Conference on Algorithms and Architecture for Parallel Processing (2000,
2002, 2005, 2007-09, 2014-15)
Â 3rd International Conference on Network Infrastructure Management Systems (2014)
Â International Conference on Soft Computing as Transdisciplinary Science and Technology
(2008)
Â International Conference on Digital Information Management (ICDIM 2007-08)
Â Generative Programming and Component Engineering for QoS Provisioning in Distributed
Systems (GPCE4QoS 2006)
Â IEEE International Conference on Digital Information Management (ICDIM 2006-07)
Â IEEE International Workshop on Trusted and Autonomic Computing Systems (TACS 2006)
Â High Performance Computing Symposium (HPC 2004)
Â 2nd International Conference on Web-based Learning (ICWL 2003)
Â IEEE Advances in Digital Libraries (ADL 2000)
Â First IEEE Computer Society International Workshop on Cluster Computing (IWCC 1999)
Â Sixth International Conference on Advanced Computing (ADCOMP 1998)
External Reviewer Â University of North Texas
Â University of Sharjah
Â East Carolina University
Â US NIST
Â National Science Foundation
Â Kansas State University
Â Naval Post-graduate School
Â University of Missouri at Kansas City
Â University of Alabama at Birmingham
Â University of West Florida
Â University of Colorado at Denver
30 

Â
University of Washington at Tacoma
Â Cleveland State University
Â University of UAE
Â The Chinese University of Hong Kong
Â The Hong Kong Polytechnic University
Â RMIT, Australia
Â Deakin University, Australia
Â IIT-Delhi, India
Â Tezapur University, India
Â NIT-Calicut, India
Â NIT-Surathkal, India
Paper Reviewer
Â ACM Transactions on Social Computing
Â IEEE Transactions on Services Computing
Â IEEE Transactions on Cloud Computing
Â IEEE Internet Computing
Â Applied Soft Computing
Â International Journal of E-Adoption
Â Engineering Reports
Â Frontiers of Information Technology & Electronic Engineering
Â IEEE Transactions on Software Engineering
Â IEEE Computer
Â IEEE Intelligent Systems
Â Journal of Parallel and Distributed Computing
Â Parallel and Distributed Computing Practice Journal
Â Cluster Computing
Â Concurrency and Computation: Practice and Experience
Â ACTA International Journal of Computers and Applications
Â IPSI Journal
Â Electronics and Telecommunications Research Institute Journal
31 

Â
Computer Languages, Systems & Structures Journal
Â Journal of Systems and Software
Â CSI Transactions on ICT
Â Data and Knowledge Engineering Journal
Â IEEE International Conference on Services Computing
Â IEEE International Conference on High Performance Computing and Communications
Â International Conference on Software and Knowledge Engineering
Â IEEE Enterprise Distributed Computing Conference
Â Hawaii International Conference On System Sciences
Â IEEE International Conference on Algorithms and Architecture for Parallel Processing
Â ACM Symposium on Applied Computing
Â IEEE Advances in Digital Libraries
Â European Conference on Parallel Computing
Â Parallel and Distributed Computing Systems Conference
Â John Wiley and Sons, Inc.
Â Prentice-Hall, Inc.
Â Scott-Jones Publishing Company
Â Morgan-Kaufman Publishing Company
Professional Memberships
ÂACM (Senior Member)
Â IEEE (Senior Member)
Service IUPUI
 Steering Committee for India-based Recruitment Initiative (2021-)
 Council of Associate Deans for Faculty Aairs (2020-)
 Top 100 Judge (2012-2016)
 Oce of Vice Chancellor for Research [Proposal Reviewer] (2011-12, 2017, 2019)
 India Interest Group (2010-15)
 Campus Promotion and Tenure Committee (2010-11)
 Centrality Study Group (1995-96)
32 


Science Olympiad [Volunteer] (1995)
School of Science  Associate Dean Search Committee [Co-Chair] (2021)
 Assistant to Dean Search Committee [Chair] (2020, 2022)
 Associate Dean Planning, Finance and Faculty Aairs (2019-)
 Unit Committee [Member and/or Chair] (2010-12; 2012-2019)
 Task Force for P&T Guidelines [Member and/or Coordinator] (2014-2019)
 Graduate Education Committee [Member] (2013-15)
 Ad-Hoc Committee for P & T Guidelines [Member] (2012-13)
 Dean Search Committee [Member] (2007-08, 2010-11)
 Graduate Aairs Committee [Member] (2003-08)
 Associate Dean Search Committee [Member] (2008)
 Graduate Training Strategic Working Group [Member] (2007)
 Faculty Grant Workshop [Panelist] (2006)
 Nomination and Awards Committee [Member and Chair] (2005-06)
 Academic Appeals Committee [Member and Chair] (1998-99)
 Windows on Science Program [Mentor] (1997-98)
 Learning and Assessment Committee [Member] (1997-98)
Department of Computer and Information Science
 Acting Chair (Fall'15)
 Interim Associate Chair and Associate Chair (2006-07; 2007-Summer'15; Spring 2016-18)
 Industrial Partnership Program [Co-Coordinator] (2017-18)
 Faculty Search Committee [Member and/or Chair] (2008-10, 2013-14, 2016-18)
 Chair Selection Committee [Member] (1997, 2007)
 Research & Infrastructure Committee [Member] (1997-00)
 Graduate Committee [Chair] and Graduate Program Director (2006-2015)
 Undergraduate Committee [Member and/or Chair] (1996-97, 1998-99, 2001-02, 2016-18)
 Seminar and Colloquia Committee [Chair] (2002-03)
 Primary Committee [Member and/or Chair] (2006-08, 2010-2018)
33 

Courses Taught
ÂGraduate Courses: Distributed-ob ject Computing, Ob ject-oriented Design and Programming,
Programming Languages, Operating Systems, Introduction to Distributed Computing, Ad-
vanced Distributed Computing
Â Undergraduate Courses: Operating Systems, Exploration in Computing, High Performance
Computing, Programming Languages, Systems Programming, Advanced Programming, Com-
puting I, Computing II, Problem Solving Using C++
Student Supervision ÂPost-doctoral: Ruchika Malhotra
Â Ph.D.: Nahida Chowdhury, Lahiru Gallege, Dimuthu Gamage, Ryan Rybarczyk, Fei Cao
(joint-UAB), Wei Zhao (joint-UAB), Alex Liu (joint-UAB)
Â M.S.: Sivakumar Chinnasamy, Mingyong Qiao, Nanditha Nayani, Girish Brahnmath, Zhisheng
Huang, Changlin Sun, Natasha Gupta, Praveen Gopalkrishna, Pradeep Mysore, Anjali Ku-
mari, Barun Devara ju, Alex Crespi, Srikanth Reddy, Zhun Li, Liying Tang, William Higdon,
Robert Berbeco, Kalpana Tummala, Jayasree Gandhamaneni, Padma Kambhapati (joint),
Ravi Bulusu, Richard Neidermyer, Pratibha Katuri, Omkar Tilak, Amruta Jejurikar, Girish
Joshi, Sucheta Phatak, Neha Talavdekar, Ketaki Pradhan, Ryan Rybarczyk, Aboli Phadke,
Allenoush Hayrapetian, Pradeep Gowda, Ra jesh Kumar, Amrita Mangaonkar, Abhinan-
dan Jayanth (joint), Zachary Reynolds (joint), Ashish Chaturvedi (joint), Apeksha Jangam
(joint), Vishwesh Janardhana (joint), Saurabh Pandey, Rohit Pawar, Ayush Maharjan, Umesh
Ra ja, Aanchal Rohira, Ankkit Sinha
Â Ph.D./M.S. Thesis/Pro ject Committee Membership: Nyalia Lui, Samira Khorshidi, Bran-
don Upp, Anushka Patil, Enas Alikhasashneh, Ping Zhang, Ra ja Dheekonda, Arvind Nair,
Salman Haider, Feng Li, Harold Owens, Weijian Zhang, Yuankun Fu, Savitha Baskaran,
Geetha Satyanarayana, Hareesh Sandupatla, Mike Du, Sricharan Lochan, Lakshmi Velicheti,
Shalini Ravishankar, Je Kriske, Mohammed Rangwala, Newlyn Erratt, Serena Gong, Tanu-
moy Pati, Venkatesh Bharadwa j, Mohit Sachan, Darshan Puranik, Omkar Tilak, Manjula
Peiris, Harold Owens, Ashhar Madni, Xiang Ran, Yogesh Karandikar, Nikesh Patel, Kumar
Abhishek, Joby Varghese, Shengquan Peng, Mathew Stephens, Michael Boyles, Srinivasan
Sikkupparbathyam, Peng Zhang, Patrick Kennedy, Anthony Wagner, Wilfred Mascarenhas,
Ming Zhong, Chih-feng Lin
Â BS: Michael Boyles, Nila Patel, Joseph Williams, Muris Ridzal, James Freeman, Joseph
Hansome, Ravi Patel, Zachary Reynolds (joint), Sujana Oruganti (joint)
34 

"
Non,"Auto-Generating Language-Specific Wrappers for Rust LibrariesSteve McCownsmccown@anonyome.com4 Nov 2021 

Common Libraries with Language WrappersCommon Library (e.g., Rust)SwiftPythonJavaC/C++Swift AppPythonAppJavaAppC/C++AppWrapper Layerâ¢Relay function callsâ¢Convert parametersâ¢Memory managementâ¢âUnsafeMutablePointerâ 

WrappersRequireUnsafeC-Style CodingRequires all of thisJust to do this 

UniFFI by Mozillaâ¢Automatically generates foreign-language bindings for Rust librariesâ¢Consolidates business logic into a portable libraryâ¢Builds wrappers forâ¢Kotlinâ¢Swiftâ¢Pythonâ¢C++ â¢https://github.com/mozilla/UniFFI-rs 

Basic Wrapper Tutorials  Tu t o r i a l sâ¢Quick FFI Introâ¢Wrapper Intro (simple UniFFI)â¢Wrapper Data Types (the main types supported by UniFFI)Source:https://github.com/sudoplatform-labs/ffi-tutorials 

UniFFI:  How it works1.Writecustom Rust library â¢Make API functions publicâ¢Build crate as linkable library2.Writea UDLrepresentation of API functionsâ¢Similar toInterface Definition Language (IDL)3.Generatea âScaffoldingâ layerâ¢FFI code that creates c-style calls,memoryconversions, etc.4.Generatelanguage-specific implementation layerâ¢Native code layer to cover up complicated FFI calls (e.g., Swift FFI feels like native Swift)5.Importgenerated code & library into native application 

Create Rust LibraryCargo.tomllib.rsSnake Case(NOTE:  the didcomm_rs library currently uses UniFFI version 0.14.0)UniFFI additions 

Create UDL for API Functionslibrary.UniFFI.udlFunctions, structures, errors, enums, etc. are defined in an independent format 

Generate Scaffolding Layer(for more details, see:   library.UniFFI.UniFFI.rs)Run thisTo   g e n e r a t e   t h i s 

Generate a Swift InterfaceRun thisGenerate library.swiftCamel Case 

MacOS Swift AppAddAddAddCall the library function  

Run thisGenerate Python InterfaceGenerate library.py 

Python Test AppAddCall the library function 

UniFFI Applied to DIDComm_rs 

UDL for didcomm_rsNote:  [Self=ByArc] is used since didcomm_rs required an Arc<T> for the self parameter (object ison the heap).UniFFI specifies how Rust objects (thatmaycontainmultiple public members & methods) are presented to calling applications.  Note: from a UniFFI perspective a referenced object can only be presented as either:1.Object (interface): â¢Contains methodsâ¢Passed byreferenceor2.Dictionary:  â¢Contains data elementsâ¢Passed by valueInUniFFI, anobjectcannotbepresentedasbothanobjectanda dictionary. 

didcomm_rs:message.rsAdded:  Using getters & setters allows access to data elements by objects without needing direct access.  This allows the didcommMessage object to be specified by UniFFI as an object interface while still allowing calling applications to access and modify the object member variables. 

Scaffolding layer is auto-generated as:didcomm-rs.UniFFI.UniFFI.rs 

Language wrapper is auto-generated as:didcomm-rs.swift 

Once the UniFFI generation is performed, the didcomm_rs test routines are easily called from Swift.Source:  https://github.com/anonyome/didcomm-rs 

Limitations (temporary?) 

libindy:  blob_storage.rsCore methodWith FutureAdded:  Method Without FutureReturn Result objectNote:  UniFFI does not (currently?) support the Rust Future designator, so a companion function, u_open_reader(), was created without the Future designator and this method was specified in the .udlfile.  

libindy:  anoncreds.rsReturn a dictionaryReturns a tupleNote:  UniFFI does not currently support the return of custom tuples.  To compensate, a custom dictionary type (containing the tuple members) was created and added to pub fnissuer_create_schema().  This allows the data to be returned and accessed by the calling application. 

Questions? 

"
Non,"WasmFX: Structured Stack Switching for WebAssembly
Daniel HillerstrÃ¶m
Computing Systems Laboratory Zurich Research Center
Huawei Technologies, Switzerland
September 22, 2023 

WebAssembly: a low-level virtual machine (Haas et al. 2017)
What is Wasm? A portable bytecode format
An abstraction of the commonly found hardware
A predictable performance model
Code format A Wasm âprogramâ is a structured module
Designed for stream compilation
The term language is
statically typedand block-structuredControl flow is structured (
i.e.all CFGs are reducible)
Exciting future prospects Running non-JavaScript code in the browser
Secure-by-compilation cloud-native applications
Efficient cross-platform portable applications           

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control flow abstractions to Wasm?Solution
Add each abstraction as a primitive to Wasm
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)        

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control flow abstractions to Wasm?Solution
Add each abstraction as a primitive to Wasm
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)        

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control flow abstractions to Wasm?Solution
Add each abstraction as a primitive to Wasm
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)        

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control flow abstractions to Wasm?Solution
Add each abstraction as a primitive to Wasm
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)        

The need for stack switching in Wasm
Non-local control is pervasive in programming languages Async/await (e.g. C
++
, C #
, Dart, JavaScript, Rust, Swift) Coroutines (e.g. C
++
, Kotlin, Python, Swift) Lightweight threads (e.g. Erlang, Go, Haskell, Java, Swift)
Generators and iterators (e.g. C
#
, Dart, Haskell, JavaScript, Kotlin, Python) First-class continuations (e.g. Haskell, Java, OCaml, Scheme)
The problem
How do I compile non-local control flow abstractions to Wasm?Solution
Add each abstraction as a primitive to Wasm
Ceremoniously transform my entire source programs (e.g. Asyncify, CPS)        

Asyncify is the current state-of-the-art (1)
(
func $doSomething ( param$argi32) (result i32 )(
call $foo (
call $bar ( local.get $arg)))) 

Asyncify is the current state-of-the-art (1)
(
func $doSomething ( param$argi32) (result i32 )(
local $call_
idx i32) (
local $reti32) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) ;; test rewind state(
then (local .set $arg ;; store local $arg(
i32 .load offset=4 ( global.get $asyncify _
heap _
ptr))) (
local .set $call _
idx ;; continuation point(
i32 .load offset=8 ( global.get $asyncify _
heap _
ptr))) (
else )) (
block $call_
foo ( result i32 )(
block $restore _
foo ( result i32 )(
block $call_
bar ( result i32 ) (
local .get $arg) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) (result i32 ) (
then (if (i32 .eq (local .get $call _
idx) ( i32.const 0)) (
then (br $call _
bar)) ;; restore $call_
bar (
else (br $restore _
foo)))) (
else (br $call _
bar)))) ;; regular $call_
bar (
local .set $ret ( call$bar ( local.get 0))) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 1)) (result i32 );; test unwind state (
then (i32 .store offset=4 ( global.get $asyncify _
heap _
ptr) ( local.get $arg)) (
i32 .store offset=8 ( global.get $asyncify _
heap _
ptr ( i32.const 0)) (
return (i32 .const 0))) ...)))))) 

Asyncify is the current state-of-the-art (1)
(
func $doSomething ( param$argi32) (result i32 )(
local $call_
idx i32) (
local $reti32) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) ;; test rewind state(
then (local .set $arg ;; store local $arg(
i32 .load offset=4 ( global.get $asyncify _
heap _
ptr))) (
local .set $call _
idx ;; continuation point(
i32 .load offset=8 ( global.get $asyncify _
heap _
ptr))) (
else )) (
block $call_
foo ( result i32 )(
block $restore _
foo ( result i32 )(
block $call_
bar ( result i32 ) (
local .get $arg) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) (result i32 ) (
then (if (i32 .eq (local .get $call _
idx) ( i32.const 0)) (
then (br $call _
bar)) ;; restore $call_
bar (
else (br $restore _
foo)))) (
else (br $call _
bar)))) ;; regular $call_
bar (
local .set $ret ( call$bar ( local.get 0))) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 1)) (result i32 );; test unwind state (
then (i32 .store offset=4 ( global.get $asyncify _
heap _
ptr) ( local.get $arg)) (
i32 .store offset=8 ( global.get $asyncify _
heap _
ptr ( i32.const 0)) (
return (i32 .const 0))) ...)))))) 

Asyncify is the current state-of-the-art (1)
(
func $doSomething ( param$argi32) (result i32 )(
local $call_
idx i32) (
local $reti32) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) ;; test rewind state(
then (local .set $arg ;; store local $arg(
i32 .load offset=4 ( global.get $asyncify _
heap _
ptr))) (
local .set $call _
idx ;; continuation point(
i32 .load offset=8 ( global.get $asyncify _
heap _
ptr))) (
else )) (
block $call_
foo ( result i32 )(
block $restore _
foo ( result i32 )(
block $call_
bar ( result i32 ) (
local .get $arg) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) (result i32 ) (
then (if (i32 .eq (local .get $call _
idx) ( i32.const 0)) (
then (br $call _
bar)) ;; restore $call_
bar (
else (br $restore _
foo)))) (
else (br $call _
bar)))) ;; regular $call_
bar (
local .set $ret ( call$bar ( local.get 0))) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 1)) (result i32 );; test unwind state (
then (i32 .store offset=4 ( global.get $asyncify _
heap _
ptr) ( local.get $arg)) (
i32 .store offset=8 ( global.get $asyncify _
heap _
ptr ( i32.const 0)) (
return (i32 .const 0))) ...)))))) 

Asyncify is the current state-of-the-art (1)
(
func $doSomething ( param$argi32) (result i32 )(
local $call_
idx i32) (
local $reti32) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) ;; test rewind state(
then (local .set $arg ;; store local $arg(
i32 .load offset=4 ( global.get $asyncify _
heap _
ptr))) (
local .set $call _
idx ;; continuation point(
i32 .load offset=8 ( global.get $asyncify _
heap _
ptr))) (
else )) (
block $call_
foo ( result i32 )(
block $restore _
foo ( result i32 )(
block $call_
bar ( result i32 ) (
local .get $arg) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 2)) (result i32 ) (
then (if (i32 .eq (local .get $call _
idx) ( i32.const 0)) (
then (br $call _
bar)) ;; restore $call_
bar (
else (br $restore _
foo)))) (
else (br $call _
bar)))) ;; regular $call_
bar (
local .set $ret ( call$bar ( local.get 0))) (
if (i32 .eq (global .get $asyncify _
mode) ( i32.const 1)) (result i32 );; test unwind state (
then (i32 .store offset=4 ( global.get $asyncify _
heap _
ptr) ( local.get $arg)) (
i32 .store offset=8 ( global.get $asyncify _
heap _
ptr ( i32.const 0)) (
return (i32 .const 0))) ...)))))) 

Characterising Asyncify
Pros
Expressive
Source-to-source transformation
Optimisable under a closed-world assumption Cons
Code size blowup
Slowdown pure code
Whole-program approach
But, what is Asyncify? The key primitives are
Unwind stack, delimit unwind, and rewind stackor expressed with a slightly different terminology:
Suspend continuation, delimit suspend, and resume continuation
Asyncify provides a particular implementation of
delimited continuations!(the state machine approach dates back at least as far as Adya et al. (2002))       

Characterising Asyncify
Pros
Expressive
Source-to-source transformation
Optimisable under a closed-world assumption Cons
Code size blowup
Slowdown pure code
Whole-program approach
But, what is Asyncify? The key primitives are
Unwind stack, delimit unwind, and rewind stackor expressed with a slightly different terminology:
Suspend continuation, delimit suspend, and resume continuation
Asyncify provides a particular implementation of
delimited continuations!(the state machine approach dates back at least as far as Adya et al. (2002))       

Characterising Asyncify
Pros
Expressive
Source-to-source transformation
Optimisable under a closed-world assumption Cons
Code size blowup
Slowdown pure code
Whole-program approach
But, what is Asyncify? The key primitives are
Unwind stack, delimit unwind, and rewind stackor expressed with a slightly different terminology:
Suspend continuation, delimit suspend, and resume continuation
Asyncify provides a particular implementation of
delimited continuations!(the state machine approach dates back at least as far as Adya et al. (2002))       

Characterising Asyncify
Pros
Expressive
Source-to-source transformation
Optimisable under a closed-world assumption Cons
Code size blowup
Slowdown pure code
Whole-program approach
But, what is Asyncify? The key primitives are
Unwind stack, delimit unwind, and rewind stackor expressed with a slightly different terminology:
Suspend continuation, delimit suspend, and resume continuation
Asyncify provides a particular implementation of
delimited continuations!(the state machine approach dates back at least as far as Adya et al. (2002))       

Characterising Asyncify
Pros
Expressive
Source-to-source transformation
Optimisable under a closed-world assumption Cons
Code size blowup
Slowdown pure code
Whole-program approach
But, what is Asyncify? The key primitives are
Unwind stack, delimit unwind, and rewind stackor expressed with a slightly different terminology:
Suspend continuation, delimit suspend, and resume continuation
Asyncify provides a particular implementation of
delimited continuations!(the state machine approach dates back at least as far as Adya et al. (2002))       

The solution: a delimited continuations instruction set
Main idea Letâs turn the gist of Asyncify into a proper instruction set!
. . . but where to start?
Many flavours of delimited continuations Felleisen (1988)âs control/prompt
Danvy and Filinski (1990)âs shift/reset
Hieb and Dybvig (1990)âs spawn
Queinnec and Serpette (1991)âs splitter
Sitaram (1993)âs run/fcontrol
Gunter, RÃ©my, and Riecke (1995)âs cupto
Longley (2009)âs catchcont
Plotkin and Pretnar (2009)âs effect handlers
(see Appendix A of my PhD thesis (HillerstrÃ¶m 2021) for a comprehensive overview of continuations)           

The solution: a delimited continuations instruction set
Main idea Letâs turn the gist of Asyncify into a proper instruction set!
. . . but where to start?
Many flavours of delimited continuations Felleisen (1988)âs control/prompt
Danvy and Filinski (1990)âs shift/reset
Hieb and Dybvig (1990)âs spawn
Queinnec and Serpette (1991)âs splitter
Sitaram (1993)âs run/fcontrol
Gunter, RÃ©my, and Riecke (1995)âs cupto
Longley (2009)âs catchcont
Plotkin and Pretnar (2009)âs effect handlers
(see Appendix A of my PhD thesis (HillerstrÃ¶m 2021) for a comprehensive overview of continuations)           

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

Why effect handlers
Design constraints, must work. . . . . . without garbage collection
. . . without closures
. . . without the use of recursion
. . . with simply typed stacks
. . . with imperative control structure
. . . with predictable cost model
. . . with legacy code reference counting suffices
first-order abstraction
no fundamental dependency
straightforward typing
compatible with native effects
transparent cost of instructions
seamless interop        

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

The WasmFX instruction set extension
Types
cont
$ft o23
Tags
tag
$tag (param Ïâ
) ( result Ïâ
) o23
Core instructions
cont.new o23
resume o23
suspend o23
Other instructions
cont.bind o23
resume
_
throw o2
barrier o2 Legend
o Specâed
2 Reference impl.
3 Wasmtime impl.We call this extension
WasmFX(the proposal was originally named âtyped continuationsâ) .
https://wasmfx.dev         

Instruction extension (1)
Continuation allocation cont.new$ct : [( ref null $ft )] â [(ref $ct )]
where $ft : [Ïâ
] â [Ï â
]
and $ct :cont $ft 

Operational interpretation of
cont.newcont.new
$t
1 (ref.func
$f ) cont.new
$t
2 (ref.func
$g ) call
$f call
$g SP
SP
SP 

Operational interpretation of
cont.newcont.new
$t
1 (ref.func
$f ) cont.new
$t
2 (ref.func
$g ) call
$f call
$g SP
SP
SP 

Operational interpretation of
cont.newcont.new
$t
1 (ref.func
$f ) cont.new
$t
2 (ref.func
$g ) call
$f call
$g SP
SP
SP 

Instruction extension (2)
Continuation resumption resume$ct (tag $tag $h )â
: [ Ïâ
(ref null $ct )] â [Ï â
]
where {$ tag
i: [
Ïâ
i ]
â [Ï â
i ]
and $h
i : [
Ïâ
i (
ref null $ct
i)]
and
$ ct
i :
cont $ft
i and
$ft
i: [
Ïâ
i ]
â [Ï â
]}
i
and $ct :cont $ft
and $ft : [Ïâ
] â [Ï â
]
The instruction fully consumes the continuation argument. 

Branching from
resumeuses the existing block instructions(
func $run ( param $task1 ( ref$task)) ( param$task2 ( ref$task)) (
local $up ( ref null $ct)) (local$down ( ref null $ct));; locals to manage continuations(
local $isOtherDone i32) ;; initialise locals(
local .set $up ( cont.new (type $ct) ( local.get $task1))) (
local .set $down ( cont.new (type $ct) ( local.get $task2))) (
loop $h ;; run $up(
block $on_
yield ( result (ref $ct)) (
resume (tag $yield $on _
yield) (
local .get $up)) (
if (i32 .eq (local .get $isOtherDone) ( i32.const 1)) ;; $up finished, check whether $down is done (
then (return ))) ;; prepare to run $down(
local .get $down) (
local .set $up) (
local .set $isOtherDone ( i32.const 1)) (
br $h) )
;; on _
yield clause, stack type: [(cont $ct)] (
local .set $up) (
if (i32 .eqz (local .get $isOtherDone)) ;; swap $up and $down(
then (local .get $down) (
local .set $down ( local.get $up)) (
local .set $up))) (
br $h))) 

Operational interpretation of
resumeresume
$t
1 (tag ...)
$x
i, . . . ,
$x
k resume
$t
2 (tag ...)
$x
j, . . . ,
$x
l call
$f call
$g SP
SP
SP
SPSP 

Operational interpretation of
resumeresume
$t
1 (tag ...)
$x
i, . . . ,
$x
k resume
$t
2 (tag ...)
$x
j, . . . ,
$x
l call
$f $x
i, . . . ,
$x
k call
$g SP
SP
SP
SPSP 

Operational interpretation of
resumeresume
$t
1 (tag ...)
$x
i, . . . ,
$x
k resume
$t
2 (tag ...)
$x
j, . . . ,
$x
l V
1 call
$g SP
SP
SP
SPSP 

Operational interpretation of
resumeV
1 resume
$t
2 (tag ...)
$x
j, . . . ,
$x
l V
1 call
$g SP
SP
SP
SPSP 

Operational interpretation of
resumeV
1 resume
$t
2 (tag ...)
$x
j, . . . ,
$x
l V
1 call
$g $x
j, . . . ,
$x
l SP
SP
SP
SPSP 

Operational interpretation of
resumeV
1 resume
$t
2 (tag ...)
$x
j, . . . ,
$x
l V
1 V
2 SP
SP
SP
SPSP 

Operational interpretation of
resumeV
1 V
2 V
1 V
2 SP
SP
SP
SPSP 

Instruction extension (4)
Continuation suspension suspend$tag : [Ïâ
] â [Ï â
]
where $tag : [Ïâ
] â [Ï â
] 

Operational interpretation of
suspend$
c
1 =
cont $
c
2 =
cont resume
$t
1 (tag ...)
$c
1 resume
$t
2 (tag ...) resume
$t
3 (tag ...) suspend
$u
1 V
1 Â· Â· Â·
Â· Â· Â·
suspend
$u
2 V
2 Â· Â· Â·SP SP
SP
SP
SP SP 

Operational interpretation of
suspend$
c
1 =
null $
c
2 =
cont resume
$t
1 (tag ...)
$c
1 resume
$t
2 (tag ...) resume
$t
3 (tag ...) suspend
$u
1 V
1 Â· Â· Â·
Â· Â· Â·
suspend
$u
2 V
2 Â· Â· Â·SP SP
SP
SP
SP SP 

Operational interpretation of
suspend$
c
1 =
null $
c
2 =
cont V
1 $
c
3 =
cont resume
$t
2 (tag ...)
V
1 $
c
2 resume
$t
3 (tag ...) Â· Â· Â·
Â· Â· Â·
suspend
$u
2 V
2 Â· Â· Â·SP SP
SP
SP
SP SP 

Operational interpretation of
suspend$
c
1 =
null $
c
2 =
null V
1 $
c
3 =
cont resume
$t
2 (tag ...)
V
1 $
c
2 resume
$t
3 (tag ...) Â· Â· Â·
Â· Â· Â·
V
1 suspend
$u
2 V
2 Â· Â· Â·SP SP
SP
SP
SP SP 

Operational interpretation of
suspend$
c
1 =
null $
c
2 =
null V
1 $
c
3 =
cont V
2 $
c
4 =
cont resume
$t
3 (tag ...)
V
2 $
c
3 Â· Â· Â·
Â· Â· Â·
V
1 Â· Â· Â·SP SP
SP
SP
SP SP 

Operational interpretation of
suspend$
c
1 =
null $
c
2 =
null V
1 $
c
3 =
null V
2 $
c
4 =
cont resume
$t
3 (tag ...)
V
2 $
c
3 V
2 Â· Â· Â·
Â· Â· Â·
V
1 Â· Â· Â·SP SP
SP
SP
SP SP 

Research prototype implementation in Wasmtime
Prototype implementation in Wasmtime
NaÃ¯ve baseline implementation
Enables running ârealâ programs
Stack switching on top of Wasmtime Fiber
Key naÃ¯ve implementation decisions Stack switching is non-Wasm native
Use
u128 as the universal type Reallocate argument buffers on each context switch WasmRust/Wasmtime runtime
l i b c a l l       

Experiments setup
Setup overview Fiber-based micro-benchmarks; three implementations: Asyncify, WasmFX, and bespoke
Fiber interface in C; instantiated with either Asyncify or WasmFX
No memory leaks allowed
Tools WASI SDK version 20.0
Binaryen version 114
Apples & oranges Bespoke and Asyncify implementations are optimised
clang -O3
wasm-opt -O2 --asyncify --pass-arg=asyncify-ignore-imports
WasmFX implementation is unoptimised and assembled by hand
Different storage
Asyncify-backed fibers in linear memory
WasmFX-backed fibers in tables
Tools do not understand function references              

Experiments setup: Fiber interface in C
/
** The signature of a fiber entry point.
**/ typedef void
*(
*fiber _
entry _
point _
t)( void
*); /
** The abstract type of a fiber object.
**/ typedef struct
fiber
*fiber _
t; /
** Allocates a new fiber with the default stack size.
**/ fiber
_
t fiber _
alloc(fiber _
entry _
point _
t entry); /
** Reclaims the memory occupied by a fiber object.
**/ void
fiber _
free(fiber _
t fiber); /
** Yields control to its parent context. This function must be called from within a fiber context.
**/ void
*fiber _
yield( void
*arg); /
** Possible status codes for âfiber _
resumeâ. **/ typedef enum
{ FIBER_
OK, FIBER _
YIELD, FIBER _
ERROR } fiber _
result _
t; /
** Resumes a given âfiberâ with argument âargâ.
**/ void
*fiber _
resume(fiber _
t fiber, void
*arg, fiber _
result _
t *result); 

Microbenchmark: C10m
Description HTTP server workload simulation.
10 million coroutines in total.
Sliding window: 10000 coroutines run concurrently, each yielding once.
Shallow call stack depth
Run-time ratio Memory footprint ratio Binary size ratio
Asyncify 1.00 1.00 (54mb) 1.00 (9.1kb)
WasmFX 0.23 0.98 (55mb) 10.78 (844b)     

Microbenchmark: C10m
Description HTTP server workload simulation.
10 million coroutines in total.
Sliding window: 10000 coroutines run concurrently, each yielding once.
Shallow call stack depth
Run-time ratio Memory footprint ratio Binary size ratio
Bespoke 1.00 1.00 (13mb) 1.00 (940b)
Asyncify 0.21 0.24 (54mb) 0.10 (9.1kb)
WasmFX 0.05 0.24 (55mb) 1.11 (844b)     

Microbenchmark: Skynet
Description Nested tree-structured concurrency simulation.
10 million coroutines in total, 6 active, each yielding once.
No auxiliary data structure; fibers are stored in the control flow state.
Deep call stack.
Run-time ratio Memory footprint ratio Binary size ratio
Asyncify 1.00 1.00 (14mb) 1.00 (30kb)
WasmFX 2.12 1.00 (14mb) 55.87 (537b)     

Microbenchmark: Skynet
Description Nested tree-structured concurrency simulation.
10 million coroutines in total, 6 active, each yielding once.
No auxiliary data structure; fibers are stored in the control flow state.
Deep call stack.
Run-time ratio Memory footprint ratio Binary size ratio
Bespoke 1.00 1.00 (13mb) 1.00 (306b)
Asyncify 0.01 0.01 (14mb) 0.01 (30kb)
WasmFX 0.14 0.24 (14mb) 0.57 (537b)     

Future experiments (1)
Benchmarks Get into pole position
Realistic workloads and use-cases
Backends Internalise Wasmtime Fiber in codegen
Cranelift native stack switching
Memory Deferred stack allocation
Stack pools
Extensions Named resume blocks
First-class & generative control tags         

Future experiments (2)
Toolchain support Compiling control abstractions
1Retrofitting existing toolchains
2Develop new (researchy) toolchains
1
The Kotlin team has shown interest in compiling to the WasmFX instruction set 2
Currently working on added the WasmFX instruction set to binaryen    

WasmFX resource list
Resources Formal specification
( https://github.com/wasmfx/specfx/blob/main/proposals/continuations/Overview.md )Informal explainer document
( https://github.com/wasmfx/specfx/blob/main/proposals/continuations/Explainer.md )Reference implementation (
https://github.com/wasmfx/specfx )Research prototype implementation in Wasmtime (
https://github.com/wasmfx/wasmfxtime)Toolchain support (
https://github.com/wasmfx/binaryenfx )OOPSLAâ23 research paper (
https://doi.org/10.48550/arXiv.2308.08347 )
https://wasmfx.dev       

ReferencesI
Felleisen, Matthias (1988).âThe Theory and Practice of First-Class Promptsâ.In:
POPL. ACM Press,
pp. 180â190. Danvy, Olivier and Andrzej Filinski (1990).âAbstracting Controlâ.In:
LISP and Functional
Programming , pp. 151â160.Hieb, Robert and R. Kent Dybvig (1990).âContinuations and Concurrencyâ.In:
PPoPP. ACM,
pp. 128â136. Queinnec, Christian and Bernard P. Serpette (1991).âA Dynamic Extent Control Operator for Partial
Continuationsâ.In: POPL. ACM Press, pp. 174â184.Sitaram, Dorai (1993).âHandling Controlâ.In:
PLDI. ACM, pp. 147â155.Gunter, Carl A., Didier RÃ©my, and Jon G. Riecke (1995).âA Generalization of Exceptions and Control
in ML-like Languagesâ.In: FPCA. ACM, pp. 12â23.Ganz, Steven E., Daniel P. Friedman, and Mitchell Wand (1999).âTrampolined Styleâ.In:
ICFP. ACM,
pp. 18â27. Adya, Atul et al. (2002).âCooperative Task Management Without Manual Stack Managementâ.In:
USENIX Annual Technical Conference, General Track . USENIX, pp. 289â302.                                 

ReferencesII
Longley, John (2009).âSome Programming Languages Suggested by Game Models (Extended
Abstract)â.In: MFPS. Vol. 249. Electronic Notes in Theoretical Computer Science. Elsevier,
pp. 117â134. Plotkin, Gordon D. and Matija Pretnar (2009).âHandlers of Algebraic Effectsâ.In:
ESOP. Vol. 5502.
LNCS. Springer, pp. 80â94. Haas, Andreas et al. (2017).âBringing the web up to speed with WebAssemblyâ.In:
PLDI. ACM,
pp. 185â200. HillerstrÃ¶m, Daniel (2021).âFoundations for Programming and Implementing Effect Handlersâ.
PhD thesis. The University of Edinburgh, Scotland, UK.Phipps-Costin, Luna et al. (2023).âContinuing WebAssembly with Effect Handlersâ.In:
Proc. ACM
Program. Lang. 7.OOPSLA2. To appear.                     

Instruction extension (3)
Partial continuation application cont.bind$sct $dct : [Ïâ
0 (
ref null $sct )]â [(ref $dct )]
where $sct :cont $ft and $ft : [Ïâ
0 Ïâ
1 ]
â [Ï â
]
and $dst :cont $ft â²
and $ft â²
: [ Ïâ
1 ]
â [Ï â
] 

Instruction extension (5)
Continuation cancellation resume_
throw $ct (tag $exn ) (tag $tag $h )â
: [ Ïâ
0 (
ref null $ct )] â [Ï â
]
where $exn : [Ïâ
0 ]
â [], {$ tag
i: [
Ïâ
i ]
â [Ï â
i ]
and $h
i : [
Ïâ
i (
ref null $ct
i)]
and
$ ct
i :
cont $ft
i and
$ft
i: [
Ïâ
i ]
â [Ï â
]}
i
and $ct :cont ([Ïâ
] â [Ï â
] 

Instruction extension (6)
Control barriers barrier$lbl $bt instr â
: [ Ïâ
] â [Ï â
]
where $bt = [ Ïâ
] â [Ï â
] and instr â
: [ Ïâ
] â [Ï â
] 

"
Non,"EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018	
Extending ROOT through Modules
OksanaShadura	1,â	, Brian Paul Bockelman	1,ââ	, Vassil Vassilev	2,âââ	
1University of Nebraska Lincoln, 1400 R St, Lincoln, NE 68588, United States2Princeton University, Princeton, New Jersey 08544, United States	
Abstract.	The ROOT software framework is foundational for the HEP ecosys-
tem, providing multiple capabilities such as I/O, a C++ interpreter, GUI, and
math libraries. It uses object-oriented concepts and build-time components to
layer between them. We believe that a new layering formalism will beneï¬t the
ROOT user community.
We present the modularization strategy for ROOT which aims to build upon
the existing source components, making available the dependencies and other
metadata outside of the build system, and allow post-install additions on top of
existing installation as well as in the ROOT runtime environment. Components
can be grouped into packages and made available from repositories in order
to provide a post-install step of missing packages. This feature implements a
mechanism for the more comprehensive software ecosystem and makes it avail-
able even from a minimal ROOT installation. As part of this work, we have
reduced inter-component dependencies in order to improve maintainability.
The modularization eï¬ort draws inspiration from similar eï¬orts in the Java,
Python, and Swift ecosystems. Keeping aligned with modern C++, this strategy
relies on forthcoming features such as C++ modules. We hope formalizing the
component layer provides simpler ROOT installs, improves extensibility, and
decreases the complexity of embedding ROOT in other ecosystems.	
1 Introduction
One of the advantages of object-oriented systems is the ability to have abstraction layers
that provide separation between the user interfaces and implementations. Ultimately, it is an
important tool to help to scale projects to larger code bases. Another technique for scaling
projects is to use a modularization. Modularisation provides a grouping of functionality into
distinct units with clear points of interaction. While ROOT [1] has a strong history in object-
oriented programming, it doesnât have as a strong a concept of modular components. Thus,
we propose to add new concepts to the ROOT ecosystem:
1.Component : A set of interdependent classes implementing coherent functionality and
providing well-deï¬ned APIs. A librarycould be deï¬ned as a component or set of
related components that expose functionality and which can be invoked by a program
or another library.	
âe-mail: oksana.shadura@cern.chââe-mail: bbockelm@cse.unl.edu	âââ	e-mail: vvasilev@cern.ch
© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creati\ve Commons Attribution License 4.0 (http://creativecommons.org/licenses/by/4.0/).  

2	
EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018  

3	
EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018	
2.Package : A distinct, self-describing resource (ï¬le, URL) that provide one or more
components (such as a library and associated non-code resources).
3. Package database: A record of all packages currently available in a ROOT installation.
4. Package manager : An actor that can locate and install packages into a ROOT instal-
lation from a package reference (such as a package URL), along with their transitive
dependencies.
Similar to ROOT, other large object-oriented software systems consist of a number of
interdependent and loosely-coupled classes and are mostly organized as a set of libraries and
build targets. In these cases, classes are often used as the lowest level of granularity and can
serve as a unit of software modularization. In ecosystems such as Java, Python, and C++, a further package structure can allow soft-
ware developers to organize their programs into components. A good organization of classes
into identiï¬able and collaborating packages simpliï¬es the understanding and the maintenance
of the software. Yet another technique to improve the quality of the software modularization
is to provide technical mechanisms for encouraging coders to utilize these package structures.	
2 Motivation
Software modularization deï¬nes a way of grouping functionalities. It outlines groups in the
form of components, which identify a particular piece of functionality that solves a speciï¬c
problem. Often, it provides a way for coders to carefully specify the visibility of interfaces
and diï¬erentiate this between inside the package and outside. In general, modularization
helps reducing management, coordination and development costs. We aim to deï¬ne a set of
mechanisms that enable a modular version of ROOT, centered around C++ modules [2] and
the concept of software packages. ROOT libraries have a very complex set of interdependencies. By introducing a com-
ponent layer on top of these libraries, we provide better boundaries between components,
allowing ROOT to scale as a project. We hope this can also make it easier for external com-
munities to develop packages on top of ROOT and easily distribute them â without needing
to get their work into ROOTâs oï¬cial codebase. A ï¬rst step is to deï¬ne a minimal ROOTpackage, removing as many dependencies as pos-
sible, until only the simplest user functionality remains; we decided the minimal functionality
should eï¬ectively be the I/O libraries and C++ interpreter. By having a smaller size and set
of external dependency, we increase the chances that its functionality can be embedded in
other contexts and enables ROOT users to interact with the broader data science ecosystem. Packages and package management provides a mechanism for ROOT users to socialize
and reuse projects built on top of ROOT. We aim to help make ROOT more ï¬exible and open
it to the new customers. This feature would allow ROOT to serve as a community nexus.
In particular, it provides the ROOT team with an improved mechanism to say ânoâ to new
components within the ROOT source itself as users can directly share their packages among
each other or in a conventional store such as GitHub.
3 ROOT components and packages
The ROOT code [3] is organized into a set of sub-directories per component, with the excep-
tion of the ROOT Core library, which has a more complex structure of subfolders. A component is a single unit of code distribution or set of classes for a framework or
an application that is built and ready to be shipped. Another C++ module [4] or another	
component can import it with a hook, such as an âinclude"" keyword in case of headers (or the
âimport"" keyword in case of C++modules).
As an example, Figure 1 shows how a package can be created based on multiple com-
ponents. Here, a hypothetical ROOT-ML packageconsists of the TMVA (âToolkit for Multi-
variate Data Analysis"") component and ROOT RDataframe as its dependencies. The ROOT modularization work is well-aligned with the eï¬ort to integrate C++ modules
work going [2] in ROOT. C++ modules provide a simple way to produce software libraries
with improved compile-time scalability and management of the API of a library. C++ mod-
ules improve encapsulation and outline a clear relationship between public and private part
of the code, splitting the two into implementation and interface. For ROOT, we are working
on an ecosystem to improve versioning and binary distribution.	
ROOT-ML package	ROOT Package
ROOT Component	TMVA component	RDF component	
Hist
MLP
Tree
XMLIO
MathCore	
Matrix
CUDA
TreePlayer	
Hist
Tree	
ROOTVecops
TreePlayer	
Imt	
Figure 1: Example of ROOT component and package.
One of the main challenges is to deï¬ne the package granularity; best practices here re-
main an open question. Too large number of components in packages defeats the purpose of
modularization. Similarly, packages should not contain too many small components as this
may introduce signiï¬cant package management overhead. A ï¬rst ROOT package to deï¬ne is ROOT Base, which includes cling (the C++ inter-
preter), ROOT I/O, and other âCoreâ components. ROOT Baseis a fundamental part from
which we start to modularize ROOT framework. Another example of a ROOT package would
be a ROOTMath package that consists of multiple mathrelated components, such as ROOT
libraries MathCore, MathMore and VecCore (see Listing 1). We deï¬ne a package to be a grouping of software and associated resources intended for
its distribution and reuse. In order to create a package, we assume a speciï¬c organization
of code for build and deploy steps. The packageâs deï¬nition, versioning, metadata, content
description, and build information is contained in a manifest ï¬le. An example of our manifest
ï¬le format is shown in Listing 1. Package resources can include build byproducts such as a
shared library or an executable, or the package documentation and unit tests. The manifest
example was inspired by Swift manifests [5], and is written in YAML [6]. Entities who may interact with manifest ï¬les may include:  

4	
EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018	
package:name: ""ROOTMath""targets: ""ROOT::MathCore ROOT::MathMore VecCore::VecCore gsl::gsl mathcore-tests	mathmore-tests""	products:package:name: ROOTMathtargets: ROOT::MathCore ROOT::MathMore VecCore::VecCore ROOT::Imt gsl::gslmodule:name: MathCorepublicheaders: inc/<enumerated headers>.hsources: src/<enumerated source files>.cxxtargets: ROOT::MathCoredependencies: VecCore Imttests: mathcore-testsmodule:name: MathMorepublicheaders: inc/<enumerated headers>.hsources: src/<enumerated source files>.cxxtargets: ROOT::MathMoredependencies: gsl MathCoretests: mathmore-testsmodule:name: VecCorepackageurl: ""https://github.com/\Croot-project/veccore/\Carchive/v0.5.1.zip""targets: VecCore::VecCoretag: 0.5.1module:name: gslpackageurl: ""https://github.com/\Campl/gsl/archive/v2\C.5.0.zip""targets: gsl::gsl	
Listing 1:Draft version of a YAML-based manifest ï¬le for the ROOTMath package.
1. ROOT subsystem developer (such as an I/O developer): Here, the purpose of the mani-
fest is strictly informational. The information for manifest is generated from the build
system; the build system will help to produce the manifest ï¬le.
2. The third-party developer : For example, a Ph.D. student who wrote a new ROOT pack-
age as a part of work on the thesis and would like to describe in a human-readable from
a build description of his package and which ROOT components it depends on.
3. A member of an experiment physics group: Provides the ability to build a particular
library on demand or to share their developments.
4 Package manager design prototype
As a part of the package manager prototype was deï¬ned two diï¬erent future work areas:
ROOT Base package as a fundamental part of ROOT, and development of ROOT package
management tool.
4.1 Evolving âMinimal ROOTâ to âROOT Baseâ
While generating the ROOT libraries dependency graph, we can quickly notice that it is hard
to visualize a small core of ROOT as desired for our ROOT Base package.
ROOTâs build system provides a âminimal ROOTâ option that is supposed to build only
the essential functionality of ROOT required for basic I/O operations. At the outset of this
project, nearly ï¬fty components were built when this minimaloption was enabled; we believe
that âMinimal ROOTâ has migrated away from its original goal of being only a core-like  

5	
EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018	
package:name: ""ROOTMath""
targets: ""ROOT::MathCore ROOT::MathMore VecCore::VecCore gsl::gsl mathcore-tests mathmore-tests""
products: package:
name: ROOTMath
targets: ROOT::MathCore ROOT::MathMore VecCore::VecCore ROOT::Imt gsl::gsl
module: name: MathCore
publicheaders: inc/<enumerated headers>.h
sources: src/<enumerated source files>.cxx
targets: ROOT::MathCore
dependencies: VecCore Imt
tests: mathcore-tests
module: name: MathMore
publicheaders: inc/<enumerated headers>.h
sources: src/<enumerated source files>.cxx
targets: ROOT::MathMore
dependencies: gsl MathCore
tests: mathmore-tests
module: name: VecCore
packageurl: ""https://github.com/\Croot-project/veccore/\Carchive/v0.5.1.zip""
targets: VecCore::VecCore
tag: 0.5.1
module: name: gsl
packageurl: ""https://github.com/\Campl/gsl/archive/v2\C.5.0.zip""
targets: gsl::gsl	
Listing 1: Draft version of a YAML-based manifest ï¬le for the ROOTMath package.
1. ROOT subsystem developer (such as an I/O developer): Here, the purpose of the mani-
fest is strictly informational. The information for manifest is generated from the build
system; the build system will help to produce the manifest ï¬le.
2. The third-party developer : For example, a Ph.D. student who wrote a new ROOT pack-
age as a part of work on the thesis and would like to describe in a human-readable from
a build description of his package and which ROOT components it depends on.
3. A member of an experiment physics group: Provides the ability to build a particular
library on demand or to share their developments.
4 Package manager design prototype
As a part of the package manager prototype was deï¬ned two diï¬erent future work areas:
ROOT Base package as a fundamental part of ROOT, and development of ROOT package
management tool.
4.1 Evolving âMinimal ROOTâ to âROOT Baseâ
While generating the ROOT libraries dependency graph, we can quickly notice that it is hard
to visualize a small core of ROOT as desired for our ROOT Base package.
ROOTâs build system provides a âminimal ROOTâ option that is supposed to build only
the essential functionality of ROOT required for basic I/O operations. At the outset of this
project, nearly ï¬fty components were built when this minimaloption was enabled; we believe
that âMinimal ROOTâ has migrated away from its original goal of being only a core-like  

6	
EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018  

7	
EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018	
For our purposes, the minimal requirement for the ROOT package manager is to be able
to deï¬ne and resolve dependencies and versions through the manifest of the package. These
should be done either for ROOT packages or its external dependencies.
Ideas for future extensions are inspired by the functionality of Swift Package Manager
[8], which is supported by a large community:
1.Automated testing;
2. Support of cross-platform packages;
3. Support for operating system package managers (homebrew, etc.);
4. Support for version control system;
5. Standardized licensing;
6. Introduction of a package index;
7. Importing dependencies by source URL;
8. Component inter-dependency determination;
9. Complex dependency resolution.
Using beneï¬ts from the introduction of C++ module infrastructure for ROOT, we fol-
lowed the ideas of Swift Package Manager and implemented a standalone tool - âroot-getâ
which provides functionality meeting the a minimal requirements described above.	
4.2.3 root-get prototype
We have implemented a dependency management tool for ROOT (Figure 2) called âroot-getâ.
It consists of the multiple modules that provide the desired package management functional-
ity:
1.Analyzer deï¬nes environment variables, checks if there are existing manifest/package
YAML ï¬les, and sets up the environment (discovering the location of components and
packages and preparing for the manifestâs generation).
2. Generator encapsulates the ROOT CMake functions for generating information for
manifest ï¬les. It allows one to conï¬gure ROOT modules and packages outside of
ROOT using special CMake ï¬les containing the deï¬nitions about ROOT CMake func-
tions and ROOT external dependencies.
3. Downloader is a set of helper routines for downloading packages from Github or other
location.
4. Resolver is a module that provides a package database generation and resolution of
dependencies via generated direct acyclic graph (DAG).
5. Builder is a module that provides ROOT packaging scripts.
6. Integrator is a module that provides installation and deployment routine for ROOT
packages.	
A root-get is using the generic approach, existing for all ""package managers"" and works
directly with the source code of components. Manifest ï¬les, in this case, are usually the ï¬les
generated by static code analysis tools or provided by the user. The core part of package
manager workï¬ow is a ""lock ï¬le"". It is a ï¬le containing the project dependencies that are
generated directly from manifest ï¬les. The expected result after package manager operations
is a delivery of the set of artifacts: compiled source code and its dependencies, which can
serve as a direct âinputâ to the interpreter and usually generated from lock ï¬le.
All compiled source code, declared by the lock ï¬le should be arranged on disk in such
way that the compiler or interpreter can use it as intended, but it will be still isolated to avoid
mutation. ROOT components arranged as a set of packages could be installed in any location,
even outside of the install path of ROOT. All necessary components for the successful setup
are ROOT Base installation and root-get installed in the system.
ROOT Base together with root-get is capable of providing proper infrastructure for cus-
tom ROOT distribution and natural extension of ROOT functionalities, which includes the
possibility to build any ROOT components and also plug-in custom user components.	
root-get	ROOT	
Generator of manifests	Analyzer	Resolver (DB +DAG)	
Downloader	Builder of package	Integrator for package	
ROOT package map	
CMake handles to build packages externally	
ROOT Base	
Figure 2: Components of root-get prototype.
5 Conclusions
We have deï¬ned the desired functionality for a package management ecosystem for ROOT.
We deï¬ned a minimal and extended set of requirements for the ROOT package manager.
These ideas have been adopted into a preliminary prototype that can download and install
packages. As it matures, the prototype will be connected directly to the ROOT runtime and
serve as a runtime dependency management tool.	
This work has been supported by U.S. National Science Foundation grants OAC-1450377, OAC-
1450323, and PHY-1624356.	
References	
[1] R. Brun, F. Rademakers, ROOT - An Object Oriented Data Analysis Framework,
Nucl. Inst. & Meth. in Phys. Res. A 389(Proceedings AIHENPâ96 Workshop,1997).
[2] V. Vassilev. Optimizing ROOTâs Performance Using C++ Modules. Journal of
Physics: Conference Series, 898. 10.1088/1742-6596/898/7/072023. (2016)  

8	
EPJ Web of Conferences 	214	, 05011 (2019)	 https://doi.org/10.1051/epjconf/201921405011	
CHEP 2018  

"
Non,"Una guÃ­a con opiniones sobre
las tecnologÃ­as de vanguardia	
Volume 26	
Radar TecnolÃ³gico 

Sobre el Radar 	3	
Un vistazo al Radar 	4	
Contribuyentes 	5
Temas   6
El Radar  8
TÃ©cnicas   11
Plataformas   20
Herramientas   27
Lenguajes y Frameworks   36
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Sobre el Radar
Thoughtworkers son personas a las que les apasiona la tecnologÃ­a. La construimos, 
la investigamos, la probamos, abogamos 
por el cÃ³digo abierto, escribimos sobre 
ella y constantemente tratamos de  mejorarla - para todas las personas. 
Nuestra misiÃ³n es defender la 
excelencia del software y revolucionar  la TI. Creamos y compartimos el Radar 
TecnolÃ³gico de Thoughtworks en apoyo  de esa misiÃ³n. El Technology Advisory Board de Thoughtworks, un grupo de  lÃ­deres tecnolÃ³gicos de alto nivel de 
Thoughtworks, crea el Radar. Se reÃºnen  periÃ³dicamente para debatir la estrategia 
tecnolÃ³gica global de Thoughtworks y las 
tendencias tecnolÃ³gicas que tienen un  impacto significativo en nuestra industria.
El Radar recoge el resultado de los 
debates del Technology Advisory Board 
en un formato que proporciona valor a  una amplia gama de partes interesadas, 
desde las personas desarrolladoras  hasta CTOs. El contenido pretende ser un 
resumen conciso.
 
Te animamos a explorar estas tecnologÃ­as.  El Radar es de naturaleza grÃ¡fica y agrupa los elementos en tÃ©cnicas, herramientas, 
plataformas y lenguajes y frameworks. 
Cuando los elementos del Radar podÃ­an  aparecer en varios cuadrantes, elegimos el que nos pareciÃ³ mÃ¡s apropiado. 
AdemÃ¡s, agrupamos estos elementos  en cuatro anillos para reflejar nuestra posiciÃ³n actual al respecto.
Para mÃ¡s informaciÃ³n sobre el Radar, 
consulta  thoughtworks.com/es/radar/faq .
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Un vistazo al Radar
El Radar se dedica a rastrear cosas interesantes, a las que nos referimos como blips. Organizamos los blips en el Radar utilizando dos elementos de categorizaciÃ³n: cuadrantes y anillos. Los cuadrantes 
representan los diferentes tipos de blips. Los anillos indican en quÃ© fase del ciclo de vida de la adopciÃ³n 
creemos que deberÃ­an estar.
Un blip es una tecnologÃ­a o tÃ©cnica que desempeÃ±a un papel en el desarrollo de software. Los blips 
son cosas que estÃ¡n âen movimientoâ, es decir, que su posiciÃ³n en el Radar estÃ¡ cambiando, lo que 
suele indicar que cada vez tenemos mÃ¡s confianza en ellos a medida que avanzan por los anillos.
Nuestro Radar estÃ¡ orientado al futuro. Para dar paso a nuevos artÃ­culos, desvanecemos los que no se han  movido recientemente, lo cual no es un reflejo de su valor, sino de nuestro limitado espacio en el Radar.	
Resistir Evalua rP robarA doptar	
Adoptar: Estamos convencidas de que
la industria deberÃ­a adoptar estos Ã­tems.
Nosotras los utilizamos cuando es
apropiado en nuestros proyectos.
Probar:  Vale la pena probarlos. Es 
importante entender cÃ³mo desarrollar estas 
capacidades. Las empresas deberÃ­an probar 
esta tecnologÃ­a en proyectos en que se  puede manejar el riesgo.
Evaluar:  Vale la pena explorar con el objetivo
de comprender cÃ³mo afectarÃ¡ a su empresa.
Resistir:  Proceder con precauciÃ³n.
Nuevo Desplazado 
adentro/afuera NingÃºn 
cambio	
4	
Equipo de traducciÃ³n al EspaÃ±ol: Alejandro Batanero, Alex Ulloa, Alexandra Ortiz, Ana JimÃ©nez Valbuena, Ana 
MarÃ­a ZÃºÃ±iga, Andrea Peralta Bravo , AndrÃ©s CÃ¡ceres, AndrÃ©s de los Reyes, Anna MascarÃ³, Araceli Correa, Ari 
Handler, Bryan ZuÃ±iga, Camila Paredes, Camila Vigneaux, Carlos Barroso, Carlos Guerrero, Catalina SolÃ­s, Christian 
Ãlvarez, Cristian Montero, Daniel Negrete, David Benitez, Diana Barreno, Diana Luna, Diana Pila, Elizabeth Parra,  Erika Vacacela, Esteban Grijalva, Esteban Villacis, Eugenia Samaniego, Eva Villarroya, Fausto de la Torre, Fernando 
Tamayo, Francisco PÃ©rez , Gabriel FrÃ­as Rivas, Gabriel Loja, Gabriela GuamÃ¡n, Gaby Gurfinkel, Geovanny Campoverde,  Giovanni Serrano, Glenn Wolfschoon, Gonzalo VÃ¡zquez Cao, Gorka LÃ³pez de Torre, Gustavo Chiriboga, Gustavo Marin, Helena Gomez , Inigo Crespo Soria, Irene Amo, Javier Alcivar, Javier Escobar, JÃ©ssica GarrigÃ³s, JesÃºs 
Cardenal, Jhosep Marin, Joan Sanchez, JoÃ£o Lucas Santana, Jorge Agudo Praena, Jorge Palacios, Jorgina ArrÃ©s 
Cardona, Jose Herdoiza, Juan Mateos, Juan Mite, Julieta Corvi, Katherine Ayala , Lara Berra, Laura MirÃ¡s, LucÃ­a  Parga Basanta, Luis Bustamante, Mafer Escudero, Magdalena Grondona, MarÃ­a JosÃ© Lalama, Mayfe YÃ©pez, Miguel 
Hernandez , Milber Champutiz, Oscar Garcia, Pablo Porto, Paola Cajilema, Paula Forero, Paula Marin, Pedro Grijalva, 
Ricardo BriceÃ±o, RubÃ©n Trujillo , Sebastian MuÃ±oz, Silvina Calderon, Valentina Morales , Viviana ProaÃ±o, Viviana 
Quilape y Yanet GarcÃ­a
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Contribuyentes
El Technology Advisory Board (TAB) es un 
grupo de 18 personas tecnÃ³logas senior 
de Thoughtworks. El TAB se reÃºne dos 
veces al aÃ±o en persona y quincenalmente  por telÃ©fono. Su funciÃ³n principal es ser un 
grupo de asesoramiento para el CTO de 
Thoughtworks, Rebecca Parsons.
El TAB actÃºa como un organismo amplio 
que puede examinar los temas que afectan 
a la tecnologÃ­a y a las personas tecnÃ³logas  de Thoughtworks. Con la actual pandemia  mundial, hemos vuelto a crear este volumen 
del Radar TecnolÃ³gico a travÃ©s de un evento 
virtual.	
Rebecca Parsons (CTO)   
Martin Fowler (Chief Scientist)  
Bharani Subramaniam   
Birgitta BÃ¶ckeler  
Brandon Byars  
Camilla Falconi Crispim  
Cassie Shum
Erik DÃ¶rnenburg   
Fausto de la Torre
Hao Xu
Ian Cartwright  
James Lewis  
Lakshminarasimhan Sudarshan   
Mike Mason  
Neal Ford
Perla Villarreal
Scott Shaw
Shangqi Liu
Zhamak Dehghani
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

El Bazar ExtraÃ±o:  El cambio econÃ³mico en Open-Source Software
En Thoughtworks, somos fieles seguidores del software de fuente abierta hace tiempo, popularizado 
en parte por el famoso ensayo de Eric Raymond âThe Cathedral and the Bazaarâ. El software de  fuente abierta abre una ventana al desarrollo de software y promueve la colaboraciÃ³n abierta hacia la 
correcciÃ³n de errores y la innovaciÃ³n. Sin embargo, los intentos de comercializaciÃ³n del software de  fuente abierta han demostrado la enorme complejidad econÃ³mica del actual ecosistema. Esto se ha 
visto, por ejemplo, en el forking de AWS âyâ Elasticsearch hacia OpenSearch en Septiembre de 2021  como respuesta al cambio en la licencia por parte de Elastic al requerir a los proveedores de cloud-
service, que se beneficiaban de su trabajo, a contribuir de vuelta. Esto muestra cuÃ¡n difÃ­cil puede 
ser para un comercial de software de fuente abierta el mantener un âfoso econÃ³micoâ. (La misma  preocupaciÃ³n se aplica con los software de fuente cerrada gratuitos, pues hemos sido testigos de 
que algunas compaÃ±Ã­as exploran alternativas a Docker Desktop debido al esfuerzo continuo de  Docker en encontrar un modelo comercial Ã³ptimo). A veces, la dinÃ¡mica de poder puede funcionar al 
contrario: debido a que Facebook fundara Presto como un producto open-source, los âmaintainersâ 
(âlos que se quedaronâ) fueron capaces de mantener la IP y cambiarle el nombre de la marca por 
Trino despuÃ©s de que estos se hubieran marchado de la compaÃ±Ã­a, beneficiÃ¡ndose asÃ­ de la inversiÃ³n  de Facebook.
La historia se vuelve incluso mÃ¡s engorrosa cuando hablamos de la la cantidad de infraestructura 
crÃ­tica que no es patrocinada por corporaciones, donde las compaÃ±Ã­as se dan cuenta de cuÃ¡nto de 
verdad necesitan apoyarse en âtrabajo no pagadoâ cuando aparecen bugs crÃ­ticos de seguridad 
(como ha pasado recientemente con Lo4J). En algunos casos, financiar a âhobbyist maintainersâ a travÃ©s de GitHub o Patreon proporciona el 
suficiente impulso para hacer la diferencia; en otros casos, simplemente se crea un sentimiento de  responsabilidad, adicional a su trabajo diario y contribuye al agotamiento. Seguimos siendo Ã¡vidos 
seguidores del software de fuente abierta pero somos conscientes que la economÃ­a alrededor se  estÃ¡ convirtiendo cada vez mÃ¡s en algo mÃ¡s bizarro, y no hay soluciones fÃ¡ciles para encontrar el 
equilibrio adecuado.
Innovaciones de la Cadena de Suministro de Software
Instancias pÃºblicas de problemas severos â La filtraciÃ³n de datos de Equifax, el ataque a SolarWinds,  la vulnerabilidad de dÃ­a cero de Log4J entre otros  â que fueron causadas por una escasa 
gobernanza de la cadena de suministro de software. Los equipos ahora entienden que las prÃ¡cticas 
de ingenierÃ­a responsables incluyen validar y gestionar las dependencias de los proyectos, esto es  un motivador de varios blips de esta ediciÃ³n del Radar. Las entradas incluyen listas de verificaciÃ³n 
y estÃ¡ndares tales como Niveles de la Cadena de Suministro para Artefactos de Software (SLSA,  por sus siglas en inglÃ©s) , un consorcio respaldado por Google para proveer guÃ­as de las amenazas 
Temas	
6
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

estÃ¡ndar a la cadena de suministro, y CycloneDX, , otro grupo de estÃ¡ndares manejados por la 
comunidad OWASP. TambiÃ©n presentamos herramientas concretas como Syft , la cual genera una 
Lista de Materiales del Software (SBOM) desde imÃ¡genes de contenedores. Los hackers estÃ¡n 
cada vez tomando mÃ¡s ventaja de la naturaleza asimÃ©trica de la ofensa y defensa en el campo de la 
seguridad â solo necesitan encontrar vulnerabilidades, mientras que los defensores deben asegurar  toda la superficie de ataque â mientras usan tÃ©cnicas de hackeo cada vez mÃ¡s sofisticadas. Una 
seguridad mejorada de la cadena de suministro es una pieza crÃ­tica de nuestra respuesta al trabajar  para mantener seguros los sistemas.
Por quÃ© los programadores siguen implementando State 
Management en React?
Distintos tipos de frameworks emergentes tienden a aparecer de manera regular en el Radar: un 
framework se hace popular, seguido por numerosas herramientas que crean un ecosistema de 
deficiencias y mejoras comunes, dando lugar a la consolidaciÃ³n de varias herramientas populares. 
Sin embargo, React state management parece ser resistente a esta tendencia. Desde que Redux se  lanzÃ³, hemos visto una corriente de herramientas y frameworks que manejan el estado de maneras 
ligeramente diferentes; cada una de ellas con una serie de desventajas. No sabemos porquÃ©, sÃ³lo 
podemos especular: Â¿Es Ã©sta la dinÃ¡mica de reemplazo que JavaScript pareciera que promueve? 
Â¿Acaso hay una deficiencia debajo de React, un problema divertido y aparentemente manejable que  anima a los desarrolladores a experimentar? O es la permanente discordancia entre el formato de lectura de documentos (navegadores web) y la interactividad (y el estado) necesarios para acoger el 
desarrollo de aplicaciones sobre documentos?  No sabemos la razÃ³n, pero esperamos a la siguiente  ronda de intentos de solucionar este problema a simple vista permanente.
La interminable bÃºsqueda de El Master Data Catalog
El deseo de sacar mÃ¡s valor de los activos de datos corporativos estÃ¡ detrÃ¡s de la mayorÃ­a de las inversiones que estamos viendo en el Ã¡mbito de la tecnologÃ­a digital. Principalmente, este esfuerzo 
se centra en crear maneras mÃ¡s efectivas de encontrar y tener acceso a datos que sean relevantes.  Desde el comienzo en el que las compaÃ±Ã­as empezaron a recabar datos digitales, han existido 
esfuerzos de racionalizar y crear un sistema de acceso Ãºnico, y de arriba hacia abajo, a un directorio 
de datos corporativo. Sin embargo, una y otra vez, esta idea en principio atractiva se da de bruces 
con la realidad compleja, redundante y ambigua propias de grandes organizaciones. Recientemente  hemos identificado un renovado interÃ©s en catÃ¡logos de datos corporativos y un surgimiento intermitente de propuestas en el Radar para nuevas e ingeniosas herramientas como Collibra  y 
DataHub . Estas herramientas pueden proveer acceso regular y detectabilidad al linaje y metadatos 
a travÃ©s de silos, pero su cada vez mÃ¡s grande conjunto de caracterÃ­sticas tambiÃ©n se extiende a la  gobernanza, gestiÃ³n de calidad, publicaciÃ³n y mÃ¡s.
En contraste con esta moda, tambiÃ©n parece haber un creciente movimiento que se aleja del manejo 
de datos desde arriba hacia abajo y centralizado, hacia una gobernanza federada y el descubrimiento  basado en una arquitectura de malla de datos. Este mÃ©todo ataja la complejidad inherente de los 
datos corporativos fijando expectativas y estÃ¡ndares de una manera central, pero segregando la 
custodia del dato a travÃ©s de las lÃ­neas de dominio. Los equipos de productos de datos orientados 
al dominio controlan y comparten sus propios metadatos incluyendo detectabilidad, calidad y otra  informaciÃ³n. En este escenario, el catÃ¡logo es sÃ³lo una forma de mostrar informaciÃ³n para bÃºsqueda 
y navegaciÃ³n. Los catÃ¡logos de datos resultantes son mÃ¡s simples, mÃ¡s fÃ¡ciles de mantener y  reducen la necesidad de plataformas enriquecidas de catalogaciÃ³n y plataformas de gobernanza.
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

ResistirResistir
Evaluar Evaluar
Probar Probar
A doptar Adoptar	
3	1	
24	3 5	
36
37	
38	
39	
4 0	
41	4 2	
43	
29	
3 4	
32	
4	12	
13	
14	
19	
2 1
15	2 0	
16	
17
2 2	
18	
5
6	
7	
9	11	
4 5	49	
60	
61	62	
63	
64	65
66	67	
68
69
70
7 1	
5 1	53
57	58	
59	
52	
74	
7 9	8 0	
81	
82	83	
84
85	
86	87
88
89	9 0
91	
93
92	
75	
7 7	
78	
3 0
31	
72	
73	
7 6	
2	
8	10	
2 3	
25
26	
27	
28	
3 3	
4 4
46	
47	
50
54	55	
56	
4 8	
El Radar	
Nuevo
Desplazado 
adentro/afuera NingÃºn cambio	
8
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Adoptar1.   Cuatro mÃ©tricas clave
2.   Single team remote wall  
Probar
3.   Malla de datos (Data mesh)
4.   DefiniciÃ³n de listo para producciÃ³n
5.   Cuadrantes de documentaciÃ³n
6.   Repensando las reuniones diarias remotas
7.   Interfaz de usuario basada en servidor
8.  Software Bill of Materials
9.   BifurcaciÃ³n TÃ¡ctica
10.  Carga cognitiva del equipo
11.  Arquitectura de transiciÃ³n  
Evaluar
12.  CUPID
13.  DiseÃ±o inclusivo
14.  PatrÃ³n de operador para recursos no 
agrupados
15.  Malla de servicios sin sidecar
16.  SLSA
1 7.  AlmacÃ©n de datos de streaming
18.  TinyML  
Resistir
19.  Azure Data Factory para orquestaciÃ³n
20.  Equipos de plataformas miscelÃ¡neas
21.  Datos de producciÃ³n en entornos de prueba
22.  SPA por defecto  
  Adoptar
â
Probar
23.  Azure DevOps
24.  Las plantillas de Azure Pipelines
25.  CircleCI
26.  Couchbase
2 7.  eBPF
28.  GitHub Actions
29.  GitLab CI/CD
30.  Google BigQuery ML
31.  Google Cloud Dataflow
32.  Flujos de trabajo reutilizables en Github 
Actions
33.  Sealed Secrets
34.  VerneMQ  
Evaluar
35.  actions-runner-controller
36.  Apache Iceberg
3 7.  Blueboat
38.  Cloudflare Pages
39.  Colima
40.  Collibra
41.  CycloneDX
42.  Embeddinghub
43.  Temporal  
Resistir â
TÃ©cnicas Plataformas
El Radar
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Adoptar
44.  tfsec
Probar
45.  AKHQ
46.  cert-manager
4 7.  Cloud Carbon Footprint
48.  Conftest
49.  kube-score
50.  Lighthouse
51.  Metaflow
52.  Micrometer
53.  NUKE
54.  Pactflow
55.  Podman
56.  Grafo de CÃ³digo Fuente
5 7.  Syft
58.  Volta
59.  Web Test Runner 
Evaluar
60.  CDKTF
61.  Chrome Recorder panel
62.  Excalidraw
63.  GitHub Codespaces
64.  GoReleaser
65.  Grype
66.  Infracost
6 7.  jc
68.  skopeo
69.  SQLFluff
70.  Validador de Terraform
71.  Typesense  
Probar â Adoptar
72.  SwiftUI
73.  Testcontainers  
Probar
74 .  Bob
75.  Widget de Flutter-Unity
76.  Kotest
7 7.  Swift Package Manager
78.  Vowpal Wabbit
Evaluar
79.  Android Gradle plugin - Kotlin DSL
80.  Azure Bicep
81.  Capacitor
82.  Java 17
83.  Jetpack Glance
84.  Jetpack Media3
85.  MistQL
86.  npm workspaces
8 7.  Remix
88.  ShedLock
89.  SpiceDB
90.  sqlc
91.  La Arquitectura Componible
92.  Ensamblaje Web
93.  Zig  
Probar â
Herramientas Lenguajes y Frameworks
El Radar
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

TÃ©cnicas	
ResistirResistir
Evaluar Evaluar
Probar Probar
A doptar Adoptar	
3	1	
24	3 5
36
37	
38	
39	
4 0	
41	4 2	
43	
29	
3 4	
32	
4	12	
13	
14	
19	
2 1
15	2 0	
16	
17
2 2	
18	
5
6	
7	
9	11	
4 5	49	
60	
61	62	63	
64	65
66	67
68
69
70
7 1	
5 1	53
57	58	
59	
52	
74	
7 9	8 0	
81	
82	83	84
85	
86	87
88
89	9 0
91	
93
92	
75	
7 7	
78	
3 0
31	
72	
73	
7 6	
2	
8	10	
2 3	
25
26	
27	28	
3 3	
4 4
46	
47	
50
54	55	
56	
4 8	
Adoptar
1.   Cuatro mÃ©tricas clave
2.   Single team remote wall  
Probar
3.   Malla de datos (Data mesh)
4.   DefiniciÃ³n de listo para producciÃ³n
5.   Cuadrantes de documentaciÃ³n
6.   Repensando las reuniones diarias remotas
7.   Interfaz de usuario basada en servidor
8.  Software Bill of Materials
9.   BifurcaciÃ³n TÃ¡ctica
10.  Carga cognitiva del equipo
11.  Arquitectura de transiciÃ³n  
Evaluar
12.  CUPID
13.  DiseÃ±o inclusivo
14.  PatrÃ³n de operador para recursos no 
agrupados
15.  Malla de servicios sin sidecar
16.  SLSA
1 7.  AlmacÃ©n de datos de streaming
18.  TinyML  
Resistir
19.  Azure Data Factory para orquestaciÃ³n
20.  Equipos de plataformas miscelÃ¡neas
21.  Datos de producciÃ³n en entornos de prueba
22.  SPA by default  	
Nuevo
Desplazado
adentro/afuera NingÃºn 
cambio
Thoughtworks Technology RadarÂ© Thoughtworks, Inc. All Rights Reserved.  

1. Cuatro mÃ©tricas clave
AdoptarPara medir el rendimiento de la entrega de software, cada vez mÃ¡s organizaciones recurren por 
defecto a las cuatro mÃ©tricas clave definidas por el programa DORA research : tiempo de espera de 
los cambios, frecuencia de despliegue, tiempo medio de restauraciÃ³n (MTTR) y porcentaje de fallos 
en los cambios. Esta investigaciÃ³n y su anÃ¡lisis estadÃ­stico han demostrado una clara relaciÃ³n entre 
el alto rendimiento de la entrega y estas mÃ©tricas; proporcionan un gran indicador de cÃ³mo lo estÃ¡  haciendo una organizaciÃ³n de entrega en su conjunto.
Seguimos siendo grandes defensores de estas mÃ©tricas, pero tambiÃ©n hemos aprendido algunas  lecciones. Seguimos observando enfoques errÃ³neos con herramientas que ayudan a los equipos 
a medir estas mÃ©tricas basÃ¡ndose Ãºnicamente en sus conductos de entrega continua (CD). En  particular, cuando se trata de las mÃ©tricas de estabilidad (MTTR y porcentaje de cambios fallidos), los datos del pipeline de CD por sÃ­ solos no proporcionan suficiente informaciÃ³n para determinar 
quÃ© es un fallo de despliegue con impacto real en el usuario. Las mÃ©tricas de estabilidad sÃ³lo tienen 
sentido si incluyen datos sobre incidentes reales que degraden el servicio para los usuarios.
Recomendamos tener siempre presente la intenciÃ³n Ãºltima de una mediciÃ³n y utilizarla para reflexionar y aprender. Por ejemplo, antes de dedicar semanas a la creaciÃ³n de sofisticadas 
herramientas de cuadros de mando, considera la posibilidad de limitarse a realizar regularmente la 
comprobaciÃ³n rÃ¡pida del DORA en las retrospectivas del equipo. Esto da al equipo la oportunidad 
de reflexionar sobre quÃ© capacidades  podrÃ­an trabajar para mejorar sus mÃ©tricas, lo que puede ser 
mucho mÃ¡s eficaz que la creaciÃ³n de herramientas demasiado detalladas. Hay que tener en cuenta 
que estas cuatro mÃ©tricas clave se originaron a partir de la investigaciÃ³n a nivel de organizaciÃ³n de  los equipos de alto rendimiento, y el uso de estas mÃ©tricas a nivel de equipo debe ser una forma 
de reflexionar sobre sus propios comportamientos, no sÃ³lo otro conjunto de mÃ©tricas para aÃ±adir al 
cuadro de mando.
2. Single team remote wall
Adoptar Un single team remote wall es una tÃ©cnica sencilla para reintroducir virtualmente el muro del 
equipo. Recomendamos que los equipos distribuidos adopten este enfoque; una de las cosas que 
escuchamos de los equipos que empezaron a trabajar en remoto es que echan de menos tener el  muro fÃ­sico del equipo. Este era un espacio Ãºnico donde se podÃ­an mostrar todas las tarjetas de 
historias de usuario, tareas, su estado y progreso, actuando como punto de informaciÃ³n y centro 
de referencia para el equipo. El muro actuÃ³ como un punto de integraciÃ³n con los datos reales 
almacenados en diferentes sistemas. A medida que los equipos se volvieron remotos, tuvieron que 
acudir de nuevo a los distintos sistemas para buscar la informaciÃ³n, por lo que obtener una âvisiÃ³n  generalâ de un proyecto se ha vuelto muy complicado. Si bien puede haber algo de esfuerzo extra  para mantenerlo actualizado, creemos que los beneficios para el equipo valen la pena. Para algunos 
equipos, actualizar el muro fÃ­sico formaba parte de las âceremoniasâ diarias que el equipo hacÃ­a en 
conjunto, y lo mismo se puede hacer con un muro remoto.
3. Malla de datos (Data mesh)
Probar
La malla de datos Data mesh  es un enfoque organizativo y tÃ©cnico descentralizado a la hora de 
compartir, acceder y gestionar los datos para la analÃ­tica y el ML. Su objetivo es crear un enfoque 
TÃ©cnicas	
12
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

sociotÃ©cnico que amplÃ­e la obtenciÃ³n de valor de los datos a medida que crezca la complejidad de la organizaciÃ³n y proliferen los casos de uso de los datos y se diversifiquen las fuentes de los mismos.  Esencialmente, crea un modelo de intercambio de datos responsable que estÃ¡ en consonancia con 
el crecimiento de la organizaciÃ³n y el cambio continuo. SegÃºn nuestra experiencia, el interÃ©s por  la aplicaciÃ³n de la malla de datos ha crecido enormemente. Este enfoque ha inspirado a muchas 
organizaciones a adoptarlo y a los proveedores de tecnologÃ­a a readaptar sus tecnologÃ­as existentes  para una implantaciÃ³n de malla. A pesar del gran interÃ©s y la creciente experiencia en la malla de 
datos, sus implantaciones se enfrentan a un elevado coste de integraciÃ³n. AdemÃ¡s, su adopciÃ³n 
sigue limitada a secciones de grandes organizaciones y los proveedores de tecnologÃ­a estÃ¡n  distrayendo a las organizaciones de los aspectos socio mÃ¡s duros de la malla de datos: la propiedad 
descentralizada de los datos y un modelo operativo de gobierno federado.
Estas ideas se exploran en Data Mesh, Delivering Data-Driven Value at Scale , que guÃ­a a los 
profesionales, arquitectos, lÃ­deres tÃ©cnicos y responsables de la toma de decisiones en su transiciÃ³n 
desde una arquitectura tradicional de big data a la malla de datos. Proporciona una introducciÃ³n 
completa a los principios de la malla de datos y sus componentes; cubre cÃ³mo diseÃ±ar una 
arquitectura de malla de datos, guiar y ejecutar una estrategia de malla de datos y navegar por el  diseÃ±o organizativo hacia un modelo de propiedad de datos descentralizado. El objetivo del libro 
es crear un nuevo marco para profundizar en las conversaciones y conducir a la siguiente fase de  madurez de la malla de datos.
4. DefiniciÃ³n de listo para producciÃ³n
Probar
En una organizaciÃ³n que practica el principio âlo construyes, lo ejecutasâ, una definiciÃ³n de listo para  producciÃ³n (DPR, por sus siglas en inglÃ©s), es una tÃ©cnica Ãºtil para ayudar a los equipos a evaluar y 
preparar la disponibilidad operacional de nuevos servicios. Implementada como una lista de puntos o 
una plantilla, un DPR da a los equipos una guÃ­a sobre quÃ© tener en cuenta y en quÃ© pensar antes de  llevar un nuevo servicio a producciÃ³n. Aunque los DRP no definen objetivos de servicio especÃ­ficos 
(SLOs, por sus siglas en inglÃ©s) a cumplir (esto pueden ser difÃ­ciles de definir de forma general),  recuerdan a los equipos en quÃ© categorÃ­a de SLOs pensar, quÃ© estÃ¡ndares organizacionales cumplir, 
y quÃ© documentaciÃ³n es necesaria. Los DRP, nos dan una fuente de informaciÃ³n que los equipos  convierten en los requerimientos de producto especÃ­ficos, por ejemplo: observabilidad y fiabilidad,  para incluir en sus backlog de producto.
Los DPR estÃ¡n Ã­ntimamente relacionados con el concepto propuesto por Google en production  readiness review (PRR) . En organizaciones demasiado pequeÃ±as como para tener un equipo de 
ingenierÃ­a de fiabilidad (SRE, por sus siglas en inglÃ©s), o preocupadas porque un proceso de panel de 
revisiÃ³n pueda impactar el flujo de salida a producciÃ³n de un equipo, tener un DPR puede al menos 
ofrecer una guÃ­a documentada de criterios acordados a nivel de organizaciÃ³n. Para nuevos servicios 
de alta criticidad, se puede aÃ±adir un mayor nivel de escrutinio en cunplimiento de DPR mediante un  PRR.
TÃ©cnicas	
13
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

5. Cuadrantes de documentaciÃ³n
Probar
Escribir buena documentaciÃ³n es un aspecto que se pasa por alto en el desarrollo de software y 
que suele dejarse para el Ãºltimo momento y se realiza de manera desordenada. Algunos de nuestros 
equipos han encontrado en los cuadrantes de documentaciÃ³n  una manera prÃ¡ctica de garantizar 
que se produzcan los artefactos correctos. Esta tÃ©cnica clasifica los artefactos sobre dos ejes: El  primer eje estÃ¡ relacionado con la naturaleza de la informaciÃ³n, prÃ¡ctica o teÃ³rica; el segundo eje 
describe el contexto en el que el artefacto es usado, estudiado o trabajado. Esta tÃ©cnica define 
cuatro cuadrantes en los cuales artefactos como tutoriales, guÃ­as prÃ¡cticas o pÃ¡ginas de referencia  pueden ser clasificadas y entendidas. Este sistema de clasificaciÃ³n no solo asegura que artefactos 
crÃ­ticos no sean pasados por alto sino que tambiÃ©n guÃ­a la presentaciÃ³n del contenido. Hemos 
encontrado esto particularmente Ãºtil a la hora de crear documentaciÃ³n de onboarding que facilita la  rÃ¡pida puesta al dÃ­a de nuevos desarrolladores cuando se unen a un nuevo equipo.
6. Repensando las reuniones diarias remotas
Probar
El tÃ©rmino standup tiene su origen en la idea de ponerse de pie durante esta reuniÃ³n de 
sincronizaciÃ³n diaria, con el objetivo de hacerla breve. Se trata de un principio comÃºn que muchos  equipos intentan cumplir en sus reuniones diarias: que sean breves y directas. Pero ahora estamos 
viendo que los equipos desafÃ­an ese principio y estÃ¡n ârepensando las reuniones diarias remotasâ.  Cuando se estÃ¡ en un mismo lugar, hay muchas oportunidades durante el resto del dÃ­a para  sincronizarnos entre nosotros de forma espontÃ¡nea, como complemento a la breve reuniÃ³n diaria.  De forma remota, algunos de nuestros equipos estÃ¡n experimentando un formato de reuniones mÃ¡s  largas, similar a lo que la gente de Honeycomb llama â sincronizaciÃ³n de equipo serpenteante.â
No se trata de deshacerse por completo de una sincronizaciÃ³n diaria; todavÃ­a lo encontramos  muy importante y valioso, especialmente en un formato remoto. En cambio, se trata de extender 
el tiempo bloqueado en los calendarios de todos para la sincronizaciÃ³n diaria hasta en una hora, y  usarlo de una manera que haga obsoletas algunas de las otras reuniones y acerque al equipo. En  las actividades aÃºn se puede incluir el bien probado recorrido por el tablero del equipo, para luego 
ampliarlo con discusiones de aclaraciÃ³n mÃ¡s detalladas, decisiones rÃ¡pidas y tomarse el tiempo para 
socializar. La tÃ©cnica se considera exitosa si se reduce la carga general de la reuniÃ³n y mejora la  uniÃ³n del equipo.
7. Interfaz de usuario basada en servidor
Probar
Al armar un nuevo volumen del Radar, a menudo nos invade una sensaciÃ³n de dÃ©jÃ  vu, y la tÃ©cnica  de server-driven UI genera un caso particularmente sÃ³lido con la llegada de los frameworks que permiten a los desarrolladores de aplicaciones mÃ³viles tomar ventaja de los ciclos de cambio mÃ¡s 
rÃ¡pidos mientras que no se falle en las polÃ­ticas de las tiendas de apps en torno a la revalidaciÃ³n de  la propia aplicaciÃ³n mÃ³vil. Lo hemos comentado antes desde la perspectiva de permitir el desarrollo 
mÃ³vil para escalar a travÃ©s de los equipos . La interfaz de usuario basada en servidor separa la 
representaciÃ³n en un contenedor genÃ©rico en la aplicaciÃ³n mÃ³vil mientras que la estructura y la 
data para cada vista es provista por el servidor. Esto significa que los cambios que alguna vez  requirieron un viaje de ida y vuelta a una tienda de apps pueden ahora ser logrados a travÃ©s de 
cambios simples en las respuestas que envÃ­a el servidor. Nota, no estamos recomendando este 
acercamiento para todos los desarrollos de interfaz de usuario, de hecho hemos experimentado 
algunos problemas terribles y demasiado configurados, pero con el respaldo de gigantes como 
TÃ©cnicas	
14
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

AirBnB y Lyft, sospechamos que no solo nosotros en Thoughtworks nos cansamos de hacer todo del lado del cliente . Mire este espacio.
8. Software Bill of Materials
Probar
Con la continua presiÃ³n de mantener los sistemas seguros y sin reducciÃ³n en el panorama general  de amenazas, un machine-readable Software Bill of Materials (SBOM) puede ayudar a los equipos 
a estar al tanto de los problemas de seguridad de las librerÃ­as en las que confÃ­an. El mÃ¡s reciente, el  dÃ­a cero del exploit remoto Log4Shell  ue critico y ampliamente conocido. Si los equipos hubieran 
tenido un SBOM listo, este hubiera escaneado y corregido rÃ¡pidamente el mismo. Ahora hemos 
tenido experiencia en ambientes de producciÃ³n usando SBOMs en proyectos que se encuentran 
tanto en compaÃ±Ã­as pequeÃ±as como en grandes multinacionales, hasta en departamentos de 
gobierno, y estamos convencidos que proveen beneficios. Herramientas como Syft  hacen fÃ¡cil el 
uso de SBOM para detectar vulnerabilidades.
9. BifurcaciÃ³n TÃ¡ctica
Probar
BifucarciÃ³n tÃ¡ctica  es una tÃ©cnica que puede ayudar con la reestructuraciÃ³n o migraciÃ³n de 
monolitos a microservicios. EspecÃ­ficamente, esta tÃ©cnica ofrece una posible alternativa al enfoque 
mÃ¡s comÃºn de primero modularizar completamente el cÃ³digo, que en muchas circunstancias puede 
tomar mucho tiempo o ser muy difÃ­cil de alcanzar. Con la bifurcaciÃ³n tÃ¡ctica un equipo puede crear  una nueva bifurcaciÃ³n del cÃ³digo y usarla para tratar y extraer un Ã¡rea o problema en particular  mientras se elimina el cÃ³digo innecesario. El uso de Ã©sta tÃ©cnica probablemente sÃ³lo serÃ­a parte de 
un plan a largo plazo para el monolito en general.
10. Carga cognitiva del equipo
Probar
La arquitectura de los sistemas imita la estructura de la organizaciÃ³n y su comunicaciÃ³n. No es  novedad que las interacciones de equipo deben ser deliberadas. Ver, por ejemplo, la maniobra inversa de Conway (Inverse Conway Maneuver) . La interacciÃ³n del equipo es una de las variables 
de la rapidez y facilidad con la que los equipos pueden entregar valor a sus clientes. Estuvimos  felices de encontrar una manera de medir estas interacciones, usamos la evaluaciÃ³n  del autor de 
TopologÃ­as de Equipo , que te ayuda a comprender quÃ© tan fÃ¡cil o difÃ­cil les resulta a los equipos 
desarrollar, testear y mantener sus servicios. Midiendo la carga cognitiva del equipo, pudimos 
aconsejar mejor a nuestros clientes sobre cÃ³mo cambiar la estructura de sus equipos y fomentar sus  interacciones.
11. Arquitectura de transiciÃ³n
Probar
Una arquitectura de transiciÃ³n  es una prÃ¡ctica Ãºtil utilizada para el reemplazo de los sistemas 
heredados. Al igual que los andamios pueden construirse, reconfigurarse y finalmente retirarse 
durante la construcciÃ³n o renovaciÃ³n de un edificio, pasos arquitectÃ³nicos provisionales durante 
el desplazamiento de lo heredado. Las arquitecturas de transiciÃ³n se eliminarÃ¡n o sustituirÃ¡n  mÃ¡s adelante, pero no son un trabajo desechable, dado el importante rol que desempeÃ±an en la 
reducciÃ³n del riesgo y en permitir que un problema difÃ­cil se divida en pasos mÃ¡s pequeÃ±os. Por 
TÃ©cnicas	
15
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

tanto, ayudan a evitar la trampa de caer en un enfoque de reemplazo heredado âbig bangâ, porque no 
se puede hacer que los pasos intermedios mÃ¡s pequeÃ±os se alineen con una visiÃ³n arquitectÃ³nica  final. Hay que tener cuidado para asegurarse de que el âandamiajeâ arquitectÃ³nico se elimine 
finalmente, para que no se convierta en una deuda tÃ©cnica mÃ¡s adelante.
12. CUPID
Evaluar
Â¿CÃ³mo te planteas la escritura de un buen cÃ³digo? Â¿CÃ³mo puedes juzgar si has escrito cÃ³digo de  calidad? Como desarrolladores de aplicaciones, siempre estamos buscando reglas, principios y patrones que podamos utilizar para compartir un lenguaje y unos valores a la hora de escribir cÃ³digo 
simple y fÃ¡cil de modificar.
Daniel Terhorst-North ha hecho recientemente un nuevo intento para crear una lista de control para un 
buen cÃ³digo. Sostiene que, en lugar de ceÃ±irse a un conjunto de reglas como  SOLID, es mÃ¡s aplicable 
el uso de un conjunto de propiedades a las que aspirar. Ha ideado lo que llama las propiedades 
CUPID  para describir lo que debemos hacer para conseguir un cÃ³digo âalegreâ: el cÃ³digo debe ser 
componible, seguir la filosofÃ­a Unix y ser predecible, idiomÃ¡tico y basado en el dominio.
13. DiseÃ±o inclusivo
Evaluar
Recomendamos a las organizaciones a evaluar diseÃ±o inclusivo  como manera de asegurar que 
la accesibilidad es considerada un requisito de primer nivel. Muy habitualmente los requisitos de 
accesibilidad e inclusividad son ignorados hasta justo antes, o incluso despuÃ©s, del lanzamiento  de software. La forma mÃ¡s barata y simple de incluir estos requisitos, a la vez que damos feedback  pronto a los equipos, es integrÃ¡ndolos completamente en el proceso de desarrollo. En el pasado, 
hemos mencionado tÃ©cnicas que priorizan los requisitos de seguridad y multifuncionales; una 
perspectiva de esta tÃ©cnica es que consigue lo mismo para accesibilidad.
14. PatrÃ³n de operador para recursos no agrupados
Evaluar
Continuamos viendo un incremento en el uso del patrÃ³n de Operador de Kubernetes  con propÃ³sitos 
mÃ¡s allÃ¡ de administrar aplicaciones desplegadas en el clÃºster. Usando el patrÃ³n de operador para 
recursos no agrupados se aprovechan las definiciones de recurso personalizado y el mecanismo 
de programaciÃ³n guiado por eventos implementados en el panel de control de Kubernetes para 
gestionar las actividades que estÃ¡n relacionadas fuera del cluster. Esta tÃ©cnica se basa en la idea 
de Kube-managed cloud services  y la extiende a otras actividades, como despliegue continuo o 
reacciÃ³n a cambios en repositorios externos. Una de las ventajas de Ã©sta tÃ©cnica por encima de 
una herramienta especialmente diseÃ±ada, es que abre una amplia gama de herramientas que ya 
vienen incluidas en Kubernetes o son parte de un ecosistema mÃ¡s amplio. Puedes usar comandos  como diff, dry-run o apply para interactuar con los recursos personalizados del operador. El  mecanismo de programaciÃ³n de Kube hace que el desarrollo sea mÃ¡s fÃ¡cil al eliminar la necesidad 
de orquestar actividades en el orden correcto. Herramientas de cÃ³digo abierto como Crossplane , 
Flux  y Argo CD  aprovechan Ã©sta tÃ©cnica, y esperamos ver mÃ¡s de estas surgir con el tiempo. 
Aunque Ã©stas herramientas tienen sus casos de uso, tambiÃ©n estamos comenzando a ver un mal  uso y abuso de Ã©sta tÃ©cnica y necesitamos repetir un viejo consejo: SÃ³lo porque puedas hacer 
algo con una herramienta no significa que debas. AsegÃºrate de descartar enfoques mÃ¡s sencillos y  convencionales antes de crear una definiciÃ³n de recurso personalizada y asumir la complejidad que 
este enfoque implica.
TÃ©cnicas	
16
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

15. Malla de servicios sin sidecar
Evaluar
Malla de servicios  suele implementarse como un proxy inverso, tambiÃ©n conocido como sidecar, 
desplegado a la par de cada instancia de servicio. Aunque estos procesos de sidecar suelen ser  ligeros, el coste general y la complejidad a la hora de adoptar una malla de servicios se incrementa 
con cada nueva instancia de servicio que requiere un sidecar adicional. Sin embargo, con los 
avances en eBPF , estamos observando un nuevo enfoque de malla de servicios sin sidecar  donde 
las funcionalidades de la malla son transferidas de forma segura al nÃºcleo del SO, habilitando a 
los servicios que se ejecutan en el mismo nodo a comunicarse de forma transparente a travÃ©s de 
sockets, sin necesidad de proxies adicionales. Puedes probar esto con Cilium service mesh  y 
simplificar el despliegue pasando de un proxy por servicio a un proxy por nodo. Estamos intrigados  por las capacidades de eBPF y por eso nos parece importante evaluar estÃ¡ evoluciÃ³n de la malla de 
servicios
16. SLSA
Evaluar
A medida que el software continÃºa aumentando en complejidad, el vector de ataque a travÃ©s de  dependencias de software se convierte en algo cada vez mÃ¡s difÃ­cil contra lo que protegerse. La reciente vulnerabilidad de Log4J nos mostrÃ³ cÃ³mo de difÃ­cil puede ser incluso conocer esas 
dependencias â muchas compaÃ±Ã­as que no usaban Log4J directamente fueron vulnerables sin 
saberlo solo por el hecho de que otro software en su ecosistema dependÃ­a de esta librerÃ­a. Supply  chain Levels for Software Artifacts, o SLSA  (pronunciado âsalsaâ), es un conjunto de guÃ­as curadas 
por un consorcio para que las organizaciones se protejan de ataques contra la cadena de suministro, 
ha sido evolucionado a partir de las guÃ­as internas que Google ha estado usando por aÃ±os. 
Apreciamos que SLSA no promete una âbala de plataâ con un enfoque centrado en herramientas para  proteger la cadena de suministro, sino que proporciona una lista de amenazas concretas y prÃ¡cticas junto con un modelo de madurez. El modelo de amenazas  es fÃ¡cil de entender ya que cuenta con 
ejemplos reales de ataques, y los requisitos  proveen una guÃ­a para ayudar a las organizaciones 
a priorizar acciones basadas en niveles incrementales de robustez para mejorar su postura de 
seguridad en la cadena de suministro. Creemos que SLSA provee consejos aplicables y esperamos  que mÃ¡s organizaciones aprendan de ello.
17. AlmacÃ©n de datos de streaming
Evaluar
La necesidad de responder rÃ¡pidamente a los conocimientos de los clientes ha impulsado la 
creciente adopciÃ³n de arquitecturas basadas en eventos y el procesamiento de flujos. Marcos como 
Spark , Flink  o Kafka Streams  ofrecen un paradigma en el que consumidores y productores de 
eventos simples pueden cooperar en redes complejas para ofrecer informaciÃ³n en tiempo real. Pero 
este estilo de programaciÃ³n requiere tiempo y esfuerzo para dominarlo y, cuando se implementa 
como aplicaciones de un solo punto, carece de interoperabilidad. Hacer que el procesamiento 
de flujos funcione universalmente a gran escala puede requerir una importante inversiÃ³n en  ingenierÃ­a. Ahora, estÃ¡ surgiendo una nueva cosecha de herramientas que ofrece las ventajas del 
procesamiento de flujos a un grupo mÃ¡s amplio y establecido de desarrolladores que se sienten 
cÃ³modos utilizando SQL para implementar los anÃ¡lisis. La estandarizaciÃ³n de SQL como lenguaje 
de flujo universal reduce la barrera para la implementaciÃ³n de aplicaciones de flujo de datos.  Herramientas como ksqlDB  y Materialize ayudan a transformar estas aplicaciones separadas en 
plataformas unificadas. En conjunto, una colecciÃ³n de aplicaciones de streaming basadas en SQL en 
toda una empresa podrÃ­a constituir un almacÃ©n de datos de streaming.
TÃ©cnicas	
17
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

18. TinyML
Evaluar
Hasta hace poco, la ejecuciÃ³n de un modelo de machine-learning (ML) se consideraba costosa 
desde el punto de vista computacional y, en algunos casos, requerÃ­a un hardware de propÃ³sito 
especial. Si bien la creaciÃ³n de los modelos todavÃ­a entra dentro de esta clasificaciÃ³n, es posible 
crearlos de forma que puedan ejecutarse en dispositivos pequeÃ±os, de bajo coste y bajo consumo 
de energÃ­a. Esta tÃ©cnica, denominada TinyML , ha abierto la posibilidad de ejecutar modelos de ML 
en situaciones que muchos podrÃ­an considerar inviables. Por ejemplo, en dispositivos que funcionan 
con baterÃ­as o en entornos desconectados con una conectividad limitada o irregular, el modelo  puede ejecutarse localmente sin un coste prohibitivo. Si te has planteado utilizar el ML pero has 
creÃ­do que no era realista debido a las limitaciones informÃ¡ticas o de red, merece la pena evaluar 
esta tÃ©cnica.
19. Azure Data Factory para orquestaciÃ³n
Resistir
Para las organizaciones que utilizan Azure como su principal proveedor de servicios cloud, Azure 
Data Factor y  es actualmente la herramienta predeterminada en cuanto a la orquestaciÃ³n de 
pipelines de procesamiento de datos. Es compatible con la ingesta de datos, copiando datos 
desde y hacia diferentes tipos de almacenamientos locales o en Azure y ejecutando lÃ³gica de  transformaciÃ³n. Aunque sÃ­ que hemos tenido una experiencia adecuada con Azure Data Factory en  migraciones sencillas de almacenamientos de datos desde local a la nube, no recomendamos el uso 
de Azure Data Factory para orquestaciÃ³n de pipelines complejas de procesamiento de datos y flujos 
de trabajo. Hemos tenido algÃºn Ã©xito con Azure Data Factory cuando lo hemos usado principalmente  para mover datos entre sistemas. Para los pipelines de datos mÃ¡s complejos, todavÃ­a tiene sus retos, incluyendo una depuraciÃ³n mediocre y el informe de errores. Adicionalmente, la observabilidad 
es limitada, ya que la capacidad de registro de Azure Data Factory no se integra con otros  productos como Azure Data Lake Storage o Databricks, lo que hace que sea complicado conseguir 
observabilidad extremo a extremo. Adicionalmente, la disponibilidad de los mecanismos de data 
source-triggering (desencadenamiento de fuente de datos) estÃ¡ limitada a ciertas regiones. En estos  momentos, animamos a usar otras herramientas de cÃ³digo libre para la orquestaciÃ³n (e.g., Airflow 
para las pipelines de datos complejas y limitar el uso de Azure Data Factory para la copia de datos o  la toma de instantÃ¡neas. Nuestros equipos siguen utilizando Data Factory para mover y extraer datos, 
pero para operaciones mÃ¡s amplias recomendamos otras herramientas con un flujo de trabajo mÃ¡s 
completo.
20. Equipos de plataformas miscelÃ¡neas
Resistir
Anteriormente, habÃ­amos presentado los equipos de productos de ingenierÃ­a de plataformas  en 
Adoptar, como una buena manera de operar para los equipos de plataformas internos, que permite  a los equipos de entrega implementar y operar sistemas en menos tiempo y con herramientas mÃ¡s sencillas. Desafortunadamente, estamos viendo la etiqueta de âequipo de plataformasâ 
aplicada a equipos dedicados a proyectos que no tienen resultados claros o un conjunto de  clientes bien definido. Como consecuencia, estos equipos de plataformas miscelÃ¡neas , como los  llamamos, tienen dificultades para entregar debido a las altas cargas cognitivas y a la falta de una 
alineaciÃ³n clara de prioridades, ya que estÃ¡n lidiando con una colecciÃ³n miscelÃ¡nea de sistemas  que no tienen relaciÃ³n entre sÃ­. De hecho, se convierten en otro equipo de apoyo general para las 
cosas que no encajan, o que no quieren en otros lados. Seguimos creyendo que los equipos de  productos de ingenierÃ­a de plataformas centrados en un producto (interno) claro y bien definido 
ofrecen una mejor serie de resultados.
TÃ©cnicas	
18
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

21. Datos de producciÃ³n en entornos de prueba
Resistir
Seguimos viendo a los datos de producciÃ³n en entornos de prueba como un Ã¡rea de preocupaciÃ³n.  En primer lugar, muchos casos de esto han resultado en un daÃ±o considerable, por ejemplo, 
cuando se ha enviado una alerta incorrecta desde un sistema de prueba a toda una poblaciÃ³n 
de clientes. En segundo lugar, el nivel de seguridad, especÃ­ficamente en torno a la protecciÃ³n de 
datos privados, tiende a ser menor para los sistemas de prueba. No tiene mucho sentido tener 
controles elaborados sobre el acceso a los datos de producciÃ³n si esos datos son copiados a una  base de datos de prueba a la que pueden acceder las personas desarrolladoras y QAs. Aunque 
es posible ofuscar la informaciÃ³n, esto tiende a aplicarse sÃ³lo a campos especÃ­ficos, tal como  nÃºmeros de tarjetas de crÃ©dito. Por Ãºltimo, copiar datos de producciÃ³n a sistemas de prueba 
puede infringir las leyes de privacidad, por ejemplo, cuando los sistemas de prueba se alojan o son 
accedidos desde un paÃ­s o regiÃ³n diferente. Este Ãºltimo escenario es especialmente problemÃ¡tico  con despliegues complejos en la nube. Los datos falsos son un enfoque mÃ¡s seguro, y existen  herramientas para ayudar en su creaciÃ³n. Reconocemos que hay razones para copiar elementos 
especÃ­ficos de los datos de producciÃ³n, por ejemplo, en la reproducciÃ³n de una incidencia o para 
el entrenamiento de modelos ML especÃ­ficos. AquÃ­ nuestro consejo es proceder con precauciÃ³n.
22. SPA por defecto
Resistir
Generalmente evitamos poner blips en Espera cuando consideramos que la recomendaciÃ³n es  demasiado obvia, lo que incluye seguir a ciegas un estilo mÃ¡s architectural sin poner atenciÃ³n 
a los trade-offs. Sin embargo, debido a la dominancia pura por parte de los equipos al elegir 
aplicaciones de una pÃ¡gina (Single-Page Applications o SPA) cuando realmente lo que necesitan  es una pÃ¡gina web, nos preocupa hasta tal punto que nos sugiere que no estÃ¡n ni tan siquiera  reconociendo SPAs como un estilo de arquitectura y, en vez de ello, se lanzan directamente 
a la elecciÃ³n de la infraestructura. SPAs generan una complejidad extra que no existe con  las tradicionales pÃ¡ginas web (server-web based): herramientas de optimizaciÃ³n, gestiÃ³n y 
administraciÃ³n del historial de bÃºsqueda, anÃ¡lisis web, tiempo de carga de la pÃ¡gina principal,  etc. Esta complejidad estÃ¡ generalmente justificada por temas relacionados con la experiencia 
de usuario, y tanto es asÃ­, que se continÃºan desarrollando herramientas para poder abordar con  mayor facilidad estas preocupaciones (aunque la agitaciÃ³n en la comunidad de React en torno 
a la gestiÃ³n de estado nos da una pista de lo difÃ­cil que puede ser el obtener una soluciÃ³n de 
aplicabilidad general).
Sin embargo, muy a menudo nos encontramos con equipos que no realizan anÃ¡lisis de trade- off, sino que aceptan a ciegas esa complejidad extra de âSPA por defectoâ incluso cuando  las necesidades de negocio no la justifican. De hecho, hemos empezado a notar que muchos 
desarrolladores ânovatosâ ni siquiera son conscientes de un mÃ©todo alternativo a SPA, pues han  pasado la mayorÃ­a de su carrera trabajando con infraestructuras similares a React. Creemos que 
muchas pÃ¡ginas web se beneficirarÃ¡n de la simplicidad de la lÃ³gica de servidor server-side, y 
nos apoyamos en tÃ©cnicas como Hotwire  que ayudan a acortar ese brecha en la experiencia de 
usuario.
TÃ©cnicas	
19
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Plataformas	
ResistirResistir
Evaluar Evaluar
Probar Probar
A doptar Adoptar	
3	1	
24	3 5
36
37	
38	
39	
4 0	
41	4 2	
43	
29	
3 4	
32	
4	12	
13	
14	
19	
2 1
15	2 0	
16	
17
2 2	
18	
5
6	
7	
9	11	
4 5	49	
60	
61	62	63	
64	65
66	67
68
69
70
7 1	
5 1	53
57	58	
59	
52	
74	
7 9	8 0	
81	
82	83	84
85	
86	87
88
89	9 0
91	
93
92	
75	
7 7	
78	
3 0
31	
72	
73	
7 6	
2	
8	10	
2 3	
25
26	
27	28	
3 3	
4 4
46	
47	
50
54	55	
56	
4 8	
Adoptar
â
Probar
23.  Azure DevOps
24.  Las plantillas de Azure Pipelines
25.  CircleCI
26.  Couchbase
2 7.  eBPF
28.  GitHub Actions
29.  GitLab CI/CD
30.  Google BigQuery ML
31.  Google Cloud Dataflow
32.  Flujos de trabajo reutilizables en Github 
Actions
33.  Kubernetes
34.  VerneMQ  
Evaluar
35.  actions-runner-controller
36.  Apache Iceberg
3 7.  Blueboat
38.  Cloudflare Pages
39.  Colima
40.  Collibra
41.  CycloneDX
42.  Embeddinghub
43.  Temporal  
Resistir â	
Nuevo Desplazado
adentro/afuera NingÃºn 
cambio
Thoughtworks Technology RadarÂ© Thoughtworks, Inc. All Rights Reserved.  

23. Azure DevOps 
Probar
Debido a que el ecosistema Azure DevOps  sigue creciendo, nuestros equipos lo utilizan con Ã©xito 
cada vez mÃ¡s. Estos servicios contienen un conjunto de servicios administrados, que incluyen  repositorios Git alojados, pipelines de compilaciÃ³n y despliegue, herramientas de pruebas 
automatizadas, utilidades de administraciÃ³n de tareas pendientes y repositorio de artefactos.  Hemos visto a nuestros equipos adquirir experiencia en esta plataforma con muy buenos resultados,  lo que significa que Azure DevOps estÃ¡ madurando. Particularmente nos gusta su flexibilidad; 
permitiÃ©ndonos usar los servicios que uno desea incluso si estos son de diferentes proveedores. Por 
ejemplo, es posible usar un repositorio Git externo al mismo tiempo que los servicios de pipelines de 
Azure DevOps. Nuestros equipos estÃ¡n especialmente entusiasmados con Azure DevOps Pipelines . 
A medida que el ecosistema madura, vemos un aumento en la incorporaciÃ³n de equipos que ya se  encuentran utilizando Azure, ya que se integra fÃ¡cilmente con el resto del mundo de Microsoft.
24. Las plantillas de Azure Pipelines
Probar
Las plantillas de Azure Pipelines  te permiten eliminar la duplicidad cuando defines tus Azure 
Pipelines, a travÃ©s de dos mecanismos. Con âincluirâ plantillas, puedes hacer referencia a una  plantilla, de tal forma que se expandirÃ¡ en lÃ­nea como una macro parametrizado de C++, permitiendo 
asÃ­ una forma sencilla de reutilizar una configuraciÃ³n comÃºn en todas las etapas, trabajos y pasos. 
Con âextenderâ plantilla, puedes definir una capa exterior con la configuraciÃ³n comÃºn de la pipeline 
y con la aprobaciÃ³n requerida de la plantilla , puedes hacer que la compilaciÃ³n falle si la pipeline no 
extiende de ciertas plantillas, previniendo ataques maliciosos contra la propia configuraciÃ³n de la  pipeline. Junto con [CircleCI]((/es/radar/platforms/circleci), Orbs y el todavÃ­a mÃ¡s reciente GitHub 
Actions Reusable Workflows , las plantillas de Azure Pipeline son parte de la tendencia sobre la 
creaciÃ³n de modularidad en el diseÃ±o de las pipelines a lo largo de mÃºltiples plataformas y varios de  nuestros equipos han estado contentos utilizÃ¡ndolo.
25. CircleCI
Probar
Muchos de nuestros equipos eligen CircleCI  para sus necesidades de integraciÃ³n continua y 
aprecian su capacidad para ejecutar pipelines complejas de manera eficiente. Los desarrolladores  de CircleCI continÃºan agregando nuevas funcionalidades con CircleCI, ahora en la versiÃ³n 3.0. 
Orbs  y executors  fueron llamados por nuestros equipos como particularmente Ãºtiles. Los Orbs son 
fragmentos de cÃ³digo reutilizables que automatizan procesos repetidos, aceleran la configuraciÃ³n 
de proyectos y facilitan la integraciÃ³n con herramientas de terceros. La amplia variedad de tipos 
de ejecutores brinda flexibilidad para configurar trabajos en mÃ¡quinas virtuales en Docker, Linux,  macOS o Windows..
26. Couchbase
Probar
Cuando cubrimos Couchbase  originalmente en 2013, se consideraba principalmente un cachÃ© 
persistente que evolucionÃ³ de una fusiÃ³n entre Membase  y CouchDB. Desde ese entonces, ha 
mejorado constantemente y un ecosistema de herramientas relacionadas y oferta comercial se ha 
desarrollado en torno a Ã©l. Entre las novedades de la suite del producto estÃ¡ Couchbase Mobile y 
el Couchbase Sync Gateway. Estas funcionalidades trabajan juntas para mantener actualizados los 
datos persistentes en dispositivos perifÃ©ricos, incluso cuando el dispositivo estÃ¡ fuera de red por 
Plataformas	
21
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

periodos extendidos de tiempo debido a conectividad intermitente. A medida que estos dispositivos 
proliferan, vemos una necesidad mÃ¡s grande de persistencia embebida que continÃºa funcionando 
estÃ© el dispositivo conectado o no. Recientemente, uno de nuestros equipos valorÃ³ Couchbase  por su capacidad de sincronizaciÃ³n offline, y descubriÃ³ que esta capacidad existente les ahorrÃ³ 
considerable esfuerzo que de otra manera lo habrÃ­an tenido que invertir en tiempo propio.
27. eBPF
Probar
Desde hace varios aÃ±os, el kernel de Linux incluye el filtro de paquetes Berkeley extendido 
( eBPF ), una mÃ¡quina virtual que ofrece la posibilidad de adjuntar filtros a determinados sockets. 
Pero eBPF va mucho mÃ¡s allÃ¡ del filtrado de paquetes y permite activar scripts personalizados 
en varios puntos del kernel con muy poca sobrecarga. Aunque esta tecnologÃ­a no es nueva, estÃ¡ 
cobrando importancia con el creciente uso de microservicios desplegados como contenedores 
orquestados. Los Kubernetes y la tecnologÃ­a de malla de servicios (Services Mesh) como Istio  se 
utilizan habitualmente, y emplean sidecars para implementar la funcionalidad de control. Con las  nuevas herramientas, â Bumblebee  en particular, hace que la construcciÃ³n, ejecuciÃ³n y distribuciÃ³n 
de programas eBPF sea mucho mÃ¡s fÃ¡cil - eBPF puede ser visto como una alternativa al sidecar  tradicional. Un mantenedor de Cilium , una herramienta en este espacio, ha llegado a proclamar 
la desapariciÃ³n del sidecar . Un enfoque basado en eBPF reduce parte de la sobrecarga de 
rendimiento y operaciÃ³n que conllevan los sidecars, pero no da soporte a funciones comunes como  la terminaciÃ³n SSL.
28. GitHub Actions
Probar
GitHub Actions  ha crecido considerablemente el aÃ±o pasado. Ha demostrado que puede asumir 
flujos de trabajo mÃ¡s complejos y llamar a otras acciones en acciones compuestas, entre otras cosas. 
Sin embargo, todavÃ­a tiene algunas deficiencias, como su incapacidad para volver a activar un solo  trabajo de un flujo de trabajo. Aunque el ecosistema en el GitHub Marketplace  tiene sus ventajas 
obvias, al dar acceso a las Acciones de GitHub de terceros a tu pipeline de construcciÃ³n se corre 
el riesgo de compartir secretos de forma insegura (recomendamos seguir los consejos de GitHub 
sobre security hardening ). Sin embargo, la comodidad de crear tu flujo de trabajo de compilaciÃ³n 
directamente en GitHub junto a tu cÃ³digo fuente, combinada con la capacidad de ejecutar las 
Acciones de GitHub localmente utilizando herramientas de cÃ³digo abierto como act  es una opciÃ³n 
convincente que ha facilitado la configuraciÃ³n y la incorporaciÃ³n de nuestros equipos.
29. GitLab CI/CD
Probar
Si estÃ¡s usando GitLab  para administrar la entrega de software, tambiÃ©n deberÃ­as darle un vistazo 
a GitLab CI/CD para tus necesidades de integraciÃ³n y entrega continua. Hemos comprobado que  es especialmente Ãºtil cuando se utiliza con GitLab con ejecutores en servidores OnPremise y auto- hospedados, ya que esta combinaciÃ³n evita los dolores de cabeza de autorizaciÃ³n que suelen 
producirse al utilizar una soluciÃ³n basada en la nube. Los ejecutores auto-hospedados pueden 
configurarse completamente para tus propÃ³sitos con el sistema operativo y las dependencias 
correctas instaladas y, como resultado, los pipelines pueden ejecutarse mucho mÃ¡s rÃ¡pido que 
cuando se utiliza un ejecutor hospedado en la nube que necesita configurarse cada vez.
AdemÃ¡s de las operaciones bÃ¡sicas de compilaciÃ³n, pruebas y despliegue de pipelines, este  producto de GitLab soporta el uso de Services, Auto Devops y ChatOps, entre otras caracterÃ­sticas 
avanzadas. Los Services son Ãºtiles al ejecutar servicios Docker como Postgres o Testcontainer  
Plataformas	
22
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

vinculados a un job para integraciÃ³n y pruebas de extremo a extremo. Auto Devops crea pipelines que no requieren configuraciones, lo que es muy Ãºtil para los equipos que son nuevos en la entrega 
contÃ­nua o para las organizaciones con muchos repositorios que de otro modo tendrÃ­an que crear  muchos pipelines manualmente.
30. Google BigQuery ML
Probar
Desde la Ãºltima vez que hablamos sobre Google BigQuery ML , se han agregado modelos mÃ¡s 
sofisticados, como Redes Neuronales Profundas y de Tablas con Autoaprendizaje, al conectar  BigQuery ML con TensorFlow y Vertex AI como backend. BigQuery tambiÃ©n ha introducido 
soporte para previsiÃ³n de series temporales. Una de nuestra preocupaciones anteriormente era  explicabilidad . A principios de este aÃ±o fue anunciado BigQuery Explainable AI , dando un paso 
para abordar esto. TambiÃ©n podemos exportar modelos de BigQuery ML a Cloud Storage como 
Tensorflow SavedModel y usarlos para predicciÃ³n online. AÃºn quedan cosas pendientes como  la dificultad de âentrega continua para aprendizaje automÃ¡ticoâ pero con su facilidad de uso, 
BigQuery ML sigue siendo una opciÃ³n atractiva, sobre todo si la informaciÃ³n ya estÃ¡ en BigQuery.
31. Google Cloud Dataf low
Probar
Google Cloud Dataflow  es un servicio de procesamiento de datos basado en la nube para 
aplicaciones de transmisiÃ³n de datos en tiempo real y por lotes. Nuestros equipos usan Dataflow  para crear flujos de procesamiento para integrar, preparar y analizar grandes conjuntos de datos, 
con el modelo de programaciÃ³n unificado de Apache Beam  para facilitar su administraciÃ³n. 
Presentamos Dataflow por primera vez en 2018, y su estabilidad, rendimiento y conjunto completo 
de funciones nos dan confianza para moverlo a la secciÃ³n Trial en esta ediciÃ³n del Radar.
32. Flujos de trabajo reutilizables en Github Actions 
Probar
Hemos visto un creciente interÃ©s en GitHub Actions  desde que se hizo blip por primera vez hace 
dos Radars. Con el lanzamiento de los flujos de trabajo reutilizables , GitHub ha continuado 
evolucionando el producto de forma que aborda algunas de sus limitaciones iniciales. Los flujos de  trabajo reutilizables en GitHub Actions aportan modularidad al diseÃ±o de pipeline, permitiendo la  reutilizaciÃ³n parametrizada incluso a travÃ©s de repositorios (siempre y cuando el repositorio de flujo 
de trabajo sea pÃºblico). Soportan el paso explÃ­cito de valores confidenciales en forma de secretos, 
y el resultado es devuelto al proceso que lo ha llamado. Con unas cuantas lineas de YAML, Github 
Actions ahora aporta el tipo de flexibilidad que se ve en CircleCI  Orbs o Azure Pipeline Templates, 
pero sin tener que dejar la plataforma GitHub.
33. Sealed Secrets
Probar
Kubernetes  soporta de manera nativa un objeto valor-clave (key-value en inglÃ©s) como secretos 
(secrets en inglÃ©s). Sin embargo, por defecto los secret en Kubernetes no son realmente secretos.  Estos son manipulados separadamente de otros datos key-value con precauciones o con controles 
de acceso que pueda ser aplicados de manera separada. Existe soporte para encriptar secrets 
antes de que sean guardados en etcd , pero estos empiezan como campos de texto plano en 
archivos de configuraciÃ³n. 
Plataformas	
23
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Secretos sellados (Sealed Secrets en inglÃ©s) es una combinaciÃ³n entre el operador y la utilidad 
de la lÃ­nea de comando que usa claves asimÃ©tricas para encriptar secrets de manera que solo  pueden ser descifrados por un controlador del clÃºster. Este proceso asegura que los secrets 
no serÃ¡n comprometidos mientras se encuentren en los archivos de configuraciÃ³n que definen 
el deployment en Kubernetes. Una vez encriptados, estos archivos pueden ser compartidos o 
almacenados de manera segura junto a otros artefactos del deployment.
34. VerneMQ
Probar
VerneMQ  es un broker MQTT de cÃ³digo abierto, de alto rendimiento y distribuÃ­do. En el pasado 
hemos analizado otros brokers MQTT como Mosquitto  y EMQ. Al igual que EMQ y RabbitMQ, 
VerneMQ estÃ¡ tambiÃ©n basado en Erlang/OTP, lo que lo convierte en altamente escalable. Escala  tanto horizontalmente como verticalmente sobre hardware estÃ¡ndar para soportar un gran nÃºmero 
de productores y consumidores, manteniendo una baja latencia y tolerancia a fallos. En nuestros  bancos de pruebas internos, hemos sido capaces de alcanzar algunos millones de conexiones 
concurrentes en un Ãºnico clÃºster. Aunque no es nuevo, llevamos utilizÃ¡ndolo ya en producciÃ³n 
durante algÃºn tiempo y nos ha funcionado bien.
35. actions-runner-controller
Evaluar
actions-runner-controller  es un controlador que opera runners auto hospedados  para GitHub 
Actions  en tu clÃºster de Kubernetes. Con esta herramienta creas un recurso runner en Kubernetes, 
que ejecutarÃ¡ y operarÃ¡ el runner auto hospedado. Los runners auto hospedados son de mucha 
ayuda en escenarios dÃ³nde el trabajo o job que tu GitHub Actions ejecuta necesita acceder a  recursos a los que los runners en la nube de GitHub no pueden acceder o tienen requerimientos 
especÃ­ficos para un sistema operativo y entorno que difieren de los que provee GitHub. En esos 
casos dÃ³nde tienes un clÃºster de Kubernetes, puedes ejecutar tus runners auto hospedados como  un pod de Kubernetes, con la capacidad de escalar conectÃ¡ndose con los eventos de los webhooks 
de GitHub.
36. Apache Iceberg
Evaluar
Apache Iceberg  Es un formato de tabla abierta para conjuntos de datos analÃ­ticos muy grandes. 
Iceberg admite operaciones de datos analÃ­ticos modernos, como la inserciÃ³n, actualizaciÃ³n 
y eliminaciÃ³n a nivel de registro, time-travel queries ,  transacciones ACID, particiÃ³n oculta y 
evoluciÃ³n completa del esquema . Soporta mÃºltiples formatos de almacenamiento de archivos 
subyacentes como Apache Parquet , Apache ORC y Apache Avro . Muchos motores de 
procesamiento de datos soportan Apache Iceberg, incluyendo motores SQL como Dremio  y Tr i n o asÃ­ 
como motores de streaming (estructurado) como Apache Spark  y Apache Flink.
Apache Iceberg estÃ¡ en la misma categorÃ­a que Delta Lake  y Apache Hudi. Todos ellos soportan 
mÃ¡s o menos caracterÃ­sticas similares, pero cada uno difiere en las implementaciones subyacentes 
y en las listas de caracterÃ­sticas detalladas. Iceberg es un formato independiente y no es nativo de  ningÃºn motor de procesamiento especÃ­fico, por lo que es soportado por un nÃºmero creciente de 
plataformas, incluyendo AWS Athena  y Snowflake. Por la misma razÃ³n, Apache Iceberg, a diferencia 
de los formatos nativos como Delta Lake, puede no beneficiarse de las optimizaciones cuando se  utiliza con Spark.
Plataformas	
24
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

37. Blueboat
Evaluar
Blueboat  es una plataforma multiplataforma para aplicaciones web sin servidor. Aprovecha el 
popular motor JavaScript V8 e implementa bibliotecas de aplicaciones web de uso comÃºn de 
forma nativa en Rust  para seguridad y rendimiento. Se puede pensar en Blueboat como una 
alternativa a CloudFlare Workers  o Deno Deploy pero con una distinciÃ³n importante â debe 
operar y administrar la infraestructura subyacente. Dicho esto, le recomendamos que evalÃºe 
cuidadosamente Blueboat para sus necesidades locales sin servidor.
38. Cloudf lare Pages
Evaluar
Cuando los Cloudflare Workers  fueron liberados, los destacamos como una temprana funciÃ³n como 
servicio (FaaS) para cÃ³mputo de borde con una implementaciÃ³n interesante. La liberaciÃ³n de las 
Cloudflare Pages  del pasado Abril no se sintiÃ³ como digna de menciÃ³n, debido a que Pages es solo 
una en una clase de muchas soluciones respaldadas por Git y de alojamiento de sitios. TenÃ­a vistas  previas continuas, una funciÃ³n Ãºtil que no se encuentra en la mayorÃ­a de las alternativas. Ahora, sin 
embargo, Cloudfare tiene unos Workers y Pages integrados , mÃ¡s enfocados, creando una soluciÃ³n 
totalmente integrada Jamstack  corriendo sobre el CDN. La inclusiÃ³n de un almacenamiento de 
clave-valor y una coordinaciÃ³n primitiva fuerte y consistente mejoran aÃºn mÃ¡s el atractivo de la  nueva versiÃ³n de Cloudflare Pages.
39. Colima
Evaluar
Colima  se estÃ¡ convirtiendo en una popular alternativa abierta para Docker de Escritorio. Provee el 
tiempo de ejecuciÃ³n del contenedor Docker en una mÃ¡quina virtual (VM) Lima, configura la interfaz 
de lÃ­nea de comando (CLI) en macOS y se encarga del port-forwarding y montaje de volÃºmenes. 
Colima utiliza containerd  como tiempo de ejecuciÃ³n, que es el mÃ¡s comÃºn en la mayorÃ­a de 
servicios Kubernetes (lo cual mejora la relaciÃ³n dev-prod). Con Colima se puede utilizar y probar  fÃ¡cilmente las Ãºltimas caracterÃ­sticas de containerd, tal como lazy loading para imÃ¡genes en 
contenedores. Por su buen desempeÃ±o, vemos a Colima como una alternativa de software abierto 
con un gran potencial frente a Docker para Escritorio.
40. Collibra 
Evaluar
En el ambiente cada vez mÃ¡s concurrido del mercado corporativo de catÃ¡logos de data, nuestros 
equipos han disfrutado al trabajar con Collibra . Les ha gustado la flexibilidad en el despliegue sea 
de un SaaS o de una instancia self-hosted, la gran amplitud de funcionalidad incluida out of the box,  tales como gobernanza de datos, linaje, calidad y observabilidad. Los usuarios tambiÃ©n tienen la 
opciÃ³n de utilizar un pequeÃ±o subconjunto de capacidades requeridas desde una aproximaciÃ³n mÃ¡s 
descentralizada como por ejemplo un data mesh . El verdadero punto extra se lo lleva su servicio al 
cliente muchas veces pasado por alto, quienes se han mostrado colaborativos y de gran soporte. Por 
supuesto, existe tensiÃ³n entre catÃ¡logos de data simples y plataformas corporativas con mayores  funcionalidades, pero hasta ahora los equipos se encuentran muy contentos en cÃ³mo Collibra ha 
soportado sus necesidades.
Plataformas	
25
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

41. CycloneDX
Evaluar
CycloneDX  es un estÃ¡ndar para describir una  Lista de materiales de software (SBOM) legible por 
una mÃ¡quina. A medida que las estructuras del software y la computaciÃ³n crecen en complejidad, 
el software se vuelve mÃ¡s difÃ­cil de definir. Originario de OWASP, CycloneDX mejora el antiguo 
estÃ¡ndar SPDX con una definiciÃ³n mÃ¡s amplia que se extiende mÃ¡s allÃ¡ de las dependencias 
de la mÃ¡quina local para incluir dependencias de servicio en tiempo de ejecuciÃ³n. TambiÃ©n 
encontrarÃ¡s implementaciones en varios lenguajes, un ecosistema  de soporte de integraciones y 
una herramienta de lÃ­nea de comando CLI que te permite analizar y cambiar SBOMs con la firma y 
verificaciÃ³n correspondientes.
42. Embeddinghub
Evaluar
Embeddinghub  es una base de datos vectorial para embeddings , de machine learning, bastante 
similar a Milvus . Sin embargo, dado que tiene soporte disponible para operaciones de vecino 
mÃ¡s cercano (NN) aproximado, particionamiento, control de versiones y control de accesos, 
recomendamos evaluar Embeddinghub para sus casos de embedding vectorial.
43. Temporal 
Evaluar
Temporal  es una plataforma para desarrollar flujos de trabajo de larga duraciÃ³n, particularmente 
para arquitecturas de microservicios. Es una variante del anterior proyecto de OSS Cadence  de 
Uber, tiene un modelo de aprovisionamiento de eventos para flujos de trabajo de larga duraciÃ³n para 
que puedan sobrepasar caÃ­das de procesos o infraestructura. A pesar de que no recomendamos 
el uso de transacciones distribuidas en arquitectura de microservicios, si realmente necesitas  implementarlas o larga duraciÃ³n Sagas , puedes considerar darle un vistazo a Temporal.
Plataformas	
26
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Herramientas
Adoptar
44.  tfsec
Probar
45.  AKHQ
46.  cert-manager
4 7.  Cloud Carbon Footprint
48.  Conftest
49.  kube-score
50.  Lighthouse
51.  Metaflow
52.  Micrometer
53.  NUKE
54.  Pactflow
55.  Podman
56.  Grafo de CÃ³digo Fuente
5 7.  Syft
58.  Volta
59.  Web Test Runner 
Evaluar
60.  CDKTF
61.  Chrome Recorder panel
62.  Excalidraw
63.  GitHub Codespaces
64.  GoReleaser
65.  Grype
66.  Infracost
6 7.  jc
68.  skopeo
69.  SQLFluff
70.  Validador de Terraform
71.  Typesense  
Resistir â	
Resistir Resistir
Evaluar Evaluar
Probar Probar
A doptar Adoptar	
3	1	
24	3 5
36
37	
38	
39	
4 0	
41	4 2	
43	
29	
3 4	
32	
4	12	
13	
14	
19	
2 1
15	2 0	
16	
17
2 2	
18	
5
6	
7	
9	11	
4 5	49	
60	
61	62	63	
64	65
66	67
68
69
70
7 1	
5 1	53
57	58	
59	
52	
74	
7 9	8 0	
81	
82	83	84
85	
86	87
88
89	9 0
91	
93
92	
75	
7 7	
78	
3 0
31	
72	
73	
7 6	
2	
8	10	
2 3	
25
26	
27	28	
3 3	
4 4
46	
47	
50
54	55	
56	
4 8
Nuevo
Desplazado
adentro/afuera NingÃºn 
cambio
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

44. tfsec
AdoptarPara nuestros proyectos en los que usamos Terraform , tfsec se ha convertido rÃ¡pidamente en una 
herramienta de anÃ¡lisis estÃ¡tico predeterminada para detectar potenciales riesgos de seguridad. Es 
fÃ¡cil de integrar en un CI pipeline y tiene una librerÃ­a en crecimiento de verificaciones contra todos  los principales proveedores y plataformas de la nube como Kubernetes. Dado que es sencillo de usar, 
creemos que tfsec podrÃ­a ser una buena adiciÃ³n para cualquier proyecto de Terraform.
45. AKHQ
Probar
AKHQ  es una interfaz de usuario para Apache Kafka que permite administrar topics, informaciÃ³n 
sobre los topics, grupos de consumidores y mucho mÃ¡s. Algunos de nuestros equipos han  encontrado a AKHQ como una herramienta efectiva para mirar en tiempo real el estado de su 
aplicaciÃ³n Kafka. Puedes, por ejemplo, buscar los distintos topics dentro de tu grupo. Por cada  topic, puedes visualizar el nombre, el nÃºmero de mensajes almacenados, el espacio de disco usado, 
el tiempo del Ãºltimo registro, el nÃºmero de particiones, el factor de replicaciÃ³n con la cantidad de  in-sync y el grupo de consumidores. Con opciones de deserializaciÃ³n para Avro y Protobuf, AKHQ 
puede ayudarte a entender el flujo de informaciÃ³n en tu aplicaciÃ³n Kafka.
46. cert-manager 
Probar
cert-manager  es una herramienta para administrar los certificados X.509 dentro de tu cluster 
de  Kubernetes.  Modela certificados y emisores como tipos de recursos de primera clase y 
provee certificados como servicio de manera segura a los desarrolladores y aplicaciones que 
se ejecutan dentro del cluster de Kubernetes. La opciÃ³n obvia cuando se utiliza el controlador  ingress predeterminado de Kubernetes, tambiÃ©n se recomienda para otros y es preferible por 
sobre el despliegue de tu propia gestiÃ³n manual de certificados. Varios de nuestros equipos han  estado utilizando cert-manager ampliamente y tambiÃ©n hemos comprobado que su usabilidad ha  mejorado mucho en los Ãºltimos meses.
47. Cloud Carbon Footprint
Probar
Cloud Carbon Footprint  (CCF) es una herramienta de cÃ³digo abierto que utiliza APIs en la nube para 
proporcionar visualizaciones de emisiones de carbono estimadas en funciÃ³n del uso de AWS, GCP 
y Azure. El equipo de Thoughtworks ha usado con Ã©xito la herramienta con varias organizaciones,  incluyendo empresas de tecnologÃ­a energÃ©tica, minoristas, proveedores de servicios digitales y 
empresas que utilizan AI. Los proveedores de plataformas en la nube se dan cuenta de que es  importante ayudar a sus clientes a comprender el impacto de carbono del uso de sus servicios, 
por lo que han comenzado a crear una funcionalidad similar por su cuenta. Debido a que CCF es  independiente de la nube, permite a los usuarios ver el uso de energÃ­a y las emisiones de carbono de 
mÃºltiples proveedores de nube en un solo lugar, al tiempo que muestra las huellas de carbono en un  impacto en el mundo real, como vuelos o Ã¡rboles plantados.
En versiones recientes, CCF comenzÃ³ a incluir recomendaciones de optimizaciÃ³n de Google Cloud y 
AWS junto con ahorros potenciales de energÃ­a y CO2, asÃ­ como para admitir mÃ¡s tipos de instancias  en la nube, como instancias de GPU. Dada la tracciÃ³n que ha recibido la herramienta y la adiciÃ³n 
continua de nuevas caracterÃ­sticas, nos sentimos confiados en trasladarla a la versiÃ³n de prueba.
Herramientas	
28
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

48. Conftest
Probar
Conftest  es una herramienta para escribir pruebas usando datos de configuraciÃ³n estructurados. 
Se apoya en el lenguaje Rego  de Open Policy Agent para escribir pruebas de configuraciones de 
Kubernetes  definiciÃ³n de pipelines de Tekton  o incluso planes de Terraform. Hemos tenido muy 
buenas experiencias con Conftest; y con su fÃ¡cil curva de aprendizaje. Con la rapidez de los tests,  nuestros equipos pueden iterar mÃ¡s rÃ¡pido y seguro cuando hacen cambios de configuraciÃ³n de 
Kubernetes.
49. kube-score
Probar kube-score  Es una herramienta que permite realizar anÃ¡lisis de cÃ³digo estÃ¡tico de las definiciones 
de objetos Kubernetes. El resultado es una lista de recomendaciones sobre lo que se puede mejorar  para que su aplicaciÃ³n sea mÃ¡s segura y resistente. Tiene una lista de verificaciones predefinidas  
que incluye las mejores prÃ¡cticas, como ejecutar contenedores con privilegios non-root y especificar 
correctamente los lÃ­mites de recursos. El kuber-score ha existido durante algÃºn tiempo y lo hemos  usado en algunos proyectos como parte de un canal CD para los manifiestos de Kubernetes. Un  inconveniente importante de kube-score es que no puede agregar polÃ­ticas personalizadas. 
Normalmente lo complementamos con otras herramientas como Conftest  en estos casos.
50. Lighthouse
Probar
Lighthouse  es una herramienta desarrollada por Google para evaluar aplicaciones y pÃ¡ginas 
web, recolectando mÃ©tricas de desempeÃ±o y aprendizajes sobre buenas prÃ¡cticas de desarrollo.  Desde hace tiempo hemos abogado por las pruebas de desempeÃ±o como ciudadanos de primera 
categorÃ­a , y las actualizaciones a Lighthouse que mencionamos hace cinco aÃ±os ciertamente 
ayudaron con eso. Nuestro concepto alrededor de funciones de aptitud arquitectural  creÃ³ una 
motivaciÃ³n fuerte para tener herramientas como Lighthouse para ser ejecutadas en secuencias 
automatizadas de compilaciÃ³n. Con la introducciÃ³n de Lighthouse CI , se ha vuelto mÃ¡s fÃ¡cil que 
nunca incluir Lighthouse en secuencias automatizadas de compilaciÃ³n administradas por varias 
herramientas .
51. Metaf low
Probar
Metaflow  es una librerÃ­a de Python y un servicio de back-end fÃ¡cil de usar que ayuda a los data 
scientists e ingenieros de data a construir y administrar el procesamiento de datos que estÃ¡ listo  para producciÃ³n, Machine Learning (ML) y flujos de inferencia. Metaflow provee APIs de Python 
que estructuran el cÃ³digo como un grafo dirigido de pasos. Cada paso puede ser decorado con 
configuraciones flexibles como los recursos requeridos de procesamiento y de almacenamiento.  Los artefactos de cÃ³digo y de data para ejecutar cada uno de los pasos (conocido como tarea) 
estÃ¡n almacenados y pueden ser recuperados ya sea para futuras ejecuciones o para los prÃ³ximos  pasos en el flujo, permitiÃ©ndole recuperarse de los errores, ejecuciones repetitivas y seguimiento de 
versiones de modelos y sus dependencias a travÃ©s de mÃºltiples ejecuciones.
La propuesta de valor de Metaflow es la simplicidad de la librerÃ­a idiomÃ¡tica de Python: se integra 
completamente con la infraestructura construida y con el tiempo de ejecuciÃ³n para habilitar la 
ejecuciÃ³n de la ingenierÃ­a de data y las tareas cientÃ­ficas en ambientes de producciÃ³n locales 
Herramientas	
29
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

y escalados. Al momento de escribir este artÃ­culo, Metaflow se integra fuertemente con los servicios AWS como S3 por su servicio de almacenamiento de data y las funciones de paso para la orquestaciÃ³n. Metaflow admite el lenguaje R ademÃ¡s de Python. Sus funcionalidades principales son 
open source.
Si estÃ¡s construyendo y desplegando tu producciÃ³n de Machine Learning (ML) y de data-processing  pipelines en AWS, Metaflow es una estructura alterna, ligera y full-stack para plataformas mÃ¡s 
complejas como MLflow .
52. Micrometer
Probar
Micrometer  es una librerÃ­a de tipo plataforma-agnÃ³stica para la instrumentaciÃ³n de mÃ©tricas en 
la mÃ¡quina virtual de Java (JVM) que soporta Graphite, New Relic, CloudWatch y muchas otras 
integraciones. Hemos encontrado que Micrometer ha beneficiado tanto a autores de librerÃ­as 
como a equipos: autores de librerÃ­as pueden incluir cÃ³digo de instrumentaciÃ³n de mÃ©tricas en sus  librerÃ­as sin necesidad de dar soporte a cada uno de los sistemas de mÃ©tricas que los usuarios estÃ©n 
utilizando; y los equipos pueden soportar muchos tipos de distintos de mÃ©tricas en registros de  back-end, lo que permite que las organizaciones colecten mÃ©tricas de forma consistente.
53. NUKE
Probar
NUKE  es una plataforma de compilaciÃ³n para .NET y una alternativa a los tradicionales MSBuild o 
Cake  y Fake  que hemos presentado anteriormente en el Radar. NUKE representa las instrucciones 
de compilaciÃ³n como un DSL de C#, lo que lo hace fÃ¡cil de aprender y con un buen soporte de  IDE. En nuestra experiencia, NUKE hizo realmente simple la automatizaciÃ³n de la compilaciÃ³n de proyectos .NET. Nos gusta la precisiÃ³n de las comprobaciones de cÃ³digo estÃ¡tico y las sugerencias. 
TambiÃ©n nos gusta que podamos utilizar cualquier paquete NuGet sin problemas y que el cÃ³digo de  automatizaciÃ³n pueda compilarse para evitar problemas en tiempo de ejecuciÃ³n. NUKE no es nuevo, pero su novedoso enfoque -utilizando un DSL de C#- y nuestra positiva experiencia en general nos llevÃ³ a incluirlo aquÃ­.
54. Pactf low
Probar
Hemos utilizado Pact  para testeo de contratos el tiempo suficiente como para ver parte de la 
complejidad que se produce a escala. Alguno de nuestros equipos han utilizado Pactflow  para 
reducir fricciÃ³n. Pactflow se ejecuta tanto como SaaS como una aplicaciÃ³n âOn premiseâ con la 
misma funcionalidad, y aÃ±ade usabilidad, seguridad y auditorÃ­as mejoradas con respecto a la versiÃ³n 
de cÃ³digo abierto Pact Broker. Estamos satisfechos con su uso hasta el momento y contentos de ver  un esfuerzo continuado para eliminar parte de la sobrecarga de gestionar el testeo de contratos a 
escala.
55. Podman
Probar
Como una alternativa a Docker , Podman ha sido validada por muchos de nuestros equipos. 
Podman presenta un motor âdaemonlessâ (sin dependencias de fondo) para el manejo y ejecuciÃ³n 
de contenedores, lo que es una acercamiento interesante en comparaciÃ³n a lo que Docker hace. 
Adicionalmente, Podman puede ser ejecutado fÃ¡cilmente como un usuario normal sin ser necesario 
Herramientas	
30
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Herramientas
tener privilegios de administrador, lo que reduce la superficie de ataque. Usando tanto Iniciativas 
Abiertas de Contenedores  (OCI su sigla en inglÃ©s) imÃ¡genes construidas por Buildah  o imÃ¡genes 
de Docker, Podman se puede adaptar a la mayorÃ­a de los casos de uso de contenedores. Aparte 
de algunos problemas de compatibilidad con macOS, nuestro equipo ha tenido en general buenas 
experiencias con Podman en distribuciones Linux.
56. Grafo de CÃ³digo Fuente
Probar
En el anterior Radar, presentamos dos herramientas que se encargaban de la bÃºsqueda y el  reemplazo de cÃ³digo usando la representaciÃ³n de un Ã¡rbol de sintaxis abstracta (AST), Comby  
y Sourcegraph . Si bien estas herramientas comparten ciertas similitudes, estas se diferencian 
de varias maneras. Sourcegraph es una herramienta comercial (con una capa gratuita de hasta 
10-usuarios). Es utilizada particularmente para bÃºsqueda, navegaciÃ³n y referenciaciÃ³n cruzada  entre largas bases de cÃ³digo, con Ã©nfasis en una experiencia de desarrollo interactiva. Por el 
contrario, Comby es una herramienta de lÃ­nea de comandos, ligera, de cÃ³digo abierto utilizada  para automatizar tareas repetitivas. Debido a que Sourcegraph es un servicio de hosting, tambiÃ©n 
tiene la capacidad de monitorear continuamente las bases de cÃ³digo y enviar alertas cuando una 
coincidencia es encontrada. Ahora que hemos ganado mÃ¡s experiencia con Sourcegraph, decidimos  moverlo al ring de prueba para reflejar nuestra experiencia positiva, lo que no significa que 
Sourcegraph sea mejor que Comby. Cada herramienta se enfoca en un nicho diferente.
57. Syf t
Probar
Uno de los elementos clave para mejorar la âseguridad de la cadena de suministroâ es usar una  Lista de materiales de software (SBOM) , por eso es cada vez mÃ¡s importante publicar un SBOM 
junto con el producto de software. Syft  Es una herramienta CLI y una biblioteca Go para generar un 
SBOM a partir de imÃ¡genes de contenedores y sistemas de archivos. Puede generar la salida SBOM  en mÃºltiples formatos, incluyendo JSON, CycloneDX  y SPDX. La salida SBOM de Syft puede ser 
utilizada por Grype  para el escaneo de vulnerabilidades. Una forma de publicar el SBOM generado 
junto con la imagen es agregarlo como testigo usando Cosign . Esto permite que los consumidores 
de la imagen verifiquen el SBOM y lo utilicen para un anÃ¡lisis posterior.
58. Volta
Probar
Cuando se trabaja con varias bases de cÃ³digo JavaScript al mismo tiempo, a menudo es necesario  utilizar diferentes versiones de Node y otras herramientas JavaScript. En los equipos de los 
desarrolladores, estas herramientas generalmente suelen estar instaladas en la cuenta de usuario 
o en la propia mÃ¡quina, lo que significa que se necesita una soluciÃ³n para cambiar entre mÃºltiples  instalaciones. Para Node ya existe nvm , pero queremos destacar Volta  como una alternativa que 
nuestros equipos estÃ¡n usando. Volta tiene varias ventajas sobre nvm: es capaz de gestionar otras 
herramientas de JavaScript como Yarn ; tambiÃ©n tiene la capacidad de fijar una versiÃ³n especÃ­fica 
de la cadena de herramientas en base al proyecto, lo que significa que los desarrolladores pueden 
simplemente utilizar las herramientas en un directorio de cÃ³digo dado sin tener que preocuparse de  cambiar manualmente entre versiones: Volta usa shims en la ruta para seleccionar la versiÃ³n fijada.  Escrito en Rust, Volta es rÃ¡pido y se distribuye como un binario sin dependencias.	
31
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

59. Web Test Runner
Probar
Web Test Runner es un paquete dentro del proyecto Modern Web , que proporciona varias 
herramientas de alta calidad para el desarrollo web moderno con soporte para estÃ¡ndares web como 
MÃ³dulos ES. Web Test Runner es un ejecutor de pruebas para aplicaciones web. Una de sus ventajas 
en comparaciÃ³n con los ejecutores de prueba existentes es que ejecuta pruebas en el navegador 
(que podrÃ­a no tener interfaz). Es compatible con mÃºltiples lanzadores de navegadores, incluidos 
Titiritero , Playwright  y Selenium, y usa Mocha de forma predeterminada para el marco de prueba. 
Las pruebas se ejecutan bastante rÃ¡pido y nos gusta que podamos abrir una ventana del navegador 
con devtools durante la depuraciÃ³n. Web Test Runner utiliza internamente Web Dev Server  lo 
que nos permite aprovechar su excelente API de complementos para agregar complementos  personalizados para nuestro conjunto de pruebas. Las herramientas de Modern Web parecen una 
cadena de herramientas para desarrolladores muy prometedora y ya las estamos usando en algunos  proyectos.
60. CDKTF
Evaluar
A estas alturas, muchas organizaciones han creado paisajes extensos de servicios en la nube.  Por supuesto, esto solo es posible cuando se usa infraestructura como cÃ³digo  y herramientas 
maduras. TodavÃ­a nos gusta Terraform , sobre todo por su rico y creciente ecosistema. Sin embargo, 
la falta de abstracciones en HCL, el lenguaje de configuraciÃ³n predeterminado de Terraform, crea 
efectivamente un techo de cristal. El uso de Terragrunt  lo lleva un poco mÃ¡s allÃ¡, pero cada vez 
mÃ¡s, nuestros equipos se encuentran anhelando las abstracciones que ofrecen los lenguajes de 
programaciÃ³n modernos. El Kit de desarrollo en la nube para Terraform (CDKTF) , que resultÃ³ de 
una colaboraciÃ³n entre el CDK  de AWS y Hashicorp, hace posible que los equipos utilicen varios 
lenguajes de programaciÃ³n, incluidos TypeScript y Java, para definir y aprovisionar la infraestructura. 
Con este enfoque sigue el ejemplo de Pulumi  mientras permanece en el ecosistema Terraform. 
Hemos tenido buenas experiencias con CDKTF, pero hemos decidido mantenerlo en el anillo de 
EvaluaciÃ³n hasta que salga de la versiÃ³n beta.
61. Chrome Recorder panel
Evaluar
Chrome Recorder panel  es una funciÃ³n de vista previa en Google Chrome 97 que permite grabar 
y reproducir de forma sencilla las interacciones de los usuarios. Aunque definitivamente no es una  idea nueva, la forma en que estÃ¡ integrada en Chrome permite crear, editar y ejecutar rÃ¡pidamente 
scripts. El panel tambiÃ©n se integra muy bien con el panel de rendimiento, lo que facilita la obtenciÃ³n  de informaciÃ³n repetida y coherente sobre el rendimiento de la pÃ¡gina. Aunque las pruebas de 
estilo de grabaciÃ³n/reproducciÃ³n siempre deben utilizarse con cuidado para evitar pruebas frÃ¡giles, 
creemos que vale la pena evaluar esta funciÃ³n de vista previa, especialmente si ya estÃ¡s utilizando el  panel de rendimiento de Chrome para medir tus pÃ¡ginas.
62. Excalidraw
Evaluar
Excalidraw  es una herramienta de dibujo online, sencilla pero potente, que les encanta utilizar 
a nuestros equipos. A veces, los equipos sÃ³lo necesitan una imagen rÃ¡pida y no un diagrama  formal; para los equipos trabajando en remoto, Excalidraw proporciona una forma rÃ¡pida de crear 
y compartir diagramas. A nuestros equipos tambiÃ©n les gusta el aspecto de âbaja fidelidadâ de los 
Herramientas	
32
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

diagramas que puede generar, que recuerda a los diagramas que habrÃ­an dibujado en una pizarra 
si estuvieran trabajando en el mismo lugar. Una advertencia: presta atenciÃ³n a la configuraciÃ³n de 
seguridad predeterminada, en el momento de escribir, cualquier persona que tenga el enlace puede 
ver el diagrama. La versiÃ³n de pago proporciona autenticaciÃ³n mÃ¡s avanzada.
63. GitHub Codespaces
Evaluar
GitHub Codespaces  permite a los desarrolladores crear entornos de desarrollo en la nube  y 
acceder a ellos a travÃ©s de un IDE como si el entorno fuera local. GitHub no es la primera empresa en  implementar esta idea; anteriormente hablamos sobre Gitpod . Nos gusta que Codespaces permite 
que los entornos se estandarizan mediante el uso de archivos de configuraciÃ³n, lo que agiliza la  incorporaciÃ³n de nuevos miembros al equipo, y que ofrecen mÃ¡quinas virtuales con hasta 32 nÃºcleos 
y 64 GB de memoria. Estas mÃ¡quinas virtuales se pueden activar en menos de diez segundos, lo que  ofrece potencialmente entornos mÃ¡s potentes que una computadora portÃ¡til de desarrollo.
64. GoReleaser
Evaluar
GoReleaser  es una herramienta que automatiza el proceso de construcciÃ³n y liberaciÃ³n de 
un proyecto Go para diferentes arquitecturas a travÃ©s de mÃºltiples repositorios y canales, una  necesidad comÃºn para los proyectos Go dirigidos a diferentes plataformas. La herramienta se 
ejecuta desde la mÃ¡quina local a travÃ©s de CI, con la herramienta disponible a travÃ©s de varios 
servicios de CI, minimizando asÃ­ la configuraciÃ³n y el mantenimiento. GoReleaser se encarga de  construir, empaquetar, publicar y anunciar cada versiÃ³n y soporta diferentes combinaciones de  formato de paquete, repositorio de paquetes y control de fuentes. Aunque existe desde hace unos 
aÃ±os, nos sorprende que no haya mÃ¡s equipos que lo utilicen. Si estÃ¡s lanzando regularmente una  base de cÃ³digo Go, vale la pena evaluar esta herramienta.
65. Grype
Evaluar
Asegurar la cadena de suministro de software se ha convertido en una preocupaciÃ³n habitual  entre los equipos de entrega, una preocupaciÃ³n que se refleja en el creciente nÃºmero de nuevas herramientas en este espacio. Grype  es una nueva y ligera herramienta de exploraciÃ³n de 
vulnerabilidades para imÃ¡genes Docker y OCI. Esta herramienta puede ser instalada como un binario,  puede escanear imÃ¡genes antes de que sean subidas a un registro docker, y no requiere un demonio 
Docker para ejecutarse en los agentes de construcciÃ³n. Grype proviene del mismo equipo que estÃ¡ 
detrÃ¡s de Syft , el cual genera SBOMs  en varios formatos a partir de imÃ¡genes de contenedores. 
Grype puede consumir la salida SBOM de Syft para escanearla en busca de vulnerabilidades.
66. Infracost
Evaluar
Una de las ventajas que se mencionan frecuentemente cuando se habla de moverse hacia la nube 
es la transparencia alrededor del coste de infraestructura. En nuestra experiencia, no siempre se da 
este caso. Los equipos no siempre piensan en las decisiones que toman en torno a la infraestructura 
en tÃ©rminos de coste financiero, y es por ello que ya mencionamos con anterioridad Ã©l coste de 
ejecuciÃ³n como funciÃ³n de aptitud de arquitectura . Tenemos curiosidad por el lanzamiento de una 
nueva herramienta llamada Infracost  que se propone dar visibilidad a los compromisos de coste en 
Herramientas	
33
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

las pull request en Terraform. Es un software de cÃ³digo abierto y estÃ¡ disponible para macOS, Linux, 
Windows y Docker y muestra los precios para AWS, GCP y Microsoft Azure listos para usar. TambiÃ©n  proporciona una API pÃºblica que puede ser consultada para conocer los datos de coste actualizados. 
Nuestros equipos estÃ¡n entusiasmados con su potencial, especialmente cuando se trata de ganar  mejor visibilidad en los costes en el IDE (Entorno de desarrollo integrado).
67. jc
Evaluar
En el Radar anterior, colocamos Modern Unix Commands  cÃ³mo tecnologÃ­a a evaluar. Uno de los 
comandos presentes en esa colecciÃ³n de herramientas era jq, un sed (stream editor) para JSON. jc  
realiza una tarea similar: recoge la salida de comandos de Unix habituales y la convierte a formato 
JSON. Los dos comandos juntos ofrecen un puente entre el mundo de Unix CLI y el conjunto de  librerÃ­as y herramientas que funcionan con JSON. Al escribir scripts simples, por ejemplo, para 
el despliegue de software o la obtenciÃ³n de informaciÃ³n de diagnÃ³stico, tener toda una gama 
de formatos de salida de diferentes comandos Unix mapeados a un formato JSON bien definido  puede ahorrarnos mucho tiempo y esfuerzo. Al igual que con jq, es necesario asegurarse de que el 
comando estÃ¡ disponible. Puede ser instalado desde muchos de los repositorios mÃ¡s conocidos.
68. skopeo
Evaluar
skopeo  es una utilidad de lÃ­nea de comando que realiza varias operaciones sobre imÃ¡genes de 
contenedores y repositorios de imÃ¡genes. No requiere que un usuario sea administrador para realizar  la mayorÃ­a de sus operaciones ni precisa de un demonio para ejecutarse. Es una parte Ãºtil del pipeline 
de integraciÃ³n continua; lo hemos utilizado para copiar imÃ¡genes de un registro a otro a medida que se  promovÃ­an las imÃ¡genes. Es mejor que realizar un âpullâ y un âpushâ ya que no es necesario almacenar las imÃ¡genes localmente. No es una herramienta nueva, pero es suficientemente Ãºtil y poco utilizada 
por lo que pensamos que vale la pena mencionarla.
69. SQLFluff
Evaluar
Si bien el linting (el anÃ¡lisis estÃ¡tico del cÃ³digo fuente) es una prÃ¡ctica antigua en el mundo del  software, su adopciÃ³n en el mundo de los datos ha sido mÃ¡s lenta. SQLFluff  es un linter (herramienta 
para hacer linting) SQL de dialecto cruzado escrito en Python, que se lanza a travÃ©s de lÃ­nea de 
comandos (CLI), lo que hace que sea fÃ¡cil incorporarlo en las pipelines de CI/CD. Si estÃ¡s cÃ³modo 
con las convenciones predeterminadas, entonces SQLFluff trabaja sin ninguna otra configuraciÃ³n 
adicional despuÃ©s de instalarlo, imponiendo un conjunto de estÃ¡ndares de formato fuertemente  definidos; establecer tus propias convenciones requiere aÃ±adir un fichero de configuraciÃ³n. La  interfaz por lÃ­nea de comandos (CLI) puede corregir automÃ¡ticamente ciertos tipos de violaciones 
que incluyen problemas de formato como los espacios en blanco o las mayÃºsculas en las palabras 
clave. SQLFluff es todavÃ­a nuevo, pero estamos emocionados por ver a SQL tener mÃ¡s presencia en 
el mundo del linting.
Herramientas	
34
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

70. Validador de Terraform
Evaluar
Las organizaciones que han adoptado infraestructura como cÃ³digo  y plataformas de infraestructura 
de autoservicio estÃ¡n buscando formas de brindar a los equipos la mÃ¡xima autonomÃ­a mientras se  mantienen aplicando buenas prÃ¡cticas de seguridad y polÃ­ticas organizacionales. Hemos resaltado 
tfsec  before and are moving it into the Adopt category in this Radar. For teams working on GCP, 
Terraform Validator  antes y lo estamos moviendo a la categorÃ­a Adoptar en este Radar. Para los 
equipos que trabajan en GCP, Terraform Validator podrÃ­a ser una opciÃ³n al crear una biblioteca de  polÃ­ticas, un conjunto de restricciones que se comparan contra las configuraciones de Terraform.
71. Typesense
Evaluar
Typesense  Es un motor de bÃºsqueda de texto rÃ¡pido y tolerante a errores tipogrÃ¡ficos. Para casos 
de uso con grandes volÃºmenes de datos, Elasticsearch podrÃ­a seguir siendo una buena opciÃ³n, ya 
que proporciona una soluciÃ³n de bÃºsqueda horizontalmente escalable basada en disco. Sin embargo, 
si estÃ¡s construyendo una aplicaciÃ³n de bÃºsqueda sensible a la latencia con un tamaÃ±o de Ã­ndice  de bÃºsqueda que puede caber en memoria, Typesense es una alternativa poderosa y otra opciÃ³n a 
evaluar junto a herramientas como Meilisearch .
Herramientas	
35
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Lenguajes y 
Frameworks
Adoptar
72.  SwiftUI
73.  Testcontainers  
Probar
74 .  Bob
75.  Widget de Flutter-Unity
76.  Kotest
7 7.  Swift Package Manager
78.  Vowpal Wabbit
Evaluar
79.  Android Gradle plugin - Kotlin DSL
80.  Azure Bicep
81.  Capacitor
82.  Java 17
83.  Jetpack Glance
84.  Jetpack Media3
85.  MistQL
86.  npm workspaces
8 7.  Remix
88.  ShedLock
89.  SpiceDB
90.  sqlc
91.  La Arquitectura Componible
92.  Ensamblaje Web
93.  Zig  
Resistir â	
Resistir Resistir
Evaluar Evaluar
Probar Probar
A doptar Adoptar	
3	1	
24	3 5
36
37	
38	
39	
4 0	
41	4 2	
43	
29	
3 4	
32	
4	12	
13	
14	
19	
2 1
15	2 0	
16	
17
2 2	
18	
5
6	
7	
9	11	
4 5	49	
60	
61	62	63	
64	65
66	67
68
69
70
7 1	
5 1	53
57	58	
59	
52	
74	
7 9	8 0	
81	
82	83	84
85	
86	87
88
89	9 0
91	
93
92	
75	
7 7	
78	
3 0
31	
72	
73	
7 6	
2	
8	10	
2 3	
25
26	
27	28	
3 3	
4 4
46	
47	
50
54	55	
56	
4 8
Nuevo
Desplazado
adentro/afuera NingÃºn 
cambio
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

72. SwiftUI
AdoptarCuando Apple presentÃ³ SwiftUI  hace unos aÃ±os, fue un gran paso para la implementaciÃ³n de 
interfaces de usuario en todo tipo de dispositivos fabricados por Apple. Desde el principio, nos gustÃ³ 
el enfoque declarativo, centrado en el cÃ³digo y el modelo de programaciÃ³n reactiva proporcionado  por Combine . Sin embargo, nos dimos cuenta de que escribir muchas pruebas de vista, que todavÃ­a 
se necesitan con un patrÃ³n modelo-vista-modelovista (MVVM), no se alineaba realmente con el  marco de automatizaciÃ³n XCUITest proporcionado por Apple. Esta brecha ha sido cerrada por 
ViewInspector . Un Ãºltimo obstÃ¡culo era la versiÃ³n mÃ­nima del sistema operativo requerida. En el 
momento del lanzamiento, sÃ³lo las Ãºltimas versiones de iOS y macOS podÃ­an ejecutar aplicaciones 
escritas con SwiftUI, pero debido a la cadencia regular de actualizaciones de Apple, las aplicaciones 
de SwiftUI ahora pueden ejecutarse en prÃ¡cticamente todas las versiones de macOS y iOS que  reciben actualizaciones de seguridad.
73. Testcontainers
Adoptar
Tenemos suficiente experiencia en el uso de Testcontainers  sobre la cual creemos que es por 
defecto la opciÃ³n mÃ¡s adecuada para crear entornos confiables para la ejecuciÃ³n de pruebas.  Es una librerÃ­a que se puede usar en mÃºltiples lenguajes , y dockeriza las librerÃ­as de test mÃ¡s 
comunes (incluyendo varios tipos de bases de datos, tecnologÃ­as de encolamiento, servicios en  la nube y dependencias de UI testing como navegadores) con la habilidad de ejecutar Dockerfiles 
personalizados cuando sea necesario. Funciona bien en frameworks de testeo como JUnit, es 
suficientemente flexible como para dejar que los usuarios gestionen el ciclo de vida y la conexiones  de red de los contenedores para que rÃ¡pidamente tengan seteado un entorno integrado de pruebas.  Nuestros equipos la encuentran como una librerÃ­a programable, ligera y una manera fÃ¡cil de manejar 
contenedores para hacer de sus tests aÃºn mÃ¡s confiables.
74. Bob
Probar
Cuando se construye una aplicaciÃ³n con React Native, a veces nos vemos en la necesidad de crear  nuestros propios mÃ³dulos. Por ejemplo, nos hemos encontrado con esta necesidad de crear una librerÃ­a de componentes de UI para una aplicaciÃ³n React Native. Desarrollar mÃ³dulos no es tan 
sencillo pero nuestros equipos han demostrado solvencia usando Bob  para automatizar esta tarea. 
Bob proporciona una CLI para crear el andamiaje (scaffolding) para diferentes tipos de proyectos. 
El andamiaje no se limita a la funcionalidad base, sino que, opcionalmente, puede incluir cÃ³digo de 
ejemplo, linters, configuraciones de pipelines de construcciÃ³n, etc.
75. Widget de Flutter-Unity 
Probar
Flutter es cada vez mÃ¡s popular para crear aplicaciones mÃ³viles multiplataforma, y    Unity es 
excelente para crear experiencias AR/VR. Una pieza clave en el rompecabezas para integrar  Unity y Flutter es el widget de Flutter-Unity , que permite incorporar aplicaciones de Unity dentro 
de aplicaciones Flutter. Una de las capacidades clave que ofrece el widget es la comunicaciÃ³n  bidireccional entre Flutter y Unity. Hemos encontrado que su rendimiento tambiÃ©n es bastante 
bueno, y esperamos aprovechar Unity en mÃ¡s aplicaciones de Flutter.
Lenguajes y Frameworks	
37
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

76. Kotest 
Probar
Kotest  (anteriormente llamado Kotlin Test) es una herramienta de testing independiente para 
el  Kotlin  ecosistema que continua ganando fuerza entre nuestros equipos a travÃ©s de varias 
implementaciones de Kotlin - nativo, JVM o JavaScript. Las principales ventajas son que ofrece 
una gran variedad de estilos de testing para organizar los test suites y que viene con un amplio y 
completo conjunto de matchers, los cuales permiten tests âexpresivosâ conjuntamente con una DSL  interna elegante. AdemÃ¡s de su soporte para property-based testing  â una tÃ©cnica que hemos 
destacado previamente en el Radar â a nuestros equipos les gusta el sÃ³lido plugin de IntelliJ y la 
creciente comunidad de soporte al rededor.
77. Swift Package Manager
Probar
Algunos lenguajes de programaciÃ³n, especialmente los mÃ¡s nuevos, tienen una soluciÃ³n integrada  para administraciÃ³n de paquetes y dependencias. Cuando fue presentado en 2014, Swift no venÃ­a 
con un administrador de paquetes, y entonces la comunidad de desarrolladores para macOS y  iOS simplemente continuÃ³ usando CocoaPods y Carthage , tlas soluciones de terceros que habÃ­an 
sido creadas para Objective-C. Un par de aÃ±os mÃ¡s tarde Swift Package Manager  ((SwiftPM) fue 
iniciado como un proyecto de cÃ³digo abierto oficial de Apple, y entonces tomÃ³ unos pocos aÃ±os 
mÃ¡s antes que Apple pudiera aÃ±adir su soporte para Xcode. Incluso en ese punto, muchos equipos 
de desarrollo continuaban usando CocoaPods y Carthage, en su mayorÃ­a porque muchos paquetes 
simplemente no estaban disponibles a travÃ©s de SwiftPM. Ahora que la mayorÃ­a de los paquetes  pueden ser aÃ±adidos a travÃ©s de SwiftPM y los procesos han sido simplificados tanto para creadores 
como consumidores de los paquetes, nuestros equipos estÃ¡n confiando cada vez mÃ¡s en SwiftPM.
78. Vowpal Wabbit 
Probar
Vowpal Wabbit  es una una librerÃ­a de uso general para aprendizaje automÃ¡tico (machine learning). 
Fue creada originalmente por Yahoo! Research hace mÃ¡s de una dÃ©cada, Vowpal Wabbit sigue  implementando nuevos algoritmos de refuerzo del aprendizaje. Queremos resaltar que Vowpal 
Wabbit 9.0 , ha publicado una nueva versiÃ³n importante despuÃ©s de seis aÃ±os, y alentarlos a 
planear la migraciÃ³n  ya que tiene varias mejoras de usabilidad, nuevas reducciones y arregla varios 
defectos.
79. Android Gradle plugin - Kotlin DSL
Evaluar
Android Gradle plugin Kotlin DSL ha agregado soporte para Kotlin Script como alternativa a  Groovy para los scripts de compilaciÃ³n de Gradle. El objetivo de reemplazar Groovy con Kotlin es proporcionar un mejor soporte para la refactorizaciÃ³n y una ediciÃ³n mÃ¡s simple en los IDE (Entorno 
de desarrollo integrado) y, en Ãºltima instancia, producir cÃ³digo que sea mÃ¡s fÃ¡cil de leer y mantener.  Para equipos que ya usan Kotlin, tambiÃ©n significa trabajar en la compilaciÃ³n en un lenguaje familiar. 
Hemos tenido un equipo con un script de compilaciÃ³n de 450 lÃ­neas de al menos siete aÃ±os de 
antigÃ¼edad que migrÃ³  en unos pocos dÃ­as. Si tienen scripts de compilaciÃ³n de Gradle grandes o 
complejos, entonces vale la pena evaluar si Kotlin Script producirÃ¡ mejores resultados para sus 
equipos.
Lenguajes y Frameworks	
38
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

80. Azure Bicep  
Evaluar
Para aquellos que prefieren un lenguaje mÃ¡s natural que JSON para cÃ³digo de infraestructura, Azure 
Bicep  es un lenguaje especÃ­fico de dominio (DSL) que utiliza una sintaxis declarativa. Soporta 
plantillas parametrizadas reutilizables para definiciones de recursos modulares. Una extensiÃ³n de 
Visual Studio Code proporciona seguridad de tipo instantÃ¡nea, intellisense y verificaciÃ³n de sintaxis,  y el compilador permite la transpilaciÃ³n bidireccional hacia y desde plantillas ARM. El DSL orientado a recursos de Bicep y la integraciÃ³n nativa con el ecosistema de Azure lo convierten en una opciÃ³n convincente para el desarrollo de infraestructura de Azure.
81. Capacitor
Evaluar
Hemos estado discutiendo los mÃ©ritos de las herramientas de desarrollo mÃ³vil multi-plataforma 
tanto tiempo como el que hemos llevado publicando el Radar TecnolÃ³gico. En 2011, nos dimos 
cuenta por primera vez de una nueva generaciÃ³n de herramientas cuando creamos el blip sobre  plataformas multi-dispositivo . Aunque permanecimos escÃ©pticos al principio, estas herramientas 
se han perfeccionado y han sido adoptadas ampliamente a lo largo de los aÃ±os. Y nadie puede  discutir la popularidad duradera y la utilidad de React Native . Capacitor es la Ãºltima generaciÃ³n 
de una lÃ­nea de herramientas que comenzÃ³ con PhoneGap, y luego fue renombrada a Apache 
Cordova . Capacitor es una reescritura completa de Ionic que abraza el estilo de las aplicaciones 
web progresivas  para aplicaciones independientes. Hasta ahora, a nuestras desarrolladoras les 
gusta que pueden conseguir crear aplicaciones web, iOS y Android con una Ãºnica base de cÃ³digo y 
que pueden gestionar las plataformas nativas de forma separada con acceso a APIs nativas cuando 
es necesario. Capacitor ofrece una alternativa a React Native, que tiene muchos aÃ±os de experiencia  multi-plataforma a sus espaldas.
82. Java 17  
Evaluar
No solemos presentar nuevas versiones de lenguajes de forma rutinaria, pero querÃ­amos destacar  la nueva versiÃ³n de soporte a largo plazo (LTS) de Java, la versiÃ³n 17. Si bien hay nuevas funciones 
prometedoras, como la vista previa de bÃºsqueda de patrones , es el cambio al nuevo proceso de 
LTS lo que deberÃ­a interesar a muchas organizaciones. Recomendamos a las organizaciones que 
evalÃºen nuevas versiones de Java a medida que estÃ©n disponibles, asegurÃ¡ndose de adoptar nuevas  funciones y versiones segÃºn convenga. Sorprendentemente, muchas organizaciones no adoptan  nuevas versiones de lenguajes de manera rutinaria, a pesar de que las actualizaciones periÃ³dicas 
ayudan a mantener las cosas pequeÃ±as y manejables. Con suerte, el nuevo proceso de LTS, junto  con las organizaciones pasando a hacer actualizaciones periÃ³dicas, ayudarÃ¡ a evitar la trampa de 
âdemasiado caro actualizarâ que acaba con el software de producciÃ³n ejecutÃ¡ndose en una versiÃ³n  de Java al final de su vida Ãºtil.
83. Jetpack Glance 
Evaluar
Android 12 ha traÃ­do cambios significativos en los widgets de las aplicaciones, las cuales han  mejorado la experiencia del usuario final y del desarrollador. Para realizar aplicaciones de Android, 
hemos expresado nuestra preferencia por Jetpack Compose  como una forma moderna de construir 
interfaces de usuario nativas. Ahora, con Jetpack Glance , el cual es construido sobre el Compose 
runtime, los desarrolladores pueden usar similares APIs declarativas de Kotlin para escribir widgets. 
Lenguajes y Frameworks	
39
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Recientemente Glance ha sido extendido para soportar Tiles en Wear OS.
84. Jetpack Media3  
Evaluar
Hoy en dÃ­a, Android tiene una gran variedad de APIs: Jetpack Media, tambiÃ©n conocido como 
MediaCompat, JetpackMedia2 y ExoPlayer. Desafortunadamente, estas bibliotecas fueron 
desarrolladas independientemente, con distintos objetivos que sobreponen funcionalidades. Las  personas que desarrollan con Android no solamente tenÃ­an que escoger quÃ© biblioteca usar, sino 
que tambiÃ©n tenÃ­an que encargarse de hacer adaptadores u otras medidas para poder usar distintas  bibliotecas a la vez. Jetpack Media3  es un esfuerzo, actualmente en acceso anticipado, de crear 
una nueva API que contiene las funcionalidades comunes de las APIs que ya existen (incluyendo  interfaz de usuario, playback y manejo de sesiones de multimedia) conbinÃ¡ndolas para crear una 
unida y refinada API. La interfaz de ExoPlayer tambiÃ©n ha sido actualizada, mejorada y simplificada  para ejercer como interfaz comÃºn para Media3.
85. MistQL
Evaluar
MistQL  es un pequeÃ±o lenguaje especÃ­fico de dominio que sirve para hacer cÃ¡lculos en estructuras 
similares a JSON. Originalmente construido para la extracciÃ³n manual de funcionalidades de  modelos de machine learning en front end, MistQL actualmente soporta una implementaciÃ³n en 
JavaScript para navegadores y una implementaciÃ³n en Python para casos de uso del lado del  servidor. Nos gusta su limpia sintaxis componible y funcional. Animamos a evaluarlo basÃ¡ndose en 
sus necesidades.
86. npm workspaces 
Evaluar
Mientras que muchas herramientas se pueden usar en el mundo de desarrollo multipaquete de node. js, npm 7 aÃ±ade soporte directo con npm workspaces . Gestionar paquetes relacionados en conjunto 
facilita el desarrollo, permitiÃ©ndote, por ejemplo, guardar multiples librerÃ­as relacionadas en un solo  repositorio. Con npm workspaces, una vez hayas aÃ±adido una configuraciÃ³n en un package.json de 
alto nivel que haga referencia a uno o mÃ¡s archivos de package.json anidados, comandos como npm  install trabajarÃ¡n a travÃ©s de multiples paquetes, enlazando simbÃ³licamente los paquetes fuente 
dependientes en el directorio raÃ­z node_modules. Otros comandos tambiÃ©n estarÃ¡n al tanto de los  workspaces, permitiÃ©ndote, por ejemplo, ejecutar los comandos npm run y npm test a travÃ©s de  mÃºltiples paquetes con un solo comando. Tener esta flexibilidad desde el primer momento reduce la 
necesidad de algunos equipos de llegar a usar otro gestor de paquetes.
87. Remix
Evaluar
Hemos sido testigos de la migraciÃ³n de renderizar sitios web desde el lado del servidor a una 
aplicaciÃ³n de una sola pÃ¡gina dentro del navegador, ahora el pÃ©ndulo del desarrollo web parece  balancearse al medio de todo. Remix es uno de estos ejemplos. Es un framework fullstack para JavaScript. El framework provee una carga rÃ¡pida para las pÃ¡ginas utilizando sistemas 
distribuidos y navegadores nativos en lugar de compilaciones estÃ¡ticas ineficientes. Ha logrado 
algunas optimizaciones en enrutamientos anidados y en la carga de pÃ¡ginas, lo que hace que  la representaciÃ³n de la pÃ¡gina parezca especialmente rÃ¡pida. Mucha gente compararÃ¡ Remix 
con  Next.js , que estÃ¡ similarmente posicionada. Nos complace ver a este framework combinar 
ingeniosamente el tiempo de ejecuciÃ³n del navegador con el tiempo de ejecuciÃ³n del servidor 
Lenguajes y Frameworks	
40
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

para dar una mejor experiencia de usuario.
88. ShedLock 
Evaluar
Ejecutar una tarea programada una Ãºnica vez en un cluster de procesadores distribuidos es un  requerimiento relativamente comÃºn. Por ejemplo, la situaciÃ³n podrÃ­a surgir cuando se ingesta un lote 
de datos, se envÃ­a una notificaciÃ³n o se ejecuta alguna actividad de limpieza periÃ³dica. Pero, este es  un problema notablemente complicado. Â¿CÃ³mo es que un grupo de procesos cooperan de manera 
segura sobre redes lentas y poco confiables? AlgÃºn mecanismo de bloqueo es necesario para  coordinar las acciones a travÃ©s del cluster. Afortunadamente, una variedad de almacenamientos 
distribuidos pueden implementar un bloqueo. Sistemas como  ZooKeeper  y Consul, asÃ­ como 
tambiÃ©n bases de datos como DynamoDB o Couchbase  poseen mecanismos subyacentes para 
administrar el consenso dentro del cluster. ShedLock  es una pequeÃ±a biblioteca para aprovecharse 
de estos proveedores en tu cÃ³digo Java, si estÃ¡s buscando implementar tus propias tareas  programadas. Cuenta con un API para adquirir y liberar bloqueos, asÃ­ como tambiÃ©n conectores para 
una amplia variedad de proveedores de bloqueo. Si estÃ¡s escribiendo tus propias tareas distribuidas  pero quieres evitar la complejidad que implica una plataforma de orquestaciÃ³n como Kubernetes , 
vale la pena darle un vistazo a ShedLock.
89. SpiceDB
Evaluar
SpiceDB  es un sistema de base de datos inspirado en Zanzibar , (creado por Google), para gestionar 
permisos en aplicaciones. Con SpiceDB, defines un esquema para modelar los requisitos y usas el 
cliente  para aplicar el esquema a una de las base de datos soportadas , insertar permisos o pedir 
respuesta a preguntas del tipo âÂ¿Tiene este usuario acceso a este rescurso?â o incluso al revÃ©s 
âÂ¿Cuales son todos los recursos a los que este usuario tiene acceso?â Normalmente abogamos por la  separaciÃ³n de las reglas de autorizaciÃ³n y el cÃ³digo, pero SpiceDB va un paso mÃ¡s allÃ¡ separando los recursos y las reglas almacenando como un grafo para responder eficientemente a consultas 
sobre autorizaciÃ³n. Debido a esta separaciÃ³n, debes asegurarte que los cambios en los datos de  tu aplicaciÃ³n principal se vean reflejados en SpiceDB. Entre otras implementaciones inspiradas en 
Zanzibar, creemos que SpiceDB es una herramienta a valorar para gestionar autorizaciones en tu  aplicaciÃ³n.
90. sqlc 
Evaluar
sqlc  es un compilador que genera cÃ³digo Go idiomÃ¡tico y con seguridad de tipos a partir de 
SQL. A diferencia de otros enfoques basados en un mapeo objeto-relacional (ORM), puedes  seguir escribiendo sencillamente SQL para lo que necesites. Una vez invocado, sqlc comprueba la correcciÃ³n del SQL y genera un cÃ³digo Go eficiente, que puede ser llamado directamente desde el  resto de la aplicaciÃ³n. Con un soporte estable tanto para PostgreSQL como MySQL, merece la pena 
echarle un vistazo a sqlc y te animamos a que lo pruebes.
91. La Arquitectura Componible
Evaluar
Desarrollar aplicaciones para iOS se ha convertido con el paso del tiempo en un proceso mÃ¡s 
eficiente, y SwiftUI  migrando a Adopt es una seÃ±al de ello. Yendo mÃ¡s allÃ¡ de la naturaleza general 
de SwiftUI y otros marcos de trabajo comunes, La Arquitectura Componible  (LAC) es a la vez una 
librerÃ­a y un estilo arquitectÃ³nico para construir aplicaciones. Fue diseÃ±ado en el transcurso de 
Lenguajes y Frameworks	
41
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Lenguajes y Frameworks	
42	
una serie de videos, y los autores han declarado que tenían en mente la composiciÃ³n, las pruebas 
y la ergonomía, basÃ¡ndose en ideas de La Arquitectura Elm y Redux. Como era de esperar, el  Ã¡mbito reducido y el adoctrinamiento son a la vez la fortaleza y la debilidad de LAC. Sentimos que  los equipos que no tienen mucha pericia escribiendo aplicaciones iOS, que a menudo son equipos 
que se encargan de mÃºltiples cÃ³digos fuente relacionados con diferentes stacks tecnolÃ³gicos, son  los que mÃ¡s pueden beneficiarse de usar un marco de trabajo doctrinario como LAC, y nos gustan 
las opiniones expresadas en LAC.
92. Ensamblaje Web 
Evaluar
WebAssembly  (WASM) es el estÃ¡ndar W3C que provee capacidades para ejecutar cÃ³digo en el 
navegador. Soportado por todos los principales navegadores y compatible con sus versiones 
anteriores, es un formato de compilaciÃ³n binaria diseÃ±ado para ejecutarse en el navegador 
a velocidades casi nativas. Abre el rango de idiomas que se puede utilizar para escribir  funcionalidades de front-end, con enfoque inicial en C, C++ y Rust, y es tambiÃ©n un objetivo de  LLVM compilation  tCuando se ejecuta en el sandbox, puede interactuar con JavaScript y compartir 
los mismos permisos y modelo de seguridad. Portabilidad y seguridad son capacidades clave, que 
habilitarÃ¡n la mayorÃ­a de las plataformas, incluyendo mobile y IoT.
93. Zig 
Evaluar
Zig  es un nuevo lenguaje que comparte muchos atributos con C pero con un tipado mÃ¡s fuerte, 
asignaciÃ³n de memoria mÃ¡s fÃ¡cil, soporte para espacios de nombres y una serie de otras  caracterÃ­sticas. Su sintaxis, sin embargo, recuerda a JavaScript mÃ¡s que a C, lo que algunos pueden  reprocharle. El objetivo de Zig es proporcionar un lenguaje muy simple con una compilaciÃ³n sencilla 
que minimice efectos laterales y produzca una ejecuciÃ³n predecible y fÃ¡cil de seguir. Zig tambiÃ©n  proporciona acceso a la capacidad de compilaciÃ³n cruzada . de LLVM. Algunas de nuestras 
desarrolladoras han encontrado esta caracterÃ­stica tan Ãºtil, que estÃ¡n utilizando Zig como un 
compilador cruzado incluso aunque no estÃ©n escribiendo cÃ³digo en Zig. Zig es un lenguaje novedoso 
e interesante de investigar para aplicaciones donde C estÃ¡ siendo considerado o ya estÃ¡ en uso, asÃ­ 
como para aplicaciones de sistema de bajo nivel que requieren manipulaciÃ³n explÃ­cita de memoria.
Thoughtworks Technology Radar© Thoughtworks, Inc. All Rights Reserved.  

Â¿Quieres estar al tanto de todas las noticias e insights relacionadas al Radar?
SÃ­guenos en tu red social favorita o suscrÃ­bete.
Thoughtworks es una consultora global de 
tecnología que integra estrategia, diseÃ±o e 
ingeniería para impulsar la innovaciÃ³n digital. 
Somos 10,000+ personas en 49 oficinas en 17
países. En los Ãºltimos 25+ aÃ±os, hemos logrado 
un impacto extraordinario junto con nuestros
clientes, ayudÃ¡ndoles a resolver problemas 
complejos de negocio a travÃ©s de la tecnología 
como elemento diferenciador.	
SuscrÃ­bete ahora 

"
Non,"Error extracting text: Secured pdf file are currently not supported."
Non,"Market Insight Report Reprint
After 10 Years, AWS continues 	 	
to expand Amazon DynamoDB 
well beyond scalability
February 24 2022
by James Curtis
DynamoDB started with a focus on scalability, but over the past 10 years, this 
nonrelational cloud database service has matured significantly, with AWS adding 
capabilities like usability, manageability and extensibility, targeting a variety of enterprise 
operational workloads and serving hundreds of thousands of customers.
This report, licensed to Amazon, developed and as provided by S&P Global Market Intelligence (S&P), was published as part of S&Pâs syndicated market insight subscription service. It shall be owned in its entirety by S&P. This report is solely intended for use by the recipient and may not be reproduced or re-posted, in whole or in part, by the recipient without express permission from S&P. 

1	
Introduction
When AWS rolled out Amazon DynamoDB in 2012 as a managed cloud database service, it was to address several 
challenges the company had seen with traditional relational databases, particularly around scaling. After 10 
years, AWS continues to iterate on this nonrelational (NoSQL) database service, having previously added or 
enhanced support for ACID transactions, replication, streaming and on-demand pricing, as just a few examples.
AWS added several enhancements throughout 2021, including capabilities for improved client-side application 
development and improved support for SQL-like querying, as well as broader integration with AWS services such 
as Amazon S3 storage and Amazon Kinesis streaming. At re:Invent 2021, the company introduced flexible table 
storage pricing and integration with AWS Backup.
THE TAKE
A decade after its introduction, Amazon DynamoDB can be considered a trailblazer in the managed 
cloud database space, a sector that began to emerge around 2008. While DynamoDB has been known 
as a purpose-built service since inception, itâs now moving into general-purpose territory, particularly 
as the scope of the service has broadened to cater to SQL developers to address a wide array of 
operational workloads. DynamoDB is a mature cloud database service, but there is still room to grow 
as it looks to tap into the wide array of AWS services. This approach will be necessary to differentiate 
the offering from a growing cadre of database services, many of which pinpoint serverless and scaling.
Context
Early on, DynamoDB played the role of relational database disruptor, with a focus on scaling operational 
transactions and doing it with a NoSQL data model as a fully managed cloud service. Amazon DynamoDB was 
serverless from the get-go, which is noteworthy because database vendors are now rushing to add serverless 
as core functionality to their cloud database offerings to meet customer demand. Based on 451 451 Researchâs 
Voice of the Enterprise: Data, AI, and Analytics, Data Analytics, Data Platforms survey, 62% of respondents report 
that their organizations are either using serverless, or have plans to adopt the technology in the near term.
The 10-year anniversary of the service is noteworthy â DynamoDB was released as a fully managed serverless 
cloud database long before serverless was an industry catchword for scaling cloud infrastructure on demand. 
The impetus behind DynamoDB was to address several challenges that Amazon had experienced with traditional 
relational databases around scaling in a production environment.
Although DynamoDBâs release appeared as a quick answer to its relational database challenges, the company 
had been chewing on the principles of a highly distributed, scalable and durable NoSQL database for some time. 
These principles were spelled out in a 2007 technical paper written by AWS engineers on Amazonâs Dynamo 
internal technology.
Fast forward to 2012, when AWS released DynamoDB as a fully managed NoSQL cloud database service, which 
specifically utilizes a key-value data model that supports a document model. In database speak, DynamoDB is a 
multimodel NoSQL database.
Market Insight Report Reprint 
Â©Copyright 2022 S&P Global Market Intelligence. All rights reserved.  

2	
Technology updates
DynamoDB is considered a mature database cloud service. A few noteworthy additions from prior updates 
include support for ACID transactions, active-active global replication, on-demand and point-in-time restore, 
streaming and on-demand pricing. Significant AWS services integrations include Key Management Service 
for encrypting data rest, and CloudWatch Contributor Insights â an AWS service for gaining insights on access 
patterns, tagging, VPN endpoints and triggers. In 2021, the company made further updates that touch on areas 
such as usability, manageability and extensibility.
Regarding usability, AWS added the NoSQL Workbench, consisting of a client-side, cross-platform IDE tool that 
provides data modeling, data visualizing and query devolvement for client-side developers. AWS also announced 
support for PartiQL â compatible with the NoSQL Workbench, the SQL-capability query language enables 
developers to interact with DynamoDB using common SQL commands.
For manageability, DynamoDB now supports AWS CloudTrail to manage governance, compliance and risk 
auditing on AWS accounts. Meanwhile, at re:Invent 2021, it announced the Amazon DynamoDB Standard-
Infrequent Access table class that provides flexible table storage pricing. This new capability targets storage-
heavy workloads in which infrequently accessed tables can be identified for lower storage costs. Customers still 
pay for throughput (to maintain consistent performance), but the throughput costs are offset by lower storage 
costs, potentially up to 60% over Standard Access tables, according to AWS.
On the integration front, AWS announced DynamoDB integration with AWS Backup, a centralized automatic 
backup service. Previously, customers had to carry out backup processes on a case-by-case basis, depending 
on the database service. With AWS Backup, however, users can manage backup policies across their AWS 
database services from a single management plane. Additionally, there is now integration with Amazon Kinesis, 
specifically the Kinesis Data Streams capabilities that capture item-level changes in the form of a data stream 
to DynamoDB tables.
Competition
DynamoDBâs customer appeal centers on its ability to scale a high number of operational transactions while 
maintaining performance. Sometimes referred to as âperformance at scale,â this approach has traditionally been 
the sweet spot for NoSQL databases. As a managed cloud service that employs a key-value data and document 
data model, DynamoDB primarily competes directly with peer cloud platform providers serving up the same or 
similar data model.
Given this, competitors include Azure Cosmos DB and Googleâs Datastore, available on Azure and GCP cloud 
platforms, respectively. There is also Oracle NoSQL Database and Oracle Autonomous JSON Database, which 
reside on Oracleâs cloud platform, while IBM has its Cloudant NoSQL database offered on IBM Cloud. Couchbase 
Capella and MongoDB Atlas are available on multiple cloud platforms and are worthy competitors, as is Redis 
Enterprise Cloud.
However, an emerging category of databases referred to as distributed SQL concentrates on handling 
operational transactions in a highly geo-distributed environment. Many of these vendors are peddling more 
traditional relational data models, although some are blending relational and nonrelational. Regardless, 
potential competitors include Cockroach Labs, PlanetScale, YugaByte, Fauna, MariaDB Xpand, AWS Amazon 
Aurora, Google Cloud Spanner, and Azure Database for PostgreSQL-Hyperscale, SingleStore and PingCAP.
Market Insight Report Reprint 
Â©Copyright 2022 S&P Global Market Intelligence. All rights reserved.  

3	
SWOT Analysis
STRENGTHS
Since its inception, DynamoDB has been serverless 
and provided performance at scale, which turned 
out to be a crucial design choice â particularly given 
the increased demand for serverless database 
architectures today.	
WEAKNESSES
As a NoSQL database, DynamoDB wonât match the 
full spectrum of SQL querying capabilities offered 
by relational database systems, although that gap 
seems to be slowly closing.	
OPPORTUNITIES
Although DynamoDB can be considered a maturing 
cloud database service now, there is certainly 
room to grow as it aims to tap into a wide array of 
AWS services, which will certainly be necessary 
to differentiate from a growing cadre of database 
services.	
THREATS
Serverless and scalability are becoming less 
distinguishable feature, since many existing 
vendors and new emerging vendors are rolling out 
serverless architectures and focusing specifically 
on SQL scaling capabilities.
Market Insight Report Reprint 
Â©Copyright 2022 S&P Global Market Intelligence. All rights reserved.  

Copyright Â© 2022 by S&P Global Market Intelligence, a division of S&P Global Inc. All rights reserved.
These materials have been prepared solely for information purposes based upon information generally available to 
the public and from sources believed to be reliable. No content (including index data, ratings, credit-related analyses 
and data, research, model, software or other application or output therefrom) or any part thereof (Content) may be 
modified, reverse engineered, reproduced or distributed in any form by any means, or stored in a database or retrieval 
system, without the prior written permission of S&P Global Market Intelligence or its affiliates (collectively, S&P Global). 
The Content shall not be used for any unlawful or unauthorized purposes. S&P Global and any third-party providers, 
(collectively S&P Global Parties) do not guarantee the accuracy, completeness, timeliness or availability of the Content. 
S&P Global Parties are not responsible for any errors or omissions, regardless of the cause, for the results obtained 
from the use of the Content. THE CONTENT IS PROVIDED ON âAS ISâ BASIS. S&P GLOBAL PARTIES DISCLAIM ANY AND 
ALL EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTIES OF MERCHANTABILITY 
OR FITNESS FOR A PARTICULAR PURPOSE OR USE, FREEDOM FROM BUGS, SOFTWARE ERRORS OR DEFECTS, THAT 
THE CONTENTâS FUNCTIONING WILL BE UNINTERRUPTED OR THAT THE CONTENT WILL OPERATE WITH ANY SOFTWARE 
OR HARDWARE CONFIGURATION. In no event shall S&P Global Parties be liable to any party for any direct, indirect, 
incidental, exemplary, compensatory, punitive, special or consequential damages, costs, expenses, legal fees, or losses 
(including, without limitation, lost income or lost profits and opportunity costs or losses caused by negligence) in 
connection with any use of the Content even if advised of the possibility of such damages.
S&P Global Market Intelligenceâs opinions, quotes and credit-related and other analyses are statements of opinion as 
of the date they are expressed and not statements of fact or recommendations to purchase, hold, or sell any securities 
or to make any investment decisions, and do not address the suitability of any security. S&P Global Market Intelligence 
may provide index data. Direct investment in an index is not possible. Exposure to an asset class represented by an 
index is available through investable instruments based on that index. S&P Global Market Intelligence assumes no 
obligation to update the Content following publication in any form or format. The Content should not be relied on and is 
not a substitute for the skill, judgment and experience of the user, its management, employees, advisors and/or clients 
when making investment and other business decisions. S&P Global Market Intelligence does not endorse companies, 
technologies, products, services, or solutions.
S&P Global keeps certain activities of its divisions separate from each other in order to preserve the independence 
and objectivity of their respective activities. As a result, certain divisions of S&P Global may have information that 
is not available to other S&P Global divisions. S&P Global has established policies and procedures to maintain the 
confidentiality of certain non-public information received in connection with each analytical process.
S&P Global may receive compensation for its ratings and certain analyses, normally from issuers or underwriters of 
securities or from obligors. S&P Global reserves the right to disseminate its opinions and analyses. S&P Globalâs public 
ratings and analyses are made available on its websites, 	www.standardandpoors.com	 (free of charge) and 	 	
www.ratingsdirect.com	 (subscription), and may be distributed through other means, including via S&P Global 	
publications and third-party redistributors. Additional information about our ratings fees is available at 	 	
www.standardandpoors.com/usratingsfees	.	
CONTACTS	
The Americas	
+1 877 863 1306	
market.intelligence@spglobal.com	
Europe, Middle East & Africa	
+44 20 7176 1234	
market.intelligence@spglobal.com	
Asia-Pacific	
+852 2533 3565	
market.intelligence@spglobal.com	
www.spglobal.com/marketintelligence 

"
Non,"	
	
Compatibility	 Matrix	 for	 CTE	 Agent	 	
with	 Data	 Security	 Manager	
Release	 7.1.0	
Document	 Version	 2	
May	 17,	 2021 


Contents	Â 	
Rebranding	Â Announcement	6	
CTE	Â Agent	Â for	Â Linux	6	
Interoperability	6	
Table	 1:	 Linux	 Interoperability	 with	 Third	 Party	 Applications	6	
ESG	 (Efficient	 Storage	 GuardPoint)	 Support	7	
Table	 2:	 Efficient	 Storage	 GuardPoint	 Support	7	
Linux	 Agent	 Raw	 Device	 Support	 Matrix	8	
Red	 Hat,	 CentOS,	 and	 OEL	 non-	UEK	 7.5-	7.9	 Raw	 Device	 Support	8	
Table	 3:	 Red	 Hat	 7.5-	7.9	 | CentOS	 7.5-	7.8	 | OEL	 non-	UEK	 7.5-	7.9	 (x86_	64)1,2	8	
Red	 Hat,	 CentOS,	 and	 OEL	 non-	UEK	 8 Raw	 Device	 Support	9	
Table	 4:	 Red	 Hat	 8.0-	8.3	 | CentOS	 8.0-	8.2	 | OEL	 non-	UEK	 8.0-	8.2	 (x86_	64)1,2	9	
SLES	 12	 Raw	 Device	 Support	9	
Table	 5:	 SLES	 12	 SP3,	 SLES	 12	 SP4,	 and	 SLES	 12	 SP5	 (x86_	64)2	9	
SLES	 15	 Raw	 Device	 Support	10	
Table	 6:	 SLES	 15,	 SLES	 15	 SP1,	 and	 SLES	 15	 SP2	 (x86_	64)	10	
Red	 Hat	 7.5	 ACFS	 Support	 with	 Secvm	10	
Table	 7:	 Oracle	 ACFS/Secvm	 Support	 on	 Red	 Hat	 7.5	 (x86_	64)	10	
Table	 8:	 Oracle	 ACFS/Secvm	 Stack	 with	 Red	 Hat	 7.5	 (x86_	64)	11	
Linux	 Agent	 File	 System	 Support	 Matrix	12	
Red	 Hat,	 CentOS,	 and OEL	 non-	UEK	 7.5	 - 7.9	 File	 System	 Support	12	
Table	 9:	 Red	 Hat	 7.5-	7.9	 | CentOS	 7.5-	7.8	 | OEL	 non-	UEK	 7.5-	7.9	 (x86_	64)1,4,5,6	12	
LDT	 Feature	 for	 Red	 Hat,	 CentOS,	 and OEL	 non-	UEK	 7.5-	7.9	 File	 System	 Support	13	
Table	 10:	 Red	 Hat	 7.5-	7.9	 | CentOS	 7.5-	7.8	 | OEL	 non-	UEK	 7.5-	7.9	 (x86_	64)2	13	
Docker	 Feature	 for	 Red	 Hat,	 CentOS,	 and OEL	 non-	UEK	 7.5	 - 7.9	 File	 System	 Support	13	
Table	 11:	 Red	 Hat	 7.5-	7.9	 | CentOS	 | OEL	 non-	UEK	 7.5-	7.9	 (x86_	64)	14 


Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
3	
Table	 12:	 Red	 Hat	 7.5-	7.8	 | CentOS	 | OEL	 non-	UEK	 7.5-	7.9	 (x86_	64)	14	
Table	 13:	 OpenShift	 Support	 on	 Docker:	 Red	 Hat	 7.5	 â 7.8	 | CentOS	 | OEL	 non-	UEK	 7.5	 â 7.9	 (x86_	64)	14	
Table	 14:	 Kubernetes	 Support	 on	 Docker:	 Red	 Hat	 7.5,	 CentOS,	 and	 OEL	 non-	UEK	 7.5	 (x86_	64)	15	
Table	 15:	 Kubernetes	 Support	 on	 Docker:	 Red	 Hat	 7.3/7.4,	 CentOS,	 and OEL	 non-	UEK	 7.3/7.4	 (x86_	64)	15	
Red	 Hat	 8 File	 System	 Support	15	
Table	 16:	 Red	 Hat	 8.0-	8.3	 (x86_	64)	 1,	 2,	 3	15	
LDT	 Feature	 for	 Red	 Hat	 8 File	 System	 Support	16	
Table	 17:	 Red	 Hat	 8.3	 (x86_	64)	16	
Docker	 Feature	 for	 Red	 Hat	 8.0	 File	 System	 Support	16	
Table	 18:	 Red	 Hat	 8.0	 (x86_	64)	16	
Table	 19:	 Red	 Hat	 8.0	 (x86_	64)	17	
HDFS	 Support	17	
Table	 20:	 Red	 Hat	 7.5-	7.8	 (x86_	64)	17	
CIFS	Â File	Â System	Â Support	18	
Table	 21:	 CIFS File	 System	 Supported	 on	 Linux	 CTE	 Distributions	18	
SLES	 Support	19	
SLES	 12	 File	 System	 Support	19	
Table	 22:	 SLES	 12	 SP3,	 SLES	 12	 SP4,	 and	 SLES	 12	 SP5	 (x86_	64)	 1,3	19	
LDT	 Feature	 for	 SLES	 12	 File	 System	 Support	20	
Table	 23:	 SLES	 12	 SP3,	 SLES	 12	 SP4,	 and	 SLES	 12	 SP5	 (x86_	64)1	20	
SLES	 15	 File	 System	 Support	21	
Table	 24:	 SLES	 15,	 SLES	 15	 SP1,	 and	 SLES	 15	 SP2	 (x86_	64)1	21	
LDT	 Feature	 for	 SLES	 15	 File	 System	 Support	22	
Table	 25:	 SLES	 15	 and	 SLES	 15	 SP1,	 and	 SLES	 15	 SP2	 (x86_	64)	22	
SAP	 HANA	 Support	 (LDT	 is	 supported	 on	 SAP	 HANA)	22	
Table	 26:	 SAP	 HANA	 Support	22	
Ubuntu	 Support	24 


Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
4	
Ubuntu	 18.04	 File	 System	 Support	24	
Table	 27:	 Ubuntu	 18.04,	 18.04.1,	 18.04.2,	 18.04.3,	 18.04.4,	 18.04.5	 (x86_	64)1,3	24	
LDT	 Feature	 for	 Ubuntu	 18.04	 File	 System	 Support	25	
Table	 28:	 Ubuntu	 18.04,	 18.04.1,	 18.04.2,	 18.04.3,	 18.04.4,	 18.04.5	 (x86_	64)1	25	
Ubuntu	 20.04	 File	 System	 Support	26	
Table	 29:	 Ubuntu	 20.04	 (x86_	64)1	26	
LDT	 Feature	 for	 Ubuntu	 20.04	 File	 System	 Support	27	
Table	 30:	 Ubuntu	 20.04	 (x86_	64)	27	
CTE	Â Agent	Â for	Â Cloud	Â Environment	27	
General	 CTE	 Support	 for	 AWS	 AMI	 and	 Microsoft	 Azure	 on	 Linux:	27	
CTE	 Support	 on	 AWS	 EFS,	 AWS	 S3	 File	 Storage	 Gateway,	 AWS	 Hadoop/HDFS:	 EMR	 and	 Hortonworks	27	
Amazon	 Linux	28	
Table	 31:	 Amazon	 Linux	 File	 System	 Support	28	
Table	 32:	 Amazon	 Linux	 Kernels	 Supported	29	
AWS	 AMI	29	
Microsoft	 Azure	 (Linux)	30	
Table	 33:	 Azure	 Files	 Supported	 Linux	 CTE	 Distributions	30	
CTE	Â Agent	Â for	Â Windows	31	
Windows	 Application	 Interoperability	31	
Table	 34:	 Windows	 interoperability	 with	 IBM Infosphere	 Guardium	 and Imperva	 Securesphere	31	
Table	 35:	 Windows	 CTE	 LDT	 Interoperability	 with	 backup	 applications	31	
Table	 36:	 Windows	 CTE	 LDT	 Interoperability	 with	 Active-	Passive	 clusters	31	
Table	 37:	 Windows	 CTE	 Quantum	 StorNext	 Support	32	
Table	 38:	 Windows	 File	 System	 Support	  (Vmfiltr	 driver)	32	
LDT	 Feature	 for	 Windows	 File	 System	 Support	34	
Table	 39:	 Windows	 (Vmfiltr	 driver)	34 


Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
5	
Table	 40:	 Windows	  (VMLFS	 Driver)	35	
Anti-	Virus	Â Support	36	
Table	 41:	 Anti-	virus	 support	 on	 Windows 	 (x64-	bit)	36	
DSM	Â and	Â VTE/CTE	Â Agent	Â Software	Â Version	Â Compatibility	36	
Table	 42:	 DSM	 vs.	 VTE/CTE	 Agent	 Version	 Support	36	
End	Â of	Â Support	Â Information	37	
Table	 43:	 End	 of	 Support	 Information	37 


Rebranding	Announcement	
Interoperability
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
6	
Rebranding	Â Announcement	
From	 release	 7.0.0	 and	 onward,	 the	 VTE	 (Vormetric	 Transparent	 Encryption)	 Agent	 had	 been	 rebranded	 to	 CTE	 (CipherTrust	 Transparent	 Encryption)	 Agent. 	 When	 a host	 is 	
registered	 with	 the	 DSM	 key	 manager,	 CTE	 is backward	 compatible	 with	 the	 VTE	 Agent	 when	 it comes	 to	 supported	 kernel	 platforms,	 applications,	 and	 configurations.	 For	 	
example,	 if a given	 kernel	 is supported	 starting	 with	 VTE	 6.3.0.130,	 it is automatically	 supported	 in	 all	 CTE	 releases	 as	 well.	 	
For	 information	 about	 VTE	 release	 support,	 see	 the	 Vormetric	 Transparent	 Encryption	 Agent	 Compatibility	 Matrix,	 Release	 6.3.1	. 	
CTE	Â Agent	Â for	Â Linux	
Interoperability
Table	Â 1:	Â Linux	Â Interoperability	Â with	Â Third	Â Party	Â Applications	
Product	Version	OS	Notes	
IBM	 Infosphere	 Guardium	v8.0,	 v9.0,	 v10.1	Linux	Compatible	
Imperva	 Securesphere	v9.0,	 v9.5,	 v10.5,	 v11.5,	 v12	Linux	Compatible 	
Symantec	 Net-	backup	7.6.1.2	 	
7.7.3	 	
Linux	Compatible 	
McAfee	 Enterprise	 Endpoint	v10.6.5    	Red	 Hat	 7	
Red	 Hat	 8	
Compatible	
Trend	 Micro	 Deep	 Security	v12.0.347	 	Red	 Hat	 7	
Red	 Hat	 8	
Compatible	
Quantum	 StorNext	 	Red	 Hat	 7.5+	LDT	 is not	 supported	
Red	 Hat	 Pacemaker	0.9.168	 and	 onward	Red	 Hat	 7	
Red	 Hat	 8	
Compatible	
Note
Guardium	 and	 Imperva	 are	 not	 supported	 with	 LDT	 and	 Docker. 


CTE	Agent	for	Linux	
ESG	(Efficient	Storage	GuardPoint)	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
7	
ESG	Â (Efficient	Â Storage	Â GuardPoint)	Â Support	
Table	Â 2:	Â Efficient	Â Storage	Â GuardPoint	Â Support	
CTE	Â Agent	CTE	Â Agent	Â and	Â DSM	Â Versions	Â 	
Supporting	Â ESG	
Operating	Â Systems	Â 	
Supported	
Virtualization	Â Platforms	Â 	
Supported	Storage	Â Systems	Â Supported	
CTE	 for	 Linux	CTE	 Version:	 7.0.103	 and	 onward	
DSM	 Version:	 6.3.0	 and	 onward	
RHEL	 7.5	 and	 onward	
RHEL	 8 and	 onward	
SLES	 12	 SP3	 ~ 12	 SP5	
SLES	 15	 and	 onward	
Ubuntu	 18	
VMWare	 (RDMs	 and	 vVols)	Pure	 FlashArray	 (minimum	 version	 5.3.7)	
 
Dell	 EMC	 PowerMax	 Family:	  PowerMax	 2000,	 	
PowerMax	 8000	  	
PowerMax	 OS:	 5978.711.711	 	
CTE	 for	 	
Windows	
CTE	 Version:	 7.0.0.107	 and	 onward	
DSM	 Version:	 6.3.0	 and	 onward	
Windows	 2012	
Windows	 2016	
Windows	 2019	
Hyper-	V,	 VMWare	 (RDMs	 and	 	
VVols)	
Pure	 FlashArray	 (minimum	 version	 5.3.7)	
Dell	 EMC	 PowerMax	 Family:	
PowerMax	 2000	
PowerMax	 8000	
PowerMax	 OS:	 5978.711.711	
Notes
 l	For	 technical	 details	 and	 use	 case	 guidelines,	 see	 the	 âEfficient	 Storageâ	 chapter	 in	 the	 CTE	 Agent	 for	 Linux	 Advanced	 Configuration	 and	 Integration	 Guide	 	
or	 the	 CTE	 Agent	 for	 Windows	 Advanced	 Configuration	 and	 Integration	 Guide	.	
 l	For	 the	 OS,	 file	 system,	 and	 database	 applications	 supported	 with	 ESG	 on	 file	 system	 use	 cases,	 see	 the	 existing	 file	 system	 compatibility	 tables.	
 l	For	 details	 about	 the	 compatible	 Pure	 Storage	 versions,	 the	 Pure	 Storage	 requirements,	 and	 other	 Pure	 Storage	 features	 supported	 with	 CTE,	 see	 your	 	
Pure	 Storage	 documentation.	
 l	Limitations	 for	 Windows	 ES	 GuardPoint	 Support:	
 o	Windows	 dynamic	 disks	 are	 not	 supported.	
 o	DFS/DFSR	 with	 shared	 disk	 scenarios	 are	 not	 supported.	
 o	All	 applications	 accessing	 the	 Pure	 LUN	 directly	 must	 be	 closed	 during	 the	 initialization	 and	 IDT	 process.	
 n	GuardPoint	 may	 fail	 to	 apply	 if resources	 are	 used.	 User	 may	 need	 to	 reboot	 the	 system	 to	 complete	 the	 GuardPoint.	
 n	User	 must	 not	 use	 any	 tools	 to	 manage	 the	 disk	 during	 these	 operations. 


CTE	Agent	for	Linux	
Linux	Agent	Raw	Device	Support	Matrix 	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
8	
Linux	Â Agent	Â Raw	Â Device	Â Support	Â MatrixÂ 	
 l	CipherTrust	 supports	 up	 to	 6000	 raw	 (secvm)	 devices.	
 l	KVM	 VirtIO	 driver	 is supported	 from	 CTE	 7.0.0	 release	 and	 onwards.	
Note
Raw	 devices	 are	 not	 supported	 with	 LDT	 and	 Docker.	
Red	Â Hat,	Â CentOS,	Â and	Â OEL	Â non-	UEK	Â 7.5-	7.9	Â Raw	Â Device	Â Support	
Table	Â 3:	Â Red	Â Hat	Â 7.5-	7.9	Â |Â CentOS	Â 7.5-	7.8	Â |Â OEL	Â non-	UEK	Â 7.5-	7.9	Â (x86_	64)	1,2	
Database	
Oracle	Â 11gR2/12c/18c/19c	Â 	DB2	Â 9.7/10.1/10.5/11.1	Â 	Sybase	Â ASE	Â 16	Â 	
RAW	 	RAW	 	RAW	 	
Native	 LVM	 	 	 	
ASM/ASMlib	 	 	 	
ASMFD	3	 	 	
Notes:
 1.	 CTE	 supports	 only	 RHEL	 7.5	 GA	 (Kernel	 3.10.0-	862.el7.x86_	64)	 and	 later	 releases.	
 2.	 Support	 for	 RAW	 Character	 Devices	 must	 be	 configured	 manually.	
 3.	 ASMFD	 is supported	 from	 CTE	 7.1.0	 GA	 onwards	 Oracle	 12c/18c/19c. 


CTE	Agent	for	Linux	
Linux	Agent	Raw	Device	Support	Matrix 	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
9	
Red	Â Hat,	Â CentOS,	Â and	Â OEL	Â non-	UEK	Â 8	Â Raw	Â Device	Â Support	
Table	Â 4:	Â Red	Â Hat	Â 8.0-	8.3	Â |Â CentOS	Â 8.0-	8.2	Â |Â OEL	Â non-	UEK	Â 8.0-	8.2	Â (x86_	64)	1,2	Â 	
Database	
Oracle	Â 12c/18c/19c	Â 	DB2	Â 10.5/11.1	Â 	
RAW	 	RAW	 	
Native	 LVM	 	 	
ASM	 	 	
Notes:
 1.	 CTE	 supports	 only	 RHEL	 8.0	 GA	 and	 later.	
 2.	 Support	 for	 RAW	 Character	 Devices	 must	 be	 configured	 manually.	
SLES	Â 12	Â Raw	Â Device	Â Support	
Table	Â 5:	Â SLES	Â 12	Â SP3,	Â SLES	Â 12	Â SP4,	Â and	Â SLES	Â 12	Â SP5	Â (x86_	64)	2	
Database	
Oracle	Â 11gR2/12c/18c/19c	DB2	Â 9.7/10.1/10.5/11.1	Informix	Â 11.5/11.7/12.1	Sybase	Â ASE	Â 16	
RAW	RAW	 	RAW	1 	RAW	 	
Native	 LVM	 	Native	 LVM	 	 	
ASM	 	 	 	
Notes:
 1.	 Support	 for	 RAW	 Character	 Devices	 must	 be	 configured	 manually.	 	
 2.	 CTE	 supports	 only	 SLES12	 SP3	 (Kernel	 4.4.126-	94.22.1)	 and	 later. 


CTE	Agent	for	Linux	
Linux	Agent	Raw	Device	Support	Matrix 	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
10	
SLES	Â 15	Â Raw	Â Device	Â Support	
Table	Â 6:	Â SLES	Â 15,	Â SLES	Â 15	Â SP1,	Â and	Â SLES	Â 15	Â SP2	Â (x86_	64)	
Database	
Oracle	Â 11gR2/12c/18c/19c	DB2	Â 9.7/10.1/10.5/11.1	Â 	
RAW	RAW	1 	
Native	 LVM	 	
ASM	 	
Note:	Â 	
 1.	 Support	 for	 RAW	 Character	 Devices	 must	 be	 configured	 manually.	 	
Red	Â Hat	Â 7.5	Â ACFS	Â Support	Â with	Â Secvm	
Table	Â 7:	Â Oracle	Â ACFS/Secvm	Â Support	Â on	Â Red	Â Hat	Â 7.5	Â (x86_	64)	
Database	
OS	Â Platform	Oracle	Â Database	Â Application	DSM	Â Policy	Â 	
Redhat	 7.5	 (x86_	64)	Oracle	 12c	
Oracle	 12c	 RAC	
Encryption-	only,	 no	 access	 control,	 no	 auditing,	 and	 do	 not	 execute	 binaries	 stored	 in the	 ACFS	 file	 system	
Note
For	 ACFS	 file	 systems	 mounted	 over	 Efficient	 Storage	 GuardPoints,	 see	 the	 âEfficient	 Storageâ	 chapter	 in	 the	 CTE	 Agent	 for	 Linux	 Advanced	 Configuration	 and	 	
Integration	 Guide	. 


CTE	Agent	for	Linux	
Linux	Agent	Raw	Device	Support	Matrix 	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
11	
Table	Â 8:	Â Oracle	Â ACFS/Secvm	Â Stack	Â with	Â Red	Â Hat	Â 7.5	Â (x86_	64)	
Oracle	 RAC	 	
Oracle	 ACFS	 (File	 System)	 	
Oracle	 ADVM	 (Volume	 Manager)	 	
Oracle	 ASM	 (Storage	 Manager)	 	
SecVM	 	
Block	 Device	(s)	  


CTE	Agent	for	Linux	
Linux	Agent	File	System	Support	Matrix	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
12	
Linux	Â Agent	Â File	Â System	Â Support	Â Matrix	
Red	Â Hat,	Â CentOS,	Â andÂ OEL	Â non-	UEK	Â 7.5	Â -Â 7.9	Â File	Â System	Â Support	
Table	Â 9:	Â Red	Â Hat	Â 7.5-	7.9	Â |Â CentOS	Â 7.5-	7.8	Â |Â OEL	Â non-	UEK	Â 7.5-	7.9	Â (x86_	64)	1,4,5,6	
FileÂ System	Database	
Unstructured	Â 	
Data	
Oracle
11gR2
12c/18c/19c	
DB2
9.7
10.1
10.5
11.1
11.5	
Informix
11.5
11.7
12.1
14.1	
Sybase
16	
MySQL
5.5
5.6
8
Maria	Â DB	
10.5	
Postgre	Â 	
SQL	MongoDB	Cassandra	
Couchbase
4.5
6.6	
GreenPlum	Â 5.21.0	
Standalone	Â &Â 	
Cluster	
HP	Â Vertica	Â 	
9.0.1	
EXT3	EXT3	EXT3	EXT3	 	EXT3	 	 	 	 	 	 	
EXT4	EXT4	EXT4	EXT4	EXT4	EXT4	 	EXT4	EXT4	 	 	 	
NFS	 V4/V3	NFS	 V4/	 V3	NFS	 	
V4/V3	
 	 	NFS	 	
V4/V3	
 	 	 	 	 	 	
XFS	 XFS	XFS	 	 	XFS	XFS	XFS	XFS	 	XFS	XFS	3	XFS	
AWS	 EFS	2	 	 	 	 	 	 	 	 	 	 	 	
Gluster	6	 	 	 	 	 	 	 	 	 	 	 	
Notes:
 1.	 For	 supported	 kernel	 versions	 see	 the	 CTE	 Compatibility	 Portal	 or	 the	 compatibility	 matrix	 PDF	 available	 at	  https://packages.vormetric.com/pub/cte_	compatibility_	
matrix.pdf.	
 2.	 AWS	 EFS	 support	 with	 and	 without	 on-	premises	 Direct	 Connect.	
 3.	 GreenPlum	 is supported	 only	 in	 Red	 Hat	 7.5	 and	 later.	
 4.	 CTE	 supports	 only	 RHEL	 7.5	 GA	 (Kernel	 3.10.0-	862.el7.x86_	64)	 and	 later.	
 5.	 Couchbase	 database	 version	 6.6	 is supported	 with	 Red	 Hat	 7.9.	
 6.	 The	 Gluster	 file	 system	 is supported	 from	 CTE	 7.1.0	 GA onward. 


CTE	Agent	for	Linux	
Linux	Agent	File	System	Support	Matrix	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
13	
LDT	Â Feature	Â for	Â Red	Â Hat,	Â CentOS,	Â andÂ OEL	Â non-	UEK	Â 7.5-	7.9	Â File	Â System	Â Support	
Table	Â 10:	Â Â Red	Â Hat	Â 7.5-	7.9	Â |Â CentOS	Â 7.5-	7.8	Â |Â OEL	Â non-	UEK	Â 7.5-	7.9	Â (x86_	64)	2	
File	Â System	Â 	Database	
Unstructured	Â 	
data	
Oracle
11gR2
12c/18c/19c	
DB2
9.7
10.1
10.5
11.1
11.5	
Sybase
16	
MySQL
5.5
5.6
8
Maria	Â DB	
10.5	
Postgre	Â SQL	MongoDB	GreenPlum	Â 5.21.0	
Standalone	Â &Â Cluster	
HP	Â Vertica	
9.0.1	
EXT3	EXT3	EXT3	 	EXT3	 	 	 	 	
EXT4	EXT4	EXT4	EXT4	EXT4	 	EXT4	 	 	
XFS	 XFS	XFS	 	XFS	XFS	XFS	XFS	XFS	1	XFS	
NFS	 V4/V3	3	NFS	 V4/	 V3	NFS	 V4/	 V3	 	NFS	 V4/	 V3	 	 	 	 	
Notes:
 1.	 GreenPlum	 is supported	 only	 in	 Red	 Hat	 7.5	 and	 later.	
 2.	 CTE	 supports	 only	 RHEL	 7.5	 GA	 (Kernel	 3.10.0-	862.el7.x86_	64)	 and	 later.	
 3.	 Single	 node	 reky	 for	 LDT	 over	 NFS supported	 in	 CTE	 7.1.0	 GA.	
Docker	Â Feature	Â for	Â Red	Â Hat,	Â CentOS,	Â andÂ OEL	Â non-	UEK	Â 7.5	Â -Â 7.9	Â File	Â System	Â Support	
Note:	
 l	Overlay	 Storage	 Driver	 is supported	 with	 Docker	 from	 CTE	 7.1.0	 GA	 and	 onwards.	 CTE	 7.0.0	 and	 previous	 versions	 only	 support	 device	 mapper	 with	 Docker. 


CTE	Agent	for	Linux	
Linux	Agent	File	System	Support	Matrix	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
14	
Table	Â 11:	Â Red	Â Hat	Â 7.5-	7.9	Â |Â CentOS	Â |Â OEL	Â non-	UEK	Â 7.5-	7.9	Â (x86_	64)	
Docker	Â Host	
Container	Â (Image-	based	Â and	Â container-	based)	
RHEL/CentOS	SLES	Ubuntu	
RHEL/CentOS	 7.5	 with	 Docker	 engine	 1.12	 or	 later	RHEL/CentOS	 7.5	SLES	 12	 SP3	Not	 supported	
RHEL/CentOS	 7.6	 with	 Docker	 engine	 1.12	 or	 later	RHEL/CentOS	 7.5/7.6	SLES	 12	 SP3	Ubuntu	 18.04	
RHEL/CentOS	 7.8/7.9	 with	 Docker	 engine	 1.12	 or	 later	RHEL/CentOS	 7.5/7.6/7.7/7.8/7.9	SLES	 12	 SP3	
SLES	 12	 SP4	
SLES	 12	 SP5	
SLES	 15	
Ubuntu	 18.04	
Table	Â 12:	Â Red	Â Hat	Â 7.5-	7.8	Â |Â CentOS	Â |Â OEL	Â non-	UEK	Â 7.5-	7.9	Â (x86_	64)	
Docker	Â Host	
Container	Â (Image-	based	Â and	Â container-	based)	
RHEL/CentOS	Â 7.5	Â -Â 7.9	SLES	Â 12	Â SP3/SP4/SP5	
SLES	Â 15	Ubuntu	Â 18.04	
RHEL/CentOS	 7.5/7.6/7.7/7.8/7.9	EXT4
XFS
NFSv3
NFSv4    	
EXT3
EXT4
XFS
NFSv3
NFSv4	
EXT4
XFS
NFSv3
NFSv4    	
Table	Â 13:	Â OpenShift	Â Support	Â on	Â Docker:	Â Red	Â Hat	Â 7.5	Â âÂ 7.8	Â |Â CentOS	Â |Â OEL	Â non-	UEK	Â 7.5	Â âÂ 7.9	Â (x86_	64)	
OpenShift	Â Version	Docker	Â Host	
OCP	 3.4	 or	 later	 version	RHEL/CentOS	 7.5/7.6/7.7/7.8 


CTE	Agent	for	Linux	
Linux	Agent	File	System	Support	Matrix	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
15	
Table	Â 14:	Â Kubernetes	Â Support	Â on	Â Docker:	Â Red	Â Hat	Â 7.5,	Â CentOS,	Â and	Â OEL	Â non-	UEK	Â 7.5	Â (x86_	64)	
Kubernetes	Â Version	Docker	Â Host	Docker	Â Version	
Kubectl	 1.10	 or	 later	 version   	RHEL/CentOS	 7.5	1.13	
Table	Â 15:	Â Kubernetes	Â Support	Â on	Â Docker:	Â Red	Â Hat	Â 7.3/7.4,	Â CentOS,	Â andÂ OEL	Â non-	UEK	Â 7.3/7.4	Â (x86_	64)	
Mesos	Â Version	Docker	Â Host	Docker	Â Version	
Mesos:1.4.1	RHEL/CentOS	 7.5	1.12.6	
Red	Â Hat	Â 8	Â File	Â System	Â Support	
Table	Â 16:	Â Â Red	Â Hat	Â 8.0-	8.3	Â (x86_	64)	Â 1,Â 2,Â 3	
FileÂ System	Database	
Unstructured	Â 	
Data	
Oracle
11gR2
12c/18c/19c	
DB2
9.7	Â |10.1	
10.5	Â |Â 11.1	
MySQL
5.5	Â |Â 5.6	Â |Â 8	
Maria	Â DB	Â 10.5	
MongoDB	
EXT3	EXT3	EXT3	EXT3	 	
EXT4	EXT4	EXT4	EXT4	EXT4	
NFS	 V4/V3	NFS	 V4/	 V3	NFS	 V4/V3	NFS	 V4/V3	 	
XFS	 XFS	XFS	 	XFS	XFS	
Gluster	 	 	 	 	
Notes:
 1.	 For	 supported	 kernel	 versions	 see	 the	 CTE	 Compatibility	 Portal	 or	 the	 compatibility	 matrix	 PDF	 available	 at	  https://packages.vormetric.com/pub/cte_	compatibility_	
matrix.pdf.	
 2.	 Thales	 has	 performed	 validation	 with	 CTE	 for	 these	 databases	 on	 Red	 Hat	 8,	 but	 the	 database	 vendors	 may	 not	 have	 completed	 their	 Red	 Hat	 validation	 yet.	
 3.	 The	 Gluster	 file	 system	 is supported	 from	 CTE	 7.1.0	 GA onward. 


CTE	Agent	for	Linux	
Linux	Agent	File	System	Support	Matrix	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
16	
LDT	Â Feature	Â for	Â Red	Â Hat	Â 8	Â File	Â System	Â Support	
Table	Â 17:	Â Red	Â Hat	Â 8.3	Â (x86_	64)	Â 	
FileÂ System	Â 	Database	
Unstructured	Â 	
data	
Oracle
12c/18c/19c	
DB2	Â 	
11.1	
MySQL	Â 5.5	Â |Â 5.6	Â |Â 8	
Maria	Â DB	Â 10.5	MongoDB	
EXT3	EXT3	EXT3	EXT3	 	
EXT4	EXT4	EXT4	EXT4	EXT4	
XFS	 XFS	XFS	 	XFS	 	
NFS	 V4/V3	1	NFS	 V4/V3	NFS	 V4/V3	NFS	 V4/V3	 	
Note:
 1.	 Single	 node	 reky	 for	 LDT	 over	 NFS supported	 in	 CTE	 7.1.0	 GA.	
Docker	Â Feature	Â for	Â Red	Â Hat	Â 8.0	Â File	Â System	Â Support	
Note:	
 l	Overlay	 Storage	 Driver	 is supported	 with	 Docker	 from	 CTE	 7.1.0	 GA	 and	 onwards.	 CTE	 7.0.0	 and	 previous	 versions	 only	 support	 device	 mapper	 with	 Docker.	
Table	Â 18:	Â Red	Â Hat	Â 8.0	Â (x86_	64)	
Docker	Â Host	
Container	Â (Image-	based	Â and	Â container-	based)	
RHEL/CentOS	SLES	Ubuntu	
RHEL/CentOS	 8.0	 with	 Docker	 engine	 1.13	1 or	 later	RHEL	 7.5	 â 7.8	
RHEL	 8.0	
SLES	 12	 SP4	
SLES	 15	
Ubuntu	 18.04.0	 â 18.04.5	
Note:
 1.	 CTE	 does	 not	 support	 bind	 mounts	 for	 Docker	 versions	 18.06	 and	 later. 


CTE	Agent	for	Linux	
HDFS	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
17	
Table	Â 19:	Â Red	Â Hat	Â 8.0	Â (x86_	64)	
Docker	Â Host	
Container	Â (Image-	based	Â and	Â container-	based)	
RHEL/CentOS	SLES	Ubuntu	
RHEL/CentOS	Â 7.5	Â -Â 7.6	
RHEL	Â 8.0	
SLES	Â 12	Â SP4	
SLES	Â 15	Ubuntu	Â 18.04â	18.04.2	
RHEL	 8.0	EXT4
XFS
NFSv3
NFSv4    	
EXT3
EXT4
XFS
NFSv3
NFSv4	
EXT4
XFS
NFSv3
NFSv4    	
HDFS	Â Support	
Table	Â 20:	Â Â Red	Â Hat	Â 7.5-	7.8	Â (x86_	64)	
HDFS	Â Access	Â Control	Â 	Platform	Â Support	Â 	File	Â System	Â 	
Hortonworks	 (HDP)	 2.6.4	 	
Apache	 Hadoop	 2.7.3	
RHEL	 7.5	EXT4
XFS	
Hortonworks	 (HDP)	 3.0.0	 	
Apache	 Hadoop	 3.1.1	
RHEL	 7.5	EXT4
XFS	
Hortonworks	 (HDP)	 3.1.0	 	
Apache	 Hadoop	 3.1.1	
RHEL	 7.8	EXT4
XFS	
Cloudera	 (CDH)	 5.14	
Apache	 Hadoop	 2.6.0	
RHEL	 7.8	EXT4
XFS	
EMR	 5.11.1	
Apache	 Hadoop	 2.7.3	
Amazon	 Linux	Default	 EXT4	  


CIFS	File	System	Support	
HDFS	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
18	
CIFS	Â File	Â System	Â Support	
Table	Â 21:	Â Â CIFSÂ File	Â System	Â Supported	Â on	Â Linux	Â CTE	Â Distributions	
CTE	Â on	Â CIFS	Â Share	Â mount	Â to	Â local	Â File	Â System	
Operating	Â System	Â 	SMB	Â Protocol	Â 2.1	Â 	SMB	Â Protocol	Â 3.0	
RHEL	 7	Supported	Supported	
RHEL	 8	Supported	Supported	
Notes:	
 l	CIFS file	 system	 is supported	 from	 CTE	 7.1.0	 GA	 and	 onwards	 for	 both	 CBC	 and	 CBC_	CS1	 encryption.	
 l	Data	 access	 across	 Linux	 and	 Windows	 with	 same	 CIFS	 share	 is supported	 with	 the	 same	 CTE	 policy	 and	 CBC	 encryption.	                     	
Note:	Â Â Do	 not	 use	 LDT	 policies	 used	 in	 this	 context. 


CIFS	File	System	Support	
SLES	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
19	
SLES	Â Support	
SLES	Â 12	Â File	Â System	Â Support	
Table	Â 22:	Â SLES	Â 12	Â SP3,	Â SLES	Â 12	Â SP4,	Â and	Â SLES	Â 12	Â SP5	Â (x86_	64)	Â 1,3	
File	Â System	Database	
Unstructured	Â 	
data	Â 	
Oracle
11gR2
12c/18c/19c	Â 	
DB2	Â 	
9.7,	Â 10.1	
10.5,	Â 11.1	Â 	
Informix
11.5,	Â 11.7,	Â 12.1	Â 	
Sybase
16	
Maria	Â DB	
10.5	MongoDB	Â 	Cassandra	Â 	
EXT3	 	 	 	 	EXT3	 	 	
EXT4	EXT4	 	 	EXT4	EXT4	 	 	
NFS	 V4/V3	 	 	 	 	NFS	 V4/V3	 	 	
XFS	 XFS	XFS	 	 	XFS	XFS	XFS	 	
AWS	 EFS	2	 	 	 	 	 	 	 	
Notes:
 1.	 For	 supported	 kernel	 versions	 see	 the	 CTE	 Compatibility	 Portal	 or	 the	 compatibility	 matrix	 PDF	 available	 at	  https://packages.vormetric.com/pub/cte_	compatibility_	
matrix.pdf.	
 2.	 AWS	 EFS	 support	 with	 and	 without	 on-	premises	 Direct	 Connect.	
 3.	 CTE	 only	 supports	 SLES12	 SP3	 (Kernel	 4.4.126-	94.22.1)	 and	 later. 


CIFS	File	System	Support	
SLES	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
20	
LDT	Â Feature	Â for	Â SLES	Â 12	Â File	Â System	Â Support	
Table	Â 23:	Â SLES	Â 12	Â SP3,	Â SLES	Â 12	Â SP4,	Â and	Â SLES	Â 12	Â SP5	Â (x86_	64)	1	
File	Â System	Database	
Unstructured	Â 	
data	
Oracle
11gR2/12c/18c/19c	
DB2	Â 	
9.7,	Â 10.1,10.5,	Â 11.1	
Sybase
16	
Maria	Â DB	
10.5	
EXT3	 	 	 	EXT3	
EXT4	EXT4	 	EXT4	EXT4	
XFS	 XFS	XFS	 	XFS	XFS	
NFS	 V4/V3	2	NFS	 V4/V3	NFS	 V4/V3	NFS	 V4/V3	 	
Note:
 1.	 CTE	 only	 supports	 SLES12	 SP3	 (Kernel	 4.4.126-	94.22.1)	 and	 later.	
 2.	 Single	 node	 reky	 for	 LDT	 over	 NFS supported	 in	 CTE	 7.1.0	 GA. 


CIFS	File	System	Support	
SLES	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
21	
SLES	Â 15	Â File	Â System	Â Support	
Table	Â 24:	Â SLES	Â 15,	Â SLES	Â 15	Â SP1,	Â and	Â SLES	Â 15	Â SP2	Â (x86_	64)	1	
File	Â System	Database	
Unstructured	Â 	
data	Â 	
Oracle
11gR2,	Â 12c/18c/19c	Â 	
DB2	Â 	
9.7,	Â 10.1,	Â 10.5,11.1	Â 	
Maria	Â DB	
10.5	MongoDB	Â 	
EXT3	 	 	EXT3	 	
EXT4	EXT4	 	EXT4	 	
NFS	 V4/V3	 	 	NFS	 V4/V3	 	
XFS	 XFS	XFS	 	XFS	XFS	
AWS	 EFS	3	 	 	 	 	
Notes:
 1.	 For	 supported	 kernel	 versions	 see	 the	 CTE	 Compatibility	 Portal	 or	 the	 compatibility	 matrix	 PDF	 available	 at	  https://packages.vormetric.com/pub/cte_	compatibility_	
matrix.pdf.	
 2.	 AWS	 EFS	 support	 with	 and	 without	 on-	premises	 Direct	 Connect. 


CIFS	File	System	Support	
SAP	HANA	Support	(LDT	is	supported	on	SAP	HANA)	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
22	
LDT	Â Feature	Â for	Â SLES	Â 15	Â File	Â System	Â Support	
Table	Â 25:	Â SLES	Â 15	Â and	Â SLES	Â 15	Â SP1,	Â and	Â SLES	Â 15	Â SP2	Â (x86_	64)	
File	Â System	Database	
Unstructured	Â 	
data	
Oracle
11gR2,	Â 12c/18c/19c	
DB2	Â 	
9.7,	Â 10.1,	Â 10.5,	Â 11.1	
Maria	Â DB	
10.5	
EXT3	 	 	EXT3	
EXT4	EXT4	 	EXT4	
XFS	XFS	XFS	 	XFS	
NFS	 V4/V3	1	NFS	 V4/V3	NFS	 V4/V3	NFS	 V4/V3	
Note:
 1.	 Single	 node	 reky	 for	 LDT	 over	 NFS supported	 in	 CTE	 7.1.0	 GA.	
SAP	Â HANA	Â Support	Â (LDT	Â is	Â supported	Â on	Â SAP	Â HANA)	
Table	Â 26:	Â SAP	Â HANA	Â Support	
SAP	Â HANA	Â Version	Platform	Â Support	File	Â System	
1.00.122.06.1485334242	SLES	 12	 SP3	EXT4
XFS	
1.00.122.06.1485334242 	 	SLES	 12	 SP3 	 	
Supports	 new	 CBC-	CS1	 key	
EXT4
XFS	
1.00.122.06.1485334242	RHEL	 7.5	EXT4
XFS	
1.00.122.06.1485334242 	 	RHEL	 7.5	 	
Supports	 new	 CBC-	CS1	 key	
EXT4
XFS 


CIFS	File	System	Support	
SAP	HANA	Support	(LDT	is	supported	on	SAP	HANA)	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
23	
SAP	Â HANA	Â Version	Platform	Â Support	File	Â System	
1.00.122.06.1485334242 	 	SLES	 12	 SP3	 (MS	 Azure)	
Supports	 new	 CBC-	CS1	 key	
EXT4
XFS	
2.00.030.00.1522210459	SLES	 12	 SP3	
Supports	 new	 CBC-	CS1	 key	
EXT4
XFS	
2.00.030.00.1522210459	RHEL	 7.5	EXT4
XFS	
2.00.030.00.1522210459	RHEL	 7.5	
Supports	 new	 CBC-	CS1	 key	
EXT4
XFS	
2.00.030.00.1522210459	SLES	 12	 SP3	
Supports	 new	 CBC-	CS1	 key	
EXT4
XFS	
2.00.030.00.1522210459	SLES	 12	 SP3	 (MS	 Azure)	
Supports	 new	 CBC-	CS1	 key	
EXT4
XFS	
2.00.030.00.1522210459	SLES	 12	 SP4	 (supported	 with	 CBC-	CS1)	EXT4
XFS	
2.00.040.00.1553674765	SLES	 15	EXT4
XFS	
2.00.040.00.1553674765	RHEL	 8	EXT4
XFS	
Note
Test	 configuration	 coverage	 included	 Physical,	 Virtual	 (VMWare	 ESX),	 Virtustream	 environment	 and	 AWS. 


CIFS	File	System	Support	
Ubuntu	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
24	
Ubuntu	Â Support	
Ubuntu	Â 18.04	Â File	Â System	Â Support	
Table	Â 27:	Â Ubuntu	Â 18.04,	Â 18.04.1,	Â 18.04.2,	Â 18.04.3,	Â 18.04.4,	Â 18.04.5	Â (x86_	64)	1,3	
File	Â System	Database	
Unstructured	Â data	DB2
10.1	Â |Â 10.5	Â |Â 11.1	Â |Â 11.5	Informix	Â 12.1	MySQL	Â 5.5/5.6	MongoDB	Cassandra	Couchbase	Â 4.5	Â 	
EXT4	EXT4	EXT4	EXT4	EXT4	EXT4	EXT4	
NFS	 V3/V4	NFS	 V3/V4	 	NFS	 V4/V3	 	 	 	
XFS	 	XFS	 	XFS	XFS	XFS	 	
AWS	 EFS	2	 	 	 	 	 	 	
Notes:
 1.	 For	 supported	 kernel	 versions	 see	 the	 CTE	 Compatibility	 Portal	 or	 the	 compatibility	 matrix	 PDF	 available	 at	  https://packages.vormetric.com/pub/cte_	compatibility_	
matrix.pdf.	
 2.	 AWS	 EFS	 support	 with	 and	 without	 on-	premise	 Direct	 Connect.	
 3.	 CTE	 only	 supports	 4.15.0-	20-	generic	 and	 5.3.0-	19-	generic	 kernels. 


CIFS	File	System	Support	
Ubuntu	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
25	
LDT	Â Feature	Â for	Â Ubuntu	Â 18.04	Â File	Â System	Â Support	
Table	Â 28:	Â Ubuntu	Â 18.04,	Â 18.04.1,	Â 18.04.2,	Â 18.04.3,	Â 18.04.4,	Â 18.04.5	Â (x86_	64)	1Â 	
File	Â System	Database	
Unstructured	Â data	DB2
10.1	Â |Â 10.5	Â |Â 11.1	Â |Â 11.5	MySQL	Â 5.5/5.6	PostgresSQL	MongoDB	
EXT3	EXT3	EXT3	EXT3	EXT3	
EXT4	EXT4	EXT4	EXT4	EXT4	
XFS	 	XFS	XFS	XFS	XFS	
NFS	 V4/V3	2	NFS	 V4/V3	NFS	 V4/V3	 	 	
Note:
 1.	 CTE	 only	 supports	 4.15.0-	20-	generic	 and	 5.3.0-	19-	generic	 kernels.	
 2.	 Single	 node	 reky	 for	 LDT	 over	 NFS supported	 in	 CTE	 7.1.0	 GA. 


CIFS	File	System	Support	
Ubuntu	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
26	
Ubuntu	Â 20.04	Â File	Â System	Â Support	
Table	Â 29:	Â Ubuntu	Â 20.04	Â (x86_	64)	1	
File	Â System	Database	
Unstructured	Â data	DB2
10.1	Â |Â 10.5	Â |Â 11.1	Â |Â 11.5	Informix	Â 12.1	MySQL	Â 5.5/5.6	MongoDB	Cassandra	Couchbase	Â 4.5	Â 	
EXT4	EXT4	EXT4	EXT4	EXT4	EXT4	EXT4	
NFS	 V3/V4	NFS	 V3/V4	 	NFS	 V4/V3	 	 	 	
XFS	 	XFS	 	XFS	XFS	XFS	 	
AWS	 EFS	2	 	 	 	 	 	 	
Notes:
 1.	 For	 supported	 kernel	 versions	 see	 the	 CTE	 Compatibility	 Portal	 or	 the	 compatibility	 matrix	 PDF	 available	 at	  https://packages.vormetric.com/pub/cte_	compatibility_	
matrix.pdf.	
 2.	 AWS	 EFS	 support	 with	 and	 without	 on-	premise	 Direct	 Connect. 


CTE	Agent	for	Cloud	Environment	
General	CTE	Support	for	AWS	AMI	and	Microsoft	Azure	on	Linux:	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
27	
LDT	Â Feature	Â for	Â Ubuntu	Â 20.04	Â File	Â System	Â Support	
Table	Â 30:	Â Ubuntu	Â 20.04	Â (x86_	64)	
File	Â System	Database	
Unstructured	Â data	DB2
10.1	Â |Â 10.5	Â |Â 11.1	Â |Â 11.5	MySQL	Â 5.5/5.6	PostgresSQL	MongoDB	
EXT3	EXT3	EXT3	EXT3	EXT3	
EXT4	EXT4	EXT4	EXT4	EXT4	
XFS	 	XFS	XFS	XFS	XFS	
NFS	 V4/V3	1	NFS	 V4/V3	NFS	 V4/V3	 	 	
Note:
 1.	 Single	 node	 reky	 for	 LDT	 over	 NFS supported	 in	 CTE	 7.1.0	 GA.	
CTE	Â Agent	Â for	Â Cloud	Â Environment	
General	Â CTE	Â Support	Â for	Â AWS	Â AMI	Â and	Â Microsoft	Â Azure	Â on	Â Linux:	
 l	CTE	 supports	 Ubuntu,	 SLES,	 and	 Red	 Hat	 OS	 images/platforms	 given	 baseline	 support	 and	 CTE	 support	 exists.	 	
 l	Please	 refer	 to	 Matrices/tables	 in	 section	 ""CTE	 Agent	 for	 Linux""	 on	 page 6	 for	 existing	 CTE	-supported	 OS	 platforms	 and	 applications.	
CTE	Â Support	Â on	Â AWS	Â EFS,	Â AWS	Â S3	Â File	Â Storage	Â Gateway,	Â AWS	Â Hadoop/HDFS:	Â EMR	Â and	Â 	
Hortonworks	
 l	CTE	 supports	 Amazon	 Elastic	 File	 System	 (EFS)	 on	 both	 AWS	 Cloud	 and	 AWS	 Direct	 Connect	 (on-	premises)	 configurations.	 Refer	 to	 the	 Matrix	 tables	 in	 the	 section	 	
""CTE	 Agent	 for	 Linux""	 on	 page 6	.   	
 l	CTE	 supports	 Amazon	 S3	 File	 Storage	 Gateway	 on	 cloud	 tiering	 and	 hybrid	 cloud	 backup. 	 S3	 buckets	 and	 their	 objects	 are	 mountable	 NFS	 to	 clients	 (one	 or	 more)	 in	 	
the	 AWS	 or	 on-	premises	 for	 data	 backup/restore	 with	 encryption	 protected	 by	 CTE	.	
 l	AWS	 Hadoop	 Hortonworks	 is now	 supported. 


CTE	Agent	for	Cloud	Environment	
Amazon	Linux	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
28	
 l	Amazon	 EMR	 with	 S3	 & S3	 File	 Storage	 Gateway	 is supported.	
 l	Cloud	 Object	 Storage	 (CTE	 COS	) is supported	 with	 Red	 Hat	 7 from	 CTE	 7.0.0	 GA	 onward	 and	 with	 Red	 Hat	 8 from	 CTE	 7.1.0	 onward.	 For	 details	 see	 the	 CTE	 Agent	 for	 	
Linux	 Advanced	 Configuration	 and	 Integration	 Guide	.	
Amazon	Â Linux	
Table	Â 31:	Â Amazon	Â Linux	Â File	Â System	Â Support	
File	Â System	Database	
Unstructured	Â data	DB2
10.1,	Â 10.5,	Â 11.1	
MySQL
5.5/5.6	MongoDB	
EXT3	EXT3	EXT3	EXT3	
EXT4	EXT4	EXT4	EXT4	
XFS	XFS	XFS	XFS	
NFSv4	NFSv4	NFSv4	 	
Note
Raw	 device	 is not	 supported	 with	 Amazon	 Linux. 


CTE	Agent	for	Cloud	Environment	
AWS	AMI	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
29	
Table	Â 32:	Â Amazon	Â Linux	Â Kernels	Â Supported	
Amazon	Â Linux	Â 2017.09	Â Kernels	
4.9.51-	10.52.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.58-	18.51.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.58-	18.55.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.62-	21.56.amzn1.x86_	64	 (6.0.3.18	 GA)	 	
4.9.70-	22.55.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.70-	25.242.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.75-	25.55.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.76-	3.78.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.77-	31.58.amzn1.x86_	64	 (6.0.3.18	 GA)	
4.9.81-	35.56.amzn1.x86_	64	 (6.0.3.23)	
4.9.91-	40.57.amzn1.x86_	64	 (6.0.3.68/6.0.3.111)	
AWS	Â AMI	
Refer	 to	 the	 Matrix	 tables	 in	 the	 section	 ""CTE	 Agent	 for	 Cloud	 Environment""	 on	 page 27	 for	 AWS	 AMI	 image	 (distributions)	 and	 application	 support. 


CTE	Agent	for	Cloud	Environment	
Microsoft	Azure	(Linux)	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
30	
Microsoft	Â Azure	Â (Linux)	
Refer	 to	 matrices/tables	 in	 section	 ""CTE	 Agent	 for	 Cloud	 Environment""	 on	 page 27	 for	 existing	 CTE	 supported	 OS	 platforms	 and	 applications	 for	 MS	 Azure	 distributions	 and	 	
application	 support.	
Table	Â 33:	Â Azure	Â Files	Â Supported	Â Linux	Â CTE	Â Distributions	
CTE	Â on	Â Azure	Â Files	Â SMB	Â 2.1	Â and	Â 3.0	Â Protocol	Â Support	Â 	
OS	Â vs	Â Azure	Â Files	Â Access	SMB	Â 2.1	
Â (VMs	Â from	Â Azure	Â cloud	Â only)	
SMB	Â 3.0	Â 	
(VMs	Â from	Â Azure	Â cloud	Â only)	Mount	Â Azure	Â files	Â from	Â on	Â Premises	Â machine	Â (SMB	Â 3.0	Â only)	
RHEL	 7.4	YES	YES	YES	1	
SLES	 11	 SP4	NO	NO	NO	
SLES	 12	 SP2/SP3	YES	YES	YES	1	
Note:
 1.	 CTE	 supports	 Ubuntu,	 SLES,	 and	 Red	 Hat	 mounted	 in	 Azure	 File	 Storage	 assuming	 baseline	 support	 and	 that	 CTE	 supports	 the	 OS	 and	 file	 system	 combination.	
The	 following	 features	 and	 configurations	 are	 not	 supported:	 	
 l	Live	 Data	 Transformation	
 l	Azure	 Blob	 storage	
 l	Rest	 API	 to	 Azure	 file	 storage	
 l	azcopy	 utility	 to	 access	 Blob	 or	 file	 storage 


CTE	Agent	for	Windows	
Windows	Application	Interoperability	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
31	
CTE	Â Agent	Â for	Â Windows	
Windows	Â Application	Â Interoperability	
Table	Â 34:	Â Windows	Â interoperability	Â with	Â IBMÂ Infosphere	Â Guardium	Â andÂ Imperva	Â Securesphere	
Product	Version	OS	Notes	
IBM	 Infosphere	 Guardium	v10.1.4	Windows	Compatible	
Imperva	 Securesphere	v9.0,	 v9.5,	 v10.5,	 v11	Windows	 2012	 R2	Compatible	
Table	Â 35:	Â Windows	Â CTE	Â LDT	Â Interoperability	Â with	Â backup	Â applications	
Product	Version	Notes	
Symantec	 Net-	backup	
 	
7.6.0,	 7.6.1.4,	 7.6.04,	 8.1.1	
 	
Compatible	
Symantec	 Backup	 Exec	 2012,	 2016	2012,	 2016	Compatible	
UltraBackup	10.0	Compatible	
Microsoft	 Backup	 with	 VSS	1	Win2012,	 Win2016,	 Win2019	Compatible	
Note:
 1.	 Thales	 has	 tested	 the	 backup	 applications	 with	 Volume	 Shadow	 Copy	 Service	 (VSS)	 for	 backup	 and	 restore.	
Table	Â 36:	Â Windows	Â CTE	Â LDT	Â Interoperability	Â with	Â Active-	Passive	Â clusters	
Product	Version	Notes	
Microsoft	 Cluster	2012	 R2/2016/2019 	SQL	 Servers,	 File	 Server	 Compatible	  with	 vmfiltr	 driver	 but	 not	 supported	 with	 VMLFS	 driver	 	
Veritas	 Cluster 	6.02 	Compatible	   with	 vmfiltr	 driver	 but	 not	 supported	 with	 VMLFS	 driver	  


CTE	Agent	for	Windows	
Windows	Application	Interoperability	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
32	
Table	Â 37:	Â Windows	Â CTE	Â Quantum	Â StorNext	Â Support	
Product	Version	Notes	
Quantum	 StorNext	2012	 R2/2016	 l	Structured	 data	 is not	 supported	 (i.e.	 database	 	
application)	
 l	LDT	 is not	 supported	
 l	CBC-	CS1	 key	 is not	 supported	
Table	Â 38:	Â Windows	Â File	Â System	Â Support	Â Â (Vmfiltr	Â driver)	
Â File	Â System	Database	Apps	
Windows	Unstructured	Â 	
data	
Oracle
10gR1
11gR1
11gR2
12c/18c	
DB2	Â 	
95/9.7
10.1
10.5
11.1	
Informix
11.5
11.7
12.1	
MySQL
5.5
5.6
8	
MS	Â 	
SQL
2012	b	
2014	Â b	
2016	Â bÂ 	
2017
2019	
MS	Â SQL	Â 	
on	Â MS	Â 	
AZURE
2012
2017	
MongoDB
3.2,	Â 3.4	
SharePoint	Â 	
2010/
2013/
2016	
Active	Â 	
Directory	Â 	
2012/
2016	
Active	Â 	
Directory
on	Â MS	Â 	
Azure
2012R2	
Microsoft	Â 	
Exchange	Â 	
DAG
2016	
Windows	 	
2012/R2
(x64)	a,c	
NTFS/CIFS/ReFS	NTFS	NTFS	NTFS	NTFS	NTFS	b 	NTFS	NTFS	NTFS	NTFS	 	 	
Windows	 10	
(x64)	a 	
NTFS/CIFS	 	 	 	 	 	 	 	 	 	 	 	
Windows	 	
2016	 (x64)	a,c	 	
NTFS/CIFS/
ReFS	
NTFS	NTFS	NTFS	NTFS	NTFS	b 	NTFS	NTFS	NTFS 	NTFS	 	NTFS	
Windows	 	
2016	 Core	 	
(x64)	a,c	
NTFS/CIFS/
ReFS	
NTFS	NTFS	NTFS	NTFS	NTFS	b 	NTFS	NTFS	NTFS 	NTFS	 	 	
Windows	 	
2019	 (x64)	a	
NTFS/CIFS/
ReFS	
NTFS	 	 	 	NTFS	b	 	 	 	 	 	 	
MS	 Azure	 	
Windows	 	
2012	 R2	
NTFS/CIFS/
ReFS	
 	 	 	 	 	NTFS	 	 	 	NTFS	  


CTE	Agent	for	Windows	
Windows	Application	Interoperability	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
33	
Â File	Â System	Database	Apps	
Windows	Unstructured	Â 	
data	
Oracle
10gR1
11gR1
11gR2
12c/18c	
DB2	Â 	
95/9.7
10.1
10.5
11.1	
Informix
11.5
11.7
12.1	
MySQL
5.5
5.6
8	
MS	Â 	
SQL
2012	b	
2014	Â b	
2016	Â bÂ 	
2017
2019	
MS	Â SQL	Â 	
on	Â MS	Â 	
AZURE
2012
2017	
MongoDB
3.2,	Â 3.4	
SharePoint	Â 	
2010/
2013/
2016	
Active	Â 	
Directory	Â 	
2012/
2016	
Active	Â 	
Directory
on	Â MS	Â 	
Azure
2012R2	
Microsoft	Â 	
Exchange	Â 	
DAG
2016	
MS	 Azure	 	
Windows	 	
2016	 	
NTFS/CIFS/
ReFS	
 	 	 	 	 	NTFS	 	 	 	 	 	
Notes:
 a.	 Supports	 AES-	NI.	
 b.	 Always	 encrypted,	 Always	 on,	 and	 File	 Table	 features	 are	 tested	 and	 supported.	
 c.	 Azure	 file	 storage	 is supported. 


CTE	Agent	for	Windows	
LDT	Feature	for	Windows	File	System	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
34	
LDT	Â Feature	Â for	Â Windows	Â File	Â System	Â Support	
Table	Â 39:	Â Windows	Â (Vmfiltr	Â driver)	
Â Â Â File	Â System	Database	Apps	
Windows	Unstructured	Â 	
data	
Oracle
10gR1
11gR1
11gR2
12c/18c	
DB2	Â 	
9.5/9.7
10.1
10.5
11.1	
Informix
11.5
11.7
12.1	
MySQL
5.5
5.6
8	
MS	Â 	
SQL
2012	b	
2014	b	
2016	b	
2017
2019	
MS	Â SQL	Â on	Â MS	Â 	
AZURE
2012
2017	
MongoDB
3.2,	Â 3.4	
SharePoint	Â 	
2010/
2013/
2016	
Active	Â 	
Directory
on	Â MS	Â 	
Azure
2012R2	
Microsoft	Â 	
Exchange	Â DAG	
2016	
Windows	 2012/R2	
(x64)	a,d	
NTFS/ReFS	NTFS	NTFS	NTFS	NTFS	NTFS	b 	 	NTFS	NTFS	 	 	
Windows	 10	
(x64)	a 	
NTFS	 	NTFS	NTFS	 	 	 	 	 	 	 	
Windows	 2016	 	
(x64)	a,c	 	
NTFS	NTFS	NTFS	NTFS	NTFS	NTFS	b	 	NTFS	NTFS	 	NTFS	
Windows	 2016	 Core	 	
(x64)	a,c	
NTFS	NTFS	NTFS	NTFS	NTFS	NTFS	b	 	NTFS	NTFS	 	 	
Windows	 2019	 	
(x64)	a,c	
NTFS	NTFS	 	 	 	NTFS	b	 	 	 	 	 	
MS	 Azure	 Windows	 	
2012	 R2	
NTFS	 	 	 	 	 	NTFS	 	 	NTFS	 	
MS	 Azure	 Windows	 	
2016	 	
NTFS	 	 	 	 	 	NTFS	 	 	 	 	
Notes:
 a.	 Supports	 AES-	NI.	
 b.	 Always	 encrypted,	 Always	 on,	 and	 File	 Table	 features	 are	 tested	 and	 supported.	
 c.	 LDT	 support	 for	 Windows	 2016 testing is	 completed	 but	 there	 is no	 ReFS	 support. 


CTE	Agent	for	Windows	
LDT	Feature	for	Windows	File	System	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
35	
Table	Â 40:	Â Windows	Â Â (VMLFS	Â Driver)	
Â Â Â File	Â System	
Windows	Unstructured	Â 	
data	
Windows	 2012/R2	
(x64)	a,d	
CIFS	
Windows	 10	
(x64)	a	
CIFS	
Windows	 2016	 (x64)	a,c	CIFS	
Windows	 2016	 Core	 (x64)	a,c	CIFS	
Windows	 2019	 (x64)	a,c	CIFS	
MS	 Azure	 Windows	 2012	 R2	CIFS	
MS	 Azure	 Windows	 2016	 	CIFS	
Note:
 1.	 Thales	 has	 tested	 the	 backup	 applications	 with	 Volume	 Shadow	 Copy	 Service	 (VSS)	 for	 backup	 and	 restore. 


Anti-	Virus	Support	
LDT	Feature	for	Windows	File	System	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
36	
Anti-	Virus	Â Support	
Table	Â 41:	Â Anti-	virus	Â support	Â on	Â WindowsÂ 	Â (x64-	bit)Â 	
Product	Version	OS	Notes	
Symantec	 End	 Point	 Protection	v12.1,	 v14.0.1_	MP2	Windows	 2012R2,	 Windows	 2016,	 Windows	 2019	Compatible	
McAfee	 VirusScan	 Enterprise	 + Antispyware	 Enterprise	8.8.0900	Windows	 2012R2,	 Windows	 2016,	 Windows	 2019	Compatible	
McAfee	 Endpoint	 Security	10.7.0	Windows	 2012R2,	 Windows	 2016,	 Windows	 2019	Compatible	
Sophos	 Home	3.2.2	Windows	 10	 	 	
Trend	 Micro	 for	 Small	 Business	11.0	 	 	
Microsoft	 Defender	4.18	 	 	
DSM	Â and	Â VTE/CTE	Â Agent	Â Software	Â Version	Â Compatibility	
The	 following	 table	 shows	 the	 compatibility	 between	 the	 DSM	 and	 VTE/CTE	 Agent.	 	
Table	Â 42:	Â DSM	Â vs.	Â VTE/CTE	Â Agent	Â Version	Â Support	
DSM	Â 	
Version	
VTE/CTE	Â Agent	Â Version	
5.2.3	5.2.4	5.2.5	6.0	6.0.1	6.0.2	6.0.3	6.1.0	6.1.1	6.1.2	6.1.3	6.2.0	6.2.1	6.3.0	6.3.1	7.0.0	7.1.0	
5.2.3	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	No	No	No	No	No	No	
5.3.0	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	No	No	No	No	No	No	
5.3.1	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	No	No	No	No	No	No	
6.0	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.0.1	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.0.2	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes 


End	of	Support	Information	
LDT	Feature	for	Windows	File	System	Support	
Compatibility	Matrix	for	CTE	Agent	with	DSM	
Copyright	Â©2009-	2021	Thales	Group.	All	rights	reserved.	
37	
DSM	Â 	
Version	
VTE/CTE	Â Agent	Â Version	
5.2.3	5.2.4	5.2.5	6.0	6.0.1	6.0.2	6.0.3	6.1.0	6.1.1	6.1.2	6.1.3	6.2.0	6.2.1	6.3.0	6.3.1	7.0.0	7.1.0	
6.0.3	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.1.0	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.2.0	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.3.0	No	No	No	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.4.0	No	No	No	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.4.1	No	No	No	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.4.2	No	No	No	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.4.3	No	No	No	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.4.4	No	No	No	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
6.4.5	No	No	No	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	Yes	
End	Â of	Â Support	Â Information	
The	 following	 third-	party	 products	 are	 no	 longer	 supported	 by	 CTE.	
Table	Â 43:	Â End	Â of	Â Support	Â Information	
Product	Last	Â VTE	Â Version	Â with	Â Product	Â Support	
VxFS	 File	 System	6.1.3 

	
 
 
 
 
 
 
 	
Contact	Â us	
For	 office	 locations	 and	 contact	 information,	 	
visit	 cpl.thalesgroup.com/contact-	us	
>Â cpl.thalesgroup.com	Â < 

"
Non,"
 
    	
GoldenGate Augments Cloud Databases 	
GoldenGate Augments Cloud 
Databases 	
by, David Floyer 	
May 21st, 2021 
Oracle GoldenGate is now offered as an integral part of Oracle Cloud Infrastructure (OCI), which dramatically 
lowers the costs of provisioning and maintaining the GoldenGate advanced features for all the Database 
Platforms supported. Previous Wikibon research established that Convergence & Distribution are important 
dimensions of any Cloud Database. 
Wikibon strongly recommends that senior enterprise executives focus intently on the development of Cloud 
Database Platforms, which are fully converged and excel across both transactional and analytic data types. The 
converged database must also support Data Mesh features and allow distributions of data where the data is 
created and the ability of lines of business to work directly and own the data that is vital to them. They will 
radically improve the cost, time-to-value, and functionality when used to implement data-driven strategies. 	
Â© 2021 Wikibon Research | Page 1  


 	
                          
                         
                  
                      
                      
                         
                  
 	
     	
                               
                         
                       	
 	
     	
                      
                           
                               
                           
                     
                            
               
     	
                           
                  
                             
     
                          
                   	
   	
                                 
                             
       
                         
                     
                       
                                 
                           
       	
GoldenGate Augments Cloud Databases 	
Premise 
Oracle  GoldenGate  is now  offered  as an  integral  part of Oracle  Cloud Infrastructure  (OCI), which 
dramatically  lowers the costs  of provisioning  and maintaining  the GoldenGate  advanced features for all the
Database  Platforms supported.  Previous Wikibon research  established  that Convergence  & Distribution  (see 
the  section  below for definitions)  are important  dimensions  of any  Cloud  Database  Platform. 
The  premise  of this  research  is that  GoldenGate  improves the convergence-dimension  and significantly
improves  the distribution-dimension  for all the  Cloud  Database  Platforms that GoldenGate  supports. As a 
result,  GoldenGate  improves the integration  between different databases  and significantly  reduces 
database  conversion. 
Scope of this  Research 
This  research  adds the impact  of GoldenGate  on the  Oracle  Cloud Database  Platform and adds  SAP HANA
into  the earlier  research  titled âCloud  Database  Platform Positioning,â  published February, 19th 2021.  The 
research  has also  been  updated  with additional  changes and feedback  from the community. 
GoldenGate  101 
The explosion  of Databases 
Databases  were originally  for systems  of record  (transactional  databases) and analytic  systems  (data 
warehouse  databases).  In the  last  decade,  enterprises  have deployed  many new databases,  such as NoSQL,
log,  graph  databases,  and many  others.  Wikibon  would point out that  the reason  for these  database  types 
is  the  emergence  of new  data  types.  Log files  have  very different  characteristics  and processing
requirements  from systems  of record.  Splunk is a very  effective  log database. 
The  increase  in data  types  is continuing.  Blockchain,  machine learning,  and inference  data types  are the 
newest,  and Wikibon  expects this trend  to continue  developing. 
Changing  Enterprise  Database Strategy 
As a result  of the  increases  in data  types,  most enterprises  have business-critical  data distributed  across 
the  enterprise  in heterogeneous  databases. However, enterprise  system designers  are increasingly
focusing  on simplifying  and automating  business processes  as part  of a drive  to increase  efficiency  and 
flexibility,  i.e., digital  transformation. 
As  a result,  the system  architects,  designers, and programmers  need to integrate  data from  different
sources  and databases  in real-time  or near-real-time.  This is not  easy. 
Avoiding  Database  Conversion 
The Irish  are famous  for their  humorous  reply when  asked  for directions,  âI wouldnât  start from  here.â  Many
database  vendors argue that the solution  to heterogeneous  databases is to  convert  every database  to their 
database  or set  of databases. 
Anybody  with experience  with database  conversions  knows this solution  does not work,  whatever  gimmicks
and  magic  conversion  software are promised.  Wikibon knows first-hand  how wholesale  conversion 
strategies  have caused  banks, insurance  companies,  and many  other enterprise  types irreparable  harm. 
The  cost  of database  conversion  and the disruption  to the  lines  of business  makes a conversion  strategy a
non-starter.  Enterprises should avoid this strategy  like the plague  and remove  any employees  or suppliers 
that  advocate  such an approach. 	
Â© 2021 Wikibon Research | Page 2  


 	
   
                              
                                 
                 
                        
                           
                                       
                        
                               	
    	
                           
                    
               
         
               	
l	
             	l	
                         	l	
                         
        	
l	
                         	l	
                         
                   
                        
                       
   
                               
                                  
       
                            
                          
                 	
        	
                      
                         
                     
       	
       	
GoldenGate Augments Cloud Databases 	
Avoiding  Database  Sprawl 
Enterprises  need to take  advantage  of different  data types  and use databases  to manage  these new data
types.  However,  introducing  a new  database  for every  new data  type will lead  to database  sprawl and make 
it  much  more difficult  to achieve  automation  and digital  transformation. 
Wikibon  strongly  recommends  an alternative  strategy of minimizing  the number  of databases  for different
data  types.  Enterprises  can achieve  this by using  converged  databases that support  multiple  data types. 
For  example,  Oracle and SAP  support  almost all data  types,  whereas  each AWS database  and Snowflakeâs
database  support very few data  types. 
Wikibon  recommends  enterprises choose databases  carefully, with the main  criteria  being the ability  to 
support  all data  types  and support  distribution  of databases  across the enterprise  in real-time  or near  real-
time. 
Augmenting  Databases with GoldenGate 
Oracle  acquired  GoldenGate  in 2009.  It is  a comprehensive  software package for real-time  data integration
and  replication.  It also  enables  high availability  solutions, transactional  change data capture, 
transformations,  and verification  between operational  and analytical  enterprise  systems. 
Oracle  GoldenGate  has the following  features: 
Moves  committed  transactions,  which enables  consistency  and improves  performance. 
Moves  data in real-time,  which reduces  end-to-end  latency. 
Supports  a wide  range  of heterogeneous  databases running on a variety  of operating  systems. For 
example,  you can replicate  data from  an Oracle  Database  to a different  heterogeneous  database. 
The  GoldenGate  microservices  architecture is âcloud-native.â 
The  tight  integration  allows high performance  with minimal  overhead  on the  underlying  databases and 
infrastructure. 
Oracle  GoldenGate  enables the exchange  and manipulation  of data  at the  transaction  level among  multiple,
heterogeneous  platforms across an enterprise.  It moves  committed  transactions  with transaction  integrity 
and  low overheads  across existing  infrastructures.  Its modular  architecture  enables users to extract  and
replicate  selected data records,  transactional  changes, and changes  to DDL  (Data  Definition  Language) 
across  various  topologies. 
Oracle  GoldenGate  is used  in many  environments  as a tool  to enable  database  integration.  For example,  it
is  certified  on the  AWS  site to support  AWS Aurora  and other  databases.  There are also  other  tools and 
software  that achieve  similar results. 
Wikibon  believes  that the close  integration  of GoldenGate  with OCI significantly  changes the skill  and time
required  to set  up and  maintain  these environments.  Wikibon believes  this integration  is a gamechanger 
and  expands  its potential  use to enterprises  of any  size. 
GoldenGate  Business Continuity  and High  Availability 
GoldenGate  is an  important  component  of the  Oracle  Maximum  Availability  Architecture  (MAA). To establish 
and  maintain  an MAA  environment,  data must  move  between  multiple servers and data  centers. 
GoldenGate  provides bidirectional  active-active  replication of Oracle  and other  databases  to support  the 
highest  levels of business  continuity. 
Initial Load and Database  Migration 	
Â© 2021 Wikibon Research | Page 3  


 	
                                                            
                     
 
                       
                         	
      	
                           
                             
             
                          
                   
                         	
m	
              
                           	
m	
                       
           	
       	
        	
                        
                           
                      
                           
                          
                           
 
       
                           
                               
                                 
                             
                            
                   
                        
                       
 	
GoldenGate Augments Cloud Databases 	
Initial  load is extracting  data records  from a source  database  and loading  those records  onto a target 
database.  Initial load is a data  migration  process that is performed  only once.  Oracle  GoldenGate  allows
you  to perform  initial load data  migrations  without taking your systems  offline. 
Data Integration 
Data integration  involves combining  data from  several  disparate  sources stored using various  technologies
and  providing  a unified  view of the  data.  Oracle  GoldenGate  provides real-time  data integration. 
Definitions  of Convergence  & Distribution 
Previous  Wikibon research  established  there are two  main  dimensions  to the  evaluation  of Cloud  Database 
Platforms. 
1.  Convergence  is the  ability  to use  multiple  data and database  types in real-time  or near-real-time  with 
predictable  performance.  Convergence capabilities include enabling  transactional  and analytic 
databases  to work  together,  as the  most  valuable  applications  will combine  both. Enterprises  can use 
GoldenGate  to improve  combining  different database  types. 
2.  Distribution  is the  ability  to: 
Distribute  copies of data  to meet  availability  and consistency  requirements  of databases  in many  local 
domains.  Goldengate  can significantly  assist in managing  this distribution. 
Distribute  domain ownership  of databases  in the  form  of a âData  Meshâ  while maintaining  metadata 
control  to enable  enterprise  compliance,  provenance,  and data  security.  (See the reference  to Data 
Mesh  in Footnotes  below for more  detail.) 
GoldenGate  & Cloud  Database  Platforms 
GoldenGate  Announcement  as a Database  Service 
Oracle has recently  announced  GoldenGate  as an  OCI  database  service. In recent  research,  Wikibon 
identified  two components  that are vital  for a successful  Cloud Database  Platform. These are Database 
Convergence  and Data  Distribution.  GoldenGate in OCI  will significantly  enhance the Data  Distribution
component  of most  Cloud  Database  Platforms,  including Oracle, AWS RDS,  IBM DB2,  and Microsoft  SQL 
Server.  This capability  dramatically  extends the range  of customers  who will take  advantage  of the
GoldenGate  functionality.  IT does  not need  to invest  resources  to install,  manage,  and maintain  the 
software  on-premises. 
Data Warehouse  & Lake  Problems 
There are many  database  analystâs views available  for subsets  of the  database  and database  management. 
Data  warehouses  and data  lakes  are often  stripped  off and  reviewed  as a stand-alone  product. Wikibon and
other  researchers  have shown  that the CIO  and  the lines  of business  think that the investments  in data 
warehouses  and data  lakes  have not yielded  the results  that the vendors  promised.  CIO concerns  include
late  delivery  of data,  data compliance  and provenance,  poor documentation,  and too many  versions  of the 
truth. 
Wikibon  believes  that database  convergence  and database  distribution  capabilities are important  because
they  allow  enterprises  to integrate  data sources,  ensure consistency  between distributed  data, and provide 
automated  tools to ensure  compliance  and provenance.  GoldenGate is an  important  element in achieving
these  goals. 	
Â© 2021 Wikibon Research | Page 4  


 	
      	
                         
                     
                        
                            
                           
       	
       	
                             
                             
                               
                               
                                   
   
                               
                               
                               
             
                      
                             	
       
 	
                         
                    
                        
                      
                   
                             
                           
           
                             
                   
                       
                 
                           
                               
                                                                                         
                         	
GoldenGate Augments Cloud Databases 	
Technology  Impacts on Database  Architecture 
Wikibon believes  that database  technologies  have evolved  to remove  the hard  break  between  transactional
systems  of record  and data  warehousing.  Data in-memory  technologies,  persistent storage layers, smart 
flash  storage,  and mixed  columnar  and row-based  architectures  have allowed  operational  systems of record
to  use  real-time  analytics when needed,  proven provenance  and compliance,  and a single  version  of the 
truth.  With instant  scaling  of database  cloud services,  these capabilities  are central  to allowing  full 
automation  of complex  business  processes. 
Technology  Impacts on Database  Distribution 
The other  technology  impact is the  cost  of computing.  Traditional linear x86 architectures  doubled in speed
or  number  of cores  every  18 months  to two  years  (Mooreâs  Law). However,  new heterogeneous  parallel 
architectures  from Arm have  improved  processing  speed by 118%  over the last  5 years,  measured  in TOPS
or  Trillions  of Operations  per Second.  The cost  of compute  is coming  down, especially  in Matrix  workloads 
such  as ML  and  AI. The  comparative  cost of compute  operations  is dropping  far faster  than the cost  of
storage  and networking. 
This  means  enterprises  will push  real-time  computing  to where  the data  is created,  mostly at the  Edge. 
Enterprises  will immediately  process most of this  data  at the  Edge,  keeping  only small  amounts  of valuable
resultant  data there.  Again,  enterprises  will push  requests  for analysis  to the  Edge  rather  than move  data 
across  the network  to a traditional  data lake. 
Databases  will need  to manage  distributed  real-time data, manage  compliance  and provenance,  and
manage  a single  version  of the  truth.  GoldenGate  will play  an important  role in this  evolution. 
Challenges  of Traditional  Databases;  How Cloud 
Databases  Help 
Complexity 
Traditional databases need expensive  people specialized  in software  and hardware  to run  large  database
systems.  DBAs, storage  specialists,  system administrators,  and network  specialists  must keep databases 
and  infrastructure  up to date,  optimize  performance,  isolate bottlenecks,  and keep  fragile  systems  running.
Data  workflows  are complex  and tortuous  for transactional,  analytic, and other  single-purpose  database 
systems,  requiring  data managers,  data architects,  data scientists,  data engineers,  statisticians,  storage
specialists,  and many  more specialized  and expensive  people to manage  the data  stored  in many  silos. 
Data  is moved  and transformed  without context.  The resulting  centralized  data lakes  and data  warehouses
are  the  triumph  of hope  over experience. 
1.  The  first  way  Cloud  Database  Platform Providers  can tackle  the problem  of complexity  is by  providing 
autonomous  capabilities. The Cloud  Database  Provider delivers automation  of upgrades,  indexing, 
performance,  recovery, backup, patching,  and more.  Providers  with volume  and who  utilize  machine 
learning  algorithms  based on many  users will drive  continuous  improvements. 
2.  The  second  way Cloud  Databases  can provide  radical simplicity  is to  support  the physical  distribution 
and  management  of data  close  to where  the enterprise  first creates  and uses  it. The  people  responsible 
for  data  creation  are usually  best positioned  to define  what the data  means  in their  domain  and keep 
the  data  in context  with other  data and processes  to support  its business  needs. 
3.  The  third  requirement  for an enterprise  Cloud Database  is to  automate  the collection  and distribution  of 
metadata  between all the  domains  and provide  the services  to optimize  and predict  workloadsâ 	
Â© 2021 Wikibon Research | Page 5  


 	
         
                             
                             
           	
     	
                             
                                              
                      
     
                      
                           
                                                     
                       
                        
                      
                      
                        
                 
                         
                       
                       
                       
                      
 	
 	
                       
                         
                                 
                       
                       
                         
                     
             
                        
                           
         
                               
                             
                               
                           
                            
   	
GoldenGate Augments Cloud Databases 	
performance  using data over multiple  locations. 
Wikibon  believes  that autonomous  databases are essential  to reduce  the cost  and complexity  of using  data. 
This  simplicity  and automation  will allow  domain  business  staff to directly  define and consume  their data
and  broadcast  the metadata  to other  domains. 	
Database  and Data  Types 
Database  and data  types  have exploded  in the  last  two  decades,  with many  providers  introducing  different
database  types for each  one. Examples  include Advanced  Analytic, AI Inference,  AI Learning,  Blockchain, 
Document,  Graph, Key-value,  Inference, In-memory,  Log-file, NoSQL, and Time-series  databases. Each of
these  databases  deals with different  data types  and provides  specialized  structures to improve 
performance  and reduce  complexity. 
Some  of these  database  types have robust  independent  implementations,  such as MongoDB  (Document),
SAP  HANA  (In-memory),  and Splunk  (Log-file).  Cloud providers  support them all; for example,  AWS has 
sixteen  different  database  types. However,  the user  workflows  between the different  database  types have
become  extraordinarily  complex and insecure.  Data transformations  and the time  to move  data result  in 
higher  costs, loss of data  context,  and significantly  reduce the dataâs  business  value. 
The  longer  elapsed  times mean  that synchronizing  applications using transaction  and analytic  databases  is
much  more challenging  (and usually  impossible)  to achieve.  Synchronous  applications enable a more 
significant  potential for automating  business processes,  making them more valuable  in creating  data-first
business  processes.  The bottom  line is converged  databases that scale  allow  faster  automation  and 
simplification  of business  processes  and reduce  complex  asynchronous  business processes. 
Oracle,  SAP HANA,  and to some  extent  Couchbase,  have developed  converged  databases with a single
database  engine providing  integrated  support for different  databases  and data  types.  The converged 
database  supports transactional  and analytic  database  types working  together.  Equally important  is the
performance  and automation  of each  database  type within  the converged  database. Converged  databases 
provide  an essential  game-changing  reduction in complexity  with equal  or better  performance  than 
specialized  databases. 
Database  Performance 
Cloud Database  Platforms need to support  synchronous  business processes  in real-time  or near-real-time.
These  database  platforms  are complex  and require  specialized  hardware and platform  services to optimize 
performance.  Wikibon is predicting  that flash  drives  will be the  same  cost as HDDs  by 2026.  Flash and
other  non-volatile  memory technologies  will allow  simple  single-tier  storage solutions  that help  enable 
distributed  database solutions.  Improved protocols such as NVMe  and RoCE  (RDMA  over Converged
Ethernet)  radically reduce protocol  overhead  and improve  latency. NVMe also provides  faster and lower 
costs  for any-to-any  connectivity  of processors  and storage.  Non-volatile  memory technology  also simplifies
recovery  and restart  and provides  lower-cost  shared caches. 
Wikibon  believes  that elastic  scalability  is a crucial  attribute  for successful  Cloud Databases.  These 
databases  must use cloud  services  to scale  resources  instantaneously  for short  periods  while the database
keeps  running  to minimize  elapsed time-to-value. 
Arm  processors  and systems  are now  faster  and cheaper  than x86 processors,  and the rate  of improvement 
is  significantly  faster. The challenge  for Arm  processors  is that  vendors  have designed  almost all database
software  for x86  processors.  However, the manufacture  of Arm  wafers  is over  ten times  larger  than x86 
wafers.  This volume  disparity  means that the learning  curves of both  manufacturing  and design  have
outstripped  x86. The latest  Apple  M1 processors  are performing  faster than the latest  equivalent  Intel Tiger 
Lake  i7 chips. 	
Â© 2021 Wikibon Research | Page 6  


 	
                                                              
                            
                               
                          
                       
                        
     	
 	
                         
                      
                             
                 
                                 
                               
                        
                          
                               
                                                         
                 
                         
                               
               
          	
                          
                      
                   	
GoldenGate Augments Cloud Databases 	
x86 currently  has an advantage  because vendors will need  time and resources  to migrate  software  to Arm. 
However,  Apple and Microsoft  are converting  their PC platforms  to run  primarily  on Arm  and extend  the
platform  to support  Arm mobile  applications . AWS  has invested  in Arm-based  Graviton and is migrating  its 
platform  to Arm  at a rapid  rate. Wikibon  expects cloud platforms  and Cloud  Database  Platforms to migrate
to  work  with Arm,  mainly  because  Apple and Arm  have  developed  heterogeneous  architectures that can 
accelerate  specific workloads  by a hundred  times compared  with the general-purpose  x86 architectures . 
Wikibon  believes  that Cloud  Database  Platforms will adopt  heterogeneous  Arm technology  early because  of 
databasesâ  particular performance  requirements. 
Data Distribution 
Enterprises  are increasingly  distributing the location  of data  creation.  The continued  reduction in the  cost 
of  Micro-Electrical  Mechanical Systems (MEMS) capabilities  is pushing  enormous  amounts of data  creation
to  the  Edge  in warehouses,  retail outlets,  energy distribution,  and more.  Edge devices  must often  be 
autonomous,  such as autonomous  cars, planes,  trains, and other  machines. 
Moving  large amounts  of data  is very  expensive.  It takes  a significant  amount of time,  and (as previously
discussed)  data loses  context  if not  processed  at the  point  of creation.  It follows  that databases  must 
support  on-premises  equipment and processing,  in addition  to the  cloud.  This distributed  data processingâs
primary  use is almost  always  to support  remote business  operations,  such as plants,  warehouses,  and 
autonomous  vehicles. The secondary  benefit is to  provide  data in context  to other  parts of the  business. 
Cloud  Database  Platforms must therefore  be available  in both  public  and private  clouds.  The business
people  who support  the business  operations  at each  location  must define  their data to support  their 
operations  and simultaneously  provide access and context  to other  sites. 
At  the  same  time, organizations  must ensure  compliance  with legal  requirements  and ensure  all dataâs
provenance  and safety  at the  local  and national  level. Databases  will need  to support  a âData  Meshâ 
approach  and the metadata  requirements  to achieve  this goal. 
Cloud  Database  Platforms:  Horses on the  Track 
Wikibon  believes  there are two  fundamental  requirements  for a Cloud  Database  Platform: the ability  to
support  converged  databases and support  a devolved  distributed  business environment.  Figure 1 provides 
Wikibonâs  assessment  of the  current  position  of Cloud  Database  Platform providers. 	
Â© 2021 Wikibon Research | Page 7  


 	
                
     	
                          
                           
                           
                           
                         
             
                           
                                                               
                                 
                             
                       
                        
 
                         	
GoldenGate Augments Cloud Databases 	
Figure 1: Oracle  with GoldenGate  lead Cloud  Database  Platform Positioning 
Source:  Â© Wikibon  2021 	
Figure 1 shows  the Wikibon  assessment  of Distribution  capability on the  y-axis  and Convergence  capability
on  the  x-axis.  The diameter  of the  circle  is the  multiplication  of the  two  scores. 
GoldenGate  support AWS RDS (yellow),  IBM DB2  (light  blue),  Microsoft  SQL Server  (grey),  and Oracle 
Databases  (red). Each of these  Cloud  Database  platforms  is assessed  with and without  GoldenGate,  which
provides  distribution  benefit and a small  convergence  benefit. The arrows  connect  the original  score 
without  GoldenGate  to a higher  score with GoldenGate. 
GoldenGate  does not support  CouchBase  (green) and provides  limited support  for SAP  HANA  (dark blue),
and  Snowflake  Cloud Database  platforms  (black). There is a single  point for these  platforms.  The individual 
scores  on each  Database  Cloud Platform  in Figure  1 are  discussed  in more  detail  in the  following  sections. 
Overall,  the chart  shows  that the Oracle  Cloud Database  platform with GoldenGate  is a clear  leader  on both
dimensions.  However, Wikibon observes  that it is  still  early  in the  evolution  of Cloud  Database  Platforms. 
There  is still  significant  potential for improvement  on the  distribution  dimension, which needs  additional
features  such as distributed  data catalogs  and additional  advanced distribution  capabilities to support  data-
meshing. 
AWS 
AWS  has taken  16 open-source  databases, forked most of them  with proprietary  improvements  not given 	
Â© 2021 Wikibon Research | Page 8  


 	
                              
                       
                                   
       
                         
                            
                      
                      
                             
                                   
                             
                                 
                           
           
                        
                   
                       
                                                         
 
                           
                          
         
              
                                 
                             
             
                          
                            
                          
                               
                   
                     
                                
   
                   	
GoldenGate Augments Cloud Databases 	
back to the  community,  and integrated  them well but separately  into the AWS  PaaS.  On the  Distribution 
axis,  AWS has delivered  Outposts,  allowing at the  moment  limited on-premises  RDS implementations,
which  is currently  a small  subset  of the  16 databases.  AWS also has good  support  for data  protection  across 
different  regions in different  countries. 
AWS  has implemented  some enhanced  data movement  between different databases  but has  no announced
strategy  to invest  in converged  database integration.  As a result,  enterprises  that need  to combine  data 
from  different  databases  must perform  time-consuming  and costly  ETL. This architecture  makes combining
data  from  different  data types  (e.g., transactional  and analytic  data) challenging  and time-consuming. 
Apart  from an interesting  blog on Data  Mesh, AWS  has not announced  any data  mesh  capabilities. 
GoldenGate  provides good support  for AWS  RDS and is available  as a tool  on the  AWS  platform.  However,  it
is  supported  as an  as-is  tool and is not  available  as a fully  managed  cloud service. 
The  availability  of GoldenGate  as a service  on OCI  is a significant  improvement  and provides  users of AWS 
RDS  with  consistent  active-active  options in real-time  analytics on OCI.  The GoldenGate  score signifies  this
type  of integration,  not an as-is  implementation. 
Couchbase 
Couchbase  focuses explicitly  on providing  support for transactional  and analytic  databases.  It claims  a
NoSQL  heritage  but has  implemented  a SQL-compliant  combination of analytical  and transactional 
databases. 
Couchbase  has limited  cloud database  distribution  or data  mesh  capabilities.  GoldenGate does not support
Couchbase. 
IBM 
IBM  offers  a Tier-1  (See Footnotes  for a definition  of Tier-1)  DB2 Cloud  Database,  which provides  integrated
transactional  and analytic  capabilities.  IBM provides  good support  for data  protection  but little  logical 
distribution  support. 
GoldenGate  gives excellent  support for DB2,  which  is found  in many  mainframe  systems and financial
organizations.  DB2 databases  are often  complex,  and avoiding  conversion  and integration  will be a sound 
strategy  for many  large financial  organizations. 
IBM  is working  on a separate  distributed  architecture  platform. 
It  is  unclear  if IBM  will integrate  the two  approaches;  Wikibon hopes they will, together  with Red Hat,
provide  a Tier-1  Cloud Database  Platform. Wikibon also hopes  that they  will improve  the open-source  legal 
foundation  to encourage  contributions  from AWS and others. 
Google 
Google  offers BigTable  and Spanner  databases  for OLTP,  and BigQuery  is a data  warehouse  database. 
There  is little  converged  capability between the OLTP  and data  warehouse  databases.  BigQuery is at  heart
a  columnar  database written to take  advantage  of cloud  scalability.  BigQuery is serverless,  has hybrid 
NoSQL  capabilities  such as record  type, and can hold  and address  raw JSON  documents.  BigQuery is a
lightly  converged  analytic database  that excels  when database  sizes are massive. 
Google  offers Google  Anthos for on-premises  Cloud Database  Platform requirements.  Google Anthos 
typically  runs on Cisco  servers  that are not used  by GCP,  meaning  a lack  of architectural  equivalency from
cloud  to on-premises. 
GoldenGate  certifies GCP BigQuery  and GCP  Object  Storage  as delivery  targets. 	
Â© 2021 Wikibon Research | Page 9  


 	
                         
                           
                           
                         
                          
                        
                               
                             
                             
                           
             
                        
                           
                         
                         
                            
                               
                     
                        
                               
                  
                    
                     
                         
                             
                          
   
                          
                  
                   
         
     
                          
                       
                    
                               
                          
                               
               
                         	
GoldenGate Augments Cloud Databases 	
Microsoft 
Microsoft has a Tier-1  SQL Server  Database  for on-premises  deployment.  Currently, it offers  Azure SQL
Cloud  Database  for transactional  cloud services  and Azure  Synapse  as a data  warehouse  Cloud Database. 
Wikibon  assesses  that users  will find  little  converged  capability between the SQL  Cloud  and Synapse 
Database  offerings. However, GoldenGate  certifies SQL Server  and Azure  Synapse  (delivery  target only) for
Microsoft  SQL Server,  and enterprises  can use it to  improve  integration  between transactional  and analytic 
workloads. 
Microsoft  is developing  Azure Cosmos  DB as a globally  distributed,  scalable, multi-model  database cloud
service  extension  to Azure  Synapse.  It is  building  this service  from the ground  up. Cosmos  DB provides 
native  support  for NoSQL  and OSS  APIs,  including  MongoDB,  Cassandra,  Gremlin, etcd, Spark,  and SQL.  It
offers  multiple  consistency  models from strong  to eventual  and says  it supports  low read  and write 
latencies.  Wikibon understands  that the emphasis  of Cosmos  DB is to  integrate  analytic requirements.  The
Microsoft  Azure SQL services  separately  support transactional  needs. 
Microsoft  offers Azure Stack for on-premises  Cloud Database  requirements.  As noted  in prior  Wikibon 
research,  Azure Stack requires  an Azure  Stack operator  on-site. Microsoft  offers a selection  of qualified 
systems  from five different  vendors.  Wikibon does not rate  this approach  as satisfactory,  lacking 
equivalency  as the  Azure  public  cloud does not deploy  any of these  hardware  systems. 
Microsoft  has the software  and architectural  capabilities to become  a leader  in providing  a Cloud  Database
Platform  and building  its strong  SQL Server  customer  base. It is  giving  mixed  signals  on intent.  GoldenGate 
certifies  both SQL Server  and Azure  Synapse  (Synapse  is delivery  target only). 
Oracle 
Wikibon  believes  Oracle has the leading  converged  database implementation  of all  other  providers  in 
Figure  1. In its  latest  Oracle  Database  21c announcement,  Oracle has improved  many aspects  of its  Tier-1
converged  Cloud Database  Platform, including  performance  improvements  for in-memory,  graph, and 
multi-tenant  processing. In addition,  JavaScript  is in-database.  AutoML for in-database  machine learning
(ML)  is an  additional  automation  capability. Blockchain  Tables provide  immutable  insert-only tables in 
Oracle  Database.  A native  JSON binary  data type was introduced,  which increases  Document  Database
performance  and function.  Oracle has a clear  ongoing  strategy  of reducing  complexity  for DBAs  and data 
users  through  automation,  performance,  and integration  of all  database  and data  types  and is executing
this  strategy  well. 
On  the  distribution  side, Oracle  has excellent  cluster and distributed  processing with RAC (Real  Application 
Clusters),  Active Data Guard  (active-passive  copy distribution),  and Sharding  (shared-nothing  geo-
distribution  of horizontally  partitioned data). Oracle  also announced  its distributed  sharding performance 
and  flexibility  enhancements  in Database  21c. 
Oracle Database  with GoldenGate 
Oracle  acquired  GoldenGate  Oracle in 2009  and is an  integral  part of Oracle  Maximum  Availability 
Architecture  (MAA). Wikibon  has always  admired  Oracle GoldenGateâs  unique ability to deploy  fault-tolerant
database  replication  combined with real-time  operational  analytic insights,  supporting  Oracle and non-
Oracle  data sources.  GoldenGate  is installed  in the  majority  of Fortune  2000 enterprises  as part  of MAA
implementations. 
The  downside  of GoldenGate  is the  complexity  and cost  of providing  and maintaining  these capabilities. 
However,  the latest  GoldenGate  release on OCI  is a managed  service, which solves  the complexity  and cost
problems  and extends  these capabilities  to every  company  size. 
GoldenGate  OCI service  in conjunction  with the Oracle  Database  Cloud platform  improves  the Convergence 	
Â© 2021 Wikibon Research | Page 10  


 	
            	
   
                                
                               
                       
                            
                     
                              
                             
                           
                              
               
                      
                             
                           
                               
                            
                                 
                                                               
                                                                 
                               
            
                         
                           
                         
                             
                            
                        
                     
                    
                      
                      
                                             
                         
         	
   	
 	
GoldenGate Augments Cloud Databases 	
dimension  slightly and significantly  improves the Distribution  dimension. 
SAP HANA  Cloud 
SAP HANA  is an  in-memory  SaaS service  where the updates  and management  of SAP  are managed  by SAP.
The  fundamental  structure of SAP  HANA  is a data  in-memory  columnar SQL database  that supports  a broad 
range  of data  and database  types, including  Analytics,  JSON documents,  Graph, Spatial,  Time-series,
Machine  Learning,  and Blockchain.  Its main  advantage  is to  shorten  the time  required  to create  SAP 
analytic  reports. SAP has an excellent  track record  in achieving  this benefit. 
The  SAP  HANA  architecture  also has a Row-based  view of the  data,  enabling  SAP to provide  transaction
processing  of SAP  systems  of record.  As discussed  in the  introduction,  this capability  is important  to 
enterprises  because it allows  applications  to have  consistent  and coherent  access to real-time  analytic data
and  provide  a single  source  of truth,  eliminating  the risk  of inconsistent  data across  the organization  and 
improving  the ability  to automate  complex business  processes  fully. 
However,  SAP does  not currently  provide the high  availability,  recovery, and transaction  performance
expected  for Tier-1  systems-of-record  in HANA  Cloud  S4. SAP  has said  that HANA  will support  all 
environments,  including HA transaction  services, by December  2025. On December  31, 2027,  SAP plans  to
withdraw  support for SAP  âLegacyâ  products running on other  databases.  This date  is the  latest  of many 
that  SAP has published.  Given customersâ  reluctance to move,  Wikibon  believes  this date  will move  out
again. 
Wikibon  would point out there  are only  three  Tier-1  Databases  at the  moment.  They are IBM  DB2,  Microsoft 
SQL  Server,  and Oracle  Database.  It took  the vendors  decades  to provide  and prove  the advanced  high-
availability  and recovery  features.  The software  market usually has a long  tail, and ERP  is no  exception. 
SAP  is a leading  vendor in ERP,  with about  6% of the  total  ERP market.  Wikibonâs  research indicates
that  most  of SAPâs  largest  customers  run on Oracle  Database.  Wikibon believes  SAP does  not have  the 
volume  of SAP  HANA  database  instances  to develop  full Tier-1  capabilities  and will not increase  volumes by
selling  SAP HANA  as a general-purpose,  stand-alone database. 
Wikibon  believes  the current  cloud databases  from Google,  IBM, Microsoft  Azure, Oracle  (and AWS, 
assuming  it invests  in developing  a fully  converged  database) will radically  improve multi-cloud  and cloud-
on-premises  converged database solutions.  Wikibon believes  it is  very  likely  and SAP  customers  will 
demand  to continue  to run  the Tier-1  database  of their  choice  for SAP  and other  software. 
Apart  from an interesting  blog on SAP  HANA  data mesh  capabilities , SAP  has limited  capabilities  for
implementing  a distributed  data mesh.  Some enterprises  use GoldenGate  with SAP HANA  with integration 
supported  via existing  certified  APIs such  as JDBC,  Kafka,  and Object  Storage . 
Snowflake 
Snowflake  has created  significant  momentum,  focusing on reducing  complexity  with improved  ease-of-use 
and  time-to-value  for data  warehouses.  Snowflake has also  introduced  enhanced capabilities  for sharing
data  warehouses,  with some  Data Mesh  capabilities.  However, Snowflake  has an isolated  cloud-only 
analytic  SQL database  with limited  advanced  functions and is still  in the  gate  regarding  convergence.
Snowflake  currently has an immature  ability to integrate  machine learning. 
Some  enterprises  use GoldenGate  with Snowflake  with integration  supported via existing  certified  APIs such 
as  JDBC,  Kafka,  and Object  Storage . 
Conclusions  and Recommendations 
General  Assessment 	
Â© 2021 Wikibon Research | Page 11  


 	
                            
                           
                            
         
                           
                          
     
                               
                          
           
                               
                          
                            
                          
             
                            
                               
 	
   
                    
       
                          
                       
                         
                       
                               
                           
                           
                         
                      
                         
                           
                        
                         
                       
                           
                                
                              
                           
                               
                           
             
                        
                           
                              	
GoldenGate Augments Cloud Databases 	
An early  study  of the  automobile  market concluded  that the number  of chauffeurs  available would constrain 
the  marketâs  size. Ford showed  that making  cars simple  and in volume  made chauffeurs  redundant.
Similarly,  the demand  for household  telephones  was thought  to be  constrained  by the  number  of telephone 
operators  before AT&T automated  phone calls. 
In  the  same  way, Wikibon  believes  that Cloud  Database  Platforms will remove  the dependence  on 
expensive  IT staff  by automating  many mundane  tasks and devolving  data management  and exploitation  to 
the  lines  of business. 
Figure  1 shows  the horses  on the  Cloud  Database  Platform track. Wikibonâs  overall assessment  is that  three
furlongs  in, Oracle  is lengths  ahead on the  Convergence  dimension. Oracle with GoldenGate  has moved 
ahead  of Snowflake  on the  Distribution  dimension. 
Both  of these  dimensions  are critical  to providing  a Cloud  Database  Platform with the automation,  ease of
use,  robustness,  and flexibility  to support  data-led  enterprises  without crippling  IT staff  overheads.  Figure 1 
also  shows  that across  providers,  convergence  is further  ahead than distribution.  There is still  room  for
innovation  in Cloud  Database  Platforms,  and companies  like Snowflake  have shown  the importance  of ease-
of-use  and devolution  to the  lines  of business. 
Wikibon  believes  that large  enterprises  understand  that the choice  of Cloud  Database  Platform is a more
critical  and strategic  decision than the choice  between  alternative  Cloud IaaS or PaaS  platforms  in the  data-
led  journey. 
Vendor  Future Assessments 
Databases  are complex  technologies  where, in addition  to convergence  and distribution,  automation, 
performance,  and reliability  are critical. 
Wikibon  assesses  that AWS  is doing  well within  its limitations.  It currently  supports mainly smaller
organizations  with smaller-scale  database requirements.  As Wikibon  noted in prior  research,  the more 
databases  and data  types  exist, the more  specialized  transfer systems  are required.  Sixteen databases
would  require  120 different  transformation  transport systems. Fifty database  types would  require  1,225. 
AWS  will undoubtedly  provide a stable  and highly  performant  IaaS and PaaS  platform  for itself  and other 
providers.  AWS is ahead  of other  vendors  moving to Arm  technology,  reducing costs and increasing
performance.  However, AWS still has to invest  in converged  and distributed  database software with built-in 
tier-1  and HA recovery  systems to be  a long-term  enterprise  player in cloud  databases. 
Suppose  AWS wants  to move  upmarket  and meet  the requirements  of enterprise-level,  mission-critical
databases  and provide  for the  aggressive  scope of future  automation  applications.  In that  case,  Wikibon 
believes  AWS must  radically  change its strategy.  Instead of leaving  developers  to integrate  the databases
with  its platformâs  high-availability  capabilities, AWS must  provide  an integrated  and fully  supported  Tier-1 
level  high-availability  and recovery  capability.  To develop  an integrated  Cloud Database  Platform, AWS will
need  to invest  significant  resources to change  from adding  extensions  to open-source  databases to 
developing  in-house AWS software  for an integrated  Cloud Database  Platform. AWS will need  to develop
both  convergence  and distribution  capabilities in such  a platform.  Also, AWS will need  to bring  more  of its 
databases  to Outposts  to meet  the distribution  capabilities fully. And AWS  will need  to allow  its databases
to  run  on other  clouds  such as Alibaba,  Google, Microsoft  Azure, Oracle  OCI, and Tencent. 
Google  has developed  its databases  primarily for its own  use in their  particular  and unique  business  but 
has  struggled  to make  its Cloud  Database  Platform offerings  relevant to enterprises.  Wikibon believes  that
Google  will probably  partner with other  database  vendors. 
IBM  has significant  experience  in Tier-1  databases,  has strong  enterprise  service and sales  capabilities, 
and  has developed  impressive  potential Data Mesh  capabilities.  Although IBM is nascent  in cloud  services,
IBM  and  Red  Hat have  the financial,  technical,  and research  capabilities  to invest  in developing  a Cloud 	
Â© 2021 Wikibon Research | Page 12  


 	
                              
                    
                        
                            
                          
 
                                                            
                                 
                           
                    
                         
                           
                 
                                                               
             
                        
                             
                             
                           
                      
                          
                                 
                        
                                                             
                         
                                  
       
                       
                       
                        
                   
                                 
 
                               
                          
                                 
                               
                           
                           
                       
                               
                           	
GoldenGate Augments Cloud Databases 	
Database  Platform. Wikibon believes  IBM and  Red  Hat should  and probably  will invest  in developing  a full-
fledged  Cloud Database  Platform with both  convergence  and distributed  data mesh  capabilities. 
Microsoft  has significant  experience  in developing  a Tier-1  database  and has strong  enterprise  marketing
and  software  distribution  capabilities.  It has  built  a strong  SaaS presence  around its Office  and Teams 
software.  Also, Microsoft  is a primary  IaaS/PaaS  cloud infrastructure  provider and is supporting  a multi-
cloud  strategy. 
Wikibon  is impressed  with the vision  for Microsoft  Cosmos DB. Wikibon  believes  that Microsoft  will need  to 
integrate  its Tier-1  transactional  SQL Server  with Cosmos  DB in the  future.  Microsoft  has robust  distributed
services  on the  distribution  axis, and Cosmos  DB allows  data to be  placed  close to the  users.  However, 
Microsoft  will need  to enhance  its Azure  Stack offering  based on third-party  hardware, which Wikibon
assesses  as inadequate,  lacking architectural  equivalency. Overall, Wikibon  believes  Microsoft  has the 
financial  resources  and technical  capabilities  to develop  a Cloud  Database  Platform and upper  management
flexibility  to partner  with others.  Wikibon  expects Microsoft  to invest  heavily  in developing  a Cloud 
Database  Platform and maintain  its lead  over  AWS  and Google. 
Figure  1 shows  Oracle  has by far  the  best  Tier-1  Cloud Database  Platform and has developed  a robust
infrastructure  platform with Oracle  Exadata  X8M that is the  basis  of database  services on Oracle  Cloud 
Infrastructure  and Exadata  Cloud@Customer,  delivering architectural  equivalency. Oracle Cloud@Customer
Cloud  Database  Platform is moving  powerfully  down the database  convergence  dimensions. In addition,  the 
Oracle  strategy  of making  all the  cloud  database  services autonomous  is shown  by the  Oracle  Database
21c  announcements  in early  2021.  Oracle  Database  21c is initially  available  on OCI  via the  Autonomous 
Database  service (in the  Always  Free Tier)  and Oracle  Exadata  Cloud Service.  Oracle Autonomous  Database
is  Oracleâs  self-managing,  self-tuning, and self-securing  cloud service  built on Oracleâs  converged  database 
engine. 
Oracle  has already  developed  a multi-cloud  agreement with Microsoft.  Wikibon expects AWS and Oracle  to
reach  an agreement  to ensure  that Oracle  runs well on AWS  and can link to Oracle  Database  hardware. 
The  announcement  of OCI  support  for GoldenGate  has significantly  improved the distribution  dimension for 
the  Oracle  Cloud Database  platform. Oracle will need  to include  data catalogs  to help  integrate  different
databases.  Oracle will also  need  to expand  its sharding  feature further and invest  strongly  in a Data 
Mesh  architecture  and functionality  to allow  enterprises  to devolve  and simplify  data-led  strategies  to
include  the lines  of business.  In other  interesting  work, Oracle  uses AI and  ML to enrich  metadata  in an 
Oracle  Cloud Infrastructure  Data Catalog. 
This  decentralization  strategy may bring  them  into potential  conflict with centralized  IT development
executives  who value  and evaluate  Oracleâs Database  products. However,  Wikibon believes  the Cloud 
Database  Platform is ripe  for disruption  driven by simple-to-use  database and data  management  software
that  will empower  less technical  users in the  lines  of business. 
Wikibon  believes  that Oracle  has the vision  and management  drive to extend  its lead  in the  Cloud  Database 
Platform  market. 
The  SAP  HANA  Cloud Platform  is difficult  to evaluate.  SAP started  developing  SAP HANA  when the cloud
was  still nascent,  and other  databases  did not  provide  the integration  between transactional  and analytics. 
For  a time,  SAP HANA  was the leading  platform  for ensuring  that analytic  data could  be distributed  rapidly
to  the  end-users  in the  lines  of business.  However,  Wikibon believes  that the predicted  investments  by AWS 
and  Microsoft,  and the investments  already made by Oracle,  reduce any SAP  HANA  differentiation.  The
most  valuable  SAP contribution  is its  software  suites, and Wikibon  believes  SAP customers  would prefer 
greater  flexibility  on which  cloud and database  platform to deploy  the SAP  software. 
SAP  is not  offering  SAP HANA  as a general-purpose  database, and therefore  the potential  number of SAP
HANA  database  instances  is limited.  In software,  volume wins. Wikibon  believes  that SAP will probably 	
Â© 2021 Wikibon Research | Page 13  


 	
                                   
                               
                     
                          
                             
                          
                  
                        
           	
 	
                         
                             
                     
                       
                        
                           
                           
       
                      
                           
                             
                             
                               
       	
 	
                   
                           
                             
                                                                  
                           
                              
           
                                                             
              
                      
                      
                         
                           
                       
       	
GoldenGate Augments Cloud Databases 	
change  its current  Cloud Database  Platform strategy. 
Snowflake  has made  good initial  progress  on ease-of-use  and devolved  data warehouses.  Snowflake needs 
to  expand  its TAM  to justify  its market  cap and  has experienced  technical architects.  Wikibon expects that
Snowflake  will invest  in convergence  with a combination  of acquisition  and integration.  Wikibon 
understands  that the Snowflake  Data Mesh  implementation  is currently  envisioned  as an  ability  to connect
with  any other  domain,  internal and external,  and move  any required  data. Wikibon  believes  that Snowflake 
will  need  to develop  a distributed  database capability,  including on-premises  deployment,  to allow  data to
remain  in place.  Also, Snowflake  lacks in-database  machine-learning  algorithms. Although all these 
requirements  are technically  challenging,  Wikibon considers  that Snowflake  must take up the  challenge  or
risk  being  sidelined  by Microsoft  and Oracle. 
Assessment  Summary 
Wikibon believes  that Oracle  has the strongest  Cloud Database  Platform with Autonomous  Database and
can  be integrated  with the GoldenGate  OCI service.  It offers  a Tier-1  database  foundation,  Oracle Exadata 
Cloud@Customer,  which provides  identical Exadata X8M hardware  and software  in on-premises  private
clouds  managed  centrally.  Oracle also provides  Dedicated  Region Cloud@Customer,  a complete  portfolio of 
public  cloud services  and Oracle  Fusion SaaS applications  into an on-premises  data center.  Wikibon
believes  that the integration  of different  databases  with the OCI  GoldenGate  service is a gamechanger. 
Oracle  should  shift its marketing  to emphasize  its ability  to provide  the best  hardware,  interconnectivity,
and  integration  for all databases. 
Wikibon  believes  that Snowflake  has impressive  ease-of-use  for end-users  and a potentially  impressive 
Data  Mesh  vision.  Snowflake  will need  to execute  well and quickly  on its overall  vision. 
Wikibon  is impressed  with the vision  for Microsoft  Cosmos DB. Microsoft  also has a Tier-1  database 
foundation  in SQL  Server.  Wikibon  believes  that Microsoft  will need  to integrate  its Tier-1  transactional  SQL 
Server  with Cosmos  DB in the  future.  Microsoft  can use the GoldenGate  OCI service  to improve  the 
integration  of multiple  heterogeneous  databases. 
Action Item 
Wikibon  strongly  recommends  that senior  enterprise  executives  focus intently  on developing  Cloud
Database  Platforms,  which are fully  converged  and excel  across  both transactional  and analytic  data types. 
The  converged  database must also support  Data Mesh  features  and allow  distributions  of data  where  the
data  is created  and the ability  of lines  of business  to work  directly  and own  the data  that is vital  to them. 
They  will radically  improve the cost,  time-to-value,  and functionality  when used to implement  data-driven
strategies. 
At  the  moment,  Oracle Database  is a Tier-1,  mission-critical  converged Cloud Database  Platform, which is 
lengths  ahead in the  convergence  dimension. The ability  to use  GoldenGate  as an  OCI  service  pulls Oracle
ahead  of Snowflake  on the  distribution  dimension. 
Microsoft  is a dominant  software developer  and has a strong  Tier-1 SQL Server  Cloud Database  platform. Its 
development  of Cosmos  DB is early  but is architecturally  sound. Microsoft  needs to expand  its convergence
and  distribution  capabilities and take  advantage  of GoldenGate  services. 
Wikibon  recommends  that senior  executives  at large  enterprises,  especially those with significant  Oracle 
installations,  start investing  in Oracle  Autonomous  Database on OCI  or on-premises  with Cloud@Customer
while  continuing  to track  the development  of other  Cloud  Database  Platforms.  Wikibon also recommends 
that  senior  executives  invest in GoldenGate  as a service  to integrate  different databases  and avoid
database  conversions.  Wikibon believes  this can radically  improve the cost,  functionality,  and time-to-value 
required  to implement  data-led strategies. 	
Â© 2021 Wikibon Research | Page 14  


 	
   
                          
                         
 
     	
                             
                           
                            
                   
                                                      
       	
   	
                       
                             
                             	
GoldenGate Augments Cloud Databases 	
Footnotes 
Data Mesh  Reference 
Zhanak  Dehghani  of ThoughtWorks  is an  authority  on the  simplification  and devolution  of Enterprise  Data 
Management.  This discussion  between Dave Vellante  and Zhanak  Dehghani  is an  excellent  introduction  to 
the  subject. 
Cloud  Database  Platform Definition 
Cloud Databases  are an emerging  category of databases.  A Cloud  Database  Platform is a service  delivered 
from  an integrated  cloud platform.  A Cloud  Database  Platform enables enterprises  to utilize  Cloud Database 
services  on demand  without an initial  investment  cost for equipment  and licenses.  It also  allows  enterprises 
to  manage  distributed  databases remotely on private  clouds or shared  clouds. 
A  Cloud  Database  Platform can reside  in a private  cloud, public cloud,  hybrid  cloud, and multi-cloud 
environments.  From an application  perspective,  the database  services are identical.  The only  difference  lies 
in  where  the database  resides. 
Tier-1 Database  Definition 
Tier-1 Databases  have a strong  track record  of performance  and reliability  for large-scale  mission-critical 
applications.  Such a track  record  takes many  years to achieve.  At the  moment  (2021), Wikibon  considers 
only  three  vendors  offer Tier-1  databases.  The three  are IBM  DB2,  Microsoft  SQL Server,  and Oracle 
Database. 	
Â© 2021 Wikibon Research | Page 15  


 	
GoldenGate Augments Cloud Databases 	
David Floyer spent more than 20 years at IBM, holding positions in research, sales, marketing, systems analysis and 
running IT operations for IBM France. He worked directly with IBMâs largest European customers, including BMW, 
Credit Suisse, Deutsche Bank and Lloydâs Bank. Floyer was a Research Vice President at International Data 
Corporation (IDC) and is a recognized expert in IT strategy, economic value justification, systems architecture, 
performance, clustering and systems software. 	
David Floyer 
@dfloyer 
david.floyer@wikibon.org 	
Â© 2021 Wikibon Research | Page 16  

"
Non,"
5 Juillet 2022
Synacktiv
Hugo VINCENT (@hugow_vincent) Finding Java deserialization gadgets with CodeQL 


Agenda
1
Why this talk
2
Java deserialization vulnerability
3
CodeQL
4
Finding gadgets with CodeQL
5
Limitations 


3/46
Why this talk
Java deserialisation vulnerabilities still exists
Finding them becomes more an more difficult
Itâs hard to find gadget chains by hand
Finding a deserialisation vulnerability without a gadget is frustrating
Since 2017 insecure deserialization is included in the OWASP Top 10 


Agenda
1
Why this talk
2
Java deserialization vulnerability
3
CodeQL
4
Finding gadgets with CodeQL
5
Limitations 


5/46
Java deserialization vulnerability
Serialisation
The process of converting an object to a byte stream such that this byte stream can be reverted back to the object 


6/46
Java deserialization vulnerability
serialisation 


7/46
Java deserialization vulnerability
Reading serialized data from an ObjectInputStream
readObject
readResolve
readExternal
â¦ 


8/46
Whatâs the problem ?
Supplying user controled data to remote method
No check are performed during the deserialisation process
Every Serializable class can be supplied in the byte stream and reconstructed
Dangerous methods can be called during the deserialisation process 


9/46
Whatâs the problem ?
deserialisation 


10/46
Whatâs the problem ?
deserialisation 


11/46
A gadget chain
Using multiple functions in the code to perform other actions
Same principle as a ROP chain in binary exploitation 


12/46
The C3P0 chain
c3p0 


13/46
The C3P0 chain
c3p0 


14/46
The C3P0 chain
PoolBackedDataSourceBase -> readObject
ReferenceIndirector$ReferenceSerialized -> getObject
RegistryContext -> lookup 


15/46
The Spring1 chain
ObjectInputStream . readObject ()
SerializableTypeWrapper . MethodInvokeTypeProvider . readObject ()
SerializableTypeWrapper . TypeProvider ( Proxy ). getType ()
AnnotationInvocationHandler . invoke ()
HashMap . get ()
ReflectionUtils . findMethod ()
SerializableTypeWrapper . TypeProvider ( Proxy ). getType ()
AnnotationInvocationHandler . invoke ()
HashMap . get ()
ReflectionUtils . invokeMethod ()
Method . invoke ()
Templates ( Proxy ). newTransformer ()
AutowireUtils . ObjectFactoryDelegatingInvocationHandler . invoke ()
ObjectFactory ( Proxy ). getObject ()
AnnotationInvocationHandler . invoke ()
HashMap . get ()
Method . invoke ()
TemplatesImpl . newTransformer ()
TemplatesImpl . getTransletInstance ()
TemplatesImpl . defineTransletClasses ()
TemplatesImpl . TransletClassLoader . defineClass ()
Pwner *( Javassist - generated ).< static init >
Runtime . exec () 


16/46
YSOSERIAL
ysoserial 


17/46
Gadget Inspector
Presented at Black Hat USA 2018 by @ianhaken
A Java bytecode analysis tool for finding gadget chains
Works by reconstructing the AST (abstract syntax tree)
Clojure1 / Jython 


Agenda
1
Why this talk
2
Java deserialization vulnerability
3
CodeQL
4
Finding gadgets with CodeQL
5
Limitations 


19/46
CodeQL
Static code analyser
build a database by parsing the code to reconstruct the AST
Analyse the code by making queries on it
Useful to find vulnerabilities by pattern
Java/C/C++/C#/Javascript/Python/Swiftâ¦
(partially) open source : https ://github.com/github/codeql 


20/46
CodeQL
import
java
from
Method m
where
m
.
hasName (â readObject â)
select
m 


21/46
CodeQL
readObject 


22/46
CodeQL
weak hash 


23/46
CodeQL
ï¿¿
cat
Security / CWE / CWE -327/ BrokenCryptoAlgorithm . ql
/*
*
*
@name
Use of a broken or risky cryptographic algorithm
*
@description Using broken or weak cryptographic algorithms can allow an attacker to
compromise security .
*
@kind
path - problem
*
@problem . severity warning
*
@security - severity 7.5
*
@precision high
*
@id
java / weak - cryptographic - algorithm
*
@tags
security
*
external / cwe / cwe -327
*
external / cwe / cwe -328
*/ 


24/46
CodeQL
bb 


Agenda
1
Why this talk
2
Java deserialization vulnerability
3
CodeQL
4
Finding gadgets with CodeQL
5
Limitations 


26/46
Finding gadgets with CodeQL
The source
The sink
The path 


27/46
The Sink
Sink methods are the dangerous methods that we want to reach. We can define them in CodeQL like this :
RuntimeExec the CodeQL class name
extends the Methodclass
defined in the java.langpackage
in the Runtime class
method name is exec
private
class RuntimeExec extends Method {
RuntimeExec (){
hasQualifiedName ( â java . lang â , â Runtime â , â exec â )
}
} 


28/46
The Sink
from
MethodAccess ma
where
ma
.
getMethod () instanceof RuntimeExec
select
ma
sink 


29/46
The Sink
class
DangerousMethod extends Callable {
DangerousMethod (){
this
instanceof ExpressionEvaluationMethod or
this
instanceof ReflectionInvocationMethod or
this
instanceof RuntimeExec or
this
instanceof URL or
this
instanceof ProcessBuilder or
this
instanceof Files or
this
instanceof FileInputStream or
this
instanceof FileOutputStream or
this
instanceof EvalScriptEngine or
this
instanceof ClassLoader or
this
instanceof ContextLookup or
this
instanceof OGNLEvaluation or
this
instanceof DriverManagerMethods or
this
instanceof System
}
} 


30/46
The Sink
We want all the methods that calls a
DangerousMethod, so we look forMethodAccessof dangerous methods, and we
use the enclosing callable as a result :
private
class CallsDangerousMethod extends Callable {
CallsDangerousMethod (){
exists
( MethodAccess ma | ma . getMethod () instanceof DangerousMethod and ma .
getEnclosingCallable () = this )
}
} 


31/46
The Sink
from
Callable c
where
c
instanceof CallsDangerousMethod
select
c
sink 


32/46
The Source
A source is a method that we can call to start the gadget chain, the first obvious one is
readObject but there are other
methods like :
readObjectNoData
readResolve
readExternal
â¦ 


33/46
The Source
There are other methods that have been used in other known chains that we can add :
hashCode
equals
compare
â¦ 


34/46
The Source
1)
private void readObject ( java . io . ObjectInputStream s )
2) throws IOException , ClassNotFoundException
3) {
4)
s . defaultReadObject ();
[...]
5)
table = new Entry <?,?>[ length ];
[...]
6)
for (; elements > 0; elements --) {
7) K key = ( K ) s . readObject ();
8) V value = ( V ) s . readObject ();
9) reconstitutionPut ( table , key , value );
}
[...] 


35/46
The Source
1)
private void reconstitutionPut ( Entry <?,?>[] tab , K key , V value )
2)
throws StreamCorruptedException
{
[...]
3)
int hash = key . hashCode ();
[...]
4)
for ( Entry <?,?> e = tab [ index ] ; e != null ; e = e . next ) {
5) if (( e . hash == hash ) && e . key . equals ( key )) {
6) throw new java . io . StreamCorruptedException ();
}
}
[...] 


36/46
The source
class
Source extends Callable {
Source
(){
getDeclaringType (). getASupertype *() instanceof TypeSerializable and (
this instanceof MapSource or
this instanceof SerializableMethods or
this instanceof Equals or
this instanceof HashCode or
this instanceof Compare or
this instanceof ExternalizableMethod or
this instanceof ObjectInputValidationMethod or
this instanceof InvocationHandlerMethod or
this instanceof MethodHandlerMethod or
this instanceof GroovyMethod
)
}
} 


37/46
The path
Finding a path between the
sourceand the sink
public
void A (){
B
()
}
public
void B (){
C
()
}
public
void C (){
dangerousMethod ()
}
public
void readObject ( ObjectInputStream in ) {
A
()
} 


38/46
The path
A
.
pollyCalls ( B )
A
.
pollyCalls ( B )
B
.
pollyCalls ( C )
... 


39/46
The path
Recursion
private
class RecursiveCallToDangerousMethod extends Callable {
RecursiveCallToDangerousMethod (){
this
instanceof CallsDangerousMethod or
exists
( RecursiveCallToDangerousMethod unsafe | this . polyCalls ( unsafe ))
} 


40/46
Click1
1)
java
.
util . PriorityQueue . readObject ()
2)
java . util . PriorityQueue . heapify ()
3)
java . util . PriorityQueue . siftDown ()
4) java . util . PriorityQueue . siftDownUsingComparator ()
5) org . apache . click . control . Column$ColumnComparator . compare ()
6) org . apache . click . control . Column . getProperty ()
7) org . apache . click . control . Column . getProperty ()
8) org . apache . click . util . PropertyUtils . getValue ()
9) org . apache . click . util . PropertyUtils . getObjectPropertyValue ()
10) java . lang . reflect . Method . invoke ()
11) com . sun . org . apache . xalan . internal . xsltc . trax . TemplatesImpl .
getOutputProperties () 


41/46
real world use case
Click1
ROME
Hibernate1
Mojarra
WildFly1 


42/46
real world use case
File
:
WildFlyDataSource . java
113:
private void readObject ( java . io . ObjectInputStream in ) throws IOException ,
ClassNotFoundException {
114: in . defaultReadObject ();
115: jndiName = ( String ) in . readObject ();
116:
117:
118: try {
119: InitialContext context = new InitialContext ();
120:
121: DataSource originalDs = ( DataSource ) context . lookup ( jndiName );
[...] 


43/46
WildFly1
Wildfly is a Java application server, with more than 10000 Java classes.
A pull request was made on ysoserial
The WildFlyDataSource class is part of the org.jboss.as.connector package and is bundled inside the WildFly
GitHub repository. 


Agenda
1
Why this talk
2
Java deserialization vulnerability
3
CodeQL
4
Finding gadgets with CodeQL
5
Limitations 


45/46
Limitations
Need to have the source code of the library/project
Need to be able to compile the project
You can analyse one project at a time 


46/46
The END
https ://github.com/synacktiv/QLinspector 


THANKS FOR YOUR ATTENTION ANY QUESTIONS ? 

"
Non,"1 	
The NoSQL RDBMS 	
One of first uses of the phrase  NoSQL is due to Carlo  Strozzi, circa 1998.  
 
NoSQL:  
ï® A fast, portable, open -source RDBMS 
ï® A derivative of the RDB database system (Walter Hobbs, RAND)  
ï® Not a full -function DBMS, per se, but a shell -level tool  
ï® User interface â  Unix shell  
ï® Based on the âoperator/stream paradigmâ  
 
 
 
 	
  

NoSQL Today 	
More recently:  
ï§	The term has taken on different meanings  	
ï§	One common interpretation is ânot only SQLâ  
 	
Most  modern  NoSQL systems diverge from the relational model or standard RDBMS functionality:  
 
The data model: relations    documents 
  tuples  vs.  graphs 
  attributes    key/values 
  domains 
  normalization 
 
The query model:  relational algebra   graph traversal 
  tuple calculus vs.  text search 
    map/reduce 
 
The implementation:  rigid schemas   vs.  flexible schemas  
    (schema -less)  
  ACID compliance  vs.  BASE 
 
In that sense, NoSQL today is more commonly meant to be something like ânon- relationalâ 	
  

3	 	
NoSQL Today 	
(a partial,  unrefined  list)  	
Hbase Cassandra  Hypertable   Accumulo Amazon  SimpleDB  SciDB  Stratosphere  flare 
Cloudata  BigTable  QD Technology  SmartFocus  KDI  Alterian  Cloudera  C-Store  
Vertica  Qbase âMetaCarta  OpenNeptune  HPCC  Mongo DB  CouchDB  Clusterpoint  ServerTerrastore  
Jackrabbit  OrientDB  Perservere  CoudKit Djondb  SchemaFreeDB  SDB  JasDB 
RaptorDB  ThruDB  RavenDB  DynamoDB   Azure Table Storage  Couchbase Server  Riak   
LevelDB  Chordless  GenieDB  Scalaris  Tokyo  Kyoto Cabinet  Tyrant Scalien  
Berkeley DB  Voldemort  Dynomite  KAI   MemcacheDB  Faircom C -Tree  HamsterDB STSdb 
Tarantool /Box  Maxtable  Pincaster  RaptorDB  TIBCO Active Spaces  allegro -C   nessDBHyperDex  
Mnesia  LightCloud  Hibari BangDB OpenLDAP /MDB/Lightning  Scality Redis  
KaTree  TomP2P Kumofs  TreapDB  NMDB  luxio   actord  Keyspace   
schema -free  RAMCloud SubRecord  Mo8onDb Dovetaildb  JDBM  Neo4  InfiniteGraph 
Sones  InfoGrid  HyperGraphDB  DEX  GraphBase  Trinity  AllegroGraph  BrightstarDB  
Bigdata  Meronymy  OpenLink  Virtuoso  VertexDB FlockDB  Execom  IOG   Java Univ Netwrk /Graph Framework  
OpenRDF /Sesame  Filament OWLim  NetworkX iGraph  Jena   SPARQL  OrientDb  
ArangoDB  AlchemyDB  Soft NoSQL Systems  Db4o   Versant  Objectivity  Starcounter   
ZODB  Magma  NEO   PicoList  siaqodb  Sterling Morantex  EyeDB  
HSS Database  FramerD Ninja Database  Pro  StupidDB  KiokuDB  Perl solution  Durus 
GigaSpaces  Infinispan  Queplix Hazelcast  GridGain  Galaxy  SpaceBase  JoafipCoherence  
eXtremeScale  MarkLogic Server  EMC Documentum  xDB  eXist  Sedna BaseX  Qizx 
Berkeley DB XML  Xindice  Tamino  Globals  Intersystems  Cache  GT.M   EGTM   
U2   OpenInsight  Reality OpenQM  ESENT  jBASE  MultiValue   Lotus/Domino 
eXtremeDB RDM Embedded  ISIS Family Prevayler  Yserial  Vmware  vFabric  GemFire  Btrieve 
KirbyBase  Tokutek  Recutils  FileDB  Armadillo  illuminate Correlation Database  FluidDB  
Fleet DB  Twisted Storage  Rindo Sherpa  tin  Dryad SkyNet  Disco  
MUMPS  Adabas  XAP In -Memory Grid  eXtreme  Scale  MckoiDDB  Mckoi  SQL Database  	
    	 	 	 	 	 	 	 	 	 	
  

4	 	
Primary NoSQL Categories 	
â¢	General Categories of NoSQL Systems:  
â¢	Key/value store 	
â¢	(wide) Column store 	
â¢	Graph store 	
â¢	Document store 
 	
â¢	Compared to the relational model:  
â¢	Query models are not as developed.  	
â¢	Distinction between abstraction & implementation is not as clear.  
 
 
 	
  

5	 	
Key/Value Store 	
âDynamo: Amazonâs Highly Available Key- value Store,â DeCandia, G., et al., SOSPâ07, 21	st ACM  
Symposium  on Operating  Systems Principles.  
 	
The basic data model:  
Database is a collection of key/value pairs  
The key for each pair is unique 
 
Primary operations:  
insert(key,value)  
delete(key) 
update( key,value ) 
lookup(key) 
 
Additional operations:  
variations on the above, e.g., reverse lookup  
iterators  
 	
 	
DynamoDB 
Azure Table Storage  
Riak  
Rdis  
Aerospike  
FoundationDB  
LevelDB  
Berkeley DB  
Oracle NoSQL Database  
GenieDb  
BangDB 
Chordless  Scalaris  
Tokyo Cabinet/Tyrant  
Scalien 
Voldemort  
Dynomite  
KAI  
MemcacheDB  
Faircom C -Tree  
LSM  
KitaroDB  
HamsterDB  
STSdb  
TarantoolBox  
Maxtable  
Quasardb  
Pincaster  
RaptorDB  
TIBCO Active Spaces  Allegro-C  
nessDB  
HyperDex  
SharedHashFile  
Symas LMDB  
Sophia  
PickleDB  
Mnesia  
LightCloud  
Hibari  
OpenLDAP  
Genomu  
BinaryRage  
Elliptics  
Dbreeze  
RocksDB  
TreodeDB  	
(www.nosql -database.org	 	www.db -engines.com	 	www.wikipedia.com	) 	
No requirement for normalization  
(and consequently dependency  
preservation or lossless join)   

6	 	
Wide Column Store 	
âBigtable: A Distributed Storage System for Structured Data,â Chang, F., et al., OSDIâ06: Seventh 
Symposium on Operating System Design and implementation, 2006.  
 
The basic data model:  
Database is a collection of key/value pairs  
Key consists of 3 parts  â a row key, a column key, and a time -stamp (i.e., the version) 
Flexible schema -  the set of columns is not fixed, and may differ from row -to -row  
 
One last column detail:  
Column key consists of two parts  â a column family, and a qualifier 	
Accumulo  
Amazon SimpleDB  
BigTable  
Cassandra  
Cloudata  
Cloudera  
Druid  
Flink  
Hbase  
Hortonworks  
HPCC  
Hyupertable  
KAI  
KDI  
MapR  
MonetDB  
OpenNeptune  
Qbase  
Splice Machine  
Sqrrl  	
(www.nosql -database.org	 	www.db -engines.com	 	www.wikipedia.com	) 	
Warning #1!  

7	 	
Wide Column Store 	
ID  First Name  Last Name  Date of 
Birth  Job
 
Category  Salary
  Date of Hire  Employer  	
Personal data  Professional data 
Row key
 	
Column families  	
Column qualifiers   

8	 	
Wide Column Store 	
ID  First Name  Last Name  Date of 
Birth  Job
 
Category  Salary
  Date of Hire  Employer  
ID  First Name  Middle 
Name   Last Name
 Job
 
Category  Employer
 Hourly Rate  	
Personal data  Professional data 	
ID  First Name  Last Name  Job 
Category  Salary
  Employer  Group  Seniority  Bldg #   Office #  
ID  Last Name  Job 
Category  Salary
  Date of Hire  Employer  Insurance ID  Emergency 
Contact  	
Medical data  	
One âtableâ  

t1 	
t0  	
9	 	
Wide Column Store 	
ID   First Name  Last Name  Date of 
Birth  Job
 
Category  Salary
  Date of Hire  Employer  	
One ârowâ  
 
 
 
 
One ârowâ in a wide -column NoSQL database table  
=  
Many rows in several relations/tables in a relational database  	
Personal data Professional data 
Row key
  

10	 	
Graph Store 	
Neo4j - âThe Neo Database  â A Technology Introduction,â 2006.  
 
The basic data model:  
Directed graphs  
Nodes & edges, with properties, i.e., âlabelsâ  
 	
 	
AllegroGraph 
ArangoDB  
Bigdata  
Bitsy  
BrightstarDB  
DEX/Sparksee  
Execom IOG  
Fallen *  
Filament  
FlockDB  
GraphBase  
Graphd  
Horton  
HyperGraphDB  
IBM System G Native Store  
InfiniteGraph  
InfoGrid  
jCoreDB Graph  
MapGraph  
Meronymy  
Neo 4j 
Orly  
OpenLink virtuoso  
Oracle  Spatial and Graph  
Oracle NoSQL Datbase  
OrientDB  
OQGraph  
Ontotext OWLIM  
R 2DF  
ROIS  
Sones GraphDB  
SPARQLCity  
Sqrrl Enterprise  
Stardog  
Teradata Aster  
Titan  
Trinity  
TripleBit  
VelocityGraph  
VertexDB  
WhiteDB  	
(www.nosql -database.org	 	www.db -engines.com	 	www.wikipedia.com	)  

11	 	
Document Store 	
MongoDB -  âHow a Database Can Make Your Organization Faster, Better, Leaner,â February 
2015.  
 
The basic data model:  	
ï®	The general notion of a document â  words, phrases, sentences, paragraphs, sections,  
  subsections, footnotes, etc.  	
ï®	Flexible schema â subcomponent structure may be nested, and vary from  
  document -to -document.  	
ï®	Metadata â title, author, date, embedded tags, etc.  	
ï®	Key/identifier. 
 
One implementation detail:  	
ï®	Formats vary greatly â PDF, XML, JSON, BSON, plain text, various binary,  
  scanned image.  
 	
 	
 	
AmisaDB  
ArangoDB  
BaseX  
Cassandra  
Cloudant  
Clusterpoint  
Couchbase  
CouchDB  
Densodb  
Djondb  
EJDB  
Elasticsearch  eXist 
FleetDB  
iBoxDB 
Inquire  
JasDB  
MarkLogic  
MongoDB  
MUMPS  
NeDB  
NoSQL embedded  db  
OrientDB  
RaptorDB  
RavenDB  
RethinkDB  
SDB  
SisoDB  
Terrastore  
ThruDB  	
(www.nosql -database.org	 	www.db -engines.com	 	www.wikipedia.com	)  

12	 	
ACID vs. BASE	 	
Database systems traditionally support ACID requirements: 
Atomicity, Consistency, Isolation, Durability 
 
In a distributed  web applications the focus shifts to:  	
Consistency, Availability, Partition tolerance  
 
CAP theorem  - At most two of the above can be enforced at any given time.  
 	
Reducing  consistency, at least  temporarily, maintains the other two . 
 
Thus, distributed NoSQL systems are typically said to support some form of BASE:  
Basic Availability  
Soft state  
Eventual consistency* 
 
 
 
 
 
 
 
 	
  

"
Non,"E En tant que conseiller dÃ©butant, vous pourrez accompagner une cohorte de collÃ¨gues engagÃ©s et talentueux 
dans un parcours de deux ans visant Ã  faire de vous l'un des meilleurs conseillers d'IBM. IBM se transforme, et 
les conseillers jouent un rÃ´le crucial 	dans cette transformation. Avec plus de 1	 000 conseillers travaillant sur 	
des centaines de projets clients diffÃ©rents, le programme de conseiller dÃ©butant permet d'expÃ©rimenter divers 
rÃ´les, d'Ã©voluer en tant que conseiller et de se transformer en chef IBM	. Vous Ãªtes appuyÃ©s par les directeurs 	
de programme, les compagnons, les directeurs de projet et par une formation en personne complÃ¨te, dÃ©butant 
par des cours intensifs pour dÃ©velopper des compÃ©tences de services	-conseils de base dÃ¨s le premier jour. 	
Chaq	ue conseiller dÃ©butant exerce une pratique permettant de dÃ©velopper une expertise en la matiÃ¨re dans 	
une ou plusieurs industries en travaillant sur plusieurs projets pendant le programme de deux ans. Vous 
profiterez d'une expÃ©rience collaborative qui vous 	mettra au dÃ©fi et qui vous transformera.  	 	
 
En tant que 	conseiller technique dÃ©butant	, vous pouvez participer Ã  l'un des domaines suivants	 :  	
â	 Technologie	 : vous aiderez Ã  concevoir, dÃ©velopper et intÃ©grer des solutions en utilisant des 	
technologies, 	outils, techniques et produits reposant sur les meilleures pratiques que nos clients 	
rÃ©clament aujourd'hui.	 	
â	 Sciences des donnÃ©es/IngÃ©nierie	 : 	
vous travaillerez sur des projets 
intersectoriels qui aident les clients 
Ã  mettre en oeuvre les solutions 
cognit	ives Watson afin de 	
rÃ©inventer numÃ©riquement leur 
stratÃ©gie et leurs opÃ©rations en 
exploitant les donnÃ©es non 
structurÃ©es, encourageant une plus 
grande efficacitÃ© et la 
transformation de l'ensemble de 
leur entreprise grÃ¢ce Ã  des 
connaissances cognitives.	 	
 
En tant que 	conseiller technique dÃ©butant	, vous pouvez participer Ã  l'un des domaines suivants	 :  	
â	 Transformation organisationnelle	 : vous travaillerez sur des projets qui aident les clients Ã  intÃ©grer 	
stratÃ©gie, processus, technologie et information dans	 le but de rehausser l'efficacitÃ©, de rÃ©duire les 	
coÃ»ts et d'amÃ©liorer les profits ainsi que la valeur pour l'actionnaire.	 	
Que ferez	-vous en tant que consultant niveau DÃ©butant IBM?	 	
  

â	 Applications d'entreprise	 : vous dÃ©velopperez votre expertise dans l'un des domaines de suite de 	
solutions SAP, Oracle, Microsoft D	ynamics, Workday ou SharePoint et aiderez nos clients Ã  intÃ©grer les 	
donnÃ©es Ã  l'Ã©chelle de l'entreprise afin d'amÃ©liorer le niveau de performance et d'efficacitÃ©.	 	
â	 Conception/ExpÃ©rience utilisateur	 : vous vous concentrerez sur la conception d'expÃ©riences	 utilisateurs 	
numÃ©riques de classe mondiale afin d'appuyer nos stratÃ©gies en matiÃ¨re d'expÃ©rience client en 
travaillant sur les besoins des utilisateurs et l'expression appropriÃ©e de leurs marques.	 	
â	 Analytique	 : vous dÃ©finirez les principaux problÃ¨mes d'a	ffaires Ã  rÃ©gler; formulerez des approches 	
mathÃ©matiques et regrouperez les donnÃ©es afin de rÃ©gler ces problÃ¨mes; tirerez des conclusions et 
testerez	 des solutions Ã  prÃ©senter au client; et amÃ©liorerez les performances grÃ¢ce Ã  la modÃ©lisation 	
mathÃ©matique,	 Ã  la simulation, Ã  l'analytique des donnÃ©es, Ã  des techniques d'optimisation et Ã  la 	
visualisation des donnÃ©es.	 	
 	
 
 
 
 	
â	 Leadership et adaptabilitÃ© dÃ©montrÃ©s avec la volontÃ© de s'approprier facilement et volontairement des 
tÃ¢ches et des problÃ¨mes trÃ¨s stimulants, mÃªme au	-delÃ  de la portÃ©e initiale de la responsabilitÃ©.	 	
â	 Une approche approfondie et analytique 	avec la capacitÃ© d'appliquer la logique pour rÃ©soudre les 	
problÃ¨mes.	 	
â	 CapacitÃ© de faire face Ã  de multiples tÃ¢ches simultanÃ©ment et de 
respecter les Ã©chÃ©ances tout en restant concentrÃ© malgrÃ© des 
demandes contradictoires.	 	
â	 Sâefforcer de surmonter les obstac	les les plus exigeants ou 	
difficiles et rechercher des moyens dâamÃ©liorer les rÃ©sultats.	 	
â	 Initiative visant Ã  rechercher activement de nouvelles 
connaissances et Ã  amÃ©liorer les compÃ©tences.	 	
â	 CompÃ©tences interpersonnelles efficaces et capacitÃ© de 
collaborer 	et de travailler efficacement avec les individus, 	
renforÃ§ant les relations pour parvenir Ã  des solutions gagnante	s. 	
â	 CapacitÃ© Ã  faire part clairement et simplement des situations 
complexes en Ã©coutant activement et en transmettant des messages 
difficiles de	 maniÃ¨re positive.	 	
â	 CapacitÃ© Ã  rÃ©diger efficacement, Ã  communiquer des messages ou des exigences complexes de faÃ§on 
claire, simple et concise.	 	
â	 Une passion pour les idÃ©es novatrices, associÃ©e Ã  la capacitÃ© de comprendre et d'assimiler diffÃ©rents points 
de vu	e. 	
Que recherchons	-nous chez un 	
consultant niveau 	DÃ©butant GBS?	  

Technologie mobile	 - Notre Ã©quipe canadienne de dÃ©veloppement de solutions numÃ©riques est un chef de file 	
mondial dans la prestation de services de dÃ©veloppement Web, de technologie mobile, de rÃ©alitÃ© virtuelle ou 
d'API basÃ©s sur des technologies Ã©mergentes. Nous concevons 	et dÃ©veloppons des solutions personnalisÃ©es 	
pour une variÃ©tÃ© de plateformes d'appareils, notamment Responsive Web, iPhone, Android, Oculus Rift/Go, 
Apple TV et Gear VR.	 Nous aidons Ã©galement notre client Ã  construire la prochaine gÃ©nÃ©ration de solutions 	
nu	mÃ©riques compatibles avec le nuage et alimentÃ©es par des solutions cognitives utilisant IBM Bluemix Cloud 	
Platform, IBM Watson Solutions	 et des API. Nos consultants 	dÃ©butants	 techniques travaillent avec des 	
technologies telles que Angular, React, SDK natif	 (Android / iOS), NodeJS, Java, Swift, JavaScript + aut	resAlors 	
que nos consultants 	dÃ©butants	 non techniques jouent le rÃ´le d'analyste d'affaires, de chef de projet, d'analyste 	
AQ pour s'assurer que nous exÃ©cutons et livrons la vision de nos clients.	 	
Ventes	 - Assister les clients dans la conception, l'architecture d'affaires, le dÃ©veloppement de processus, 	
l'exÃ©cution fonctionnelle et le support des applications CRM Salesforce. ExÃ©cuter efficacement des sÃ©ances de 
conception 
dâutilisateur 
professionnel	 et 	
d'expÃ©rience 
utilisateur. Fournir 
de l'expÃ©rience 
dans la conception 
de rÃ¨gles de flux 
de travail pour 
mettre les 
processus 
d'affaires Ã  
exÃ©cution. 	 	
 
Centre de contact	 	
- Travailler avec 
des consultants en 
stratÃ©gie et des 
architectes qui 
faÃ§onnent l'av	enir 	
de la 	maniÃ¨re	 	
dont les plus 
grandes 
entreprises 
canadiennes interagissent avec leurs clients. PossibilitÃ© de conceptualiser, de concevoir et de livrer des 
solutions complexes qui amÃ©liorent l'expÃ©rience client. Travailler sur des projets qui apportent	 de la valeur non 	
seulement Ã  nos clients, mais aussi Ã  tous les clients qui interagissent avec eux quotidiennement.	 	
 
Plateformes de marketing (Adobe)	 â Dans cette pratique, nous avons des analystes commerciaux, des 	
dÃ©veloppeurs, des architectes et des str	atÃ¨ges. Nous analysons les besoins du client et voyons comment une 	
solution de plateforme de marketing d'entreprise pourrait aider l'entreprise Ã  atteindre ses objectifs de la 
stratÃ©gie jusqu'au point de mise en Åuvre.	 	
 
Interaction avec les clients et 	conception (UX)	 â Nos consultants dÃ©fendent les intÃ©rÃªts de l'utilisateur tout en 	
aidant les clients Ã  trouver de nouvelles faÃ§ons de faire des affaires en crÃ©ant des expÃ©riences agrÃ©ables pour 
leurs clients. Pour ce faire, nous utilisons une approche cent	rÃ©e sur l'utilisateur, comme des ateliers de 	
rÃ©flexion sur la conception et des recherches pour cerner les besoins des utilisateurs, valider le problÃ¨me, 
obtenir les commentaires des clients et les 	prendre en compte	, crÃ©er des concepts et des prototypes, l	es mettre 	
Ã  l'essai et rÃ©pÃ©ter l'expÃ©rience. Fonctions	 : Concepteur UX, architecte UX. Chercheur UX, concepteur UI	   	
 
 
 	
StratÃ©gie commerciale numÃ©rique	 â Notre travail inspire les clients Ã  rÃ©imaginer la faÃ§on dont leur organisation 	
innove, fonctionne et s'engage avec leurs clients, leurs employÃ©s et leur Ã©cosystÃ¨me en utilisant les 
technologies numÃ©riques et Ã©mergentes. Nous aidons nos clients Ã  exploit	er la puissance des grandes donnÃ©es 	
et des analyses pour accroÃ®tre l'efficacitÃ© et l'efficience de l'activitÃ© principale, monÃ©tiser les actifs de donnÃ©es 
existants et construire de nouveaux modÃ¨les d'affaires.	 	
 	
Nos pratiques	 	
 iX (Interactive Experience)	 	
 
StratÃ©gie numÃ©rique	 	
  

ActivitÃ©s connexes	 - Nous dÃ©terminons un modÃ¨	le pour nos clients qui dÃ©finit le meilleur dÃ©ploiement des 	
Ã©lÃ©ments fonctionnels d'une entreprise pour tirer parti du passage aux technologies numÃ©riques, aux 
technologies mobiles et aux interactions sociales. Cette offre aide les clients Ã  	dÃ©terminer	 et 	Ã  	
systÃ©matiquement mettre en prioritÃ©	 comment les capacitÃ©s numÃ©riques peuvent aider Ã  rÃ©duire les dÃ©penses 	
et Ã  gÃ©nÃ©rer de nouvelles sources de valeur grÃ¢ce Ã  l'optimisation des fonctions commerciales, opÃ©rationnelles 
et d'entreprise.	 	
 
Technologies et 	donnÃ©es	 - Cette solution aide les entreprises Ã  adopter une mÃ©thode de travail souple qui 	
permet de rÃ©duire les dÃ©lais de mise sur le marchÃ©, d'innover en exploitant les nouvelles technologies, de 
mieux comprendre leurs donnÃ©es et de passer aux compÃ©tences	 qui habiliteront le mieux les parties prenantes 	
internes. Nous fournissons un plan directeur sur la meilleure faÃ§on de moderniser le bureau des TI, de former 
des Ã©quipes souples, de rÃ©partir le travail et de mesurer ce qui compte.	 	
 
 
 	
 
Soutien cognitif Ã  la prise de dÃ©cisions commerciales	 	
 
IdO Watson	 - Travailler avec des clients de toutes tailles et de tous les secteurs d'activitÃ© qui cherchent Ã  	
obtenir des renseignements exploitables Ã  partir de leurs donnÃ©es 	obscures et Ã  pousser l'automatisation 	
jusqu'Ã  la limite de leurs activitÃ©s en utilisant l'IdO. Notre consultant aide les organisations Ã  formuler des 
stratÃ©gies autour de l'IdO (Internet des objets), ce qui inclut (mais sans s'y limiter) l'utilisation de 
l'apprentissage machine et de l'IA avec l'IdO pour l'analyse prÃ©dictive et l'automatisation, la visualisation des 
donnÃ©es, l'intelligence Ã©conomique et la vision par ordinateur et l'IA pour le contrÃ´le de la qualitÃ© Ã  grande 
Ã©chelle et l'optimisation des p	rocessus.	 	
 
IA Watson	 â La pratique de Watson et l'IA applique des technologies d'IA de pointe pour transformer 	
l'expÃ©rience client, les opÃ©rations et les donnÃ©es de nos clients et avoir un impact sur la faÃ§on dont nous 
voyageons, effectuons nos 
transactio	ns bancaires, 	
apprenons, interagissons et 
recevons des soins de 
santÃ©.	  Nous sommes une 	
Ã©quipe innovante et 
dynamique, passionnÃ©e par 
l'IA, l'innovation et la 
fourniture de solutions 
innovantes pour nos clients.	 	
 
Plateformes de donnÃ©es	 â 	
Vous allez 	inÃ©vitablement 	
exploiter vos compÃ©tences 
avec des langages et 
technologies libres, acquÃ©rir 
une expÃ©rience approfondie 
dans le domaine de l'industrie 
Ã  travers une variÃ©tÃ© 
d'engagements dans tous les 
secteurs d'activitÃ©. De faÃ§on gÃ©nÃ©rale, vous apprendrez 	des meilleurs de cette Ã©quipe multidisciplinaire et 	
dynamique et travaillerez sur une plÃ©thore de projets	 uniques dont vous serez fiers!	 	
 
Analytique Ã©voluÃ©e	 â Nos consultants utilisent des modÃ¨les et des techniques de science des donnÃ©es pour 	
rÃ©soudre les 	problÃ¨mes commerciaux de nos clients. Pour trouver des renseignements axÃ©s sur les donnÃ©es 	
pour le client, nous nous renseignons sur le processus d'affaires, recueillons les donnÃ©es et explorons les 
donnÃ©es pour comprendre le problÃ¨me et les prochaines Ã©ta	pes requises.	 	
 
 
Remaniement du processus cognitif	 	
 
Talents et implication	 - Aider les clients Ã  dÃ©terminer les enjeux commerciaux et recommander des solutions 	
stratÃ©giques de changement d'organisation afin d'accroÃ®tre la valeur de l'entreprise. Ãlaborer une stratÃ©gie et 
une amÃ©lioration des processus pour les fonctions Ressourc	es humaines, Main	-d'Åuvre et Talent afin 	
Transformation du pro	cessus cognitif	 	
  

d'amÃ©liorer la valeur et de rÃ©duire les coÃ»ts des processus et des systÃ¨mes RH. Il peut s'agir de la conception, 
de la construction et de la mise en Åuvre de centres de services partagÃ©s en RH, de la conception de m	odÃ¨les 	
d'organisation et de prestation des RH ou d'autres processus RH. 	 	
 
ChaÃ®ne de blocs	 â Les consultants conÃ§oivent, dÃ©veloppent ou rÃ©inventent des composants d'applications trÃ¨s 	
complexes et intÃ¨grent des progiciels, des programmes et des objets rÃ©util	isables rÃ©sidant sur de multiples 	
plateformes. Cela se fait en utilisant vos connaissances et votre expertise des systÃ¨mes pour concevoir et 
modÃ©liser des applications, dÃ©velopper des solutions d'application et les intÃ©grer aux applications du client.	 	
 
 
Services du processus cognitif	 	
 
Automatisation du processus cognitif	 â Notre pratique fournit l'automatisation des processus d'affaires de bout 	
en bout de faÃ§on Ã  optimiser le soutien humain nÃ©cessaire. Ce changement, qui est de dÃ©placer le fardeau des 
pro	cessus de l'homme Ã  la technologie, a le potentiel de redÃ©finir la faÃ§on dont le travail se fait au sein d'une 	
entreprise. L'automatisation simple des processus peut Ã©liminer les erreurs, rÃ©duire les prÃ©jugÃ©s et effectuer le 
travail transactionnel en une f	raction du temps qu'il faut aux humains.	 	
 
 
 	
 
Microsoft	 â Les consultants de la pratique Microsoft assument divers rÃ´les, allant de la prÃ©paration de nouvelles 	
offres, au codage, en passant par la mise Ã  niveau de l'infrastructure technologique d'un client. 	 	
 
Oracle	 â Le rÃ´le de nos consultants serait de compr	endre et de travailler avec un module spÃ©cifique (HCM, ERP, 	
SCM) proposÃ© par Oracle pour aider Ã  transformer et	 Ã  optimiser les processus d'affaires d'un client. DiffÃ©rentes 	
tÃ¢ches sont prises en considÃ©ration : analyse des donnÃ©es, saisie des donnÃ©es, personnalisation des modules et 
mise en Åuvre.	 	
 
SAP (modules Ariba, Hybris et Finance)	 - En tant que nouveau consult	ant SAP, vous travaillerez en Ã©troite 	
collaboration avec le client et d'autres d'IBMistes afin de dÃ©terminer comment nous pouvons amÃ©liorer les 
processus du client grÃ¢ce aux solutions SAP.	 	
Vous serez impliquÃ© dans toutes les phases du projet : de lâexplora	tion et la conception Ã  la rÃ©alisation et au 	
dÃ©veloppement, Ã  la rÃ©alisation et aux tests et enfin au dÃ©ploiement.	 	
 
SuccessFactors	 - Un consultant SuccessFactors fournira des services de consultation d'affaires innovateurs, de 	
conception de processus d'aff	aires, d'intÃ©gration de systÃ¨mes et de conception et de gestion d'applications Ã  	
des organisations sectorielles de premier plan Ã  l'Ã©chelle mondiale	. Nous utilisons la mÃ©thodologie IBM Ascend 	
exÃ©cutÃ©e par SAP Activate SuccessFactors pour mettre en Åuvre, m	ettre Ã  niveau ou dÃ©velopper la solution 	
SuccessFactors.	 	
 
CAMS	 â Les membres de l'Ã©quipe CAMS (services de gestion des applications infonuagiques) travaillent sur les 	
plus importants et les plus complexes programmes de modernisation et de transformation 	d'IBM.	 	
Nous commenÃ§ons avec une stratÃ©gie de transformation, dÃ©finissons l'architecture, procÃ©dons au 
dÃ©veloppement d'applications personnalisÃ©es et rÃ©alisons des intÃ©grations complexes, appuyÃ©s par des 
techniques Agile et DevOps poussÃ©es.  	 	
Notre Ã©quipe d	e consultants s'appuie sur des plateformes infonuagiques hybrides comme Azure, AWS, Google 	
Cloud, Openshift et IBM Cloud; suivies d'architectures de dÃ©veloppement modernes de conteneurs, d'interfaces 
de programme d'application (API), de microservices et d'	outils de dÃ©veloppement de A Ã  Z comme Angular, 	
Node.JS et React. 	 	
Nous sommes Ã  la recherche de consultants de niveau dÃ©butant en tant que stratÃ¨ges, analystes des systÃ¨mes 
et dÃ©veloppeurs.	 	
 
 	
Innovation en matiÃ¨re dâapplications infonuagiques	 	
  

"
Non,"The Forrester Waveâ¢: Big Data NoSQL, Q1 2019
The 15 Providers That Matter Most And How They Stack Up
by Noel Yuhanna
March 14, 2019
LiceNsed for iNdividuaL use oNLY
ForreSTer.coM	
Key Takeaways
MongoDB, Microsoft, couchbase, AWS, 
Google, And redis Labs Lead The Pack
forresterâs research uncovered a market in 
which MongodB, Microsoft, couchbase, aWs, 
Google, and redis Labs are Leaders; MarkLogic, 
datastax, aerospike, oracle, Neo4j, and iBM 
are strong Performers; and saP, arangodB, and 
ravendB are contenders.
Performance, Scalability, Multimodel, And 
Security Are Key Differentiators
The Leaders we identified support a broader 
set of use cases, automation, good scalability 
and performance, and security offerings. The 
strong Performers have turned up the heat on the 
incumbents. contenders offer lower costs and are 
ramping up their core NosQL functionality.	
Why read This report
NosQL has become critical for all businesses to 
support modern business applications. in our 
26-criterion evaluation of NosQL providers, we 
identified the 15 most significant ones â 
aerospike, amazon Web services (a Ws), 
arangodB, couchbase, datastax, Google, iBM, 
MarkLogic, Microsoft, MongodB, Neo4j, oracle, 
ravendB, redis Labs, and saP â and 
researched, analyzed, and scored them. This 
report shows how each provider measures up and 
helps enterprise architecture (ea) professionals 
select the right one for their needs.
This Pdf is only licensed for individual use when downloaded from forrester.com or reprints.forrester.com. all other distribution prohibited. 

2
3
6
7
11
13	
Â© 2019 forrester research, inc. opinions reflect judgment at the time and are subject to change. forrester	Â®, 
Technographics	Â®, forrester Wave, Techradar, and Total economic impact are trademarks of forrester research, 
inc. all other trademarks are the property of their respective companies. unauthorized copying or distributing 
is a violation of copyright law. citations@forrester.com or +1 866-367-7378	
forrester research, inc., 60 acorn Park drive, cambridge, Ma 02140 usa
+1 617-613-6000  |  fax: +1 617-613-5000  |  forrester.com	
Table of contents	
The Rise Of Big Data NoSQL Platforms
NosQL v endors continue To deliver 
enterprise features To support complex 
Workloads
Evaluation Summary
Vendor Offerings
Vendor Profiles
Leaders
strong Performers
contenders
Evaluation Overview
v endor inclusion criteria
Supplemental Material	
related research documents
The forrester Waveâ¢: cloud data Warehouse, 
Q4 2018
The forrester Waveâ¢: cloud Hadoop/spark 
Platforms, Q1 2019
The forrester Waveâ¢: Translytical data 
Platforms, Q4 2017	
for eNTerPrise arcHiTecTure ProfessioNaLs
The Forrester Waveâ¢: Big Data NoSQL, Q1 2019
The 15 Providers That Matter Most And How They Stack Up
by Noel Yuhanna
with Gene Leganza and robert Perdoni	
Share reports with colleagues. 
enhance your membership with 
research share.  

2	
The rise of Big data NosQL Platforms
NosQL is more than a decade old. it has gone from supporting simple schemaless apps to becoming a 
mission-critical data platform for large fortune 1000 companies. it has already disrupted the database 
market, which was dominated for decades by relational database vendors. Today, half of global data and 
analytics technology decision makers either have implemented or are implementing NosQL platforms, 
taking advantage of the benefits of a flexible database that serves \a broad range of use cases.	
1 
enterprises like NosQLs ability to scale out using low-cost servers and a exible, schemale\ss model that 
can store, process, and access any type of business data. NosQL platforms give ea pros greater control 
over data storage and processing, along with a configuration that accelerates application depl\oyments. 
While many organizations are complementing their relational databases with NosQL, some have started 
to replace them to support improved performance, scale, and lower their database costs.
forrester defines big data NosQL as:
A nonrelational database management system that provides storage, processing, \and accessing 
of any type of data and which supports a horizontal, scale-out architect\ure based on a 
schemaless and flexible data model.
NoSQL Vendors continue To Deliver enterprise Features To Support complex Workloads
NosQL covers a range of nonrelational databases, such as key-value, document, and graph 
databases, that are optimized for a new generation of business apps, including social medi\a, customer 
360, advanced insights, real-time, and operational apps. NosQL vendors are delivering innovative 
features, such as ai and machine learning automation, integration with apache spark and streaming 
technology, support for sQL and simplified aPis, and extensive administrative tools. enterprises 
seeking NosQL platforms should look for vendors that:  
âº Focus on AI/machine learning automation deployment capabilities. With organizations ramping 
up their NosQL database deployments, the need for greater automation has become critical. 
This includes automating provisioning, tuning, optimization, indexing, patching, upgrading, high 
availability, and security. These capabilities not only accelerate deployments; they also support \
larger and more complex applications with minimal effort. enterprise buyers should take heed of 
the various differences in ai/machine learning automation offerings and carefully map them to their 
specific requirements, now and for the future.
 
âº Press the advantages of open source. digital transformation often runs on open source 
software.	
2 Today, open source NosQL solutions are stable and ready for primetime. enterprises like 
NosQLs rapidly evolving ecosystem, low cost, minimized vendor lock-in, and ea\sier customization 
for complex deployments. While many NosQL vendors offer open source solutions, look for 
vendors that lead active NosQL communities, contribute software to enhance the open source 
platform, and engage with customers to drive innovation. Pricing, packag\ing, and support services 
also differentiate the vendors.
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

3	
 
âº Integrate with large ecosystems for tools and technologies. NosQL databases are the 
foundation on which to build great, modern business applications and support advanced insights. 
However, NosQL needs a strong ecosystem of tools and technologies offered by partners that can 
help accelerate deployments. These include tools to support data integra\tion, data quality, security, 
governance, distributed management, and data pipelining. When acquiring a NosQL platform, ea 
pros should look at the breadth and depth of partnerships the vendor supports to take advantage 
of their tools and services.	
evaluation summary
The forrester Wave evaluation highlights Leaders, strong Performers, contenders, and challengers. 
itâs an assessment of the top vendors in the market and does not represent the entire vendor 
landscape. Youâll find more information about this market in our overview report on NosQL providers.	
3	
We intend this evaluation to be a starting point only and encourage clien\ts to view product evaluations 
and adapt criteria weightings using the excel-based vendor comparison tool (see figure 1 and see 
figure 2). click the link at the beginning of this report on forrester.com to download the tool.
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

4	
FIGUre 1 forrester Waveâ¢: Big data NosQL, Q1 2019	
ChallengersContenders Leaders
Str
ong
Performers	
Str onger
curr ent
of fering
W eaker
curr ent
of fering
Weaker strategy Stronger strategy	
Market pr esence*
*A  gr ay bubble indica tes a nonpar ticipating vendor .
ArangoDB Couchbase
DataStax
MarkLogic
Neo4j
Oracle
RavenDB Redis Labs
SAP	
MongoDB	
Micr osoft	Aerospike
Amazon W
eb Services	
Google	
IBM	
Big Data NoSQL
Q1 2019
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

5	
FIGUre 2 forrester Waveâ¢: Big data NosQL scorecard, Q1 2019	
Aerospike
Amazon We	
b Services
ArangoDB CouchbaseDataStaxGoogleIBMMarkLogic	
3.72
3.30
4.00
2.90
3.00
3.00
1.00
3.00
3.00
1.70
1.00
1.00
3.00
1.00
5.00 3.92
4.10
3.80
3.90
5.00
3.00
5.00
3.00
5.00
5.00
5.00
5.00
5.00
5.00
5.001.98
3.00
1.30
2.00
3.00
1.00
1.00
3.00
3.00
1.50
1.00
1.00
3.00
1.00
1.004.34
3.50
4.90
3.90
5.00
3.00
1.00
5.00
5.00
3.00
1.00
3.00
5.00
3.00
5.003.30
3.00
3.50
3.70
5.00
3.00
1.00
3.00
5.00
3.60
3.00
3.00
5.00
3.00
5.003.72
3.30
4.00
3.90
5.00
3.00
5.00
3.00
5.00
3.90
3.00
5.00
3.00
5.00
5.002.84
2.30
3.20
3.10
3.00
3.00
3.00
3.00
5.00
3.90
3.00
5.00
3.00
5.00
5.003.96
4.50
3.60
3.10
3.00
3.00
3.00
3.00
5.00
3.70
3.00
5.00
3.00
3.00
5.00	
weighting
Forr
esterâ s	
50%
40%
60%
50%
35%
45%
5%
10% 5%
0%
30%
30%
25%
10% 5%
Curr
ent of fering
Development
Deployment
Strategy
Ability to execute
Road map
Pr ofessional services
Open sour ce
T echnical support
Market pr esence
Pr oduct r evenue
Install base
Market awar eness
Partnerships
Reach
All scor es are based on a scale of 0 (weak) to 5 (str ong).
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

6	
FIGUre 2 forrester Waveâ¢: Big data NosQL scorecard, Q1 2019 (cont.)	
Microsoft
MongoDB Neo4jOracleRavenDB*	Redis Labs	SAP	
4.00
4.00
4.00
4.60
5.00
5.00
5.00
1.00
5.00
4.40
3.00
5.00
5.00
5.00
5.00 4.80
4.50
5.00
4.00
5.00
3.00
3.00
5.00
5.00
4.40
3.00
5.00
5.00
5.00
5.003.00
3.00
3.00
3.20
3.00
3.00
1.00
5.00
5.00
2.90
1.00
3.00
5.00
3.00
3.003.46
3.40
3.50
3.00
3.00
3.00
3.00
3.00
3.00
3.70
3.00
5.00
3.00
3.00
5.001.68
2.40
1.20
2.00
1.00
3.00
1.00
1.00
3.00
1.00
1.00
1.00
1.00
1.00
1.004.24
3.70
4.60
3.20
3.00
3.00
1.00
5.00
5.00
3.60
1.00
5.00
5.00
3.00
5.002.68
3.40
2.20
2.10
3.00
1.00
3.00
3.00
3.00
2.70
1.00
3.00
3.00
5.00
5.00	
weighting
Forr
esterâ	
s	
50%
40%
60%
50%
35%
45%
5%
10% 5%
0%
30%
30%
25%
10% 5%
Curr
ent of fering
Development
Deployment
Strategy
Ability to execute
Road map
Pr ofessional services
Open sour ce
T echnical support
Market pr esence
Pr oduct r evenue
Install base
Market awar eness
Partnerships
Reach
All scor es are based on a scale of 0 (weak) to 5 (str ong).
*Indicates a nonparticipating vendor .	
vendor offerings
forrester included 15 vendors in this assessment: aerospike, amazon Web services, arango dB, 
couchbase, datastax, Google, iBM, MarkLogic, Microsoft, MongodB, Neo4j, oracle, ravendB, redis 
Labs, and saP (see figure 3).
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

7	
FIGUre 3 evaluated vendors and Product information	
OffcialÂ companyÂ name\t
Aer ospike
Amazon W eb Services (A WS)
ArangoDB
Couchbase
DataStax
Google
IBM
MarkLogic
Micr osoft
MongoDB
Neo4j
Oracle
RavenDB
Redis Labs
SAP	
Pr oductÂ evaluated
Aer ospike Database
DynamoDB
ArangoDB
Couchbase
DataStax
Cloud Fir estore
Cloudant
MarkLogic
Azur e Cosmos DB
MongoDB
Neo4j Enterprise Edition
Oracle NoSQL
Hiber nating Rhinos
Redis Enterprise
OrientDB	
v endor Profiles
our analysis uncovered the following strengths and weaknesses of individual vendors.
Leaders
 
âº MongoDB remains the most popular NoSQL database. MongodB is used by more than 8,000 
companies, including many fortune 100 companies. it is popular among app developers largely 
becauseÂ ofÂ itsÂ easeÂ ofÂ use,Â simpliOedÂ model,Â on-demandÂ andÂ elasticÂ sc\tale,Â multicloudÂ support,Â 
and comprehensive tooling. With the release of 4.0, MongodB offers multidocument acid 
transactions. it supports autosharding, built-in replication, search, and mixed-workload capabilities. 
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

8	
Top use cases include 360-degree analytics, real-time analytics, streaming analytics, fraud and 
risk management, and other multiple workloads. customer references like MongodBâs innovative 
features, faster time-to-value platform, and technical support.
 
âº Microsoft starts to get strong traction with Azure cosmos DB. azure cosmos dB is Microsoftâs 
globally distributed, multimodel database that enables users to elastica\lly scale compute and 
storage across azure geographic regions. as a foundational azure service, it is available in all azure 
regions. its simplified database with relaxed consistency levels and low-latency access makes 
it easier to develop globally distributed apps. cosmos dB automatically indexes all data without 
requiring schema or secondary indices, supports rich sQL and Javascript queries, and offers 
multidocument acid transactions. customer references like its resilience, low maintenance, cost 
effectiveness, high scalability, multimodel support, and faster time-to-value. They use cosmos dB 
for operational apps, real-time analytics, streaming analytics, and internet-of-things (ioT) analytics.
 
âº couchbase has a high-performance and scalable NoSQL offering. couchbase is an open 
source, distributed, multimodel NosQL database that is optimized for interactive applications. 
couchbase is designed to provide easy-to-scale key-value or document storage, processing, and 
access with low-latency requirements. firms use couchbase to support massively interactive use 
cases, some of which include social and mobile/ioT applications, healthcare, financial services, 
content and metadata stores, ecommerce, and online gaming applications. couchbase provides 
full support for documents, a flexible data model, indexing, full-text\ search, and Mapreduce 
for real-time analytics. customer references use couchbase to support various mission-critical 
workloads, including operational, analytical, and mixed workloads.
 
âº Amazon Web Services offers several NoSQL databases to support various use cases. aW s 
dynamodB is a fully managed NosQL database-as-a-service (dBaas) that is deployed across 
various types of workloads, including operational, transactional, and an\alytical. it automatically 
shards the data across a compute farm to support large-scale, high-performance database 
deployments. dynamodB is tightly integrated with a Ws eMr and aWs s3, offering the ability to 
store and run big data initiatives. dynamodB natively supports both key-value and document models 
and geospatial data sets. Besides dynamodB, a Ws also offers other NosQL databases such as 
Neptune, a graph database, and a time-series database. enterprises use dynamodB to support 
social media apps, gaming, real-time and operational workloads, ioT apps, and other ecommerce 
apps. customer references like dynamodBâs support, performance, scale, and high availability.
 
âº Google offers multiple NoSQL database solutions. Google offers two NosQL platforms: cloud 
firestore and cloud Bigtable. Google cloud firestore is a serverless, schema-agnostic database 
that supports automatic sharding, high availability, acid transactions, strong consistency, sQL-like 
queries, indexes, and durability for various types of workloads, but it\s targeted at mobile, web, and 
ioT apps. cloud Bigtable is scalable, wide-column database service  the same da\tabase that 
powers many Google services, such as search, analytics, Maps, and Gmail. cloud Bigtable scales 
to handle massive workloads at consistent low latency and high throughput for both operational 
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

9	
and analytical applications, including ioT, user analytics, advertising technology (adtech), and 
financial data analysis. customer references like Googleâs performance, developer-level flexibility, 
automated scalability, and ease of use.
 
âº redis Labs delivers a high-performing NoSQL database. redis is a multimodel, open source, 
in-memory database platform whose key development is currently sponsored by redis Labs. 
redis supports both relaxed and strong consistency, a flexible schemaless model, high availability, 
and ease of deployment. an enterprise version encapsulates the open source software and 
provides additional capabilities for geodistributed active-active deployme\nts (multicloud, hybrid, 
on-premises) with high availability and linear scaling, while supporting the\ open source aPi. 
companies use redis for customer 360, machine learning, ioT, search, and real-time analytics as 
well as for ecommerce, social metering solutions, and other use cases. customer references like 
its innovation for machine learning apps, performance, scale, customer support, and support for 
diverse NosQL use cases.
Strong Performers  
âº MarkLogic offers a mature NoSQL database for various use cases. MarkLogic is the only 
NosQL document database vendor that has offered a NosQL product for more than a decade. 
it provides enterprise-class capabilities to store, process, and access all kinds of data sets for 
a variety of use cases. MarkLogic runs on on-premises, hybrid-cloud, and multicloud platforms, 
including a Ws, azure, and Google cloud Platform. customers most commonly deploy MarkLogic 
for mixed workloads  including transactional, analytical, and operat\ional. some use it for 
customer 360 analytics, healthcare analytics, real-time analytics, fraud detection, information 
discovery, content delivery, and digital supply chain management. customer references like its 
ability to eliminate data silos, faster time-to-value, security, search, and simplicity.
 
âº DataStax delivers a viable NoSQL geodistributed database. datastax distributes, contributes, 
and supports the commercial enterprise version of apache cassandra, an open source project. 
datastax continues to execute well to support global apps that demand low-lat\ency access 
to critical data. datastax enterprise (dse) has a masterless, shared-nothing architecture with 
multimodel and in-memory capabilities, along with built-in analytics and\ enterprise search that 
can run on-premises or in the cloud. datastax supports various types of business applications, 
including transactional, analytical, translytical, predictive analytics, graph, and mixed 
workloads. Top use cases include fraud detection, product catalogs, consumer personalization, 
recommendation engines, and ioT apps. customer references like its distributed architecture, 
technical support, performance, and low cost of ownership.
 
âº Aerospike leverages memory to support high-performance NoSQL. aerospike is a key-value 
distributed NosQL database that provides horizontal scale and features a multitiered storage 
engine, hybrid-memory architecture designed for all-draM, draM/flash, and all-flash storage. itâs 
deployed at scale in public and private clouds  as well as in data c\enters  and is provided as 
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

10	
a dual-license product. aerospike is used by enterprises in financial services, telecommunication\s, 
technology, retail, ecommerce, adtech, and gaming. use cases include recommendations engines, 
digital payments, fraud detection, and other real-time applications. customer references like its 
speed, ease of scale, support, low latency, high-availability access and simplified storage, and 
access for strategic operational applications.
 
âº oracleâs NoSQL is a viable option for oracle customers. oracle NosQL database appeals to 
companies looking for acid transactions; geodistributed data; granular security controls; and 
integration with oracle database, oracle Wallet, spark, and Hadoop. oracle NosQL has its roots 
as a key-value database, but it also supports wide-column tables, schema\less JsoN, and property 
graph data models, delivering excellent performance, scale, security, and high availability. Many 
oracle customers use oracle NosQL to balance the need for scale-out workloads of simpler 
key-value data with the rich set of relational data management capabilities needed in their core 
business systems â or when supporting new applications that have frequently changing key-value 
data, such as profiles for fraud detection, customer 360, and ioT apps.
 
âº Neo4j is a popular NoSQL graph database that supports many use cases. Neo4j provides 
an open source property graph database and has an enterprise edition that includes cluste\ring, 
multiple geographies, and security as well as extensions that support gr\aph analytics and 
algorithms, visual graph discovery and exploration, and big data integra\tion. Thousands of 
community deployments and more than 300 customers harness connected data with Neo4j. Neo4j 
is driving a multivendor initiative to develop an iso/WG3 standard Graph Query Language with 
contribution from its cypher language and the opencypher.org community project. customer 
references like its native storage and processing of graph data models and its full acid-compliant, 
flexible data models, and high performance for connected data. customers often use it for real-
time recommendations, ai, graph-based search, social networking, ioT analytics, fraud detection, 
and master data management.
 
âº IBM cloudant is a viable cloud NoSQL offering. iBM acquired cloudant in 2014 to expand its 
dBaas and support various NosQL cases. iBM cloudant is a fully managed NosQL document 
database that is compatible with apache couchdB. cloudant documents are stored in triplicate 
across three separate availability zones for in-region high availability and disaster recovery. 
customers use cloudant for real-time analytics, ioT analytics, streaming analytics, and operational 
workloads. iBM provides hosting, administrative tools, analytics, and support for cloudant, and 
it has deployments spread across a variety of industries, including financial services, gaming, 
manufacturers, telcos, retailers, and healthcare.
contenders  
âº SAP offers a new multimodel NoSQL database: orientDB. saP acquired callidus software in 
2018, which in turn acquired orientdB in 2017.	
4 orientdB is the key contributor to and supporter 
of orientdB, an open source, multimodel NosQL database written in Java that was first released 
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

11	
in 2010. orientdB supports schemaless, hybrid, and schema-based models and leverages sQL as 
a query language in addition to Gremlin and sParQL. orientdB has a fully acid-compliant graph 
database to support transactional and operational use cases. customer references mention its 
multimodel engine, ease of use, reliable performance, and small footprint as core strengths. Key 
use cases for orientdB include asset management, network management, cybersecurity, social 
networking, recommendation engines, and fraud detection.
 
âº ArangoDB offers good NoSQL databases for specific use cases. arangodB is a multimodel 
NosQL database that supports key-value, document, and graph data models wit\h one database 
core and a unified query language called aQL. arangodB provides scalable queries when working 
with graph data. arangodB can be deployed on-premises and in the cloud, including a Ws, Google 
cloud Platform, and Microsoft azure. customer references like its graph support, flexible data 
model, query language, and simple approach. They use arangodB for transactional, operational 
workloads, and they like its faster time-to-value for business initiativ\es.
 
âº ravenDB has a viable transactional NoSQL database. Hibernating rhinos, a database provider, 
offers ravendB a NosQL transactional database that supports document, key-value, and graph 
data models. ravendB runs on-premises and in the cloud, including a Ws and azure. it offers 
support for spatial data, full-text search, popular programming and query languages, and various 
security features. ravendB declined to participate in our research.	
evaluation overview
We evaluated vendors against 26 criteria, which we grouped into three high-level categories:
 
âº current offering. each vendorâs position on the vertical axis of the forrester Wave graphic 
indicates the strength of its current offering. Key criteria for these solutions are data types, 
application development, streaming, data consistency, self-service and automation, transactions, 
data security, multimodel, architecture, performance, scalability, global distributed, high availability/
disaster recovery, tooling, workloads, and use cases.
 
âº Strategy. Placement on the horizontal axis indicates the strength of the vendorsâ strategies. We 
evaluated ability to execute, road map, professional services, open source, and technical support.
 
âº Market presence. represented by the size of the markers on the graphic, our market presence 
scores reflect each vendorâs product revenue, install base, market awareness, partnerships, and 
reach.
Vendor Inclusion criteria
forrester included 15 vendors in the assessment: aerospike, amazon Web services, arangodB, 
couchbase, datastax, Google, iBM, MarkLogic, Microsoft, MongodB, Neo4j, oracle, ravendB, redis 
Labs, and saP. each of these vendors:
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

12	
 
âº offers a comprehensive, enterprise-class NoSQL database. The vendor must offer the following 
core NosQL components, tools, and features: 1) support for core NosQL features and functionality, 
including high availability, concurrency, security, performance, scalability and administration; 2) 
support for data storage for persistence, integrity, storage, backup, and access; 3) native tools 
developed by the vendor or integration with third-party vendors to support data loading, unloading, 
security management, integration, data quality, archiving, etc.; 4) support for acid compliance 
or eventually consistency; 5) support for a flexible data model; 6) \support for multiple concurrent 
queries, transactions, and operational reporting; and 7) the ability to be deployed on-premises, in 
the cloud, or both.
 
âº Provides a standalone NoSQL solution. The solution isnât technologically tied or bundled to 
any particular application, product, or solution. v endors must market the NosQL solution as a 
standalone product. a customer must be able to buy the NosQL solution independent of other 
products.
 
âº Has a referenceable install base. evaluated vendors have 25 or more unique enterprise 
customers using the NosQL product that span more than one major geographical region.
 
âº Is publicly available. The participating vendors must have actively marketed their NosQL product 
as of November 1, 2018.
 
âº Has customer interest. forrester included only vendors that were mentioned several times by 
customers during forrester inquiry calls during the past 12 months.
 
âº Has client inquiries and/or technologies that put the vendor on Forresterâs radar. forrester 
clients often discuss the vendors and products through inquiries and interviews; alternatively, the 
vendor may, in forresters judgment, warrant inclusion or exclusion in this evaluation because of\ 
technology trends and market presence.
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

13	
supplemental Material
online resource
We publish all our forrester Wave scores and weightings in an excel file that provides detailed product 
evaluations and customizable rankings; download this tool by clicking th\e link at the beginning of this 
report on forrester.com. We intend these scores and default weightings to serve only as a starting 
point and encourage readers to adapt the weightings to fit their individual needs.
The Forrester Wave Methodology
a forrester Wave is a guide for buyers considering their purchasing options in a technology 
marketplace. To offer an equitable process for all participants, forrester follows The forrester Waveâ¢ 
Methodology Guide to evaluate participating vendors.	
engage With an analyst
Gain greater confidence in your decisions by working with forrester thought leaders to apply  
our research to your specific business and technology initiatives.	
Forresterâs research apps for ioS and Android.
stay ahead of your competition no matter where you are.
Analyst Inquiry
To help you put research 
into practice, connect 
with an analyst to discuss 
your questions in a 
30-minute phone session 
â or opt for a response 
via email.
Learn more.
Analyst Advisory
Translate research into 
action by working with  
an analyst on a specific 
engagement in the form 
of custom strategy 
sessions, workshops,  
or speeches.
Learn more.
Webinar
Join our online sessions 
on the latest research 
affecting your business. 
each call includes analyst 
Q&a and slides and is 
available on-demand. 
Learn more.
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  

14	
in our review, we conduct primary research to develop a list of vendors to consider for the evaluation. 
from that initial pool of vendors, we narrow our final list based on the inclusion criteria. We then gather 
details of product and strategy through a detailed questionnaire, demos/briefings, and customer 
reference surveys/interviews. We use those inputs, along with the analystâs experience and expertise in 
the marketplace, to score vendors, using a relative rating system that compares each vendor against 
the others in the evaluation.
We include the forrester Wave publishing date (quarter and year) clearly in the title of each forrester 
Wave report. We evaluated the vendors participating in this forrester Wave using materials they 
provided to us by december 19, 2019, and did not allow additional information after that po\int. We 
encourage readers to evaluate how the market and vendor offerings change over time.
in accordance with The forrester Waveâ¢ vendor review Policy, forrester asks vendors to review our 
findings prior to publishing to check for accuracy. v endors marked as nonparticipating vendors in the 
forrester Wave graphic met our defined inclusion criteria but declined to partici\pate in or contributed 
only partially to the evaluation. We score these vendors in accordance with The forrester Waveâ¢ and 
The forrester New Waveâ¢ Nonparticipating and incomplete Participation v endor Policy and publish 
their positioning along with those of the participating vendors.
Integrity Policy
We conduct all our research, including forrester Wave evaluations, in accordance with the integrity 
Policy posted on our website.
endnotes
1  source: forrester analytics Global Business TechnographicsÂ® data and analytics survey, 2018.
2  see the forrester report âThe i&o Proâs Guide To enterprise open source cloud adoption, Q1 2018.â
3  see the forrester report âv endor Landscape: Graph databases.â
4  source: âsaP completes acquisition of callidus software inc.,â saP press release, april 5, 2018 (https://news.sap.
com/2018/04/sap-completes-acquisition-of-callidus-software-inc/).
For eNTerPrISe ArcHITecTUre ProFeSSIoNALS
The forrester Waveâ¢: Big data NosQL, Q1 2019	
March 14, 2019	
© 2019 Forrester research, Inc. Unauthorized copying or distributing is a violation of copyright law. 	 	citations@forrester.com or +1 866-367-7378 
The 15 Providers That Matter Most And How They Stack Up  


We work with business and technology leaders to develop  
customer-obsessed strategies that drive growth.
ProDUcTS AND SerVIceS 

core research and tools
 

Data and analytics
 

Peer collaboration
 

Analyst engagement
 

consulting
 

events
Forrester  research (Nasdaq: Forr) is one of the most infiuential research and advisory rms in the world. We work with 
business and technology leaders to develop customer-obsessed strategies that drive growth. Through proprietary 
research, data, custom consulting, exclusive executive peer groups, and events, the Forrester experience is about a 
singular and powerful purpose: to challenge the thinking of our clients \to help them lead change in their organizations. 
For more information, visit forrester.com.
c LIeNT SU PPorT
For information on hard-copy or electronic reprints, please contact client S upport at  
+1 866-367-7378, +1 617-613-5730, or clientsupport@forrester.com . We offer quantity  
discounts and special pricing for academic and nonprofit institutions.
Forresterâs research and insights are tailored to your role and 
critical business initiatives.
r oLeS We S erVe
Marketing & Strategy  
Professionals
cM o
B2B Marketing
B2 c Marketing
c ustomer e xperience
c ustomer I nsights
eBusiness & c hannel 
S trategy Technology Management 
Professionals
cIo
A
pplication D evelopment  
& D elivery
 

enterprise A rchitecture
I nfrastructure & o perations
S ecurity & r isk
S ourcing & V endor 
Management Technology Industry  
Professionals
A
nalyst relations	
136481  

"
Non,"SL1	PowerFlow	Platform	
Version	2.11 

Table	of	Contents	
Introduction	to	SL1	PowerFlow	and	the	PowerFlow	Builder	9	
What	is	SL1	PowerFlow?	10	
What	is	the	SL1	PowerFlow	Builder?	11	
What	is	aStep?	12	
What	is	aPowerFlow	Application?	14	
What	is	aConfiguration	Object?	15	
Creating	and	Saving	PowerFlow	Components	15	
Elements	of	the	PowerFlow	User	Interface	15	
Logging	In	and	Out	of	the	PowerFlow	User	Interface	15	
PowerFlowÂ Pages	16	
Additional	Navigation	17	
Monitoring	PowerFlow	on	the	Dashboard	Page	18	
Installing	and	Configuring	SL1	PowerFlow	20	
PowerFlowÂ Architecture	21	
PowerFlow	ContainerÂ Architecture	21	
Integration	Workflow	22	
High-	Availability,	Off-	site	Backup,	and	Proxy	Architecture	22	
Reviewing	Your	Deployment	Architecture	23	
System	Requirements	25	
Hardened	Operating	System	26	
Additional	Prerequisites	for	PowerFlow	26	
Installing	PowerFlow	27	
Installing	PowerFlow	via	ISO	27	
Locating	the	ISO	Image	27	
Installing	from	the	ISO	Image	28	
Troubleshooting	the	ISOÂ Installation	31	
Installing	PowerFlow	via	RPM	32	
Troubleshooting	aCloud	Deployment	of	PowerFlow	35	
Upgrading	PowerFlow	36	
Upgrading	to	Version	2.2.0	from	Version	2.0.x	36	
Upgrading	to	Version	2.2.0	from	Version	1.8.x	with	the	Upgrade	Script	38	
Manually	Upgrading	to	Version	2.2.0	from	Version	1.8.x	40	
Step	1.	Upgrading	Host	Packages	and	Python	3.6	40	
Step	2.	Upgrading	to	Oracle	7.3	or	Later	42	
Step	3.	Upgrading	to	Docker	18.09.2	or	later	42	
Installing	Docker	in	Clustered	Configurations	44	
Step	4.	Installing	the	PowerFlow	RPM	44	
Updating	Cluster	Settings	when	Upgrading	from	1.8.x	to	2.0.0	or	Later	45	
Troubleshooting	Upgrade	Issues	45	
To	roll	back	to	aversion	before	PowerFlow	2.0.0	or	later	45	
Cannot	access	PowerFlow	or	an	InternalÂ Server	Error	occurs	46	
After	upgrading,	the	syncpack_	steprunner	fails	to	run	46	
Licensing	PowerFlow	46	
Configuring	Additional	Elements	of	PowerFlow	49	
Setting	aHard	Memory	Limit	in	Docker	49	
Setting	aSoft	Memory	Limit	in	the	Worker	Environment	49	
Changing	the	PowerFlow	System	Password	50	
Configuring	aProxy	Server	51	
Configuring	Security	Settings	52	
Changing	the	HTTPS	Certificate	52 

Using	Password	and	Encryption	Key	Security	52	
PowerFlow	Management	Endpoints	53	
Flower	API	53	
Couchbase	API	54	
RabbitMQ	55	
Docker	Swarm	Visualizer	56	
Docker	Statistics	56	
ManagingÂ Users	in	SL1	PowerFlow	58	
Configuring	Authentication	with	PowerFlow	59	
User	Interface	Login	Administrator	User	(Default)	59	
Basic	Authentication	Using	aREST	Administrator	User	(Default)	60	
User	Interface	Login	Using	aThird-	party	Authentication	Provider	60	
OAuth	Client	Authentication	Using	aThird-	party	Provider	63	
Basic	Authentication	Lockout	Removal	63	
Role-	based	Access	Control	(RBAC)	Configuration	63	
Assigning	aRole	to	aSpecific	User	64	
Assigning	Roles	to	aSpecific	User	Group	64	
Viewing	User	and	Group	Information	64	
Changing	Roles	and	Permissions	64	
Configuring	Authentication	Settings	in	PowerFlow	64	
UserÂ Groups,	Roles,	andÂ Permissions	65	
Creating	aUserÂ Group	in	PowerFlow	66	
Managing	Synchronization	PowerPacks	68	
What	is	aSynchronization	PowerPack?	69	
Viewing	the	List	of	Synchronization	PowerPacks	70	
Importing	and	Installing	aSynchronization	PowerPack	71	
Locating	and	Downloading	aSynchronization	PowerPack	72	
Importing	aSynchronization	PowerPack	72	
Installing	aSynchronization	PowerPack	73	
Default	Synchronization	PowerPacks	74	
Base	Steps	Synchronization	PowerPack	74	
System	UtilsÂ Synchronization	PowerPack	75	
Managing	PowerFlow	Applications	76	
Viewing	the	List	of	PowerFlow	Applications	77	
Elements	of	an	ApplicationÂ Page	79	
Creating	aPowerFlow	Application	81	
Working	with	Flow	Control	Operators	84	
Creating	an	Application	with	aÂ Condition	Operator	85	
Creating	an	Application	with	aTransform	Operator	89	
Editing	aPowerFlow	Application	97	
Creating	aStep	99	
Aligning	aConfiguration	Object	with	an	Application	99	
Running	aPowerFlow	Application	101	
Viewing	Previous	Runs	of	anÂ Application	with	the	Timeline	102	
Scheduling	aPowerFlow	Application	105	
Viewing	PowerFlowÂ System	Diagnostics	108	
Backing	up	Data	110	
Creating	aBackup	111	
Restoring	aBackup	114	
Viewing	aReport	for	aPowerFlow	Application	117	
Managing	Configuration	Objects	119	
What	is	aConfiguration	Object?	120 

Creating	aConfiguration	Object	122	
Editing	aConfiguration	Object	125	
Viewing	Logs	in	SL1	PowerFlow	127	
Logging	Data	in	PowerFlow	128	
Local	Logging	128	
Remote	Logging	128	
Viewing	Logs	in	Docker	128	
Logging	Configuration	129	
PowerFlow	Log	Files	129	
Logs	for	the	gui	Service	130	
Logs	for	the	api	Service	130	
Logs	for	the	rabbitmq	Service	130	
Working	with	Log	Files	130	
Accessing	DockerÂ Log	Files	130	
Accessing	Local	File	System	Logs	131	
Understanding	the	Contents	of	Log	Files	131	
Viewing	the	Step	Logs	and	Step	Data	for	aPowerFlow	Application	132	
Removing	Logs	on	aRegular	Schedule	133	
Using	SL1	to	Monitor	SL1	PowerFlow	135	
Monitoring	PowerFlow	136	
Configuring	the	Docker	PowerPack	137	
Configuring	the	SL1	PowerFlow	PowerPack	139	
Configuring	the	PowerPack	140	
Events	Generated	by	the	PowerPack	141	
Configuring	the	Couchbase	PowerPack	142	
Configuring	the	RabbitMQ	PowerPack	144	
Stability	of	the	PowerFlow	Platform	147	
What	makes	up	ahealthy	SL1	system?	147	
What	makes	up	ahealthy	PowerFlow	system?	147	
Using	the	pfctl	Command-	line	Utility	149	
What	is	the	pfctl	Utility?	150	
healthcheck	and	autoheal	150	
healthcheck	151	
autoheal	151	
Example	Output	151	
Using	pfctl	healthcheck	on	the	docker-	compose	file	153	
autocluster	153	
upgrade	154	
open_	firewall_	ports	155	
Troubleshooting	SL1	PowerFlow	157	
Initial	Troubleshooting	Steps	159	
SL1	PowerFlow	159	
ServiceNow	159	
Resources	for	Troubleshooting	159	
Useful	PowerFlow	Ports	159	
pfctl	healthcheck	and	autoheal	160	
Helpful	Docker	Commands	160	
Viewing	Container	Versions	and	Status	160	
Restarting	aService	160	
Stopping	all	PowerFlowÂ Services	160	
Restarting	Docker	160	
Viewing	Logs	for	aSpecificÂ Service	161 

Clearing	RabbitMQ	Volume	161	
Viewing	the	Process	Status	of	All	Services	162	
Deploying	Services	from	aDefined	Docker	Compose	File	162	
Dynamically	Scaling	for	More	Workers	162	
Completely	Removing	Services	from	Running	162	
Helpful	Couchbase	Commands	162	
Checking	the	Couchbase	Cache	to	Ensure	an	SL1	Device	ID	is	Linked	to	aServiceNowÂ Sys	ID	162	
Accessing	Couchbase	with	the	Command-	line	Interface	163	
Useful	APIÂ Commands	163	
GettingÂ PowerFlow	Applications	from	the	PowerFlow	API	163	
Creating	and	Retrieving	Schedules	with	the	PowerFlow	API	163	
Diagnosis	Tools	164	
Identifying	Why	aService	or	Container	Failed	165	
Step	1:	Obtain	the	ID	of	the	failed	container	for	the	service	165	
Step	2:	Check	for	any	error	messages	or	logs	indicating	an	error	165	
Step	3:	Check	for	out	of	memory	events	166	
Troubleshooting	aCloud	Deployment	of	PowerFlow	166	
Identifying	Why	aPowerFlow	Application	Failed	167	
Determining	Where	an	Application	Failed	167	
Retrieving	Additional	Debug	Information	(Debug	Mode)	167	
Frequently	Asked	Questions	169	
What	is	the	first	thing	Ishould	do	when	Ihave	an	issue	with	PowerFlow?	169	
Can	the	steprunner_	syncpacks	service	can	be	limited	to	just	workers?	169	
What	is	the	difference	between	the	steprunner_	syncpacks	and	the	steprunner	services?	169	
What	is	the	minimal	image	required	for	workers?	169	
Ifthe	GUI	server	is	constrained	to	use	only	the	manager	nodes,	do	the	worker	nodes	need	to	have	their	
isconfig.yml	file	updated	with	the	correct	HOST	value?	170	
Can	Iunload	unwanted	images	from	aworker	node?	170	
IfIdedicated	workers	to	one	SL1	stack,	how	are	jobs	configured	to	run	only	on	those	workers?	170	
Approximately	how	much	data	is	sent	between	distributed	PowerFlow	nodes?	170	
How	can	Ioptimize	workers,	queues,	and	tasks?	170	
Why	do	IÂ get	a""Connection	refused""Â error	when	trying	to	communicate	with	Couchbase?	173	
Why	do	Ihave	client-	side	timeouts	when	communicating	with	Couchbase?	173	
What	causes	aTask	Soft	Timeout?	174	
How	do	Iaddress	an	""Error	when	connecting	to	DBÂ Host""	message	when	access	is	denied	to	user	
""root""?	174	
How	do	Iremove	aschedule	that	does	not	have	aname?	174	
How	do	Iidentify	and	fix	adeadlocked	state?	175	
Why	are	incident	numbers	not	populated	in	SL1	on	Incident	creation	in	ServiceNow?	178	
Why	am	Inot	getting	any	Incidents	after	disabling	the	firewall?	178	
Why	are	Incidents	not	getting	created	in	ServiceNow?	178	
What	ifthe	PowerFlow	user	interface	is	down	and	Incidents	are	not	being	generated	in	ServiceNow?	178	
What	ifmy	Incident	does	not	have	aCI?	178	
How	can	Ipoint	the	""latest""	container	to	my	latest	available	images	forÂ PowerFlow?	179	
Why	does	the	latest	tag	not	exist	after	the	initial	ISO	installation?	179	
How	do	Irestore	an	offline	backup	of	my	PowerFlow	system?	179	
What	do	IÂ do	ifÂ I	get	aCode	500	Error	when	Itry	to	access	the	PowerFlow	user	interface?	180	
What	should	IÂ do	ifIÂ get	a500	Error?	181	
What	are	some	common	examples	of	using	the	iscli	tool?	181	
How	do	Iview	aspecific	run	of	an	application	in	PowerFlow?	182	
Why	am	Igetting	an	""ordinal	not	in	range""	step	error?	182	
How	do	IÂ clear	abacklog	of	Celery	tasks	in	Flower?	182 

Why	does	traffic	from	specific	subnets	not	get	aresponse	from	the	PowerFlow?	183	
API	Endpoints	in	SL1	PowerFlow	184	
Interacting	with	the	API	185	
Available	Endpoints	185	
POST	185	
Querying	for	the	State	of	aPowerFlow	Application	186	
GET	186	
REST	187	
DELETE	187	
Configuring	the	PowerFlow	System	for	High	Availability	188	
Types	of	High	Availability	Deployments	for	PowerFlow	189	
Standard	Single-	node	Deployment	(1	Node)	190	
Requirements	190	
Risks	190	
Configuration	190	
Standard	Three-	node	Cluster	(3	Nodes)	191	
Requirements	191	
Risks	192	
Mitigating	Risks	192	
Configuration	192	
3+	Node	Cluster	with	Separate	Workers	(4	or	More	Nodes)	195	
Requirements	195	
Worker	Node	Sizing	196	
Risks	196	
Mitigating	Risks	196	
Configuration	196	
3+	Node	Cluster	with	Separate	Workers	and	Drained	Manager	Nodes	(6	or	More	Nodes)	196	
Requirements	197	
Risks	197	
Configuration	197	
Additional	Deployment	Options	198	
Cross-	Data	Center	Swarm	Configuration	198	
Additional	Notes	199	
Requirements	Overview	199	
Docker	Swarm	Requirements	for	High	Availability	200	
Couchbase	Database	Requirements	for	High	Availability	201	
RabbitMQ	Clustering	and	Persistence	for	High	Availability	201	
RabbitMQ	Option	1:	Persisting	Queue	to	Disk	on	aSingle	Node	(Default	Configuration)	201	
RabbitMQ	Option	2:	Clustering	Nodes	with	Persistent	Queues	on	EachÂ Node	202	
Checking	the	Status	of	aRabbitMQ	Cluster	203	
Preparing	the	PowerFlow	System	for	High	Availability	204	
Configuring	Clustering	and	High	Availability	204	
Automating	the	Configuration	of	aThree-	Node	Cluster	205	
Configuring	Docker	Swarm	206	
Configuring	the	Couchbase	Database	207	
Code	Example:	docker-	compose-	override.yml	210	
Scaling	iservices-	contentapi	213	
Manual	Failover	213	
Additional	Configuration	Information	215	
OptimizationÂ Settings	to	Improve	Performance	of	Large-	ScaleÂ Clusters	215	
Exposing	Additional	Couchbase	Cluster	Node	Management	Interfaces	over	TLS	216	
HAProxy	Configuration	(Optional)	217 

Known	Issues	218	
Docker	Network	Alias	is	incorrect	218	
Docker	container	on	last	swarm	node	cannot	communicate	with	other	swarm	nodes	219	
Couchbase	service	does	not	start,	remains	at	nc	-zlocalhost	219	
Couchbase-	worker	fails	to	connect	to	master	219	
Couchbase	rebalance	fails	with	""Rebalance	exited""	error	219	
When	setting	up	athree-	node	High	Availability	Couchbase	cluster,	the	second	node	does	not	appear	219	
TheÂ PowerFlow	user	interface	fails	to	start	after	amanual	failover	of	the	swarm	node	220	
TheÂ PowerFlow	user	interface	returns	504	errors	220	
NTP	should	be	used,	and	all	node	times	should	be	in	sync	220	
Example	Logs	from	Flower	220	
Configuring	the	PowerFlowÂ System	for	Multi-	tenant	Environments	221	
Quick	Start	Checklist	for	Deployment	222	
Deployment	222	
Core	Service	Nodes	222	
Requirements	223	
Configuring	Core	Service	Nodes	223	
Critical	Elements	to	Monitor	on	Core	Nodes	223	
Worker	Service	Nodes	223	
Requirements	224	
Event	Sync	Throughput	Node	Sizing	224	
Test	Environment	and	Scenario	224	
Configuring	the	Worker	Node	224	
Initial	Worker	Node	Deployment	Settings	225	
Worker	Failover	Considerations	and	Additional	Sizing	225	
Knowing	When	More	Resources	are	Necessary	for	aWorker	225	
Keeping	aWorker	Node	on	Standby	for	Excess	Load	Distribution	225	
Critical	Elements	to	Monitor	in	aSteprunner	226	
Advanced	RabbitMQ	Administration	and	Maintenance	226	
Using	an	External	RabbitMQ	Instance	226	
Setting	aUser	other	than	Guest	for	Queue	Connections	226	
Configuring	the	Broker	(Queue)	URL	227	
Creating	aCustom	Configuration	Object	227	
Create	the	Configuration	Object	227	
Label	the	Worker	Node	Specific	to	the	Customer	227	
Creating	aNode	Label	227	
Placing	aService	on	aLabeled	Node	228	
Dedicating	Queues	Per	Integration	or	Customer	228	
Add	Workers	for	the	New	Queues	228	
Create	Application	Schedules	and	Automation	Settings	to	Utilize	Separate	Queues	230	
Scheduling	an	Application	with	aSpecific	Queue	and	Configuration	231	
Configuring	Automations	to	Utilize	aSpecific	Queue	and	Configuration	231	
Failure	Scenarios	231	
Worker	Containers	231	
API	232	
Couchbase	233	
RabbitMQ	234	
PowerFlow	UserÂ Interface	234	
Redis	235	
Known	Issue	for	Groups	of	Containers	235	
Examples	and	Reference	236	
Example	of	aÂ PowerFlowÂ Configuration	Object	236 

Example	of	aSchedule	Configuration	237	
Test	Cases	241	
Load	Throughput	Test	Cases	241	
Failure	Test	Cases	242	
Backup	Considerations	243	
What	to	Back	Up	243	
Fall	Back	and	Restore	to	aDisaster	Recovery	(Passive)	System	243	
Resiliency	Considerations	244	
The	RabbitMQ	Split-	brain	Handling	Strategy	(SL1	Default	Set	to	Autoheal)	244	
ScienceLogic	Policy	Recommendation	244	
Changing	the	RabbitMQ	Default	Split-	brain	Handling	Policy	244	
Using	Drained	Managers	to	Maintain	SwarmÂ Health	245	
Updating	the	PowerFlow	Cluster	with	Little	to	No	Downtime	245	
Updating	Offline	(No	Connection	to	aDocker	Registry)	245	
Updating	Online	(All	Nodes	Have	aConnection	to	aDocker	Registry)	246	
Additional	Sizing	Considerations	246	
Sizing	for	Couchbase	Services	246	
Sizing	for	RabbitMQ	Services	246	
Sizing	for	Redis	Services	246	
Sizing	for	contentapi	Services	246	
Sizing	for	the	GUI	Service	247	
Sizing	for	Workers:Â Scheduler,	Steprunner,	Flower	247	
Node	Placement	Considerations	247	
Preventing	aKnown	Issue:	Place	contentapi	and	Redis	services	in	the	Same	Physical	Location	247	
Common	Problems,	Symptoms,	and	Solutions	248	
Common	Resolution	Explanations	255	
Elect	aNew	Swarm	Leader	255	
Recreate	RabbitMQ	Queues	and	Exchanges	255	
Resynchronize	RabbitMQ	Queues	256	
Identify	the	Cause	of	aService	not	Deploying	256	
Repair	Couchbase	Indexes	257	
Add	aBroken	Couchbase	Node	Back	into	the	Cluster	258	
Restore	Couchbase	Manually	258	
PowerFlow	Multi-	tenant	Upgrade	Process	259	
Perform	Environment	Checks	Before	Upgrading	259	
Prepare	the	Systems	259	
Perform	the	Upgrade	261	
Upgrade	Core	Services	(Rabbit	and	Couchbase)	263	
Update	the	GUI	265	
Update	Workers	and	contentapi	265 

Chapte r	
1	
Introduction	to	SL1	PowerFlow	and	the	
PowerFlow	Builder	
O v e r v i e w
SL1	PowerFlow	provides	ageneric	platform	for	integrations	between	SL1	and	third-	party	platforms.	From	the	
PowerFlow	user	interface,	you	can	use	the	PowerFlow	Builder	to	create	complicated	automations	with	logical	
branching	using	drag-	and-	drop	components.	
NOTE:	After	the	2.1.0	platform	release,	the	Integration	Service	was	rebranded	as	SL1	Â PowerFlow	,which	is	
available	in	SL1	Standard	solutions.	Also,	the	Automation	Builder	was	rebranded	as	SL1	Â PowerFlow	
builder	,which	is	available	in	SL1	Premium	solutions.	For	more	information,	see	ScienceLogic	
Pricing	.	
This	chapter	covers	the	following	topics:	
What	is	SL1	PowerFlow?	10	
What	is	the	SL1	PowerFlow	Builder?	11	
What	is	a	Step?	12	
What	is	a	PowerFlow	Application?	14	
What	is	a	Configuration	Object?	15	
Creating	and	Saving	PowerFlow	Components	15	
Elements	of	the	PowerFlow	User	Interface	15	
Monitoring	PowerFlow	on	the	Dashboard	Page	18
9 

10
W h a t	i s	S L 1	P o w e r F l o w	?	
SL1	PowerFlow	enables	intelligent,	bi-	directional	communication	between	the	ScienceLogic	data	platform	and	
external	data	platforms	to	promote	aunified	management	ecosystem.	PowerFlow	allows	users	to	translate	and	
share	data	between	SL1	and	other	platforms	without	the	need	for	programming	knowledge.	PowerFlow	is	
designed	to	provide	high	availability	and	scalability.	
The	following	image	shows	an	example	of	aPowerFlow	application	and	its	steps	in	the	PowerFlow	user	interface:	
The	key	elements	of	the	PowerFlow	user	interface	include	the	following:	
l	Steps	.A	step	is	ageneric	Python	class	that	performs	asingle	action.	Steps	can	accept	zero	or	many	input	
parameters	or	data	from	previous	steps,	and	steps	can	specify	output	to	be	used	by	other	steps.	The	input	
parameters	are	configurable	variables	and	values	used	during	execution.	Steps	can	be	re-	used	in	multiple	
PowerFlow	applications.	When	these	steps	are	combined	in	an	application,	they	provide	aworkflow	that	
satisfies	abusiness	requirement.	All	Python	step	code	should	be	Python	3.7	or	later.	In	the	image	above,	the	
steps	display	as	part	of	the	flowchart	in	the	main	viewing	pane	as	well	as	the	StepsÂ Registry	pane.	
l	Applications	.An	application	is	aJSONÂ object	that	includes	all	the	information	required	for	executing	an	
integration	on	the	PowerFlow	platform.	An	application	combines	aset	of	steps	that	execute	aworkflow.	The	
input	parameters	for	each	step	are	also	defined	in	the	application	and	can	be	provided	either	directly	in	the	
step	or	in	the	parent	application.	In	the	image	above,	the	group	of	connected	steps	in	the	large	pane	make	
up	the	""SyncÂ Organizations	from	SL1	to	ServiceNow	Companies""	application.	You	can	access	all	
applications	on	the	Applications	page	(	),	and	you	can	create	new	applications	using	the	SL1	PowerFlow	
builder	.	
What	is	SL1	PowerFlow? 

What	is	the	SL1	PowerFlow	Builder?	
l	Configuration	Objects	.A	configuration	object	is	astand-	alone	JSON	file	that	contains	aset	of	configuration	
variables	used	as	input	for	an	application.	Configurations	can	include	variables	like	hostname,	user	name,	
password,	or	other	credential	information.	Configuration	objects	allow	the	same	application	to	be	deployed	
in	multiple	PowerFlow	instances,	with	different	configurations.	Click	the	[Configure	]button	from	an	
application	in	the	PowerFlow	user	interface	to	access	the	configuration	object	for	that	application.	You	can	
access	all	configuration	objects	on	the	Configurations	page	(	).	
l	Synchronization	PowerPacks	.A	Synchronization	PowerPack	(also	called	aSyncPack)	contains	all	the	code	
and	logic	needed	to	perform	integrations	on	the	PowerFlow	platform.	You	can	access	the	latest	steps,	
applications,	and	configurations	for	PowerFlow	or	athird-	party	integration	(such	as	ServiceNow,	Cherwell,	or	
Restorepoint)	by	downloading	the	most	recent	Synchronization	PowerPack	for	that	integration	from	
ScienceLogic.	You	can	access	all	Synchronization	PowerPacks	on	the	SyncPacks	page	(	).	
W h a t	i s	th e	S L 1	P o w e r F l o w	B u i l d e r	?	
You	can	use	the	SL1	PowerFlow	builder	in	the	PowerFlow	user	interface	to	create	complicated	applications	with	
logical	branching	and	data	transformation	features	using	drag-	and-	drop	components.	You	access	the	PowerFlow	
builder	on	the	Applications	page	in	the	PowerFlow	user	interface.	
For	example,	from	the	Steps	Registry	paneÂ on	an	Application	page,	you	can	drag	aCondition	operator	(	)	
onto	an	application	workflow	to	create	the	option	for	branching	flows,	such	as	If-	Else	or	If-	Then-	Else	statements:	
11 

12
Also,	you	can	drag	aTransform	operator	(	)from	the	Steps	Registry	pane	onto	an	application	workflow	to	pull	
data	gathered	by	aprevious	step	and	modify	or	transform	the	data	to	fit	into	the	next	step:	
For	more	information	about	the	PowerFlow	builder	,see	Managing	PowerFlow	Applications	.	
W h a t	i s	a	S te p ?	
In	PowerFlow	,astep	is	ageneric	Python	class	that	performs	asingle	action,	such	as	caching	device	data:	
Steps	accept	arguments	called	input	parameters	.The	parameters	specify	the	values,	variables,	and	
configurations	to	use	when	executing	the	step.	Parameters	allow	steps	to	accept	arguments	and	allow	steps	to	be	
re-	used	in	multiple	integrations.	For	example,	you	can	use	the	same	step	to	query	both	the	local	system	and	
another	remote	system;	only	the	arguments,	such	as	hostname,	username,	and	password	change.	
What	is	aStep? 

What	is	aStep?	
You	can	view	and	edit	the	parameters	for	astep	by	opening	aPowerFlow	application	from	the	Applications	page,	
clicking	[Open	Editor	](	),	and	then	clicking	the	gear	icon	(	)on	astep.	The	Configuration	pane	for	that	
step	appears:	
A	step	can	pass	the	data	itgenerates	during	execution	to	asubsequent	step.	A	step	can	use	the	data	generated	by	
another	step.	Also,	you	can	add	test	data	to	the	step	and	click	the	down	arrow	next	to	the	[Run	]button	(	)and	
select	Custom	Run	to	run	test	data	for	that	step.	
PowerFlow	analyzes	the	required	parameters	for	each	step	and	alerts	you	ifany	required	parameters	are	missing	
before	PowerFlow	runs	the	step.	
Steps	are	grouped	into	the	following	types:Â 	
l	Standard	.Standard	steps	do	not	require	any	previously	collected	data	to	perform.	Standard	steps	are	
generally	used	to	generate	data	to	perform	atransformation	or	adatabase	insert.	These	steps	can	be	run	
independently	and	concurrently.	
l	Aggregated	.Aggregated	steps	require	data	that	was	generated	by	apreviously	run	step.	Aggregated	steps	
are	not	executed	by	PowerFlow	until	all	data	required	for	the	aggregation	is	available.	These	steps	can	be	run	
independently	and	concurrently.	
l	Trigger	.Trigger	steps	are	used	to	trigger	other	PowerFlow	applications.	These	steps	can	be	configured	to	be	
blocking	or	not.	
A	variety	of	generic	steps	are	available	from	ScienceLogic	,and	you	can	access	alist	of	steps	by	sending	aGET	
request	using	the	APIÂ /steps	endpoint	.	
13 

14
W h a t	i s	a	P o w e r F l o w	A p p l i c a ti o n ?	
In	PowerFlow	,an	application	is	aJSONÂ file	that	specifies	which	steps	to	execute	and	the	order	in	which	to	
execute	those	steps.	An	application	also	defines	variables	and	provides	arguments	for	each	step.	
The	following	is	an	example	of	aPowerFlow	application:	
PowerFlow	application	JSON	objects	are	defined	by	configuration	settings,	steps	that	make	up	the	application,	
and	application-	wide	variables	used	as	parameters	for	each	step.	The	parameters	of	each	step	can	be	configured	
dynamically,	and	each	step	can	be	named	uniquely	while	still	sharing	the	same	underlying	class,	allowing	for	
maximum	re-	use	of	code.	
Applications	can	be	executed	through	the	REST	API	and	are	processed	as	an	asynchronous	task	in	PowerFlow	.	
During	processing	the	user	is	provided	aunique	task	ID	for	the	application	and	each	of	its	tasks.	Using	the	task	IDs,	
the	user	can	poll	for	the	status	of	the	application	and	the	status	of	each	individual	running	step	in	the	application.	
Executing	an	application	from	the	REST	API	allows	the	user	to	dynamically	set	one-	time	parameter	values	for	the	
variables	defined	in	the	application.	
The	required	parameters	of	applications	are	strictly	enforced,	and	PowerFlow	will	refuse	to	execute	the	application	
ifall	required	variables	are	not	provided.	
For	more	information	about	applications,	see	Managing	PowerFlow	Applications	.	
What	is	aPowerFlow	Application? 

What	is	aConfiguration	Object?	
W h a t	i s	a	C o n f i g u r a ti o n	O b j e c t?	
Configuration	variables	are	defined	in	astand-	alone	JSON	file	called	aconfiguration	object	that	lives	on	the	
PowerFlow	system	and	can	be	accessed	by	all	PowerFlow	applications	and	their	steps.	
Each	global	variable	is	defined	as	aJSON	object	in	the	configuration.	Typically,	aconfiguration	object	looks	like	
the	following:
{	
""encrypted"":	true,	
""name"":	""var_	name"",	
""value"":""var_	value""	
}	
Configuration	objects	can	map	variables	from	the	SL1	platform	to	athird-	party	platform.	For	instance,	SL1	has	
device	classes	and	ServiceNow	has	CI	classes;	the	configuration	object	maps	these	two	sets	of	variables	together.	
Each	global	variable	in	the	configuration	has	the	option	of	being	encrypted.	The	values	of	encrypted	variables	are	
encrypted	within	PowerFlow	upon	upload	through	the	REST	API.	
For	more	information	about	configuration	objects,	see	Managing	Configuration	Objects	.	
C r e a ti n g	a n d	S a v i n g	P o w e r F l o w	C o m p o n e n ts	
Instead	of	using	the	PowerFlow	user	interface,	you	can	create	steps,	applications,	and	configurations	in	your	own	
editor	and	then	upload	them	using	the	API	or	the	command	line	tool	(iscli).	
For	more	information,	see	the	SL1	PowerFlow	for	Developers	manual	.	
E l e m e n ts	o f	th e	P o w e r F l o w	U s e r	I n te r f a c e	
With	the	2.0.0	release	of	the	PowerFlow	platform,	the	PowerFlow	user	interface	was	updated	to	match	the	8.12.0	
and	later	release	of	the	SL1	user	interface,	with	the	navigation	tabs	now	located	on	the	left-	hand	side	of	the	
window.Â The	tabs	provide	access	to	the	following	pages:	Dashboard	,SyncPacks	,Â Applications	,	
Configurations	,Reports	,and	Admin	Panel	.	
TIP:	To	view	apop-	out	list	of	menu	options,	click	the	menu	iconÂ 	(	).	Click	the	menu	icon	again	to	close	the	
pop-	out	menu.	
L og g i n g	I n	a n d	O u t	of	t h e	P ow e r F l ow	U s e r	I n t e r f a c e	
You	can	log	in	to	PowerFlow	using	one	of	the	following	authorization	types:	
15 

16	
l	Local	Authentication	.The	same	local	Administrator	user	(isadmin	)is	supported	by	default	with	2.0.0	
installations.	Ifyou	are	migrating	from	aprevious	version	of	PowerFlow	to	version	2.0.0,	you	can	log	in	and	
authenticate	with	the	same	user	and	password.	
l	Basic	Authentication	.PowerFlow	2.0.0	continues	to	support	Basic	Authentication	as	well.	Because	
PowerFlow	PowerPacks	,diagnostic	scripts,	and	the	iscli	tool	continue	to	use	Basic	Authentication,	
ScienceLogic	does	not	recommend	disabling	Basic	Authentication	with	PowerFlow	version	2.0.0	or	later.	
l	OAuth	.Â OAuth	lets	PowerFlow	administrators	use	their	own	authentication	providers	to	enforce	user	
authentication	and	lockout	policies.	Authentication	using	athird-	party	provider,	such	as	LDAP	or	Active	
Directory,	requires	additional	configuration.	For	optimal	security,	ScienceLogic	recommends	that	you	disable	
the	local	Administrator	user	(isadmin	)and	exclusively	use	your	own	authentication	provider.	
Depending	on	the	authentication	used	by	yourÂ 	PowerFlow	system	,your	login	page	will	display	asingle	option	for	
logging	in,	or	more	than	one	option,	as	in	the	following	example:	
For	more	information	about	configuring	authorization	for	users,	see	Managing	Users	in	PowerFlow	.	
To	log	out	of	PowerFlow	,click	your	user	name	in	the	navigation	bar	in	the	top	right	of	any	window	and	select	Log	
off	.
TIP:	Ifyou	get	aâSyncPacks	service	is	not	reachableâ	pop-	up	message	in	the	user	interface	and	the	various	
pages	are	empty,	log	out	of	the	PowerFlow	user	interface	and	log	back	in	again.	You	can	also	click	
[Refresh	]in	your	browser	to	automatically	log	out.	This	situation	occurs	only	ifthe	user	interface	is	idle	for	
along	period	of	time.	
P ow e r F l ow	Â P a g e s	
The	Dashboard	page	(	)provides	agraphical	view	of	the	various	tasks,	workers,	and	applications	that	are	
running	on	you	PowerFlow	system.	For	more	information,	see	Monitoring	PowerFlow	on	the	Dashboard	
Page	.	
The	SyncPacks	page	(	)lets	you	import,	install,	view,	and	edit	Synchronization	PowerPacks	,which	are	also	
called	""SyncPacks"".Â For	more	information,	see	Managing	Synchronization	PowerPacks	.	
Elements	of	the	PowerFlow	User	Interface 

Elements	of	the	PowerFlow	User	Interface	
The	Applications	page	(	)provides	alist	of	the	applications	available	on	your	PowerFlow	system.	This	page	was	
called	the	Integrations	page	in	previous	versions	of	PowerFlow	.From	this	page	you	can	run,	schedule,	and	
create	applications.	You	can	also	create	applications	that	use	logical	branching	and	data	transformation	between	
steps.	For	more	information,	see	Managing	PowerFlow	Applications	.	
The	Configurations	page	(	)lets	you	create	or	use	aconfiguration	object	to	define	aset	of	variables	that	all	
steps	and	PowerFlow	applications	can	use.	For	more	information,	see	Managing	Configuration	Objects	.	
The	Reports	page	(	)contains	alist	of	reports	associated	with	PowerFlow	applications	that	have	the	reporting	
feature	enabled,	such	as	the	""PowerFlow	System	Diagnostics""	application.	For	more	information,	see	Viewing	a	
Report	for	an	Application	.	
The	Admin	Panel	pageÂ 	(	)contains	alist	of	user	groups,	which	lets	you	determine	the	roles	and	access	for	
your	users.Â Only	users	with	the	Administrator	role	for	this	PowerFlow	system	can	edit	this	page.	For	more	
information,	see	ManagingÂ Users	in	PowerFlow	.	
TIP:	While	the	SyncPacks	,Â Applications	,Configurations	,Reports	and	Admin	Panel	pages	are	loading	or	
running	aprocedure,	you	will	see	adark	green,	animated	line	running	across	the	top	of	the	page	until	the	
process	completes.	
A d d i t i on a l	N a v i g a t i on	
The	user	name	drop-	down,	which	is	found	in	the	navigation	bar	in	the	top	right	of	any	window	in	the	PowerFlow	
user	interface,	contains	the	following	options:	
l	About	.Displays	version	information	about	the	PowerFlow	version	and	the	licenses	used	by	PowerFlow	.	
l	Help	.Â Displays	online	Help	for	PowerFlow	in	anew	browser	window.	
l	Notifications	.Opens	the	Notification	Center	pane,	which	contains	alog	of	all	previous	notifications	that	
appeared	on	the	PowerFlow	system	about	applications	that	were	run	successfully	or	with	warnings	or	failures:	
17 

18	
The	different	notifications	are	color-	coded:	green	for	success,	yellow	for	warning,	and	red	for	failure.	The	
number	of	notifications	displays	as	abadge	in	the	menu.	For	more	information	about	anotification,	click	the	
link	for	the	page	where	the	notification	appeared	and	review	the	Step	Log	and	Step	Data	tabs	for	the	
application	steps.	
l	Log	off	.Logs	you	out	of	the	PowerFlow	user	interface.	
M o n i to r i n g	P o w e r F l o w	o n	th e	D a s h b o a r d	P a g e	
You	can	use	the	Dashboard	page	(	)in	the	PowerFlow	user	interface	to	monitor	the	status	of	the	various	tasks,	
workers,	and	applications	that	are	running	on	your	PowerFlow	system.	You	can	use	this	information	to	quickly	
determine	ifyour	PowerFlow	instance	is	performing	as	expected:	
The	Dashboards	page	is	the	initial	landing	page	after	you	log	in	to	PowerFlow	.This	page	displays	high-	level	
statistics	about	the	health	of	the	worker	services	that	are	being	used	by	the	PowerFlow	instance.	
Monitoring	PowerFlow	on	the	Dashboard	Page 

Monitoring	PowerFlow	on	the	Dashboard	Page	
To	view	more	information	on	the	Dashboard	page:	
1.	Hover	over	acircle	graph	or	abar	chart	item	to	view	apop-	up	field	that	contains	the	count	for	that	item	on	the	
graph	or	chart,	such	as	""Success:	48""	for	successful	tasks	on	the	All	Tasks	graph.	
2.	Click	the	List	iconÂ 	(	)for	the	All	Tasks	,Workers	,or	Applications	graphs	to	view	alist	of	relevant	tasks,	
workers,	or	applications.	Use	the	left	and	right	arrow	icons	to	move	through	the	list	of	items.	
3.	To	view	additional	details	about	aspecific	tasks,	click	the	List	iconÂ 	(	)for	the	All	Tasks	graph	and	then	click	
the	link	for	the	task,	where	relevant.	The	application	aligned	with	that	task	appears,	and	you	can	select	astep	
and	view	the	Step	Log	details	for	that	step.	
19 

Chapte r	
2	
Installing	and	Configuring	SL1	PowerFlow	
O v e r v i e w
This	chapter	describes	how	to	install,	upgrade,	and	configure	PowerFlow	,and	also	how	to	set	up	security	for	
PowerFlow	.	
This	chapter	covers	the	following	topics:	
PowerFlowÂ Architecture	21	
System	Requirements	25	
Additional	Prerequisites	for	PowerFlow	26	
Installing	PowerFlow	27	
Upgrading	PowerFlow	36	
Licensing	PowerFlow	46	
Configuring	Additional	Elements	of	PowerFlow	49	
Changing	the	PowerFlow	System	Password	50	
Configuring	a	Proxy	Server	51	
Configuring	Security	Settings	52	
PowerFlow	Management	Endpoints	53
20 

21
P o w e r F l o w	Â A r c h i te c tu r e	
This	topic	describes	the	different	aspects	of	PowerFlow	architecture.	
P ow e r F l ow	C on t a i n e r Â A r c h i t e c t u r e	
PowerFlow	is	acollection	of	purpose-	built	containers	that	are	charged	to	pass	information	to	and	from	SL1	.	
Building	PowerFlow	architecture	in	containers	allows	you	to	add	more	processes	to	handle	the	workload	as	
needed.
The	following	diagram	describes	the	container	architecture	for	PowerFlow	:	
The	PowerFlow	includes	the	following	containers:	
l	GUI	.The	GUIÂ container	provides	the	user	interface	for	PowerFlow	.	
l	RESTÂ API	.TheÂ RESTÂ APIÂ container	provides	access	to	the	Content	Store	on	the	PowerFlow	instance.	
l	Content	Store	.The	ContentÂ Store	container	is	basically	adatabase	service	that	contains	all	the	reusable	
steps,	applications,	and	containers	in	the	PowerFlow	instance.	
l	Step	Runners	.Step	Runner	containers	execute	steps	independently	of	other	Step	Runners.	All	Step	Runners	
belong	to	aWorker	Pool	and	can	run	steps	in	order,	based	on	the	instructions	in	the	applications.	By	default	
there	are	five	StepÂ RunnersÂ 	(worker	nodes)	include	in	the	PowerFlow	platform.Â 	PowerFlow	users	can	scale	up	
or	scale	down	the	number	of	worker	nodes,	based	on	the	workload	requirements.	
PowerFlowÂ Architecture 

PowerFlowÂ Architecture
I n t e g r a t i on	W or k f l ow	
The	following	high-	level	diagram	for	aServiceNow	Integration	provides	an	example	of	how	PowerFlow	
communicates	with	both	the	SL1	Central	Database	and	the	third-	party	(ServiceNow)	APIs:	
The	workflow	includes	the	following	components	and	their	communication	methods:	
l	SL1	Central	Database	.PowerFlow	communicates	with	the	SL1	database	over	port	7706.	
l	SL1	RESTÂ API	.PowerFlow	communicates	with	the	SL1	RESTÂ API	over	port	443.	
l	GraphQL	.PowerFlow	communicates	with	GraphQL	over	port	443.	
l	ServiceNowÂ Base	PowerPack	.In	this	example,	theÂ Run	Book	Automations	from	the	ServiceNow	Base	
PowerPack	(and	otherÂ 	SL1	Â PowerPacks)	communicate	with	PowerFlow	over	port	443.	
l	PowerFlow	.PowerFlow	communicates	with	both	the	SL1	Central	Database	and	an	external	endpoint.	
l	ServiceNow	API	.In	this	example,	the	ServiceNow	applications	in	PowerFlow	communicate	with	the	
ServiceNow	API	over	port	443.	
NOTE:	PowerFlow	both	pulls	data	from	SL1	and	has	data	pushed	to	itfrom	SL1	.PowerFlow	both	sends	and	
retrieves	information	to	and	from	ServiceNow,	but	PowerFlow	is	originating	the	requests.	
H i g h -	A v a i l a b i l i t y ,	O f f -	s i t e	B a c k u p ,	a n d	P r ox y	A r c h i t e c t u r e	
You	can	deploy	PowerFlow	as	aHighÂ Availability	cluster,	which	requires	at	least	three	nodes	to	achieve	automatic	
failover.	While	PowerFlow	can	be	deployed	as	asingle	node,	the	single-	node	option	does	not	provide	redundancy	
through	HighÂ Availability.	PowerFlow	also	supports	off-	site	backup	and	connection	through	aproxy	server.	
22 

23
The	following	diagram	describes	these	different	configurations:	
l	High	Availability	for	PowerFlow	is	acluster	of	PowerFlow	nodes	with	aLoadÂ Balancer	managing	the	
workload.Â In	the	above	scenario,	ifone	PowerFlow	node	fails,	the	workload	will	be	redistributed	to	the	
remaining	PowerFlow	nodes.Â High	Availability	provides	local	redundancy.	For	more	information,	see	
Appendix	A:	Configuring	PowerFlow	for	High	Availability	.	
l	Off-	site	Backup	can	be	configured	by	using	PowerFlow	to	back	up	and	recover	data	in	the	Couchbase	
database.	The	backup	process	creates	abackup	file	and	sends	that	file	using	Secure	Copy	Protocol	(SCP)	to	
auser-	defined,	off-	site	destination	system.	You	can	then	use	the	backup	file	from	the	remote	system	and	
restore	its	content.	For	more	information,	see	Backing	up	Data	.	
l	A	Proxy	Server	is	adedicated	computer	or	software	system	running	as	an	intermediary.	The	proxy	server	in	
the	above	scenario	handles	the	requests	between	PowerFlow	and	the	third-	party	application.	For	more	
information,	see	Configuring	a	Proxy	Server	.	
In	addition,	you	can	deploy	PowerFlow	in	amulti-	tenant	environment	that	supports	multiple	customers	in	ahighly	
available	fashion.	After	the	initial	High	Availability	(HA)	core	services	are	deployed,	the	multi-	tenant	environment	
differs	in	the	deployment	and	placement	of	workers	and	use	of	custom	queues.	For	more	information,	see	
Appendix	B:	Configuring	PowerFlow	for	Multi-	tenant	Environments	.	
R e v i e w i n g	Y ou r	D e p l oy m e n t	A r c h i t e c t u r e	
Review	the	following	aspects	of	your	architecture	before	deploying	PowerFlow	:	
A.	How	many	SL1	stacks	will	you	use	to	integrate	with	the	third-	party	platform	(such	as	ServiceNow,Â Cherwell,	
or	Restorepoint)?	
B.	What	is	agood	estimate	of	the	number	of	devices	across	all	of	your	SL1	stacks?	
C.	How	many	data	centers	will	you	use?	
PowerFlowÂ Architecture 

PowerFlowÂ Architecture	
D.	Specify	the	location	of	each	data	center.	
E.	What	is	the	latency	between	each	data	center?Â 	(Latency	must	be	less	than	80	ms.)	
F.	How	many	SL1	stacks	are	in	each	data	center?	
G.	Are	there	any	restrictions	on	data	replication	across	regions?	
H.	What	is	the	location	of	the	third-	party	platform	(if	applicable)?	
I.	What	is	the	VIP	for	Cluster	Node	Management?	
Based	on	the	above	list,	ScienceLogic	recommends	the	following	deployment	paths:	
l	For	question	A,	ifyou	answered	three	or	fewer	SL1	stacks,	consider	astandard	High-	Availability	
deployment.	For	more	information,	see	Appendix	A:	Configuring	PowerFlow	for	High	Availability	.	
l	For	question	A,	ifyou	answered	more	than	three	SL1	stacks	to	question	A,	consider	configuring	PowerFlow	
in	amulti-	tenant	configuration.	For	more	information,	see	Appendix	B:	Configuring	PowerFlow	for	Multi-	
tenant	Environments	.	
l	ForÂ question	G,	ifyou	answered	""Yes	""to	data	replication	restrictions,	consider	the	following	deployment	
options:	
o	Deploy	separate	PowerFlow	clusters	per	region	.This	deployment	requires	more	management	of	
PowerFlow	clusters,	but	itensures	that	the	data	is	completely	separated	between	regions.	This	
deployment	also	ensures	that	ifasingle	region	goes	down,	you	only	lose	operations	for	that	region.	
o	Deploy	a	single	PowerFlow	cluster	in	the	restrictive	region	.This	deployment	is	easier	to	manage,	
as	you	are	only	dealing	with	asingle	PowerFlow	cluster.	As	an	example,	ifEurope	has	alaw	that	
requires	that	data	in	Europe	cannot	be	replicated	to	the	United	States,	but	that	law	does	not	prevent	
data	from	the	United	States	from	coming	into	Europe,	you	can	deploy	asingle	PowerFlow	cluster	in	
Europe	to	satisfy	the	law	requirements.	
l	Ifyou	are	deploying	amulti-	tenant	configuration,	check	to	see	ifyour	environment	meets	one	the	following:	
o	You	have	three	or	more	data	centers	and	the	latency	between	each	data	center	is	less	than	
80	ms	(question	E),	consider	deploying	amulti-	tenant	PowerFlow	where	each	node	is	in	aseparate	
data	center	to	ensure	data	center	resiliency.	This	deployment	ensures	that	ifasingle	data	center	goes	
down,	PowerFlow	will	remain	operational.	
o	You	have	only	two	data	centers	and	the	latency	between	data	centers	is	less	than	80	ms	,	
consider	deploying	amulti-	tenant	PowerFlow	where	two	nodes	are	in	one	data	center	and	the	other	
node	is	in	the	other	data	center.	This	deployment	does	not	ensure	data	center	resiliency,	but	itdoes	
provide	standard	High	Availability	ifasingle	node	goes	down.	Ifthe	data	center	with	one	node	goes	
down,	PowerFlow	will	remain	operational.	However,	ifthe	data	center	with	two	nodes	goes	down,	
PowerFlow	will	no	longer	remain	operational.	
o	You	have	only	two	data	centers	but	the	latency	between	data	centers	is	more	than	80	ms	.In	
this	situation,	you	can	still	deploy	amulti-	tenant	PowerFlow	,but	all	nodes	must	be	located	in	asingle	
data	center.	This	deployment	still	provides	standard	High	Availability	so	that,	ifasingle	node	goes	
down,	the	other	two	nodes	ensure	PowerFlow	operations.	Ifyou	require	more	resiliency	than	asingle-	
node	failure,	you	can	deploy	five	nodes,	which	will	ensure	resiliency	with	two	down	nodes.	However,	if	
the	data	center	goes	down,	PowerFlow	will	not	be	operational.	
24 

25	
o	You	only	have	one	data	center,	you	can	still	deploy	a	multi-	tenant	PowerFlow	,but	all	nodes	
are	located	in	a	single	data	center	.This	deployment	still	provides	standard	High	Availability	so	that,	
ifasingle	node	goes	down,	the	other	two	nodes	ensure	PowerFlow	operations.	Ifyou	require	more	
resiliency	than	asingle-	node	failure,	you	can	deploy	five	nodes,	which	will	ensure	resiliency	with	two	
down	nodes.	However,	ifthe	data	center	goes	down,	PowerFlow	will	not	be	operational.	
S y s te m	R e q u i r e m e n ts	
PowerFlow	itself	does	not	have	specific	minimum	required	versions	for	SL1	or	AP2.	However,	certain	
Synchronization	PowerPacks	for	PowerFlow	have	minimum	version	dependencies.	Please	see	the	documentation	
for	those	Synchronization	PowerPacks	for	more	information	on	those	dependencies.	
The	following	table	lists	the	port	access	required	by	PowerFlow	:	
Source	IP	PowerFlow
Destination	
PowerFlow
SourceÂ Port	
Destination	Port	Requirement	
PowerFlow	SL1	API	Any	TCP	443	SL1	API	Access	
SL1	Run	Book	Action	PowerFlow	Any	TCP	443	SendÂ 	SL1	data	to	
PowerFlow	
Devpi	PowerFlow	Any	TCP	3141	Internal	Python	
package	repository	
for	Synchronization	
PowerPacks	;check	
for	self-	certification	
for	PowerFlow	
Dex	Server	PowerFlow	Any	TCP	5556	Enable
authentication	for	
PowerFlow	
PowerFlow	SL1	Database	Any	TCP	7706	SL1
DatabaseÂ Access	
Docker	Visualizer	PowerFlow	8081	n/a	Docker	Visualizer	
(http://	only)	
Couchbase
Dashboard	
PowerFlow	8091	n/a	Couchbase
Dashboard	(use	
yourÂ 	PowerFlow	
credentials)	
RabbitMQ
Dashboard	
PowerFlow	15672	n/a	RabbitÂ MQ
Dashboard	(use	
guest	/guest	for	
credentials)	
ScienceLogic	highly	recommends	that	you	disable	all	firewall	session-	limiting	policies.	Firewalls	will	drop	HTTPS	
requests,	which	results	in	data	loss.	
System	Requirements 

Additional	Prerequisites	for	PowerFlow	
NOTE:	The	default	internal	network	used	by	PowerFlow	services	is	172.21.0.1/16	.Please	ensure	that	this	
range	does	not	conflict	with	any	other	IP	addresses	on	your	network.	Ifneeded,	you	can	change	this	
subnet	in	the	docker-	compose.yml	file.	
TIP:	For	more	information	about	system	requirements	for	your	PowerFlow	environment,	see	the	System	
Requirements	page	at	the	ScienceLogic	Support	site	.	
H a r d e n e d	O p e r a t i n g	S y s t e m	
The	operating	system	for	PowerFlow	is	pre-	hardened	by	default,	with	firewalls	configured	only	for	essential	port	
access	and	all	services	and	processes	running	inside	Docker	containers,	communicating	on	asecure,	encrypted	
overlay	network	between	nodes.	Please	refer	to	the	table,	above,	for	more	information	on	essential	ports.	
You	can	apply	additional	Linux	hardening	policies	or	package	updates	as	long	as	Docker	and	its	network	
communications	are	operational.	
NOTE:	The	PowerFlow	operating	system	is	an	Oracle	Linux	distribution,	and	all	patches	are	provided	within	
the	standard	Oracle	Linux	repositories.	The	patches	are	not	provided	by	ScienceLogic.	
A d d i ti o n a l	P r e r e q u i s i te s	f o r	P o w e r F l o w	
To	work	with	PowerFlow	,ScienceLogic	recommends	that	you	have	knowledge	of	the	following:	
l	Linux	and	vi	(or	another	text	editor).	
l	Python.	
l	Postman	or	another	APIÂ tool	for	interacting	with	theÂ 	PowerFlow	API.	
l	Docker.	For	more	information,	see	Helpful	Docker	Commands	and	
https://docs.docker.com/engine/reference/commandline/cli/	.In	addition,	you	must	give	your	Docker	Hub	
ID	to	your	ScienceLogic	Customer	Success	Manager	to	enable	permissions	to	pull	the	containers	from	
Docker	Hub.	
l	Couchbase.	For	more	information,	see	Helpful	Couchbase	Commands	.	
26 

27
I n s ta l l i n g	P o w e r F l o w	
You	can	install	PowerFlow	for	the	first	time	in	the	following	ways:	
l	Via	ISO	to	a	server	on	your	network	
l	Via	RPM	to	a	cloud-	based	server	
Ifyou	are	upgrading	an	existing	version	of	the	PowerFlow	,see	Upgrading	PowerFlow	.	
Ifyou	are	installing	PowerFlow	in	aclustered	environment,	see	Configuring	the	PowerFlow	System	for	High	
Availability	.	
I n s t a l l i n g	P ow e r F l ow	v i a	I S O	
L o c a t i n g	t h e	I S O	I m a g e	
To	locate	the	PowerFlow	ISO	image:	
1.	Go	to	the	ScienceLogic	Support	site	at	https://support.sciencelogic.com/s/	.	
2.	Click	the	ProductÂ Downloads	tab	and	select	PowerFlow	.The	PowerFlow	Release	Version	page	appears.	
3.	Click	the	link	to	the	current	release.	The	Release	Version	page	appears.	
4.	In	the	Release	Files	section,	click	the	ISOÂ link	for	the	PowerFlow	image.	A	Release	File	page	appears.	
5.	ClickÂ 	[Download	File	]at	the	bottom	of	the	Release	File	page.	
Installing	PowerFlow 

Installing	PowerFlow	
I n s t a l l i n g	f r o m	t h e	I S O	I m a g e	
To	install	PowerFlow	viaÂ ISO	image:	
1.	Download	the	latest	PowerFlow	ISO	file	to	your	computer	or	avirtual	machine	center.	
2.	Using	your	hypervisor	or	bare-	metal	(single-	tenant)	server	of	choice,	mount	and	boot	from	the	PowerFlow	
ISO.	The	PowerFlow	Installation	window	appears:	
28 

29	
3.	Select	Install	Integration	Service	.After	the	installer	loads,	the	Network	Configuration	window	appears:	
4.	Complete	the	following	fields:	
l	IP	Address	.Type	the	primary	IP	address	of	the	PowerFlow	server.	
l	Netmask	.Type	the	netmask	for	the	primary	IPÂ address	of	the	PowerFlow	server.	
l	Gateway	.Type	the	IPÂ address	for	the	network	gateway.	
l	DNS	Server	.Type	the	IPÂ address	for	the	primary	nameserver.	
l	Hostname	.Type	the	hostname	for	PowerFlow	.	
Installing	PowerFlow 

Installing	PowerFlow	
5.	Press	[Continue	].The	Root	Password	window	appears:	
6.	Type	the	password	you	want	to	set	for	the	root	user	on	the	PowerFlow	host	(and	the	service	account	
password)Â and	press	[Enter	].The	password	must	be	at	least	six	characters	and	no	more	than	24	characters,	
and	all	special	characters	are	supported.	
NOTE:	You	use	this	password	to	log	into	the	PowerFlow	user	interface,	to	SSH	to	the	PowerFlow	server,	and	
to	verify	API	requests	and	database	actions.	This	password	is	set	as	both	the	""Linux	host	isadmin""	user	
and	in	the	/etc/iservices/is_	pass	file	that	is	mounted	into	the	PowerFlow	stack	as	a""Docker	secret"".	
Because	itis	mounted	as	asecret,	all	necessary	containers	are	aware	of	this	password	in	asecure	
manner.	For	more	information,	see	Changing	the	PowerFlow	Password	.	
7.	Type	the	password	for	the	root	user	again	and	press	[Enter	].The	PowerFlow	installer	runs,	and	the	system	
reboots	automatically.Â This	process	will	take	afew	minutes.	
8.	After	the	installation	scripts	run	and	the	system	reboots,	SSHÂ into	your	system	using	PuTTY	or	asimilar	
application.	The	default	username	for	the	system	is	isadmin	.	
30 

31	
9.	To	start	the	Docker	services,	change	directory	to	run	the	following	commands:	
cd	/opt/iservices/scripts	
./pull_	start_	iservices.sh	
NOTE:	This	process	will	take	afew	minutes	to	complete.	
10.	To	validate	that	iservices	is	running,	run	the	following	command	to	view	each	service	and	the	service	
versions	for	services	throughout	the	whole	stack:	
docker	service	ls	
11.	Navigate	to	the	PowerFlow	user	interface	using	your	browser.	The	address	of	the	PowerFlow	user	interface	is:	
https://	<IP	address	entered	during	installation>	
12.	Log	in	with	the	default	username	of	isadmin	and	the	password	you	specified	in	step	6.	
13.	After	installation,	you	must	license	yourÂ 	PowerFlow	system	ifyou	want	to	enable	all	of	the	features.	For	more	
information,	see	Licensing	PowerFlow	.	
14.	Ifyou	are	setting	up	High	Availability	for	the	PowerFlow	on	amultiple-	node	cluster,	see	Preparing	the	
PowerFlow	System	for	High	Availability	.	
NOTE:	The	HOST_	ADDRESS	value	in	the	/etc/iservices/isconfig.yml	file	should	be	the	fully	
qualified	domain	nameÂ 	(FQDN)	of	either	the	host	ifthere	is	no	load	balancer,	or	the	FQDN	of	
the	load	balancer	ifone	exists.	Ifyou	change	the	HOST_	ADDRESS	value,	you	will	need	to	
restart	the	PowerFlow	stack.	
T r o u b l e s h o o t i n g	t h e	I S O Â I n s t a l l a t i o n	
To	verify	that	your	stack	is	deployed,	view	your	Couchbase	logs	by	executing	the	following	command:	
docker	service	logs	--follow	iservices_	couchbase	
Ifno	services	are	found	to	be	running,	run	the	following	command	to	start	them:	
docker	stack	deploy	-c	docker-	compose.yml	iservices	
Installing	PowerFlow 

Installing	PowerFlow	
To	add	or	remove	additional	workers,	run	the	following	command:Â 	
docker	service	scale	iservices_	steprunner=10	
I n s t a l l i n g	P ow e r F l ow	v i a	R P M	
NOTE:	The	HOST_	ADDRESS	value	in	the	/etc/iservices/isconfig.yml	file	should	be	the	fully	qualified	
domain	nameÂ 	(FQDN)	of	either	the	host	ifthere	is	no	load	balancer,	or	the	FQDN	of	the	load	
balancer	ifone	exists.	Ifyou	change	the	HOST_	ADDRESS	value,	you	will	need	to	restart	the	
PowerFlow	stack.	
The	following	procedure	describes	how	to	install	PowerFlow	via	RPM	to	Amazon	Web	Service	(AWS)	EC2.	You	
can	also	install	PowerFlow	on	other	cloud-	based	environments,	such	as	Microsoft	Azure.	For	other	cloud-	based	
deployments,	the	process	is	essentially	the	same	as	the	following	steps:	PowerFlow	provides	the	containers,	and	
the	cloud-	based	environment	provides	the	operating	system	and	server.	
NOTE:	Ifyou	install	the	PowerFlow	system	in	acloud-	based	environment	using	amethod	other	than	an	ISO	
install,	you	are	responsible	for	setting	up	and	configuring	the	requirements	of	the	cloud-	based	
environment.	
You	can	install	PowerFlow	on	any	Oracle	Linux	7	or	later	operating	system,	even	in	the	cloud,	as	long	as	you	meet	
all	of	the	operating	system	requirements.	These	requirements	include	CPU,	memory,	Docker	and	adocker-	
compose	file	installed,	and	open	firewall	settings.	When	these	requirements	are	met,	you	can	install	the	RPM	and	
begin	to	deploy	the	stack	as	usual.	
NOTE:	Ifyou	install	the	PowerFlow	system	on	any	operating	system	other	than	Oracle	Linux	7,	ScienceLogic	
will	only	support	the	running	application	and	associated	containers.	ScienceLogic	will	not	assist	with	
issues	related	to	host	configuration	for	operating	systems	other	than	Oracle	Linux	7.	
To	install	asingle-	node	PowerFlow	via	RPM	to	acloud-	based	environmentÂ 	(using	AWSÂ as	an	example):	
1.	In	Amazon	Web	Service	(AWS)	EC2,	click	[Launch	instance	].The	Choose	an	Amazon	Machine	Image	
(AMI)	page	appears.	
TIP:	Ifyou	are	installing	PowerFlow	to	another	cloud-	based	environment,	such	as	Microsoft	Azure,	set	
up	the	operating	system	and	server,	and	then	go	to	step	7.	
2.	Deploy	anew	Oracle	Linux	7.6	virtual	machine	by	searching	for	OL7.6-	x86_	64-	HVM	in	the	Search	for	an	
AMI	field.	
3.	Click	the	results	link	for	Community	AMIs.	
32 

33	
4.	Click	[Select	]for	avirtual	machine	running	Oracle	Linux	7.6	or	greater	for	installation.Â The	following	image	
shows	an	example	of	an	OL7.6-	x86_	64-	HVM-	*AMI	file:	
5.	From	the	Choose	an	Instance	Type	page,	select	at	least	at2.xlarge	AMI	instance,	depending	on	your	
configuration:	
l	Single-	node	deployments	.The	minimum	is	t2.xlarge	(four	CPUs	with	16	GB	memory),	and	
ScienceLogic	recommends	t2.2xlarge	(8	CPUs	with	32	GB	memory).	
l	Cluster	deployments	.Cluster	deployments	depend	on	the	type	of	node	you	are	deploying.	Refer	to	the	
separate	multi-	tenant	environment	guide	for	more	sizing	information.Â ScienceLogic	recommends	that	
you	allocate	at	least	50	GB	or	more	for	storage.	
6.	Go	to	the	Step	6:	Configure	Security	Group	page	and	define	the	security	group:	
l	Only	inbound	port	443	needs	to	be	exposed	to	any	of	the	systems	that	you	intend	to	integrate.	
l	For	PowerFlow	version	1.8.2	and	later,	port	8091	is	exposed	through	https	.ScienceLogic	
recommends	that	you	make	port	8091	available	externally	to	help	with	troubleshooting:	
Installing	PowerFlow 

Installing	PowerFlow	
7.	Upload	the	sl1-	powerflow-	2.x.x-	1.x86_	64.rpm	file	to	the	server	using	SFTP	or	SCP.	
8.	Ensure	that	the	latest	required	packages	are	installed	by	running	the	following	commands	on	the	server	
instance:	
sudo	yum	install	-y	wget	
pip	install	docker-	compose	
wget	https://download.docker.com/linux/centos/7/x86_	64/stable/Packages/docker-	ce-	
19.03.13-	3.el7.x86_	64.rpm	
&&	sudo	yum	install	docker-	ce-	19.03.13-	3.el7.x86_	64.rpm	
NOTE:	You	will	need	to	update	both	instances	of	the	Docker	version	in	this	command	ifthere	is	amore	
recent	version	of	Docker	CE	on	the	Docker	Download	page:	
https://download.docker.com/linux/centos/7/x86_	64/stable/Packages/	.	
WARNING:	You	might	need	to	remove	spaces	from	the	code	that	you	copy	and	paste	from	this	
manual.	For	example,	in	instances	such	as	the	wget	command,	above,	line	breaks	were	
added	to	long	lines	of	code	to	ensure	proper	pagination	in	the	document.	
9.	Create	the	Docker	group:	
sudo	groupadd	docker	
10.	Add	your	user	to	the	Docker	group:	
sudo	usermod	-aG	docker	$USER	
11.	Log	out	and	log	back	in	to	ensure	that	your	group	membership	is	re-	evaluated.	
12.	Run	the	following	commands	for	the	configuration	updates:	
setenforce	0	
vim	/etc/sysconfig/selinux	
SELINUX=permissive
sudo	systemctl	enable	docker	
sudo	systemctl	start	docker	
sudo	yum	install	yum-	utils	
sudo	yum-	config-	manager	âenable	ol7_	addons	ol7_	optional_	latest	ol7_	latest	
sudo	yum-	config-	manager	âdisable	ol7_	ociyum_	config	
wget	http://dl.fedoraproject.org/pub/epel/epel-	release-	latest-	7.noarch.rpm	
rpm	-Uvh	epel-	release-	7*.rpm	
sudo	yum	update	
sudo	pip	install	docker-	compose	
sudo	yum	install	firewalld	
systemctl	enable	firewalld	
systemctl	start	firewalld	
systemctl	disable	iptables	
34 

35
13.	Run	the	following	firewall	commands:	
firewall-	cmd	--add-	port=2376/tcp	--permanent	
firewall-	cmd	--add-	port=2377/tcp	--permanent	
firewall-	cmd	--add-	port=7946/tcp	--permanent	
firewall-	cmd	--add-	port=7946/udp	--permanent	
firewall-	cmd	--add-	port=4789/udp	--permanent	
firewall-	cmd	--add-	protocol=esp	--permanent	
firewall-	cmd	-âreload	
TIP:	To	view	alist	of	all	ports,	run	the	following	command:	firewall-	cmd	--list-	all	
14.	Copy	the	PowerFlow	RPM	to	the	instance	of	installation	and	install	the	RPM:	
sudo	yum	install	sl1-	powerflow	
systemctl	restart	docker	
15.	Create	apassword	for	PowerFlow	:	
sudo	printf	<password>	>	/etc/iservices/is_	pass	
where	<password>	is	anew,	secure	password.	
16.	Pull	and	start	iservices	to	start	PowerFlow	:	
/opt/iservices/scripts/pull_	start_	iservices.sh	
NOTE:	For	an	AWS	deployment,	ScienceLogic	recommends	that	you	switch	to	an	Amazon	EC2	user	as	soon	
as	possible	instead	of	running	all	the	commands	on	root.	
NOTE:	For	aclustered	PowerFlow	environment,	you	must	install	the	PowerFlow	RPM	on	every	server	that	you	
plan	to	cluster	into	PowerFlow	.You	can	load	the	Docker	images	for	the	services	onto	each	server	
locally	by	running	/opt/iservices/scripts/pull_	start_	iservices.sh	.Installing	the	RPM	
onto	each	server	ensures	that	the	PowerFlow	containers	and	necessary	data	are	available	on	all	
servers	in	the	cluster.	
NOTE:	After	installation,	you	must	license	yourÂ 	PowerFlow	system	to	enable	all	of	the	features.	Licensing	is	
required	for	production	systems	only,	not	for	test	systems.	For	more	information,	see	Licensing	
PowerFlow	.	
T r ou b l e s h oot i n g	a	C l ou d	D e p l oy m e n t	of	P ow e r F l ow	
After	completing	the	AWS	setup	instructions,	ifnone	of	the	services	start	and	you	see	the	following	error	during	
troubleshooting,	the	problem	is	that	you	need	to	restart	Docker	after	installing	the	RPM	installation.	
Installing	PowerFlow 

Upgrading	PowerFlow	
sudo	docker	service	ps	iservices_	couchbase	--no-	trunc	
""error	creating	external	connectivity	network:	Failed	to	Setup	IP	tables:	Unable	to	
enable	SKIP	DNAT	rule:	(iptables	failed:	iptables	--wait	-t	nat	-I	DOCKER	-i	docker_	
gwbridge	-j	RETURN:	iptables:	No	chain/target/match	by	that	name.""	
U p g r a d i n g	P o w e r F l o w	
Please	note	that	upgrading	to	version	2.2.0	will	involve	some	downtime	of	PowerFlow	.Before	upgrading	your	
version	of	PowerFlow	,ScienceLogic	recommends	that	you	make	abackup	of	your	PowerFlow	system.	For	more	
information,	see	Backing	up	Data	.	
You	can	upgrade	PowerFlow	from	any	1.8.x	version	or	later	to	the	2.2.0	version.	Select	the	relevant	procedure	
below	based	on	your	upgrade	path:	
l	Upgrading	to	Version	2.2.0	from	Version	2.0.x	
l	Upgrading	to	Version	2.2.0	from	Version	1.8.x	with	the	Upgrade	Script	
l	Manually	Upgrading	to	Version	2.2.0	from	Version	1.8.x	
WARNING:	Ifyou	made	any	customizations	to	default	applications	or	steps	that	shipped	with	previous	
versions	of	PowerFlow	,you	will	need	to	make	those	customizations	compatible	with	Python	3.6	
or	later	before	upgrading	to	version	2.0.0	or	later	of	PowerFlow	.	
WARNING:	Ifyou	made	any	modifications	to	the	nginx	configuration	or	to	other	service	configuration	files	
outside	of	the	docker-	compose.yml	file,	you	will	need	to	modify	or	back	up	those	custom	
configurations	before	upgrading,	or	contact	ScienceLogicÂ Support	to	prevent	the	loss	of	those	
modifications.	
U p g r a d i n g	t o	V e r s i on	2 . 2 . 0	f r om	V e r s i on	2 . 0 . x	
To	upgrade	to	PowerFlow	version	2.2.0	from	version	2.0.x:	
1.	Download	the	PowerFlow	2.2.0	RPM	and	copy	the	RPM	file	to	the	PowerFlow	system.	
2.	Either	go	to	the	console	of	the	PowerFlow	system	or	use	SSH	to	access	the	server.	
3.	Log	in	as	isadmin	with	the	appropriate	(root)	password.	You	must	be	root	to	upgrade	using	the	RPM	file.	
36 

37	
4.	Type	the	following	at	the	command	line:	
rpm	âUvh	full_	path_	of_	rpm	
where:
full_	path_	of_	rpm	is	the	name	and	path	of	the	RPM	file,	such	as	sl1-	powerflow-	2.2.0-	1.x86_	64	.	
NOTE	:Â If	you	are	running	PowerFlow	in	aclustered	environment,	install	the	RPM	on	all	nodes	in	the	
cluster	before	continuing	with	the	remaining	steps.	
5.	Ifthe	upgrade	process	recommends	restarting	Docker,	run	the	following	command:	
systemctl	restart	docker	
NOTE:	Ifyou	want	to	upgrade	your	services	in	place,	without	bringing	them	down,	you	may	skip	this	
step.	Please	note	that	skipping	this	step	might	take	the	services	slightly	longer	to	update.	
6.	After	the	RPM	is	installed,	run	the	following	Docker	command:	
docker	stack	rm	iservices	
7.	Re-	deploy	the	Docker	stack	to	update	the	containers:	
docker	stack	deploy	-c	/opt/iservices/scripts/docker-	compose.yml	iservices	
8.	After	you	re-	deploy	the	Docker	stack,	the	services	automatically	update	themselves.	Wait	afew	minutes	to	
ensure	that	all	services	are	updated	and	running	before	using	the	system.	You	can	use	the	visualizer	at	port	
8080	to	monitor	the	progress	of	the	updates.	
9.	To	view	updates	to	each	service	and	the	service	versions	for	services	throughout	the	whole	stack,	type	the	
following	at	the	command	line:	
docker	service	ls	
You	will	notice	that	each	service	now	uses	the	new	version	of	PowerFlow	.	
Upgrading	PowerFlow 

Upgrading	PowerFlow	
U p g r a d i n g	t o	V e r s i on	2 . 2 . 0	f r om	V e r s i on	1 . 8 . x	w i t h	t h e	U p g r a d e	
S c r i p t
The	process	for	upgrading	to	version	2.2.0	of	PowerFlow	from	version	1.8.x	includes	the	following	required	steps:	
1.	Upgrade	the	host	packages	and	Python	3.6	(previous	versions	of	PowerFlow	used	Python	2.6).	
2.	Upgrade	to	Oracle	7.3	or	later.	
3.	Upgrade	to	Docker	version	18.09.2	or	later.	
NOTE:	PowerFlow	version	2.0.0	or	later	requires	the	docker-	ce	18.09.2	or	later	version	of	Docker.	
The	PowerFlow	ISO	installs	the	docker-	ce	19.03.13	version	of	Docker	by	default,	but	ifyou	
are	upgrading	to	this	version	from	the	RPM,	you	must	upgrade	Docker	before	you	upgrade	
PowerFlow	with	the	RPM.	
4.	Install	the	PowerFlow	upgrade	RPM.	
5.	Update	the	PowerFlow	system	from	Basic	Authentication	to	OAuth	2.0.	For	more	information,	see	
Configuring	Authentication	with	the	PowerFlow	.	
6.	Set	up	licensing	for	PowerFlow	.You	must	license	yourÂ 	PowerFlow	system	to	enable	all	of	the	features.	Ifyou	
are	not	deploying	PowerFlow	on	aproduction	or	pre-	production	environment,	you	can	skip	licensing.	For	
more	information,	see	Licensing	theÂ 	PowerFlow	.	
NOTE:	Ifyou	are	upgrading	from	aversion	before	1.8.3,	be	sure	to	review	the	release	notes	for	the	older	
version	for	any	relevant	update	considerations	before	upgrading.	For	example,	there	is	asmall	port	
change	that	you	might	need	to	apply	ifyou	are	upgrading	acustomized	cluster	from	aversion	of	
PowerFlow	before	version	1.8.3.	
You	will	need	to	run	the	is_	upgrade_	to_	v2.sh	script	to	perform	the	upgrade	steps	automatically.	The	script	
upgrades	the	PowerFlow	system	from	1.8.x	to	2.0.0	or	later.	
To	locate	the	upgrade	script:	
1.	Go	to	the	ScienceLogic	Support	site	at	https://support.sciencelogic.com/s/	.	
2.	Click	the	ProductÂ Downloads	tab	and	select	PowerFlow	.The	PowerFlow	Release	page	appears.	
3.	Click	the	""PowerFlow	2.0""	link.	The	PowerFlow	2.0	Release	Version	page	appears.	
4.	In	the	Release	Files	section,	click	the	""1.8.X	to	2.	x.x	Upgrade""	link	for	the	script.	A	Release	File	page	
appears.	
5.	ClickÂ 	[Download	File	]at	the	bottom	of	the	Release	File	page.	The	is_	upgrade_	to_	v2.sh	script	is	in	the	is_	
upgrade_	tools.zip	file.	
38 

39
The	upgrade	script	runs	the	following	steps:	
1.	Checks	to	see	ifthe	Oracle	version	is	greater	or	equal	to	7.3.	Ifnot,	the	script	stops	and	displays	amessage	
that	you	need	to	update	to	Oracle	7.3	or	later.	
2.	Sets	the	requirements	location	and	either	mounts	the	ISO	or	verifies	that	the	RPM	exists.	For	this	step,	the	
script	asks	ifthe	installation	will	be	offline	or	online,	and	italso	asks	you	for	the	location	of	the	RPM.	
3.	Installs	Python	3.6.	
4.	Installs	Docker	19.03.5.	
5.	Installs	the	PowerFlow	RPM.	
6.	Runs	the	pull_	start_	iservices.sh	script	to	deploy	and	initialize	PowerFlow	.	
7.	Ifthe	upgrading	process	was	offline,	cleans	the	changes	made	for	the	upgrading	process,	unmounts	the	ISO,	
and	removes	the	localiso.repo	file.	
To	run	the	upgrade	script:	
1.	Download	the	is_	upgrade_	to_	v2.sh	script	and	add	itto	adirectory	on	the	PowerFlow	system	.	
2.	Download	the	sl1-	powerflow-	2.	x.x	-1.x86_	64.rpm	file	or	the	ISO	file	and	add	itto	adirectory	on	the	
PowerFlow	system	.Make	anote	of	this	directory,	because	you	will	need	itfor	Step	2	in	the	script.	
TIP:	Alternately,	instead	of	downloading	the	RPM	file,	you	can	specify	an	online	location	for	the	RPM	
file.	
3.	Ifneeded,	run	the	following	command	on	the	PowerFlow	system	to	give	the	script	execution	permissions:	
sudo	chmod	+x	is_	upgrade_	to_	v2.sh	
NOTE:	Ifyou	are	installing	aversion	of	PowerFlow	later	than	2.0.0,	you	will	need	to	update	the	
version	number	in	the	command,	above.	
4.	Change	directory	to	the	directory	containing	the	is_	upgrade_	to_	v2.sh	script,	such	as	/home/isadmin/	,	
and	then	run	the	following	command	to	execute	the	upgrade	script:	
sudo	./is_	upgrade_	to_	v2.sh	
NOTE:	Ifyou	are	installing	aversion	of	PowerFlow	later	than	2.0.0,	you	will	need	to	update	the	
version	number	in	the	command,	above.	
5.	For	Step	2	of	the	script,	you	will	need	to	specify	ifyou	want	to	run	the	upgrade	online,	or	run	itoffline.	Type	""1""	
ifyou	have	Internet	access,	or	""2""	ifyou	want	to	run	the	update	offline.	
Upgrading	PowerFlow 

Upgrading	PowerFlow	
6.	For	Step	2	of	the	script,	you	will	need	to	specify	the	location	of	the	RPM	or	ISO	file	for	2.0.	x.You	can	use	a	
location	on	the	PowerFlow	system	for	the	RPMÂ or	ISO	file,	or	an	online	location	for	the	RPM.	For	example:	
/home/isadmin/sl1-	powerflow-	2.2.0-	1.x86_	64.rpm	
7.	After	the	upgrade	script	completes,	perfor	m	the	following	steps	to	verify	the	upgrade:	
l	Review	the	docker-	compose.yml	file	and	ensure	that	all	environmental	changes	are	in	place.	
l	Ifthe	docker-	compose.yml	file	is	ready	to	be	deployed,	you	can	re-	deploy	the	PowerFlow	stack.	
l	After	the	PowerFlow	stack	is	up	and	running,	run	the	healthcheck	Â action	with	the	pfctl	command-	line	
utility	to	verify	that	you	had	ahealthy	deployment.	
l	Ifneeded,	run	the	healthcheck	Â action	with	the	pfctl	utility	to	automatically	fix	any	remaining	
inconsistencies	after	the	upgrade.	
TIP:	For	more	information	about	the	iservicecontrol	utility,	see	Using	the	pfctl	Command-	line	
Utility	.	
8.	To	view	updates	to	each	service	and	the	service	versions	for	services	throughout	the	whole	stack,	type	the	
following	at	the	command	line:	
docker	service	ls	
9.	As	needed,	update	the	PowerFlow	system	from	Basic	Authentication	to	OAuth	2.0.	For	more	information,	
see	Configuring	Authentication	with	PowerFlow	.	
10.	As	needed,	set	up	licensing	for	the	PowerFlow	.For	more	information,	see	Licensing	PowerFlow	.	
NOTE:	Ifyou	are	upgrading	aclustered	PowerFlow	environment	from	1.8.x	to	2.0.0	or	later,	see	Updating	
Cluster	Settings	when	Upgrading	from	1.8.x	to	2.0.0	or	Later	.	
M a n u a l l y	U p g r a d i n g	t o	V e r s i on	2 . 2 . 0	f r om	V e r s i on	1 . 8 . x	
Instead	of	running	the	upgrade	script,	you	can	manually	upgrade	to	2.2.0	from	1.8.x,	following	the	detailed	
instructions	below.Â 	
NOTE:	Ifyou	are	upgrading	from	aversion	before	1.8.3,	be	sure	to	review	the	release	notes	for	the	older	
version	for	any	relevant	update	considerations	before	upgrading.	For	example,	there	is	asmall	port	
change	that	you	might	need	to	apply	ifyou	are	upgrading	acustomized	cluster	from	aversion	of	
PowerFlow	before	version	1.8.3.	
S t e p	1 .	U p g r a d i n g	H o s t	P a c k a g e s	a n d	P y t h o n	3 . 6	
To	access	the	host	packages	online:	
40 

41	
1.	To	make	sure	that	all	repositories	can	access	the	required	host-	level	packages,	enable	the	necessary	
repositories	by	running	the	following	commands	on	the	PowerFlow	system	:	
yum	install	yum-	utils	
yum-	config-	manager	--enable	ol7_	latest	
yum-	config-	manager	--enable	ol7_	optional_	latest	
2.	Run	the	following	commands	to	update	and	install	the	host-	level	packages,	and	to	upgrade	to	Python	3.6:	
yum	remove	python34-	pip	python34-	setuptools	python3	
yum	--setopt=obsoletes=0	install	python36-	pip	python36	python36-	setuptools	
python36-	devel	openssl-	devel	gcc	make	kernel	
yum	update	
3.	Continue	the	upgrade	process	by	upgrading	to	Oracle	7.3	or	later.	
Ifyou	need	to	upgrade	the	host	packages	offline,	without	Internet	access,	you	can	mount	the	latest	PowerFlow	ISO	
file	onto	the	system	and	create	ayum	repository	configuration	that	points	to	the	local	mount	point	in	
/etc/yum.repos.d	.After	the	ISOÂ is	mounted,	you	can	import	the	latest	GNU	Privacy	Guard	(GPG)	key	used	by	
the	repository.	
To	access	the	host	packages	offline:	
1.	Mount	the	PowerFlow	ISO	onto	the	system:	
mount	-o	loop	/dev/cdrom	/mnt/tmpISMount	
2.	After	you	mount	the	ISO,	add	anew	repository	file	to	access	the	ISO	as	ifitwere	ayum	repository.	Create	a	
/etc/yum.repos.d/localiso.repo	file	with	the	following	contents:	
[localISISOMount]
name=Locally	mounted	IS	ISO	for	packages	
enabled=1
baseurl=file:///mnt/tmpISMount
gpgcheck=0
After	you	create	and	save	this	file,	theÂ Linux	system	can	install	packages	from	the	PowerFlow	ISO.	
3.	Optionally,	you	can	import	the	latest	GNU	Privacy	Guard	(GPG)	key	to	verify	the	packages	by	running	the	
following	command:	
rpm	--import	/mnt/repo_	keys/RPM-	GPG-	KEY-	Oracle	
rpm	--import	/mnt/tmpISMount/repo_	keys/RPM-	GPG-	KEY-	Docker-	ce	
rpm	--import	/mnt/tmpISMount/repo_	keys/RPM-	GPG-	KEY-	EPEL-	7	
Upgrading	PowerFlow 

Upgrading	PowerFlow	
4.	Ifyou	cannot	install	Docker	or	Python	offline,	delete	the	other	repository	references	by	running	the	following	
command	(ScienceLogic	recommends	that	you	back	up	those	file	first):	
rm	-rf	/etc/yum.repos.d/epel.repo	/etc/yum.repos.d/epel-	testing.repo	
/etc/yum.repos.d/public-	yum-	ol7.repo	
5.	Run	the	following	commands	to	update	and	install	the	host-	level	packages,	and	to	upgrade	to	Python	3.6:	
yum	remove	python34-	pip	python34-	setuptools	python3	
yum	--setopt=obsoletes=0	install	python36-	pip	python36	python36-	setuptools	
python36-	devel	openssl-	devel	gcc	make	kernel	
yum	update	
6.	Continue	the	upgrade	process	by	upgrading	to	Oracle	7.3	or	later.	
S t e p	2 .	U p g r a d i n g	t o	O r a c l e	7 . 3	o r	L a t e r	
ScienceLogic	recommends	that	you	update	the	version	of	Oracle	Linux	running	on	the	PowerFlow	to	64-	bit	
version	7.3	or	later.	
NOTE:	Ifyou	want	to	upgrade	to	Oracle	Linux	to	7.6,	see	https://docs.oracle.com/en/operating-	
systems/oracle-	linux/7/relnotes7.6/	.	
To	upgrade	to	Oracle	7.3	or	later:	
1.	To	check	the	current	version	of	Oracle	Linux	on	your	PowerFlow	system,	run	the	following	command	on	the	
PowerFlow	system:	
cat	/etc/oracle-	release	
2.	To	install	Oracle,	choose	one	of	the	following	procedures:	
l	Using	apublic	Oracle	Linux	repository,	run	the	yum	update	command.	
l	By	mounting	the	latest	PowerFlow	ISO	to	the	system	and	installing	the	latest	packages	from	the	ISO.	
3.	Continue	the	upgrade	process	by	updating	Docker.	
S t e p	3 .	U p g r a d i n g	t o	D o c k e r	1 8 . 0 9 . 2	o r	l a t e r	
PowerFlow	systems	before	version	2.0.0	included	Docker	18.06.	Ifyou	are	running	aversion	of	the	PowerFlow	
before	version	2.0.0,	you	will	need	to	update	Docker	18.09.2	or	later	to	be	able	to	upgrade	toÂ 	PowerFlow	version	
2.0.0	or	later.	For	more	information	about	the	security	updates	included	in	Docker	18.09.2,	see	
https://cve.mitre.org/cgi-	bin/cvename.cgi?name=CVE-	2019-	5736	.	
Before	upgrading	Docker,	ScienceLogic	recommends	that	you	review	the	following	information	from	the	Docker	
product	manual:	https://docs.docker.com/ee/upgrade/	.	
42 

43	
NOTE:	Run	the	following	process	on	DockerÂ Swarm	node,	starting	with	the	manager	nodes.	
WARNING:	For	clustered	configurations,	see	the	information	in	Installing	Docker	in	Clustered	
Configurations	before	running	the	upgrade	steps	below.	
To	upgrade	to	Docker	18.09.2	or	later:	
1.	Review	the	steps	in	the	Docker	product	manual:	https://docs.docker.com/ee/docker-	ee/oracle/#install-	
with-	a-	package	.	
2.	To	install	docker-	io,	run	the	following	command	on	the	PowerFlow	instance:	
sudo	yum	install	-y	https://download.docker.com/linux/centos/7/x86_	
64/stable/Packages/containerd.io-	1.3.7-	3.1.el7.x86_	64.rpm	
https://download.docker.com/linux/centos/7/x86_	64/stable/Packages/docker-	ce-	
19.03.13-	3.el7.x86_	64.rpm	
https://download.docker.com/linux/centos/7/x86_	64/stable/Packages/docker-	ce-	cli-	
19.03.13-	3.el7.x86_	64.rpm	
NOTE:	You	will	need	to	update	the	Docker	versions	in	this	command	ifthere	are	more	recent	versions	
on	the	Docker	Download	page:	https://download.docker.com/linux/centos/7/x86_	
64/stable/Packages/	.	
WARNING:	You	might	need	to	remove	spaces	from	the	code	that	you	copy	and	paste	from	this	
manual.	For	example,	in	instances	such	as	the	yum	command,	above,	line	breaks	were	
added	to	long	lines	of	code	to	ensure	proper	pagination	in	the	document.	
3.	To	install	docker-	ce	offline	using	the	IS	2.0.	xISO,	run	the	following	commands:	
rpm	-e	--nodeps	docker-	ce	
yum	install	-y	/mnt/tmpISMount/Packages/containerd.io-	1.3.7-	3.1.el7.x86_	64.rpm	
yum	install	-y	/mnt/tmpISMount/Packages/docker-	ce-	cli-	19.03.13-	3.el7.x86_	64.rpm	
yum	install	-y	/mnt/tmpISMount/Packages/docker-	ce-	19.03.13-	3.el7.x86_	64.rpm	
systemctl	enable	docker	
systemctl	start	docker	
NOTE:	Ifthe	node	is	amember	of	acluster,	wait	for	afew	minutes,	and	the	node	will	automatically	rejoin	the	
swarm	cluster,	and	re-	deploy	the	services	running	on	that	node.	Wait	until	all	services	are	
operational,	then	proceed	to	upgrade	the	next	node.	For	more	information,	see	Installing	Docker	in	
Clustered	Configurations.	
3.	Continue	the	upgrade	process	by	installing	the	PowerFlow	RPM	.	
Upgrading	PowerFlow 

Upgrading	PowerFlow	
I n s t a l l i n g	D o c k e r	i n	C l u s t e r e d	C o n f i g u r a t i o n s	
Follow	the	best	practices	for	upgrading	acluster	described	in	the	Docker	product	manual:	
https://docs.docker.com/ee/upgrade/#cluster-	upgrade-	best-	practices	.	
NOTE:	You	should	upgrade	all	manager	nodes	before	upgrading	worker	nodes.	Upgrading	manager	nodes	
sequentially	is	recommended	iflive	workloads	are	running	in	the	cluster	during	the	upgrade.	After	you	
upgrade	the	manager	nodes,	you	should	upgrade	worker	nodes,	and	then	the	Swarm	cluster	
upgrade	is	complete.	
Docker	recommends	that	you	drain	manager	nodes	of	any	services	running	on	those	nodes	Ifalive	migration	is	
expected,	all	workloads	must	be	running	on	swarm	workers,	not	swarm	managers,	or	the	manager	under	upgrade	
needs	to	be	completely	drained.	
Also,	anew	Python	Package	Index	(PyPI)	service	was	added	to	the	PowerFlow	stack.	When	deploying	PowerFlow	
in	acluster	setup,	and	not	using	network-	aware	volumes,	the	PyPI	server	must	be	""pinned""	to	aspecific	node	with	
constraints.	Pinning	the	PyPI	server	to	asingle	node	ensures	that	its	persistent	volume	containing	the	
Synchronization	PowerPacks	will	always	be	available	to	PowerFlow	.	
S t e p	4 .	I n s t a l l i n g	t h e	P o w e r F l o w	R P M	
To	update	PowerFlow	using	the	RPM:	
1.	Download	the	PowerFlow	RPM	and	copy	the	RPM	file	to	the	PowerFlow	system.	
2.	Log	in	as	isadmin	with	the	appropriate	(root)	password.	You	must	be	root	to	upgrade	the	RPM	file.	
3.	Type	the	following	at	the	command	line:	
rpm	âUvh	full_	path_	of_	rpm	
where	full_	path_	of_	rpm	is	the	full	path	and	name	of	the	RPM	file.	
4.	Run	the	pull_	start_	iservices.sh	script	to	deploy	and	initialize	2.	x.x	:	
/opt/iservices/scripts/pull_	start_	iservices.sh	
WARNING:	Do	not	run	the	pull_	start_	iservices.sh	script	ifyou	are	using	PowerFlow	in	aclustered	
environment.	
5.	To	view	updates	to	each	service	and	the	service	versions	for	services	throughout	the	whole	stack,	type	the	
following	at	the	command	line:	
docker	service	ls	
6.	Verify	that	each	service	now	uses	the	new	version	of	PowerFlow	.	
44 

45	
7.	As	needed,	update	the	PowerFlow	system	from	Basic	Authentication	to	OAuth	2.0.	For	more	information,	
see	Configuring	Authentication	with	PowerFlow	.	
8.	As	needed,	set	up	licensing	for	PowerFlow	.For	more	information,	see	Licensing	PowerFlow	.	
NOTE:	Ifyou	are	upgrading	aclustered	PowerFlow	environment	from	1.8.x	to	2.0.0	or	later,	see	Updating	
Cluster	Settings	when	Upgrading	from	1.8.x	to	2.0.0	or	Later	.	
U p d a t i n g	C l u s t e r	S e t t i n g s	w h e n	U p g r a d i n g	f r om	1 . 8 . x	t o	2 . 0 . 0	or	
L a t e r
After	upgrading	PowerFlow	cluster	nodes	from	1.8.x	to	2.0.0	or	later,	you	need	to	verify	the	following	details	in	the	
docker-	compose	file:	
1.	Verify	that	the	pypiserver	is	pinned	to	the	node	that	contains	the	Synchronization	PowerPacks	:	
node.hostname	==	<node	hostname	from	docker	node	ls>	.	
2.	Verify	that	the	dexserver	is	replicated	three	times	to	keep	the	pypiserver	service	from	moving	from	one	node	
to	another	and	so	you	no	longer	have	the	persisted	Synchronization	PowerPack	storage.	
You	can	use	the	healthcheck	action	with	the	pfctl	command-	line	utility	to	validate	the	docker-	compose	file.	The	
healthcheck	action	will	show	amessage	ifthe	pypiserver	or	dexserver	services	are	not	well-	configured	in	the	
docker-	compose	file.	You	can	fix	these	issues	manually,	or	you	can	fix	them	using	the	autoheal	action	with	the	
pfctl	utility.	The	utility	corrects	the	docker-	compose	file	and	copies	itto	all	the	nodes	in	the	cluster	environment.	
NOTE:	When	using	the	version	of	the	pfctl	command-	line	utility	that	comes	with	PowerFlow	version	2.1.0	or	
later,	the	autocluster	action	validates	and	fixes	the	pypiserver	and	dexserver	services	definitions	in	
the	docker-	compose	file.	
For	more	information,	see	Using	the	pfctl	Command-	line	Utility	.	
T r ou b l e s h oot i n g	U p g r a d e	I s s u e s	
The	following	topics	describe	issues	that	might	occur	after	the	upgrade	to	version	2.0.0	or	later,	and	how	to	address	
those	issues.	
T o	r o l l	b a c k	t o	a	v e r s i o n	b e f o r e	P o w e r F l o w	2 . 0 . 0	o r	l a t e r	
After	aschedule	is	accessed	or	modified	on	the	2.0.0	or	later	PowerFlow	API	or	scheduler,	that	schedule	will	not	
be	accessible	again	in	a1.x	version.	Ifyou	upgrade	to	2.0.0	from	1.x	and	you	want	to	go	back	to	the	1.x	release,	
you	must	delete	the	schedule	and	recreate	the	schedule	in	1.x	(if	the	schedule	was	modified	in	2.0.0).	
Upgrading	PowerFlow 

Licensing	PowerFlow	
C a n n o t	a c c e s s	P o w e r F l o w	o r	a n	I n t e r n a l Â S e r v e r	E r r o r	o c c u r s	
PowerFlow	version	2.0.0	and	later	use	anew	type	of	authentication	session.	This	change	might	cause	problems	if	
your	browser	attempts	to	load	the	PowerFlow	user	interface	using	a""stale""	cache	from	version	1.8.4.	Ifyou	have	
issues	accessing	the	user	interface,	or	ifyou	see	an	""Internal	server	error""	message	when	you	log	in,	be	sure	to	
clear	the	local	cache	of	your	browser.	
A f t e r	u p g r a d i n g ,	t h e	s y n c p a c k _	s t e p r u n n e r	f a i l s	t o	r u n	
This	error	flow	tends	to	happen	when	the	syncpack_	steprunner	is	deployed,	but	the	database	is	not	yet	updated	
with	the	indexes	necessary	for	the	Synchronization	PowerPack	processes	to	query	the	database.	In	most	
deployments,	the	index	should	be	automatically	created.	Ifthe	index	is	not	automatically	created,	which	itmight	do	
in	aclusterd	configuration,	you	can	resolve	this	issue	by	manually	creating	the	indexes.	
In	this	situation,	ifyou	check	the	logs,	you	will	most	likely	see	the	following	message:	
couchbase.exceptions.HTTPError:	<RC=0x3B	[HTTP	Operation	failed.	Inspect	status	code	
for	details],	HTTP	Request	failed.	Examine	'objextra'	for	full	result,	Results=1,	C	
Source=	(src/http.c,144),	OBJ=ViewResult<rc=0x3B	[HTTP	Operation	failed.	Inspect	status	
code	for	details],	value=	{'requestID':	'57ad959d-	bafb-	46a1-	9ede-	f80f692b0dd7',	
'errors':	[{'code':	4000,	'msg':	'No	index	available	on	keyspace	content	that	matches	
your	query.	Use	CREATE	INDEX	or	CREATE	PRIMARY	INDEX	to	create	an	index,	or	check	that	
your	expected	index	is	online.'}],	'status':	'fatal',	'metrics':	{'elapsedTime':	
'5.423085ms',	'executionTime':	'5.344487ms',	'resultCount':	0,	'resultSize':	0,	
'errorCount':	1}},	http_	status=404,	tracing_	context=0,	tracing_	output=None>,	Tracing	
Output=	{"":nokey:0"":	null}>	
To	address	this	issue,	wait	afew	minutes	for	the	index	to	be	populated.	Ifyou	are	still	getting	an	error	after	the	
database	has	been	running	for	afew	minutes,	you	can	manually	update	the	indexes	by	running	the	following	
command:
initialize_	couchbase	-s	
L i c e n s i n g	P o w e r F l o w	
Before	users	can	access	all	of	the	features	of	version	2.0.0	or	later	of	PowerFlow	,the	Administrator	user	must	
license	theÂ 	PowerFlow	instance	through	the	ScienceLogic	Support	site.	For	more	information	about	accessing	
PowerFlow	files	at	the	ScienceLogic	Support	site,	see	the	following	Knowledge	Base	article:	SL1	PowerFlow	
Download	and	Licensing	.	
NOTE:	The	administrator	and	all	users	cannot	access	certain	production-	level	capabilities	until	the	
administrator	licenses	the	instance.	For	example,	users	cannot	create	schedules	or	upload	PowerFlow	
applications	and	steps	that	are	not	part	of	aSynchronization	PowerPack	until	PowerFlow	has	been	
licensed.	
46 

47	
TIP:	Ifyou	are	not	deploying	PowerFlow	on	aproduction	or	pre-	production	environment,	you	can	skip	the	
licensing	process.	
NOTE:	Ifyou	are	licensing	aPowerFlow	High	Availability	cluster,	you	can	run	the	following	licensing	process	
on	any	node	in	the	cluster.	The	node	does	not	have	to	be	the	leader,	and	the	licensing	process	does	
not	have	to	be	run	on	all	nodes	in	the	Swarm.	
To	license	aPowerFlow	system:	
1.	Run	the	following	command	on	your	PowerFlow	system	to	generate	the	.iskey	license	file:	
iscli	--license	--customer	""<user_	name>	""	--email	<user_	email>	
where	<user_	name>	is	the	first	and	last	name	of	the	user,	and	<user_	email>	is	the	user's	email	address.	
For	example:	
iscli	--license	--customer	""John	Doe""	--email	jdoe@sciencelogic.com	
2.	Run	an	ls	command	to	locate	the	new	license	file:	customer_	key.iskey	.	
3.	Using	WinSCP	or	another	utility,	copy	the	.iskey	license	file	to	your	local	machine.	
4.	Go	to	theÂ 	PowerFlow	License	Request	page	at	the	ScienceLogic	Support	site:	
https://support.sciencelogic.com/s/integration-	service-	license-	request	.	
5.	Click	the	relevant	license	link.	The	Generate	License	File	page	appears:	
TIP:	You	already	covered	Step	1	of	the	""Generate	License	File""	process	in	steps	1-	3	of	this	procedure.	
Licensing	PowerFlow 

Licensing	PowerFlow	
6.	For	Step	2	of	the	""Generate	License	File""	process,	select	the	type	of	PowerFlow	you	are	using.	
7.	For	Step	3	of	the	""Generate	License	File""	process,	upload	the	.iskey	license	file	you	created	in	steps	1-	3	of	
this	procedure	and	click	[Upload	File	].	
8.	After	uploading	the	license	file,	click	[Generate	License	].A	new	Licensing	page	appears:	
9.	Click	the	.crt	file	in	the	Files	pane	to	download	the	new	.crt	license	file.	
10.	Using	WinSCP	or	another	file-	transfer	utility,	copy	the	.crt	license	file	to	your	PowerFlow	system.	
11.	Upload	the	.crt	license	file	to	the	PowerFlow	server	by	running	the	following	command	on	that	server:	
iscli	-l	-u	-f	./	<license_	name>	.crt	-H	<IP_	address>	-U	<user_	name>	-p	<user_	
password>
where	<license_	name>	is	the	system-	generated	name	for	the	.crt	file,	<IP_	address>	is	the	IP	address	of	
the	PowerFlow	system,	<user_	name>	is	the	user	name,	and	<user_	password>	is	the	user	password.	For	
example:
iscli	-l	-u	-f	./aCx0x000000CabNCAS.crt	-H	10.2.33.1	-U	isadmin	-p	passw0rd	
NOTE:	ScienceLogic	determines	the	duration	of	the	license	key,	not	the	customer.	
TIP:	Ifyou	have	any	issues	licensing	your	PowerFlow	system	,please	contact	your	ScienceLogic	Customer	
Success	Manager	(CSM)	or	open	anew	Service	Request	case	under	the	""Integration	Service""	category.	
48 

49
C o n f i g u r i n g	A d d i ti o n a l	E l e m e n ts	o f	P o w e r F l o w	
Ifyou	have	multiple	workers	running	on	the	same	PowerFlow	system	,you	might	want	to	limit	the	amount	of	
memory	allocated	for	each	worker.	This	helps	prevent	memory	leaks,	and	also	prevents	one	worker	using	too	
many	resources	and	starving	other	workers.	You	can	apply	these	limits	in	two	ways:	
l	Set	ahard	memory	limit	in	Docker	(this	is	the	default)	
l	Set	asoft	memory	limit	in	the	worker	environment	
S e t t i n g	a	H a r d	M e m or y	L i m i t	i n	D oc k e r	
Setting	amemory	limit	for	the	worker	containers	in	your	docker-	compose.yml	file	sets	ahard	limit.	Ifyou	set	a	
memory	limit	for	the	workers	in	the	docker-	compose	file	and	aworker	exceeds	the	limit,	the	container	is	
terminated	via	SIGKILL.	
Ifthe	currently	running	task	caused	memory	usage	to	go	above	the	limit,	that	task	might	not	be	completed,	and	the	
worker	container	is	terminated	in	favor	of	anew	worker.	This	setting	helps	to	prevent	aworker	from	endlessly	
running	and	consuming	all	memory	on	the	PowerFlow	system.	
You	can	configure	the	hard	memory	limit	in	the	steprunner	service	of	the	docker-	compose.yml	file:	
deploy:	
resources:	
limits:	
memory:	2G	
S e t t i n g	a	S of t	M e m or y	L i m i t	i n	t h e	W or k e r	E n v i r on m e n t	
You	can	set	the	memory	limit	for	aworker	application,	and	not	at	the	Docker	level.	Setting	the	memory	limit	at	the	
application	level	differs	from	the	hard	memory	limit	in	Docker	in	that	ifaworker	exceeds	the	specified	memory	
limit,	that	worker	is	not	immediately	terminated	via	SIGKILL.	
Instead,	ifaworker	exceeds	the	soft	memory	limit,	the	worker	waits	until	the	currently	running	task	is	completed	to	
recycle	itself	and	start	anew	process.	As	aresult,	tasks	will	complete	ifaworker	crosses	the	memory	limit,	but	ifa	
task	is	running	infinitely	with	amemory	leak,	that	task	might	consume	all	memory	on	the	host.	
NOTE:	The	soft	memory	limit	is	less	safe	from	memory	leaks	than	the	hard	memory	limit.	
You	can	configure	the	soft	memory	limit	with	the	worker	environment	variables.	The	value	is	in	KiB	(1024	bytes).	
Also,	each	worker	instance	contains	three	processes	for	running	tasks.	The	memory	limit	applies	to	each	individual	
instance,	and	not	the	container	as	awhole.	For	example,	a2	GB	memory	limit	for	the	container	would	translate	to	
2	GB	divided	by	three,	or	about	700	MB	for	each	worker:	
steprunner:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:1.8.1	
environment:	
additional_	worker_	args:	'	--max-	memory-	per-	child	700000'	
Configuring	Additional	Elements	of	PowerFlow 

Changing	the	PowerFlow	System	Password	
C h a n g i n g	th e	P o w e r F l o w	S y s te m	P a s s w o r d	
The	PowerFlow	system	uses	two	primary	passwords.	For	consistency,	both	passwords	are	the	same	after	you	install	
PowerFlow	,but	you	can	change	them	to	separate	passwords	as	needed.	
PowerFlow	uses	the	following	passwords:	
l	The	PowerFlow	Administrator	(isadmin	)user	password	.This	is	the	password	that	you	set	during	the	
PowerFlow	ISOÂ installation	process	,and	itis	only	used	by	the	default	local	Administrator	user	(isadmin	).	
You	use	this	password	to	log	into	the	PowerFlow	user	interface	and	to	verify	API	requests	and	database	
actions.	This	password	is	set	as	both	the	""Linux	host	isadmin""	user	and	in	the	/etc/iservices/is_	pass	file	that	
is	mounted	into	the	PowerFlow	stack	as	a""Docker	secret"".	Because	itis	mounted	as	asecret,	all	necessary	
containers	are	aware	of	this	password	in	asecure	manner.	Alternatively,	you	can	enable	third-	party	
authentication,	such	as	LDAP	or	AD,	and	authenticate	with	credentials	other	than	isadmin.	However,	you	will	
need	to	set	the	user	policies	for	those	LDAP	users	first	with	the	default	isadmin	user.	For	more	information,	
see	Managing	Users	in	PowerFlow	.	
l	The	Linux	Host	OS	SSH	password	.This	is	the	password	you	use	to	SSH	and	to	log	in	to	isadmin.	You	can	
change	this	password	using	the	standard	Linux	passwd	command	or	another	credential	management	
application	to	manage	this	user.	You	can	also	disable	this	Linux	user	and	add	your	own	user	ifyou	want.	The	
PowerFlow	containers	and	applications	do	not	use	or	know	this	Linux	login	password,	and	this	password	does	
not	need	to	be	the	same	between	nodes	in	acluster.	This	is	astandard	Linux	Host	OS	password.	
To	change	the	PowerFlow	Administrator	(isadmin	)user	password:	
1.	You	can	change	the	mounted	isadmin	password	secret	(which	is	used	to	authenticate	via	API	by	default)	and	
the	Couchbase	credentials	on	the	PowerFlow	stack	by	running	the	ispasswd	script	on	any	node	running	
PowerFlow	in	the	stack:	
/opt/iservices/scripts/ispasswd	
2.	Follow	the	prompts	to	reset	the	password.	The	password	must	be	at	least	six	characters	and	no	more	than	24	
characters,	and	all	special	characters	are	supported.	
NOTE:	Running	the	ispasswd	script	automatically	changes	the	password	for	all	PowerFlow	
application	actions	that	require	credentials	for	the	isadmin	user.	
3.	Ifyou	have	multiple	nodes,	copy	/etc/iservices/is_	pass	file,	which	was	just	updated	by	the	ispasswd	script,	
to	all	other	manager	nodes	in	the	cluster.	You	need	to	copy	this	password	file	across	all	nodes	in	case	you	
deploy	from	adifferent	node	than	the	node	where	you	changed	the	password.	The	need	to	manually	copy	the	
password	to	all	nodes	will	be	removed	in	afuture	release	of	PowerFlow	.	
NOTE:	IfaPowerFlow	user	makes	multiple	incorrect	login	attempts,	PowerFlow	locks	out	the	user.	To	unlock	
the	user,	run	the	following	command:	unlock_	user	-u	<username>	
50 

51
C o n f i g u r i n g	a	P r o xy	S e r v e r	
To	configure	PowerFlow	to	use	aproxy	server:	
1.	Either	go	to	the	console	of	the	PowerFlow	system	or	use	SSH	to	access	the	PowerFlow	server.	
2.	Log	in	as	isadmin	with	the	appropriate	password.	
3.	Using	atext	editor	like	vi,	edit	the	file	/opt/iservices/scripts/docker-	compose-	override.yml	.	
NOTE:	PowerFlow	uses	adocker-	compose-	override.yml	file	to	persistently	store	user-	specific	
configurations	for	containers,	such	as	proxy	settings,	replica	settings,	additional	node	settings,	
and	deploy	constraints.	The	user-	specific	changes	are	kept	in	this	file	so	that	they	can	be	re-	
applied	when	the	/opt/iservices/docker-	compose.yml	file	is	completely	replaced	on	an	
RPM	upgrade,	ensuring	that	no	user-	specific	configurations	are	lost.	
4.	In	the	environment	section	of	the	steprunner	service,	add	the	following	lines:	
services:	
steprunner:	
environment:	
https_	proxy:Â ""<proxy_	host>""	
http_	proxy:	""<proxy_	host>""	
no_	proxy:	"".isnet""	
5.	In	the	environment	section	of	the	syncpack_	steprunner	service,	add	the	following	lines:	
services:	
syncpack_	steprunner:	
environment:	
https_	proxy:Â ""<proxy_	host>""	
http_	proxy:	""<proxy_	host>""	
no_	proxy:	"".isnet""	
6.	Save	the	settings	in	the	file	and	then	run	the	/opt/iservices/compose_	override.sh	script.	
NOTE:	The	compose_	override.sh	script	validates	that	the	configured	docker-	compose.yml	and	
docker-	compose-	override.yml	files	are	syntactically	correct.	Ifthe	settings	are	correct,	the	
script	applies	the	settings	to	your	existing	docker-	compose.yml	file	that	is	used	to	actually	
deploy.	
7.	Re-	deploy	the	steprunners	to	use	this	change	by	typing	the	following	commands:	
docker	service	rm	iservices_	steprunner	
docker	stack	deploy	-c	/opt/iservices/scripts/docker-	compose.yml	iservices	
Configuring	aProxy	Server 

Configuring	Security	Settings	
C o n f i g u r i n g	S e c u r i ty	S e tti n g s	
This	topic	explains	how	to	change	the	HTTPS	certificate	used	by	PowerFlow	,and	italso	describes	password	and	
encryption	key	security.	
C h a n g i n g	t h e	H T T P S	C e r t i f i c a t e	
The	PowerFlow	user	interface	only	accepts	communications	over	HTTPS.	By	default,	HTTPS	is	configured	using	an	
internal,	self-	signed	certificate.	
You	can	specify	the	HTTPS	certificate	to	use	in	your	environment	by	mounting	the	following	two	files	in	the	user	
interface	(gui)	container:	
l	/etc/iservices/is_	key.pem	
l	/etc/iservices/is_	cert.pem	
NOTE:	Ifyou	are	using	aclustered	configuration	for	PowerFlow	,you	will	need	to	copy	the	key	and	certificate	
to	the	same	location	on	the	node.	
To	specify	the	HTTPS	certificate	to	use	in	your	environment:	
1.	Copy	the	key	and	certificate	to	the	PowerFlow	host.	
2.	Modify	the	/opt/iservices/scripts/docker-	compose-	override.yml	file	and	mount	avolume	to	the	gui	
service.	The	following	code	is	an	example	of	the	volume	specification:	
volumes:	
-	""<path	to	IS	key>	:/etc/iservices/is_	key.pem""	
-	""<path	to	IS	certificate>	:/etc/iservices/is_	cert.pem""	
3.	Run	the	following	script	to	validate	and	apply	the	change:	
/opt/iservices/scripts/compose_	override.sh	
4.	Re-	deploy	the	gui	service	by	running	the	following	commands:	
docker	service	rm	iservices_	gui	
/opt/iservices/scripts/pull_	start_	iservices.sh	
U s i n g	P a s s w or d	a n d	E n c r y p t i on	K e y	S e c u r i t y	
When	you	install	the	PowerFlow	platform,	you	specified	the	PowerFlow	root	password.	This	root	password	is	also	
the	default	isadmin	password:	
l	The	root/admin	password	is	saved	in	aroot	read-	only	file	here:	/etc/iservices/is_	pass	
l	A	backup	password	file	is	also	saved	in	aroot	read-	only	file	here:	/opt/iservices/backup/is_	pass	
52 

53
The	user-	created	root	password	is	also	the	default	PowerFlow	password	for	couchbase	(:8091)	and	all	API	
communications.	The	PowerFlow	platform	generates	aunique	encryption	key	for	every	platform	installation:	
l	The	encryption	key	exists	in	aroot	read-	only	file	here:	/etc/iservices/encryption_	key	
l	A	backup	encryption	key	file	is	also	saved	in	aroot	read-	only	file	here:	/opt/iservices/backup/encryption_	
key	
You	can	use	the	encryption	key	to	encrypt	all	internal	passwords	and	user-	specified	data.	You	can	encrypt	any	
value	in	aconfiguration	by	specifying	""encrypted"":	true,	when	you	POST	that	configuration	setting	to	the	API.	
There	is	also	an	option	in	the	PowerFlow	user	interface	to	select	encrypted	.Encrypted	values	use	the	same	
randomly-	generated	encryption	key.	
User-	created	passwords	and	encryption	keys	are	securely	exposed	in	the	Docker	containers	using	Docker	secrets	at	
https://docs.docker.com/engine/swarm/secrets/	to	ensure	secure	handling	of	information	between	containers.	
NOTE	:The	encryption	key	must	be	identical	between	two	PowerFlow	systems	ifyou	plan	to	migrate	from	one	
to	another.Â The	encryption	key	must	be	identical	between	High	Availability	or	Disaster	Recovery	
systems	as	well.	
TIP:	PowerFlow	supports	all	special	characters	in	passwords.	
P o w e r F l o w	M a n a g e m e n t	E n d p o i n ts	
This	section	provides	technical	details	about	managing	PowerFlow	.The	following	information	is	also	available	in	
the	PowerPacks	in	Using	SL1	to	Monitor	SL1	PowerFlow	.	
F l ow e r	A P I	
Celery	Flower	is	aweb-	based	tool	for	monitoring	PowerFlow	tasks	and	workers.	Flower	lets	you	see	task	progress,	
details,	and	worker	status:	
PowerFlow	Management	Endpoints 

PowerFlow	Management	Endpoints	
The	following	Flower	API	endpoints	return	data	about	the	Flower	tasks,	queues,	and	workers.	The	tasks	endpoint	
returns	data	about	task	status,	runtime,	exceptions,	and	application	names.	You	can	filter	this	endpoint	to	retrieve	a	
subset	of	information,	and	you	can	combine	filters	to	return	amore	specific	data	set.	
/flower/api/tasks	.Retrieve	alist	of	all	tasks.	
/flower/api/tasks?app_	id=	{app_	id}	.Retrieve	alist	of	tasks	filtered	by	app_	id.	
/flower/api/tasks?app_	name=	{app_	name}	.Retrieve	alist	of	tasks	filtered	by	app_	name.	
/flower/api/tasks?started_	start=1539808543&started_	end=1539808544	.Retrieve	alist	of	all	tasks	
received	within	atime	range.	
/flower/api/tasks?state=FAILURE|SUCCESS	.Retrieve	alist	of	tasks	filtered	by	state.	
/flower/api/workers	.Retrieve	alist	of	all	queues	and	workers	
To	view	this	information	in	the	Flower	user	interface	navigate	to	<hostname_	of_	PowerFlow	_system>	/flower	.	
For	more	information,	see	the	Flower	API	Reference	.	
NOTE:	Ifyou	use	the	ScienceLogic:	PowerFlow	PowerPack	to	collect	this	task	information,	the	PowerPack	will	
create	events	in	SL1	ifaFlower	task	fails.	For	more	information,	seeÂ 	Using	SL1	to	Monitor	
PowerFlow	.	
C ou c h b a s e	A P I	
The	Couchbase	Server	is	an	open-	source	database	software	that	can	be	used	for	building	scalable,	interactive,	
and	high-	performance	applications.	Built	using	NoSQL	technology,	Couchbase	Server	can	be	used	in	either	a	
standalone	or	cluster	configuration.	
The	following	image	shows	the	CouchBase	user	interface,	which	you	can	access	at	port	8091:	
54 

55
The	following	Couchbase	API	endpoints	return	data	about	the	Couchbase	service.	The	pools	endpoint	represents	
the	Couchbase	cluster.	In	the	case	of	PowerFlow	,each	node	is	aDocker	service,	and	buckets	represent	the	
document-	based	data	containers.	These	endpoints	return	configuration	and	statistical	data	about	each	of	their	
corresponding	Couchbase	components.	
<hostname_	of_	PowerFlow	_system>	:8091/pools/default	.Retrieve	alist	of	pools	and	nodes.	
<hostname_	of_	PowerFlow	_system>	:8091/pools/default/buckets	.Retrieve	alist	of	buckets.	
To	view	this	information	in	the	Couchbase	AdministratorÂ user	interface,	navigate	to	<hostname_	of_	PowerFlow	_	
system>	:8091	.	
For	more	information,	see	the	Couchbase	API	Reference	.	
NOTE:	You	can	also	use	the	Couchbase	PowerPack	to	collect	this	information.	For	more	information,	
seeÂ 	Using	SL1	to	Monitor	PowerFlow	.	
R a b b i t M Q
RabbitMQ	is	an	open-	source	message-	broker	software	that	originally	implemented	the	Advanced	Message	
Queuing	Protocol	and	has	since	been	extended	with	aplug-	in	architecture	to	support	Streaming	Text	Oriented	
Messaging	Protocol,	Message	Queuing	Telemetry	Transport,	and	other	protocols.Â 	
The	following	image	shows	the	RabbitMQ	user	interface,	which	you	can	access	at	port	15672:	
PowerFlow	Management	Endpoints 

PowerFlow	Management	Endpoints	
D oc k e r	S w a r m	V i s u a l i z e r	
Docker	Swarm	Visualizer	is	avisualizer	for	Docker	Swarm	using	the	Docker	Remote	API,	Node.JS,	and	D3.	Each	
node	in	the	swarm	shows	all	tasks	running	on	it.	When	aservice	goes	down,	the	service	is	be	removed.	When	a	
node	goes	down	itwill	not	be	removed;	instead	the	circle	icon	in	Visualizer	turns	red	to	indicate	itwent	down.	
The	following	image	shows	the	Visualizer	user	interface,	which	you	can	access	at	port	8081:	
D oc k e r	S t a t i s t i c s	
You	can	collect	Docker	information	by	using	SSH	to	connect	to	the	Docker	socket.	You	cannot	currently	retrieve	
Docker	information	by	using	the	API.	
56 

57
To	collect	Docker	statistics:	
1.	Use	SSH	to	connect	to	the	PowerFlow	instance.	
2.	Run	the	following	command:	
curl	--unix-	socket	/var/run/docker.sock	http://docker	<PATH>	
where	<PATH>	is	one	of	the	following	values:	
l	/info	
l	/containers/json	
l	/images/json	
l	/swarm	
l	/nodes	
l	/tasks	
l	/services	
NOTE:	You	can	also	use	the	Docker	PowerPack	to	collect	this	information.	For	more	information,	seeÂ 	Using	
SL1	to	Monitor	PowerFlow	.	
PowerFlow	Management	Endpoints 

Chapte r	
3	
ManagingÂ Users	in	SL1	PowerFlow	
O v e r v i e w
This	chapter	describes	how	to	configure	authentication	for	PowerFlow	to	allow	access	to	multiple	users	with	a	
variety	of	roles	and	permissions.	
This	chapter	also	describes	how	to	use	the	Admin	Panel	pageÂ 	(	)to	manage	user	group	access	to	the	
PowerFlow	user	interface.Â Only	users	with	the	Administrator	role	for	the	PowerFlow	system	can	edit	this	page.	
This	chapter	covers	the	following	topics:	
Configuring	Authentication	with	PowerFlow	59	
Role-	based	Access	Control	(RBAC)	Configuration	63	
Configuring	Authentication	Settings	in	PowerFlow	64	
UserÂ Groups,	Roles,	andÂ Permissions	65	
Creating	a	UserÂ Group	in	PowerFlow	66
58 

59
C o n f i g u r i n g	A u th e n ti c a ti o n	w i th	P o w e r F l o w	
SL1	PowerFlow	version	2.0.0	or	later	supports	the	following	authentication	methods:	
l	Local	Authentication	.The	same	local	Administrator	user	(isadmin	)is	supported	by	default	with	2.0.0	or	
later	installations.	Ifyou	are	migrating	from	aprevious	version	of	PowerFlow	to	version	2.0.0	or	later,	you	can	
log	in	and	authenticate	with	the	same	user	and	password.	
l	Basic	Authentication	.PowerFlow	2.0.0	or	later	continues	to	support	Basic	Authentication	as	well.	Because	
the	PowerFlow	PowerPacks	,diagnostic	scripts,	and	the	iscli	tool	continue	to	use	Basic	Authentication,	
ScienceLogic	does	not	recommend	disabling	Basic	Authentication	with	PowerFlow	version	2.0.0	or	later.	
l	OAuth	.Â Lets	PowerFlow	administrators	use	their	own	authentication	providers	to	enforce	user	authentication	
and	lockout	policies.	Authentication	using	athird-	party	provider,	such	as	LDAP	or	Active	Directory,	requires	
additional	configuration.	For	optimal	security,	ScienceLogic	recommends	that	you	disable	the	local	
Administrator	user	(isadmin	)and	exclusively	use	your	own	authentication	provider.	
Regardless	of	authentication	strategy,	authorization	and	role	access	is	configured	separately,	based	on	user	or	user	
group.Â For	more	information,	see	Creating	a	UserÂ Group	in	PowerFlow	.	
The	following	topics	describe	how	to	configure	each	authentication	strategy	for	PowerFlow	:	
l	User	Interface	LoginÂ Administrator	User	(default)	
l	Basic	Authentication	Using	a	REST	Administrator	User	(default)	
l	User	Interface	Login	Using	a	Third-	party	Authentication	Provider	
l	OAuth	Client	Authentication	Using	a	Third-	party	Provider	
U s e r	I n t e r f a c e	L og i n	A d m i n i s t r a t or	U s e r	( D e f a u l t )	
The	local	Administrator	user	is	the	default	login	user	for	PowerFlow	.The	username	is	""isadmin""	by	default,	and	the	
password	gets	set	during	the	ISO	installation	process.	The	PowerFlow	administrator	can	change	the	default	
password	or	the	Administrator	user.	
This	authentication	strategy	allows	authentication	of	the	local	Administrator	user	through	the	PowerFlow	user	
interface.	The	""isadmin""	user	is	the	local	Administrator	by	default,	but	you	can	change	the	username	of	the	local	
Administrator	to	something	other	than	""isadmin""	ifneeded.	
WARNING:	Ifyou	disable	this	authentication	strategy,	you	must	first	configure	an	alternative	provider	to	
appropriately	authenticate	to	the	PowerFlow	system	and	also	configure	auser	or	user	group	
policy	that	has	the	Administrator	role.	Ifyou	disable	this	user	without	asecond	authentication	
provider	configured,	PowerFlow	will	not	be	able	to	authenticate	any	users.	
To	disable	this	user,	set	the	following	environment	variable	in	the	/etc/iservices/isconfig.yml	file:	
BASIC_	AUTH:	False	
Configuring	Authentication	with	PowerFlow 

Configuring	Authentication	with	PowerFlow	
To	change	the	local	Administrator	username,	set	the	following	environment	variable	in	the	
/etc/iservices/isconfig.yml	file:	
BASIC_	AUTH_	USER:	""username""	
NOTE:	Before	changing	the	local	Administrator	username,	make	sure	that	the	user	has	Administrator	
permissions	in	the	PowerFlow	system.	For	more	information,	see	UserÂ Groups,	Roles,	
andÂ Permissions	.	
ScienceLogic	recommends	that	you	use	the	same	system-	wide	password	for	the	local	Administrator	user,	which	is	
located	in	/etc/iservices/is_	pass	.To	change	this	password,	run	the	/opt/iservices/scripts/ispasswd	script.	
Alternatively,	you	can	set	adifferent	password	for	the	local	Administrator	user	by	setting	the	following	environment	
variable	in	/etc/iservices/isconfig.yml	:	
BASIC_	AUTH_	PASSWORD:	""BASIC_	AUTH_	PASSWORD""	
B a s i c	A u t h e n t i c a t i on	U s i n g	a	R E S T	A d m i n i s t r a t or	U s e r	( D e f a u l t )	
Basic	Authentication	through	aREST	administrator	user	enables	the	local	Administrator	user	to	access	the	
PowerFlow	API	through	REST	using	Basic	Authentication.	This	authentication	strategy	enables	users	to	make	
queries	through	the	API	using	<isadmin:password>	basic	authentication.	
This	Basic	Authentication	is	enabled	by	default	in	PowerFlow	2.0.0	or	later.	The	REST	administrator	uses	the	same	
environment	variables	and	configuration	as	the	user	interface	login	Administrator	user	.	
Basic	Authentication	is	limited	to	only	the	local	Administrator	user.	Ifyour	PowerFlow	system	is	configured	to	use	a	
different	authentication	provider,	you	must	authenticate	using	OAuth	2.0	and	abearer	token.	For	more	
information,	see	OAuth	Client	Authentication	Using	an	Internal	Provider	.	
WARNING:	This	Basic	Authentication	user	is	enabled	by	default	to	support	backward	compatibility	with	any	
scripts	or	queries	running	against	the	versions	of	PowerFlow	before	2.0.0.	Ifyou	disable	this	
authentication	strategy,	PowerPacks	and	end-	user	scripts	will	not	be	able	to	query	or	access	the	
API	unless	they	are	updated	to	use	OAuth	2.0	with	bearer	atoken.	
U s e r	I n t e r f a c e	L og i n	U s i n g	a	T h i r d -	p a r t y	A u t h e n t i c a t i on	P r ov i d e r	
This	authentication	strategy	lets	you	set	up	user	authentication	through	athird-	party	authentication	provider,	such	
as	LDAP	or	ActiveÂ Directory	(AD).	You	configure	additional	providers	and	their	connectors	in	the	
/etc/iservices/isconfig.yml	file,	following	the	Dex	connector	configuration	described	below.	This	authentication	
strategy	is	automatically	disabled	when	no	connectors	are	present	in	the	configuration.	
After	you	configure	the	providers,	users	can	select	one	of	the	defined	providers	or	the	local	Administrator	
authentication,	and	authenticate	using	that	provider.	
60 

61
When	using	this	authentication	strategy,	the	PowerFlow	system	automatically	retrieves	the	LDAPÂ or	AD	groups	to	
which	the	user	belongs.	You	can	use	these	groups	can	be	used	to	apply	specific	role	permissions.	For	more	
information,	see	Role-	based	Access	Control	(RBAC)	Configuration	.	
NOTE:	By	default,	no	third-	party	authentication	providers	are	configured	in	PowerFlow	.	
Credentials	for	user	authentication	exist	only	with	the	third-	party	authentication	provider,	and	the	credentials	are	
not	imported	into	PowerFlow	.The	only	information	that	PowerFlow	retains	for	these	users	are	the	roles	and	
permissions	attached	to	the	user	names.	
To	configure	athird-	party	authentication	provider:	
1.	Go	to	https://github.com/dexidp/dex#connectors	and	locate	the	connector	type,	such	as	LDAP	or	SAML	2.0,	
that	you	would	like	to	use	for	authentication.	
2.	Click	the	link	for	the	connector	type	to	view	example	configuration	options	for	the	connector.	
3.	Update	the	/etc/iservices/isconfig.yml	file	and	add	aDEX_	CONNECTORS	section	with	the	configuration	
for	the	connector	you	want	to	use.	The	DEX_	CONNECTORS	section	in	the	isconfig.yml	file	is	identical	to	
the	CONNECTORS	section	described	in	the	Dex	documentation.	
4.	Re-	deploy	the	PowerFlow	stack	and	check	for	errors	in	docker	service	logs	iservices_	dexserver	.Ifthe	
dexserver	starts	successfully	and	PowerFlow	is	running,	then	PowerFlow	has	accepted	the	configuration.	
5.	Next,	log	in	to	the	PowerFlow	system	with	auser	provided	by	the	newly	configured	authentication.	
6.	Look	for	errors	with	searching	users	or	user	groups	in	the	user	interface	or	the	Docker	service	logs.	
For	reference,	the	following	is	an	example	/etc/iservices/isconfig.yml	file	with	an	Active	Directory	authentication	
provider	configured	to	look	up	users	and	their	groups:	
HOST_	ADDRESS:	10.2.11.222	
CLIENT_	ID:	isproxy	
CLIENT_	SECRET:	knivq7uDPVORdrSlWJ0I4YiiwuQgGDsf9rMWquoInYs	
SESSION_	SECRET:	BDLO2xPrBs_	s-	YkqY-	j4lN6VPeBzyrVsYt_	P10oWbn0	
DB_	HOST:	couchbase.isnet,localhost	
DEX_	CONNECTORS:	
-	type:	ldap	
#	Required	field	for	connector	id.	
name:	ldap	
id:	ldap	
#	Required	field	for	connector	name.	
config:	
host:	rstcsdc01.sciencelogic.local:636	
#	Host	and	optional	port	of	the	LDAP	server	in	the	form	""host:port"".	
#	If	the	port	is	not	supplied,	it	will	be	guessed	based	on	""insecureNoSSL"",	
#	and	""startTLS""	flags.	389	for	insecure	or	StartTLS	connections,	636	
#	otherwise.	
insecureNoSSL:	false	
#	The	insecureNoSSL	field	is	required	if	the	LDAP	host	is	not	using	TLS	(port	389).	
#	Because	this	option	inherently	leaks	passwords	to	anyone	on	the	same	network	
#	as	dex,	THIS	OPTION	MAY	BE	REMOVED	WITHOUT	WARNING	IN	A	FUTURE	RELEASE.	
Configuring	Authentication	with	PowerFlow 

Configuring	Authentication	with	PowerFlow	
insecureSkipVerify:	true	
#	If	a	custom	certificate	isn't	provide,	this	option	can	be	used	to	turn	on	
#	TLS	certificate	checks.	As	noted,	it	is	insecure	and	shouldn't	be	used	outside	
#	of	explorative	phases.	
bindDN:	auser	
bindPW:	password	
#	The	DN	and	password	for	an	application	service	account.	The	connector	uses	
#	these	credentials	to	search	for	users	and	groups.	Not	required	if	the	LDAP	
#	server	provides	access	for	anonymous	auth.	
#	Please	note	that	if	the	bind	password	contains	a	`$`,	it	has	to	be	saved	in	an	
#	environment	variable	which	should	be	given	as	the	value	to	the	bindPW	field.	
usernamePrompt:	silo	credentials	
#	The	attribute	to	display	in	the	provided	password	prompt.	If	unset,	will	
#	display	""Username""	
userSearch:	
#	User	search	maps	a	username	and	password	entered	by	a	user	to	a	LDAP	entry.	
baseDN:	OU=Domain	User	Accounts,DC=ScienceLogic,DC=local	
#	BaseDN	to	start	the	search	from.	
filter:	""(objectClass=user)""	
#	Optional	filter	to	apply	when	searching	the	directory.	
username:	userPrincipalName	
#	username	attribute	used	for	comparing	user	entries.	
#	The	following	three	fields	are	direct	mappings	of	attributes	on	the	user	entry.	
idAttr:	DN	
#	String	representation	of	the	user.	
emailAttr:	userPrincipalName	
#	Required.	Attribute	to	map	to	email.	
nameAttr:	cn	
#	The	nameAttr	field	maps	to	the	display	name	of	users.	No	default	value.	
groupSearch:	
#	Group	search	queries	for	groups	given	a	user	entry.	Group	searches	must	match	a	
user	
#	attribute	to	a	group	attribute.	
baseDN:	OU=Domain	Groups,DC=ScienceLogic,DC=local	
#	BaseDN	to	start	the	search	from.	
filter:	""(objectClass=group)""	
#	Optional	filter	to	apply	when	searching	the	directory.	
#	The	following	list	contains	field	pairs	that	are	used	to	match	a	user	to	a	group.	
#	It	adds	an	additional	requirement	to	the	filter	that	an	attribute	in	the	group	
#	must	match	the	user's	attribute	value.	
userAttr:	DN	
#	The	userAttr	field	is	the	attribute	used	from	the	user	search	query.	
#	to	match	to	an	element	of	the	group	search.	
groupAttr:	member	
#	The	groupAttr	field	is	the	attribute	of	the	group	results	that	should	match	the	
userAttr	value.
nameAttr:	cn	
#	This	value	must	exactly	match	the	group	defined	in	the	IS	Group	configuration.	
62 

63
O A u t h	C l i e n t	A u t h e n t i c a t i on	U s i n g	a	T h i r d -	p a r t y	P r ov i d e r	
OAuth	Client	Authentication	provides	user	authentication	with	aconfigured	third-	party	provider,	such	as	LDAPÂ or	
AD.	This	allows	users	to	use	clients	authenticating	with	OAuth	2.0	bearer	tokens	to	authenticate	against	their	
configured	provider.	
In	PowerFlow	2.0.0	or	later,	the	OAuth	Client	Authentication	strategy	is	automatically	enabled	when	a	
authentication	provider	is	configured.Â You	can	configure	this	strategy	using	the	same	parameters	as	the	User	
Interface	Login	Using	a	Third-	party	Authentication	Provider	.	
The	PowerFlow	system	provides	discovery	endpoints	for	all	OAuth	2.0	required	endpoints.	The	discovery	address	
is:	https://IS-	IP:5556/dex/.well-	known/openid-	configuration	.	
Using	these	discovery	endpoints,	you	can	use	an	OAuth	2.0	client	to	generate	asecure	token	using	athird-	party	
authentication	provider.	You	can	use	the	generated	secure	token	as	abearer	authentication	token	(specified	in	
request	headers)	to	authenticate	and	make	requests.	
The	client_	secret	and	session_	secret	are	unique,	randomly	generated	strings	generated	for	each	IS	deployment.	
To	obtain	an	OAuth	2.0	token,	you	will	need	these	values,	which	you	can	find	in	the	/etc/iservices/isconfig.yml	
file.
B a s i c	A u t h e n t i c a t i on	L oc k ou t	R e m ov a l	
PowerFlow	2.0.0	or	later	supports	OAuth	2.0	with	OpenID	Connect,	which	lets	PowerFlow	administrators	use	
third-	party	authentication	providers,	such	as	LDAP	or	Active	Directory,	to	enforce	user	authentication	and	lockout	
policies.
PowerFlow	continues	to	support	Basic	Authentication	as	well,	but	PowerFlow	no	longer	enforces	automatically	
locking	out	the	isadmin	user	ifthat	user	has	too	many	failed	login	attempts.	
Ifyou	are	concerned	about	removing	the	lockout	functionality	for	the	isadmin	user,	you	can	perform	one	of	the	
following	actions:	
1.	Change	the	default	username	from	isadmin	to	adifferent	name	to	prevent	brute	force	type	attacks	on	the	
isadmin	user.	
2.	Disable	Basic	Authentication	and	use	third-	party	authentication	providers,	such	as	your	company's	LDAP	
server,	to	enforce	user	authentication	and	lockout	policies.	
NOTE:	Before	disabling	Basic	Authentication,	make	sure	that	all	scripts	or	tools	that	query	the	PowerFlow	API	
have	been	updated	to	use	OAuth	2.0.	
R o l e -	b a s e d	A c c e s s	C o n tr o l	( R B A C )	C o n f i g u r a ti o n	
Regardless	of	the	authentication	method	you	have	chosen	to	use,	the	role-	based	permissions	assigned	to	users	is	
applied	in	the	same	way.	
Role-	based	Access	Control	(RBAC)	Configuration 

Configuring	Authentication	Settings	in	PowerFlow	
A s s i g n i n g	a	R o l e	t o	a	S p e c i f i c	U s e r	
By	applying	arole	permission	to	asingle	username	in	the	admin	panel,	or	through	the	API,	permissions	will	be	
granted	to	any	user	who	logs	in	through	aprovider	matching	that	username.	
A s s i g n i n g	R o l e s	t o	a	S p e c i f i c	U s e r	G r o u p	
By	applying	arole	permission	to	agroup	name	in	the	admin	panel,	or	through	the	API,	permissions	will	be	granted	
to	any	user	who	logs	in	and	is	amember	of	the	specified	group.	
For	authentication	to	work	properly,	group_	search	must	be	configured	in	the	authentication	provider's	connector	
information.	When	requesting	authentication	tokens,	be	sure	to	also	request	the	groups	claim.	
NOTE:	Ifauser	belongs	to	multiple	groups,	with	varying	permissions	defined	to	each	group,	the	user	will	be	
permitted	to	do	all	actions	provided	by	the	group	that	provides	the	most	roles.	
V i e w i n g	U s e r	a n d	G r ou p	I n f or m a t i on	
After	configuring	your	third-	party	authentication	connector	in	Dex,	you	can	run	the	following	command	to	view	the	
search	strategy	and	group	results	for	any	user	that	attempts	to	authenticate	by	looking	at	the	Dexserver	logs:	
docker	service	logs	-f	iservices_	dex	
C h a n g i n g	R o l e s	a n d	P e r m i s s i o n s	
To	change	roles	and	permissions	through	the	PowerFlow	user	interface,	go	to	the	Admin	Panel	pageÂ 	(	)to	
create	and	edit	the	roles	and	permissions	of	the	user	groups.	For	more	information,	see	Creating	a	UserÂ Group	
in	PowerFlow	.	
To	change	roles	and	permissions	through	the	API,	refer	to	the	swagger.yml	file	for	API	required	parameters	and	
endpoints	to	update	roles	and	permissions.	
C o n f i g u r i n g	A u th e n ti c a ti o n	S e tti n g s	i n	P o w e r F l o w	
The	following	configuration	settings	can	be	configured	and	used	with	the	PowerFlow	authentication	and	
authorization	strategies:	
l	DEX_	CONNECTORS	.This	environment	variable	specifies	which	authentication	providers	Dex	will	use.	For	
more	information,	see	User	Interface	Login	Using	a	Third-	party	Authentication	Provider	.	
l	BASIC_	AUTH_	USER	.This	environment	variable	specifies	the	basic_	auth	and	admin	login	username.	For	
more	information,	see	User	Interface	Login	Administrator	User	.	
l	BASIC_	AUTH_	PASSWORD	.This	environment	variable	specifies	the	basic_	auth	and	admin	login	
password.	For	more	information,	see	User	Interface	Login	Administrator	User	.	
64 

65	
l	BASIC_	AUTH	.This	environment	variable	specifies	whether	the	local	Administrator	user	will	be	enabled.	For	
more	information,	see	User	Interface	Login	Administrator	User	.	
l	CLIENT_	ID	.The	IS	client_	id	in	use	by	default	is	isproxy	,and	you	should	not	change	this	value.	
l	CLIENT_	SECRET	.The	client_	secret	is	arandomly	generated	string	unique	to	each	PowerFlow	deployment.	
In	most	configurations,	you	do	not	need	to	change	this	secret.	
U s e r Â Gr o u p s ,	R o l e s ,	a n d Â P e r m i s s i o n s	
On	the	Admin	Panel	pageÂ 	(	),	you	can	edit	and	create	user	groups	that	define	the	different	roles	and	
permissions	for	your	users.	Depending	on	their	assigned	permissions,	users	have	access	to	certain	features,	or	they	
are	blocked	from	certain	features.	
The	available	roles	and	permissions	for	PowerFlow	include	the	following:	
l	View	.The	user	can	view	and	get	via	the	API	the	list	of	applications,	the	list	of	installed	Synchronization	
PowerPacks	,the	dashboards,	the	reports,	the	configuration	objects,	and	the	results	of	application	runs.	
l	Execute	.The	user	has	all	of	the	privileges	of	the	View	permission,	but	the	user	can	also	trigger	application	
runs	through	the	user	interface	or	the	API.	
l	Configuration	.The	user	has	all	of	the	privileges	of	the	Execute	permission,	but	the	user	can	also	add	and	
edit	configuration	objects	and	modify	the	application	variables.	
l	Developer	.The	user	has	all	of	the	privileges	of	the	Configure	permission,	but	the	user	can	also	add,	copy,	
and	edit	step	definitions;	add,	copy,	and	edit	application	definitions;	and	create	Synchronization	PowerPacks	.	
l	Administrator	.The	user	has	all	of	the	privileges	of	the	Develop	permission,	but	the	user	can	also	add,	
install,	activate,	and	deleteÂ 	Synchronization	PowerPacks	;delete	applications,	steps,	and	configuration	
objects;	and	access	to	(but	not	authorization	to)	the	user	interfaces	for	Couchbase,	Flower,	andÂ RabbitMQ.	
Additional	information	about	roles	and	permissions	in	PowerFlow	:	
l	Roles	in	PowerFlow	are	automatically	assigned	to	auser	based	on	the	user's	group	with	the	external	provider	
(such	as	LDAP	or	AD)	.	
l	IfaPowerFlow	system	does	not	have	an	external	provider	configured,	such	as	LDAPÂ or	Active	Directory,	that	
PowerFlow	system	can	only	support	asingle	isadmin	Administrator	user.	
l	The	only	password	saved	in	the	PowerFlow	system	is	the	password	for	the	isadmin	Administrator	user.	All	
other	user	passwords	are	saved	in	the	third-	party	authentication	provider	configured	for	PowerFlow	.	
l	The	authentication	configuration	used	by	PowerFlow	is	also	supported	in	SL1	.	
l	API	queries	made	by	users	will	also	be	checked	for	proper	authorization.	
l	The	PowerFlow	administrator	can	configure	which	users	are	in	which	roles.	
l	The	PowerFlow	administrator	can	test	and	configure	the	LDAPÂ and	Active	Directory	settings	to	ensure	that	the	
settings	work	before	proceeding.	
l	The	PowerFlow	administrator	can	always	log	in	with	the	isadmin	Administrator	user,	even	ifthe	underlying	
LDAP	provider	is	inaccessible.	
UserÂ Groups,	Roles,	andÂ Permissions 

Creating	aUserÂ Group	in	PowerFlow	
C r e a ti n g	a	U s e r Â Gr o u p	i n	P o w e r F l o w	
Ifyou	have	the	Administrator	role,	you	can	modify	the	permissions	for	each	user	group	on	the	page.	You	cannot	
change	the	permissions	for	the	user	group	to	which	you	currently	belong.	
A	user	with	the	Administrator	role	can	also	create	auser	group	and	assign	permissions	to	that	group.	
NOTE:	The	only	password	saved	in	the	PowerFlow	system	is	the	password	for	the	isadmin	Administrator	user.	
All	other	user	passwords	are	saved	in	the	third-	party	authentication	provider	configured	for	
PowerFlow	.	
To	create	auser	group:	
1.	In	theÂ 	PowerFlow	user	interface,	go	to	the	Admin	Panel	pageÂ 	(	):	
66 

67	
2.	Click	[AddÂ User	Group	].The	Create	User	Group	window	appears.	
3.	Complete	the	following	fields:	
l	User	Group	Name	.Type	aname	for	this	user	group,	without	spaces.	This	is	the	name	that	the	users	
in	this	group	will	use	when	logging	in	to	PowerFlow	.	
l	Choose	Permissions	Group	.Click	Add	Permission	to	select	the	permission	to	assign	to	this	new	
user	group.	Your	choices	include	Developer	,Configuration	,View	,Â Execute	,and	Administrator	,and	
these	choices	are	defined	in	UserÂ Groups,	Roles,	andÂ Permissions	.	
4.	Click	[Create	User	Group	].The	group	is	added	to	the	Admin	Panel	page.	
5.	Ifyou	want	to	give	auser	group	more	permissions,	select	the	empty	checkbox	(	)for	that	role	from	the	
Admin	Panel	page.	You	must	have	the	Administrator	role	to	change	these	settings.	
6.	Ifyou	want	to	remove	permissions	from	auser	group,	select	the	highest	level	role,	which	has	ablue	check	
mark	(	).	The	role	to	the	right	(with	fewer	permissions)	becomes	the	new	role	for	that	user	group.	
Creating	aUserÂ Group	in	PowerFlow 

Chapte r	
4	
Managing	Synchronization	PowerPacks	
O v e r v i e w
This	chapter	describes	how	to	use	the	SyncPacks	page	(	)to	import,	install,	view,	and	edit	Synchronization	
PowerPacks	.	
This	chapter	covers	the	following	topics:	
What	is	a	Synchronization	PowerPack?	69	
Viewing	the	List	of	Synchronization	PowerPacks	70	
Importing	and	Installing	a	Synchronization	PowerPack	71	
Default	Synchronization	PowerPacks	74
68 

69
W h a t	i s	a	S y n c h r o n i z a ti o n	P o w e r P a c k	?	
A	Synchronization	PowerPack	contains	all	of	the	code	and	logic	needed	to	perform	integrations	on	the	PowerFlow	
platform.	A	Synchronization	PowerPack	is	saved	as	aPython	.whl	file,	and	ittypically	includes	Python	steps	that	are	
listed	in	applications,	and	then	encapsulated	within	configurations:	
You	can	access	the	latest	steps,	applications,	and	configurations	for	PowerFlow	or	athird-	party	integration,	such	as	
ServiceNow,	by	downloading	the	most	recent	Synchronization	PowerPack	for	that	integration	from	ScienceLogic.	
You	can	access	all	Synchronization	PowerPacks	on	the	SyncPacks	page.	
A	Synchronization	PowerPack	is	highly	customizable,	and	you	can	modify	the	contents	of	aSynchronization	
PowerPack	to	meet	your	business	needs,	instead	of	changing	your	SL1	configuration.	You	can	categorize	and	
segment	your	business	integration	solutions	using	aSynchronization	PowerPack	.	
Synchronization	PowerPacks	let	you	distribute	content	to	simplify	installations,	upgrades,	and	maintenance	of	
integration	solutions.	Just	like	PowerPacks	,you	can	also	share	Synchronization	PowerPacks	.IfaPowerFlow	user	
creates	aSynchronization	PowerPack	,that	user	can	share	the	Synchronization	PowerPack	with	other	users	through	
the	ScienceLogic	Customer	Portal.	In	addition,	Synchronization	PowerPacks	protect	acompany's	intellectual	
property	using	licensing	and	encryption	technologies.	
What	is	aSynchronization	PowerPack? 

Viewing	the	List	of	Synchronization	PowerPacks	
V i e w i n g	th e	L i s t	o f	S y n c h r o n i z a ti o n	P o w e r P a c k s	
The	SyncPacks	page	provides	alist	of	the	Synchronization	PowerPacks	on	your	PowerFlow	system.	From	this	page	
you	can	import,	install,	and	edit	Synchronization	PowerPacks	:	
You	can	search	for	aspecific	Synchronization	PowerPack	by	typing	the	name	of	that	Synchronization	PowerPack	in	
the	Search	field	at	the	top	of	the	SyncPacks	page.Â The	user	interface	filters	the	list	as	you	type.	You	can	also	sort	
the	list	of	Synchronization	PowerPacks	by	clicking	the	headers	of	the	columns.	
TIP:	Click	the	Filter	iconÂ 	(	)at	the	top	right	of	the	SyncPacks	page	and	click	the	Show	all	SyncPacks	toggle	
to	show	all	available	Synchronization	PowerPacks	or	to	show	only	activated	Synchronization	
PowerPacks	.Â An	activated	Synchronization	PowerPack	has	been	installed	and	is	ready	to	be	used.	
The	[Actions	]button	(	)for	aSynchronization	PowerPack	gives	you	the	option	to	activate	and	install	or	uninstall	
that	Synchronization	PowerPack	.Also,	ifyou	have	older	versions	of	the	same	Synchronization	PowerPack	,you	can	
select	Change	active	version	to	activate	adifferent	version	other	than	the	version	that	is	currently	running.	
70 

71
Click	aSynchronization	PowerPack	from	the	list	to	view	theÂ 	SyncPacks	page	for	the	Synchronization	PowerPack	.	
From	this	page	you	can	view	the	contents	of	the	Synchronization	PowerPack	,including	the	applications,	
configuration	objects,	and	steps	included	in	the	Synchronization	PowerPack	.You	can	also	view	additional	
metadata,	including	minimum	PowerFlow	platform	version	and	descriptions:	
I m p o r ti n g	a n d	I n s ta l l i n g	a	S y n c h r o n i z a ti o n	P o w e r P a c k	
The	SyncPacks	page	in	the	PowerFlow	user	interface	lets	you	import	and	install	the	latest	version	of	a	
Synchronization	PowerPack	.After	you	install	aSynchronization	PowerPack	,PowerFlow	considers	that	
Synchronization	PowerPack	to	be	activated	and	ready	to	be	used.	By	default,	the	SyncPacks	page	displays	all	
activated	Synchronization	PowerPacks	.	
After	you	install	aÂ Synchronization	PowerPack	,that	Synchronization	PowerPack	is	available	on	the	SyncPacks	page	
of	the	PowerFlow	user	interface.	You	can	click	the	name	of	that	Synchronization	PowerPack	to	view	the	detail	page	
for	that	Synchronization	PowerPack	.On	the	Synchronization	PowerPack	detail	page,	you	can	view	metadata	about	
the	Synchronization	PowerPack	,and	you	can	also	view	the	applications,	configuration	objects,	and	steps	for	that	
Synchronization	PowerPack	.Also,	the	applications	from	that	Synchronization	PowerPack	are	added	to	the	
Applications	page	of	the	user	interface.	
You	can	click	the	Filter	icon	(	)at	the	top	of	the	SyncPacks	page	and	select	Show	All	SyncPacks	to	view	all	
versions	of	the	available	Synchronization	PowerPacks	.A	Synchronization	PowerPack	must	be	aPython	.whl	file	to	
upload	and	install	in	PowerFlow	.	
Importing	and	Installing	aSynchronization	PowerPack 

Importing	and	Installing	aSynchronization	PowerPack	
NOTE:	You	must	have	the	Develop	or	Administrator	role	to	install	aSynchronization	PowerPack	.For	more	
information,	see	Managing	Users	in	PowerFlow	.	
NOTE:	Ifyour	PowerFlow	system	uses	self-	signed	certificates,	you	will	need	to	manually	accept	the	certificate	
before	you	can	upload	Synchronization	PowerPacks	.Go	to	https://	<IP	address	of	
PowerFlow	>	:3141/isadmin	,accept	the	certificate,	and	then	exit	out	of	the	tab.Â When	you	log	into	
PowerFlow	again,	you	will	be	able	to	upload	Synchronization	PowerPacks	.	
L oc a t i n g	a n d	D ow n l oa d i n g	a	S y n c h r on i z a t i on	P ow e r P a c k	
A	Synchronization	PowerPack	file	has	the	.whl	file	extension	type.	You	can	download	the	Synchronization	
PowerPack	file	from	the	ScienceLogic	Support	site.	
To	locate	and	download	the	Synchronization	PowerPack	:	
1.	Go	to	the	ScienceLogic	Support	site	at	https://support.sciencelogic.com/s/	.	
2.	Click	the	[ProductÂ Downloads	]tab,	select	PowerPacks	,and	then	click	the	""Synchronization""	link.	The	
Synchronization	PowerPack	Downloads	page	appears.	
3.	Click	the	name	of	the	Synchronization	PowerPack	you	want	to	install.	The	PowerPack	page	appears.	
4.	In	the	Files	list,	locate	the	Synchronization	PowerPack	.whl	file,	click	the	down	arrow	button,	and	select	
Download	.	
NOTE:	Synchronization	PowerPacks	do	not	require	aspecific	license.	After	you	download	aSynchronization	
PowerPack	,you	can	import	itto	PowerFlow	using	the	PowerFlow	user	interface.	
I m p or t i n g	a	S y n c h r on i z a t i on	P ow e r P a c k	
WARNING:	Ifyou	are	upgrading	to	anewer	version	of	aSynchronization	PowerPack	from	aprevious	version,	
make	anote	of	any	settings	you	made	on	the	Configuration	tab	for	the	various	PowerFlow	
applications	in	that	Synchronization	PowerPack	,as	those	settings	are	not	retained	when	you	
upgrade.	
To	import	aSynchronization	PowerPack	in	the	PowerFlow	user	interface:	
1.	On	the	SyncPacks	page	of	the	PowerFlow	user	interface,	click	[Import	SyncPack	].TheÂ 	Import	SyncPack	
page	appears.	
2.	Click	[Browse	]and	select	the	.whl	file	for	the	Synchronization	PowerPack	you	want	to	install.	
TIP:	You	can	also	drag	and	drop	a.whl	file	to	the	SyncPacks	page.	
72 

73	
3.	ClickÂ 	[Import	].PowerFlow	registers	and	uploads	the	Synchronization	PowerPack	.Â The	Synchronization	
PowerPack	is	added	to	the	SyncPacks	page.	
NOTE:	You	cannot	edit	the	content	package	in	aSynchronization	PowerPack	published	by	ScienceLogic.	You	
must	make	acopy	of	aÂ ScienceLogic	Synchronization	PowerPack	and	save	your	changes	to	the	new	
Synchronization	PowerPack	to	prevent	overwriting	any	information	in	the	original	Synchronization	
PowerPack	when	upgrading.	
I n s t a l l i n g	a	S y n c h r on i z a t i on	P ow e r P a c k	
To	install	aSynchronization	PowerPack	in	the	PowerFlow	user	interface:	
1.	On	the	SyncPacks	page	of	the	PowerFlow	user	interface,	click	the	[Actions	]button	(	)Â for	the	
Synchronization	PowerPack	you	want	to	install	and	select	Activate	&Â Install	.The	Activate	&Â Install	SyncPack	
modal	appears.	
TIP:	By	default,	the	SyncPacks	page	displays	only	activated	and	installed	PowerPacks.	Ifyou	do	not	see	
the	PowerPack	that	you	want	to	install,	click	the	Filter	icon	(	)on	the	SyncPacks	page	and	select	
Show	All	SyncPacks	to	see	alist	of	the	uninstalled	PowerPacks	.	
2.	Click	[Yes	]to	confirm	the	activation	and	installation.	When	the	Synchronization	PowerPack	is	activated,	the	
SyncPacks	page	displays	agreen	check	mark	icon	(	)for	that	Synchronization	PowerPack	.Â If	the	activation	
or	installation	failed,	then	ared	exclamation	mark	icon	(	)appears.	
TIP:	While	the	Synchronization	PowerPack	is	installing,	you	cannot	click	any	of	the	options	that	appear	
when	you	click	the	[Actions	]button	(	).	
3.	For	more	information	about	the	activation	and	installation	process,	click	the	check	mark	icon	(	)or	the	
exclamation	mark	icon	(	)in	the	Activated	column	for	that	Synchronization	PowerPack	.For	asuccessful	
installation,	the	""ActivateÂ &Â Install	SyncPack""	application	appears,	and	you	can	view	the	Step	Log	for	the	
steps.	For	afailed	installation,	the	Error	Logs	window	appears.
Importing	and	Installing	aSynchronization	PowerPack 

Default	Synchronization	PowerPacks	
D e f a u l t	S y n c h r o n i z a ti o n	P o w e r P a c k s	
When	you	install	PowerFlow	,the	following	Synchronization	PowerPacks	are	added	to	the	SyncPacks	page	by	
default:	
l	Base	StepsÂ 	Synchronization	PowerPack	
l	SystemÂ Utils	Synchronization	PowerPack	
NOTE:	The	Flow	Control	Synchronization	PowerPack	contains	the	""IfStep""	step	that	enables	the	logical	
branching	used	by	the	PowerFlow	builder	.This	Synchronization	PowerPack	is	included	in	PowerFlow	.	
Ifyou	need	to	install	these	Synchronization	PowerPacks	,click	the	[Actions	]button	(	)and	select	Activate	&Â Install	
for	each	Synchronization	PowerPack	.	
B a s e	S t e p s	S y n c h r on i z a t i on	P ow e r P a c k	
The	Base	Steps	Synchronization	PowerPack	contains	the	""TemplateÂ App""	application.	You	can	use	the	""Template	
App""Â application	as	atemplate	for	building	new	PowerFlow	applications.	
The	Base	Steps	Synchronization	PowerPack	also	includes	the	following	steps,	which	you	can	use	in	new	and	
existing	applications:	
l	Cache	Delete	
l	Cache	Read	
l	Cache	Save	
l	DirectÂ Cache	Read	
l	Jinja	Template	Data	Render	
l	MS-	SQL	Describe	
l	MS-	SQL	Insert	
l	MS-	SQL	Select	
l	MySQL	Describe	
l	MySQL	Insert	
l	MySQL	Select	
l	Query	GraphQL	
l	Query	REST	
l	Run	acommand	through	an	SSH	tunnel	
l	Trigger	Application	
74 

75	
TIP:	To	view	the	code	for	astep,	select	aSynchronization	PowerPack	from	the	SyncPacks	page,	click	the	
[Steps	]tab,	and	select	the	step	you	want	to	view.	
S y s t e m	U t i l s Â 	S y n c h r on i z a t i on	P ow e r P a c k	
The	System	Utils	Synchronization	PowerPack	is	aStandard	Synchronization	PowerPack	that	contains	applications,	
aconfiguration	object,	and	steps.	
NOTE:	You	must	install	the	Base	Steps	Synchronization	PowerPack	before	you	can	install	this	Synchronization	
PowerPack	.	
The	SystemÂ Utils	â¦	Synchronization	PowerPack	includes	the	following	PowerFlow	applications:	
l	PowerFlow	Backup	.Creates	abackup	file	of	the	Couchbase	database	used	by	PowerFlow	and	sends	the	
file	to	aremote	location.	For	more	information,	see	Backing	up	Data	.	
l	PowerFlow	Restore	.Restores	aCouchbase	backup	file	that	was	created	up	with	the	""PowerFlow	Backup""	
application.	For	more	information,	see	Restoring	a	Backup	.	
l	PowerFlow	SystemÂ Diagnostics	.Displays	areport	of	platform	diagnostics	for	the	services	used	by	
PowerFlow	.For	more	information,	see	Viewing	PowerFlow	Â System	Diagnostics	.	
l	Timed	Removal	.Removes	logs	from	Couchbase	on	aregular	schedule.	For	more	information,	see	
Removing	Logs	on	a	Regular	Schedule	.	
This	Synchronization	PowerPack	includes	the	following	configuration	objects:	
l	IS	-System	Backup	Configuration	Example	.Contains	the	structure	needed	for	the	""PowerFlow	Backup""	
and	""PowerFlow	Â Restore""	applications.	
l	IS	-System	DiagnosticÂ Configuration	Example	.Contains	the	structure	needed	for	the	""PowerFlow	System	
Diagnostics""	application.	
This	Synchronization	PowerPack	also	includes	the	following	steps,	which	are	used	in	the	four	PowerFlow	
applications	listed	above:	
l	CollectÂ ISÂ Diagnostics	
l	CreateÂ 	PowerFlow	Backup	
l	Diagnostic	reporter	
l	Query	an	ISÂ Node	
l	Query	Application	Logs	and	Remove	
l	QueryÂ IS	Manager	node	
l	Restore	CouchbaseÂ From	Backup	
Default	Synchronization	PowerPacks 

Chapte r	
5	
Managing	PowerFlow	Applications	
O v e r v i e w
This	chapter	describes	how	to	use	the	Applications	page	(	)of	the	PowerFlow	user	interface	to	view,	run,	and	
schedule	PowerFlow	applications.	You	can	use	the	PowerFlow	builder	to	create	custom	applications,	and	those	
applications	can	use	""flow	control""	operators	that	enable	logical	branching	and	data	transformation	between	steps.	
NOTE:	The	Flow	Control	Synchronization	PowerPack	contains	the	""IfStep""	step	that	enables	the	logical	
branching	used	by	the	PowerFlow	builder	.This	Synchronization	PowerPack	is	included	in	PowerFlow	.	
This	chapter	covers	the	following	topics:	
Viewing	the	List	of	PowerFlow	Applications	77	
Elements	of	an	ApplicationÂ Page	79	
Creating	a	PowerFlow	Application	81	
Working	with	Flow	Control	Operators	84	
Editing	a	PowerFlow	Application	97	
Creating	a	Step	99	
Aligning	a	Configuration	Object	with	an	Application	99	
Running	a	PowerFlow	Application	101	
Viewing	Previous	Runs	of	anÂ Application	with	the	Timeline	102	
Scheduling	a	PowerFlow	Application	105	
Viewing	PowerFlowÂ System	Diagnostics	108	
Backing	up	Data	110
76 

77	
Viewing	a	Report	for	a	PowerFlow	Application	117	
V i e w i n g	th e	L i s t	o f	P o w e r F l o w	A p p l i c a ti o n s	
The	Applications	page	(	)provides	alist	of	the	PowerFlow	applications	on	your	PowerFlow	system.	From	this	
page	you	can	schedule,	edit,	view,	and	create	applications	and	steps:	
You	can	search	for	aspecific	application	by	typing	the	name	of	that	in	the	Search	field	at	the	top	of	the	
Applications	page.Â The	user	interface	filters	the	list	as	you	type.	You	can	also	sort	the	list	of	applications	by	clicking	
the	headers	of	the	columns.	
Some	of	the	applications	on	the	Applications	page	of	the	PowerFlow	user	interface	are	internal	applications	that	
you	should	not	run	directly.	Instead,	other	""parent""	applications	run	these	internal	applications.	To	view	the	internal	
applications,	click	the	Filter	iconÂ 	(	)at	the	top	right	of	the	Applications	page	and	select	ShowÂ Hidden	
Applications	.Internal	applications	are	hidden	by	default.	
TIP:	To	view	the	applications	that	belong	to	only	one	Synchronization	PowerPack	or	some	of	the	
Synchronization	PowerPacks	,click	the	Filter	iconÂ 	(	)and	select	the	Synchronization	PowerPacks	that	
contain	the	applications	you	want	to	view	from	the	Filter	by	SyncPack	drop-	down.	You	can	also	search	
for	aSynchronization	PowerPack	from	this	drop-	down.	To	go	back	to	seeing	all	applications	on	the	
Applications	page,	click	Clear	selected	items	.	
Viewing	the	List	of	PowerFlow	Applications 

Viewing	the	List	of	PowerFlow	Applications	
The	SyncPack	column	on	the	Applications	page	lists	the	name	of	the	Synchronization	PowerPack	to	which	a	
specific	application	belongs,	where	relevant.	You	can	click	the	name	of	the	Synchronization	PowerPack	to	which	
an	application	belongs	to	go	to	the	Synchronization	PowerPack	page	for	that	pack.	
The	Last	Run	column	shows	the	current	status	of	all	of	the	PowerFlow	applications:	
Icon	Status
The	application	ran	successfully.	The	date	and	time	of	the	last	run	appears	next	to	the	icon.	
The	application	is	currently	running.	
The	application	failed	to	run	successfully.	
The	application	has	not	been	run.	
The	[Schedule	]button	lets	you	use	theÂ Scheduler	to	define	how	often	or	at	what	time	to	run	an	application.	A	
scheduled	application	displays	acheck	mark	in	the	[Schedule	]button	on	this	page.	For	more	information,	see	
Scheduling	a	PowerFlow	Application	.	
The	[Actions	]button	(	)for	an	application	gives	you	the	option	to	run,	view,	or	delete	that	application.	You	
cannot	run	an	application	in	DebugÂ Mode	or	run	an	application	with	custom	attributes	from	this	menu.	
TIP:	To	open	the	Notification	Center	pane,	which	contains	alist	of	all	of	the	pop-	up	messages	about	
applications	that	were	run	successfully	or	with	warnings	or	failures,	click	your	user	name	in	the	navigation	
bar	in	the	top	right	and	select	Notifications	.The	different	notifications	are	color-	coded:	green	for	success,	
yellow	for	warning,	and	red	for	failure.	The	number	of	notifications	displays	as	abadge	in	the	menu.	For	
more	information	about	anotification,	click	the	link	for	the	notification	and	review	the	Step	Log	andÂ 	Step	
Data	tabs	for	the	application	steps.	
78 

79
E l e m e n ts	o f	a n	A p p l i c a ti o n Â P a g e	
When	you	click	the	name	of	an	application	on	the	Applications	page,	an	Application	detail	page	appears:	
The	Application	page	contains	the	logic	for	the	application.	In	the	main	viewing	pane,	the	steps	for	the	
application	are	organized	as	aflowchart.	Each	rectangular	block	is	astep,	and	the	arrows	indicate	the	order	in	
which	the	steps	will	execute	when	you	run	the	application.	The	color	of	each	step	changes	to	show	the	progress	of	
the	application	run	as	itruns:	green	for	success,	red	for	failure,	and	blue	for	running.	
Ifastep	triggers	achild	application,	ablue	information	icon	(	)appears	in	the	upper	left	corner	of	the	step.	Click	
the	icon	to	display	the	triggered	application's	run	ID	as	alink	in	apop-	up	window.	You	can	click	that	link	to	view	the	
detail	page	for	the	triggered	application.	Ifno	run	IDÂ is	present,	the	pop-	up	window	displays	""No	runs	available"".	
The	buttons	at	the	top	of	an	Application	page	include	the	following:	
l	[Open	Editor	](	).Â Opens	the	Steps	Registry	pane	and	launches	the	PowerFlow	builder	interface.	After	
you	click	the	button,	the	following	buttons	appear	in	the	top	navigation	bar:	
o	[Close	Editor	](	).	Closes	the	Steps	Registry	pane.	
o	[Edit	Metadata	](	).	Opens	the	Integration	Metadata	window	for	that	application	so	you	can	
update	the	description	or	version	of	the	application.	
o	[Save	].Lets	you	save	any	changes	to	the	steps	and	application.	You	can	also	save	the	edited	
application	as	anew	application	with	new	metadata	using	the	Save	as	option.	
For	more	information,	see	Editing	an	Application	.	
Elements	of	an	ApplicationÂ Page 

Elements	of	an	ApplicationÂ Page	
l	[Reports	](	).	Displays	areport	of	the	results	of	this	application,	where	applicable.	For	more	information,	
see	Viewing	a	Report	for	an	Application	.	
l	[Timeline	](	).	Opens	the	Timeline	view	at	the	top	of	the	window	where	you	can	view	past	runs	of	this	
application,	and	whether	the	application	failed	or	succeeded.	For	more	information,	see	Viewing	Previous	
Runs	of	anÂ Application	.	
l	[Replay	](	).	Replays	the	last	run	of	this	application.	
l	[Run	](	).	Runs	the	application	ifyou	click	the	button	without	hovering	over	it.	Ifyou	hover	over	the	[Run	]	
button,	you	can	choose	from	the	following	options:	
o	Debug	Run	.Run	the	application	in	Debug	Mode,	which	provides	more	log	data	in	the	StepÂ Logs	to	
help	you	with	troubleshooting.	
o	Custom	Run	.Run	the	application	with	custom	parameters	for	testing	or	troubleshooting.	
TIP:	You	can	click	[Run	](	)to	run	an	application	and	then	navigate	to	another	page	in	the	
PowerFlow	,and	the	application	will	complete	on	its	own.	
For	more	information,	see	Running	an	Application	.	
l	[Configure	](	).	Opens	the	Configuration	pane	for	the	application,	where	you	can	change	the	
configuration	object	aligned	with	the	application	and	edit	other	fields	as	needed.	For	more	information,	see	
Aligning	a	Configuration	Object	with	an	Application	.	
TIP:	Click	the	Zoom	icons	(	,	)to	change	the	size	of	the	steps	on	the	PowerFlow	builder	.	
TIP:	Click	the	Rotate	iconÂ 	(	)to	turn	the	PowerFlow	builder	90	degrees.	
In	the	bottom	left-	hand	corner	of	the	page,	pop-	up	messages	appear	temporarily	with	the	status	of	the	application	
or	an	action.	In	the	example	above,	the	status	is	""Run:	success"".	After	afew	seconds,	the	pop-	up	message	
disappears.
The	Step	pane	at	the	bottom	of	an	Application	page	displays	two	tabs:	
l	[Step	Log	].Displays	the	time,	the	type	of	log,	and	the	log	messages	from	astep	that	you	selected	in	the	main	
pane.	All	times	that	are	displayed	in	this	pane	are	in	seconds.	
l	[Step	Data	].Displays	the	JSONÂ data	that	was	generated	by	the	selected	step.	
Click	the	Step	pane	again	to	hide	the	pane.	
80 

81	
TIP:	For	longer	step	log	messages,	click	the	down	arrow	icon	(	)in	the	Message	column	of	the	[Step	Log	]	
tab	to	open	the	message.	Also,	you	can	triple-	click	alog	message	to	highlight	the	entire	text	block	so	you	
can	easily	copy	the	log	message	to	another	text	viewer.	
To	generate	more	detailed	logs	when	you	run	an	application,	select	Debug	Run	by	hovering	over	[Run	](	).	
In	the	bottom	right-	hand	corner	of	the	page	is	asmaller	version	of	the	application.	You	can	click	and	drag	on	this	
version	to	move	or	scroll	through	the	steps	in	the	main	viewing	pane.	
C r e a ti n g	a	P o w e r F l o w	A p p l i c a ti o n	
On	the	Applications	page,	you	can	create	new	applications	using	the	PowerFlow	builder	interface.	
To	create	an	application:	
1.	From	the	Applications	page	(	),	click	[Create	Application	].A	CreateÂ Application	window	appears:	
2.	Complete	the	following	fields:	
l	Friendly	Name	.The	name	that	you	want	users	to	see	for	this	application.	Required.	
l	Description	.A	short	description	of	what	this	application	does.	
l	Author	.The	name	of	the	person	or	company	that	created	this	application.	Use	the	same	name	for	
multiple	applications.	Required.	
l	Version	.The	version	for	this	application.	
l	Configuration	.Â Select	aconfiguration	object	to	align	with	the	new	application,	or	select	Make	New	
Configuration	to	create	anew	configuration	object.	
Creating	aPowerFlow	Application 

Creating	aPowerFlow	Application	
3.	Click	[Set	Values	].TheÂ PowerFlowÂ builder	interface	appears:	
4.	Click	the	step	you	want	to	add	from	the	Steps	Registry	pane	and	drag	itto	the	main	viewing	pane	to	add	the	
step	to	the	application.	The	Configuration	pane	for	the	step	appears:	
5.	Type	aname	for	the	new	step	and	update	the	other	fields	on	the	Configuration	pane	as	needed.	
82 

83	
6.	Click	the	down	arrow	on	the	Advanced	section	to	update	the	advanced	fields.	
7.	Click	[Save	]to	save	the	new	step.	
TIP:	To	adjust	the	position	of	any	step	on	the	main	viewing	pane,	click	the	step	you	want	to	move	and	
drag	itto	its	new	location.	To	add	an	arrow	from	one	step	to	another,	click	and	drag	the	mouse	
from	the	first	step	to	the	second	step.	An	arrow	appears	between	the	two	steps,	which	you	can	click	
and	drag	to	reposition.	
8.	Repeat	steps	4-	7	to	add	any	remaining	steps	to	the	application.	
TIP:	To	remove	astep	from	the	main	viewing	pane,	right-	click	the	step	and	select	Delete	.	
9.	To	adjust	the	position	of	any	step	in	the	application,	click	the	step	you	want	to	move	and	drag	itto	its	new	
location.	
10.	To	add	an	arrow	between	steps,	click	the	outline	of	the	first	step	and	drag	the	arrow	that	appears	to	the	
second	step.	
TIP:	To	add	aflow	control	operator,	such	as	aCondition	or	aTransform	operator,	see	Working	with	
Flow	Control	Operators.	
11.	When	you	are	done	adding	and	configuring	steps	for	the	new	application,	click	[Save	].Â The	application	is	
added	to	the	Applications	page.	
Creating	aPowerFlow	Application 

Working	with	Flow	Control	Operators	
W o r k i n g	w i th	F l o w	C o n tr o l	O p e r a to r s	
When	you	are	creating	or	editing	aapplication	in	the	PowerFlow	builder	interface,	you	can	use	""flow	control""	
operators,	including	aCondition	operator	and	aTransform	operator	that	you	can	drag	and	drop	into	the	step	
workflow	on	the	canvas:	
l	The	Condition	operator	(	)lets	you	create	for	branching	workflows,	such	as	If-	Else	or	If-	Then-	Else	
statements.	TheÂ 	Conditional	Wizard	pane	lets	you	modify	the	conditions	that	enable	branching	in	the	
workflow.	
l	The	Transform	operator	(	)lets	the	application	pull	data	gathered	by	aprevious	step	and	modify	or	
transform	that	data	to	fit	into	the	next	step.	The	Transform	Wizard	pane	lets	you	specify	which	data	you	want	
to	use	or	transform	from	the	previous	steps.	
NOTE:	The	Flow	Control	Synchronization	PowerPack	contains	the	""IfStep""	step	that	enables	the	logical	
branching	used	by	the	PowerFlow	builder	.This	Synchronization	PowerPack	is	included	in	PowerFlow	.	
The	following	example	uses	aCondition	operator	that	allows	the	workflow	to	branch	depending	on	the	data	
gathered	by	the	previous	steps	in	the	application:	
84 

85
C r e a t i n g	a n	A p p l i c a t i on	w i t h	a Â C on d i t i on	O p e r a t or	
You	can	drag	aCondition	operator	(	)onto	an	application	workflow	to	create	the	option	for	branching	flows,	
such	as	If-	Else	or	If-	Then-	Else	statements.	
To	create	an	automation	that	contains	aCondition	operator:	
1.	From	the	Applications	page	(	),	click	[Create	Application	].A	CreateÂ Application	window	appears.	
2.	Complete	the	following	fields:	
l	Friendly	Name	.The	name	that	you	want	users	to	see	for	this	application.	Required.	
l	Description	.A	short	description	of	what	this	application	does.	
l	Author	.The	name	of	the	person	or	company	that	created	this	application.	Use	the	same	name	for	
multiple	applications.	Required.	
l	Version	.The	version	for	this	application.	
l	Configuration	.Â Select	aconfiguration	object	to	align	with	the	new	application,	or	select	Make	New	
Configuration	to	create	anew	configuration	object.	
3.	Click	[Set	Values	].TheÂ 	Application	page	and	the	StepsÂ Registry	pane	appear	in	Edit	mode:	
4.	On	the	Steps	Registry	pane,	you	can	search	for	astep	or	filter	the	list	of	steps	to	help	you	find	the	step	you	
need:	
Working	with	Flow	Control	Operators 

Working	with	Flow	Control	Operators	
l	Click	the	[Search	Steps	]tab	(	)Â to	search	the	entire	list	of	steps	from	the	Search	Steps	Registry	field.	
l	Click	the	[GroupÂ Steps	]Â tab	(	)to	view	the	steps	by	Synchronization	PowerPack	or	by	atag,	or	to	
show	all	of	the	steps	in	one	list.	
TIP:	Click	the	[Actions	]button	(	)on	astep	in	the	Steps	Registry	pane	to	view	more	
information	about	that	step,	including	the	stepÂ ID,	the	Synchronization	PowerPack	for	that	
step,	the	version,	and	creator	of	the	step.	You	can	also	click	[Edit	Step	Code	]to	edit	the	
code	for	that	step.	
5.	Select	the	step	you	want	to	add	from	the	Steps	Registry	pane	and	drag	itto	the	main	viewing	pane	(""canvas"")	
to	add	the	step	to	the	application.	The	Configuration	pane	for	that	step	appears:	
6.	On	the	Configuration	pane,	type	aname	for	the	new	step	and	update	the	other	fields	as	needed.	Click	the	
down	arrow	on	the	Advanced	section	to	update	the	advanced	fields.	
7.	Click	[Save	]to	save	the	new	step.	
TIP:	To	adjust	the	position	of	any	step	on	the	main	viewing	pane	(""canvas""),	click	the	step	you	want	to	
move	and	drag	itto	its	new	location.	To	add	an	arrow	from	one	step	to	another,	click	on	the	step	
and	drag	the	mouse	to	the	second	step.	An	arrow	appears	between	the	two	steps,	which	you	can	
click	and	drag	to	reposition,	or	you	can	click	the	arrow	and	press	the	[Delete	]key	to	delete	the	
arrow.	
86 

87	
8.	Repeat	steps	4-	7	to	add	any	remaining	steps	to	the	application.	
TIP:	To	remove	astep	from	the	PowerFlow	builder	interface,	right-	click	the	step	and	select	Delete	.	
9.	Click	[Save	](	)to	save	your	work.	
10.	On	the	Steps	Registry	pane,	click	the	[Advanced	]tab	(	).	The	flow	control	operators	appear.	
11.	To	add	the	option	for	branching	flows,	such	as	If-	Else	or	If-	Then-	Else	statements,	drag	the	Condition	
operator	(	)onto	the	canvas.	The	Condition	operator	appears	on	the	canvas	with	ared	exclamation	mark	
next	to	it,	and	the	Conditional	Wizard	pane	appears:	
12.	Click	[Save	]on	the	Conditional	Wizard	pane	to	save	your	work	so	far.	
13.	Connect	the	steps	for	the	branching	workflow	to	the	Condition	operator	by	clicking	the	outline	of	each	step	
and	dragging	the	arrow	that	appears	to	the	Condition	operator.	Repeat	this	process	for	all	of	the	steps	that	
should	be	part	of	the	branching	workflow.	
TIP:	The	arrows	connecting	the	steps	and	the	Condition	operator	display	as	green	to	show	the	flow	of	
the	data,	based	on	the	values	in	the	application.	In	Edit	mode,	you	can	click	the	blue	circle	above	a	
branched	step	to	open	the	Conditional	Wizard	pane	to	see	the	conditions	for	that	step.	
Working	with	Flow	Control	Operators 

Working	with	Flow	Control	Operators	
14.	Click	the	gear	icon	(	)on	the	Condition	operator.	The	Conditional	Wizard	pane	for	that	operator	
appears,	this	time	with	fields	related	to	the	steps	you	connected	to	the	Condition	operator	in	step	13:	
15.	Click	[Add	New	Condition	]for	each	new	condition	you	want	to	configure.	
16.	Complete	the	following	fields	on	the	Conditional	Wizard	pane	for	each	step	you	want	to	include	in	the	
branching:	
l	Bound	to	this	Step	.Change	the	step	connected	to	the	Condition	operator.	
l	Label	.Type	abrief	description	of	the	branching	condition,	such	as	""ChangeÂ Request	Found"".	This	text	
will	display	above	the	selected	step	when	you	are	out	of	Edit	mode.	
88 

89	
l	Condition	type	.Select	the	condition	that	this	step	needs	to	meet	to	allow	branching.	The	condition	
type	you	select	here	determines	what	options	display	in	the	field	or	fields	below	this	field.	You	can	use	
variables	from	the	previous	step,	which	you	can	find	on	the	[Step	Data	]tab	of	the	Step	pane	for	that	
step.	Your	options	include:	
o	equal	.In	the	Equal	to	field,	type	True	or	False	.In	the	Value	field,	specify	the	variable	that	
should	be	true	or	false.	
o	numeric	.In	the	Value	field,	specify	the	variable	that	should	fall	within	the	range	specified	by	the	
Above	and	Below	fields	.	
o	template	.In	the	Value	field,	type	acondition	that	can	include	numeric	or	another	kind	or	
comparison.	For	example:	${step_	name.stop_	step}	==	true	
o	and	.Joins	two	or	more	conditions	with	the	AND	logic	operator,	and	these	conditions	can	be	
simple	or	complex	conditions.	
o	not	.Inverts	the	result	of	the	simple	condition	type.	This	condition	can	contain	only	one	condition	
inside	of	it.	
o	or	.Joins	two	or	more	conditions	with	the	OR	logic	operator.	
TIP:	In	aValue	field	under	the	Condition	type	,you	can	also	click	the	[Actions	]button	(	)and	
click	Select	property	to	use	data	from	one	of	the	previous	steps,	ifavailable.	
17.	You	can	add	asub-	condition	for	the	same	step	by	clicking	the	[Actions	]button	(	)for	the	Condition	type	
field	and	selecting	Add	sub-	condition	.You	can	also	click	the	[New	condition	]button	to	add	asub-	condition.	
NOTE:	You	can	add	more	than	one	Condition	operator	to	the	same	application.	You	can	also	add	
one	or	more	Transform	operators	to	the	same	application.	For	more	information,	see	
Creating	an	Application	with	aTransformÂ Operator.	
18.	Edit	the	remaining	conditions	on	the	Conditional	Wizard	pane,	and	then	click	[Save	].	
19.	On	theÂ 	Application	detail	page,	add	additional	steps	and	operators	to	the	new	application	as	needed,	and	
then	click	[Save	].Â The	application	is	added	to	the	Applications	page.	
NOTE:	Running	and	scheduling	an	application	works	the	same	way	as	with	an	application.	For	more	
information,	see	Running	a	PowerFlow	Application	or	Scheduling	a	PowerFlow	Application	.	
C r e a t i n g	a n	A p p l i c a t i on	w i t h	a	T r a n s f or m	O p e r a t or	
In	the	PowerFlow	builder	interface,	you	can	drag	aTransform	operator	(	)from	the	Steps	Registry	pane	onto	
an	application	workflow.	
Working	with	Flow	Control	Operators 

Working	with	Flow	Control	Operators	
The	Transform	operator	can	pull	data	gathered	by	aprevious	step,	and	then	modify	or	transform	the	data	to	fit	
into	the	next	step.	The	operator	uses	aJinja2	template	to	merge	the	data	from	previous	steps.	For	example,	you	
might	want	to	use	the	Transform	operator	ifastep	in	an	application	is	pulling	in	alarge	amount	of	data,	but	you	
only	want	to	focus	on	aspecific	sub-	set	of	that	data.	
To	create	an	application	with	aTransform	operator:	
1.	From	the	Applications	page	(	),	click	[Create	Application	].A	CreateÂ Application	window	appears.	
2.	Complete	the	following	fields:	
l	Friendly	Name	.The	name	that	you	want	users	to	see	for	this	application.	Required.	
l	Description	.A	short	description	of	what	this	application	does.	
l	Author	.The	name	of	the	person	or	company	that	created	this	application.	Use	the	same	name	for	
multiple	applications.	Required.	
l	Version	.The	version	for	this	application.	
l	Configuration	.Â Select	aconfiguration	object	to	align	with	the	new	application,	or	select	Make	New	
Configuration	to	create	anew	configuration	object.	
3.	Click	[Set	Values	].TheÂ 	Application	page	and	the	StepsÂ Registry	pane	appear	in	Edit	mode:	
4.	On	the	Steps	Registry	pane,	you	can	search	for	astep	or	filter	the	list	of	steps	to	help	you	find	the	step	you	
need:	
90 

91	
l	Click	the	[Search	Steps	]tab	(	)Â to	search	the	entire	list	of	steps	from	the	Search	Steps	Registry	field.	
l	Click	the	[GroupÂ Steps	]Â tab	(	)to	view	the	steps	by	Synchronization	PowerPack	or	by	atag,	or	to	
show	all	of	the	steps	in	one	list.	
TIP:	Click	the	[Actions	]button	(	)on	astep	in	the	Steps	Registry	pane	to	view	more	
information	about	that	step,	including	the	stepÂ ID,	the	Synchronization	PowerPack	for	that	
step,	the	version,	and	creator	of	the	step.	You	can	also	click	[Edit	Step	Code	]to	edit	the	
code	for	that	step.	
5.	Select	the	step	you	want	to	add	from	the	Steps	Registry	pane	and	drag	itto	the	main	viewing	pane	(""canvas"")	
to	add	the	step	to	the	application.	The	Configuration	pane	for	that	step	appears:	
6.	On	the	Configuration	pane,	type	aname	for	the	new	step	and	update	the	other	fields	as	needed.	Click	the	
down	arrow	on	the	Advanced	section	to	update	the	advanced	fields.	
7.	Click	[Save	]to	save	the	new	step.	
TIP:	To	adjust	the	position	of	any	step	on	the	main	viewing	pane	(""canvas""),	click	the	step	you	want	to	
move	and	drag	itto	its	new	location.	To	add	an	arrow	from	one	step	to	another,	click	on	the	step	
and	drag	the	mouse	to	the	second	step.	An	arrow	appears	between	the	two	steps,	which	you	can	
click	and	drag	to	reposition,	or	you	can	click	the	arrow	and	press	the	[Delete	]key	to	delete	the	
arrow.	
Working	with	Flow	Control	Operators 

Working	with	Flow	Control	Operators	
8.	Repeat	steps	4-	7	to	add	any	remaining	steps	to	the	application.	
TIP:	To	remove	astep	from	the	PowerFlow	builder	interface,	right-	click	the	step	and	select	Delete	.	
9.	Click	[Save	](	)to	save	your	work.	
10.	On	the	Steps	Registry	pane,	click	the	[Advanced	]tab	(	).	The	flow	control	operators	appear.	
11.	To	add	an	operator	that	pulls	data	gathered	by	aprevious	step	and	modify	or	transform	the	data	to	fit	into	the	
next	step,	drag	the	Transform	operator	(	)onto	the	canvas.	The	Transform	operator	appears	on	the	
canvas	with	ared	exclamation	mark	next	to	it.	
12.	Locate	the	step	or	steps	that	contain	data	you	want	to	send	to	the	following	step	or	steps.	
13.	Connect	the	step	or	steps	to	the	Transform	operator	by	clicking	the	outline	of	each	step	and	dragging	the	
arrow	that	appears	to	the	Transform	operator.	
14.	Connect	the	step	or	steps	that	will	receive	the	transformed	data	by	clicking	the	outline	of	the	Transform	
operator	and	dragging	the	arrow	that	appears	to	those	steps.	
TIP:	The	arrows	connecting	the	steps	and	the	Transform	operator	display	as	green	to	show	the	flow	of	
the	data,	based	on	the	values	in	the	application.	
15.	Click	[Save	](	)to	save	your	work	so	far.	
16.	Ifneeded,	click	[CloseÂ Editor	](	)and	click	[Run	](	)to	gather	data	for	the	steps	that	will	send	data	to	
the	Transform	operator.	
92 

93
17.	From	the	Application	page,	click	the	gear	icon	(	)on	the	Transform	operator.	The	Transform	Wizard	
pane	for	that	operator	appears:	
18.	As	needed,	click	the	Jinja/UI	toggle	to	switch	between	the	Jinja	code-	only	view	or	the	UI	drag-	and-	drop	
view.	
l	Ifyou	are	using	the	Jinja	code-	only	view,	see	steps	19	to	21.	
l	Ifyou	are	using	the	UI	drag-	and-	drop	view,	see	steps	22	to	26.	
TIP:	Use	the	Search	field	at	the	top	of	the	left-	hand	pane	to	search	for	the	data	you	want	to	transform.	
Use	the	up	and	down	arrows	to	move	through	the	search	results.	
19.	In	the	Jinja	code-	only	view,	on	the	left-	hand	pane	of	the	Transform	Wizard	pane,	you	can	view	all	of	the	
data	gathered	from	the	steps	that	you	selected	to	send	to	the	Transform	operator	in	step	13.	You	will	use	this	
data	to	create	aJinja2	template	on	the	[Jinja	Template	]tab.	
TIP:	The	data	on	the	left-	hand	pane	matches	the	data	you	would	see	ifyou	selected	each	step	
connected	to	the	Transform	operator	and	clicked	the	[StepÂ Data	]tab.	
Working	with	Flow	Control	Operators 

Working	with	Flow	Control	Operators	
20.	On	theÂ 	[Jinja	Template	]tab,	you	can	use	data	from	the	left-	hand	pane	to	create	aJinja2	template	to	
manipulate	and	change	that	data	to	use	in	the	next	step	of	the	application.	A	Jinja2	template	lets	you	create	
complex,	concatenated	(linked)	fields.	For	more	information	about	Jinja2Â Templates,	see	the	
TemplateÂ Designer	Documentation	.	
Example	1	
For	asimple	example	for	astep	that	gathers	specific	data	about	adevice,	you	could	create	the	following	
Jinja2	template	on	theÂ 	[Jinja	Template	]tab:	
{%	set	output	=	
{""sys_	name"":	input	[""name""],	
""sys_	ip"":	input	[""ip""]}	%}	{{output|tojson}}	
TIP:	ClickÂ 	[Check	the	Output	Data	]on	theÂ 	[Jinja	Template	]tab	to	run	the	Jinja2	template	you	
created	on	theÂ 	[Jinja	Template	]tab.	The	results	of	this	process	appear	on	the	[Output	Data	]tab.	
After	running	the	above	Jinja2	template,	you	would	see	the	following	data	on	the	[Output	Data	]tab:	
{
""sys_	ip"":	""10.2.11.154"",	
""sys_	name"":	""pm-	aio-	11-	154""	
}	
Example	2	
For	amore	complicated	example,	you	could	create	the	following	Jinja2Â template	to	gather	and	merge	data	
from	the	""GetIncidents""	and	the	""GetCompletedCRs""	steps:	
{%	set	d=dict	()	%}	{%	set	crupdate	=	
d.update	(	
{'changeRequests':	GetCompletedCRs,	
'correlation':	GetIncidents.0.correlation,	
'device_	id':	GetIncidents.0.events.0.device.id,	
'event_	id':	GetIncidents.0.events.0.event_	id,	
'device_	name':	GetIncidents.0.events.0.device.name,	
'device_	sysid':	GetIncidents.0.events.0.device.sys_	id,	
'found_	cr':	GetIncidents.0.sys_	id	in	GetCompletedCRs,	
'incident_	number':	GetIncidents.0.number,	
'incident_	sys_	id':	GetIncidents.0.sys_	id	})	%}	{{Â 	d|tojson	}}	]	
After	running	the	above	Jinja2	template,	you	would	see	the	following	data	on	the	[Output	Data	]tab:	
{
""changeRequests"":	[],	
""correlation"":	""Bus_	Services_	test+DEV+15+EVENT+3636"",	
""device_	id"":	""15"",	
""device_	name"":	""enagley-	cmdr-	34-	242"",	
""device_	sysid"":	""7b01681edb1df300dc44f00fbf9619e7"",	
""event_	id"":	""12503"",	
""found_	cr"":	false,	
""incident_	number"":	""INC0010104"",	
""incident_	sys_	id"":	""06685154db35b300dc44f00fbf961933""	
}	
94 

95
21.	Ifyou	are	satisfied	with	the	results	of	the	Jinja2	template	theÂ 	[Jinja	Template	]tab	and	the	data	on	the	
[Output	Data	]tab,	go	to	step	27.	
22.	In	the	UI	drag-	and-	drop	view,	search	for	the	value	that	you	want	to	transform	in	the	left-	hand	pane	and	drag	
and	drop	that	value	onto	the	middle	pane.	A	new	value	box	appears:	
You	can	drag	and	drop	add	multiple	values	from	the	left-	hand	pane	onto	the	middle	pane.	PowerFlow	uses	
these	values	to	create	the	Jinja	template	for	this	Transform	step.	You	can	also	drag	and	drop	agroup	of	
values	onto	the	middle	pane	by	clicking	the	blue	oval	at	the	top	of	the	list	of	values.	
TIP:	To	edit	the	name	of	avalue	box	in	the	middle	pane,	click	the	pencil	icon	(	)and	type	anew	
name.	To	see	the	link	between	avalue	from	the	left-	hand	pane	and	the	value	in	the	middle	pane,	
click	the	link	iconÂ 	(	).	
23.	To	delete	avalue	box	from	the	middle	pane,	drag	the	box	toward	the	bottom	of	the	middle	pane.	A	""drop	
here	to	delete""	rectangle	appears,	and	when	you	drag	the	box	into	that	rectangle,	itturns	red.	Drop	the	value	
box	into	the	red	rectangle	to	delete	the	box.	
24.	You	can	create	anew	value	box	by	clicking	[New	Value	]and	typing	aname	for	itin	the	middle	pane.	Then	
you	can	drag	and	drop	data	from	the	left-	hand	pane	into	that	value	box.	
Working	with	Flow	Control	Operators 

Working	with	Flow	Control	Operators	
25.	To	use	aJinja	filter	on	avalue	in	the	middle	pane,	drag	one	or	more	of	the	yellow	filter	ovals	into	the	relevant	
value	box	in	the	middle	pane.	The	user	interface	will	show	a""Not	supported""	message	for	any	filters	that	are	
not	compatible	with	certain	value	types,	such	as	a""capitalize""	filter	with	anumber	value.	For	more	
information	about	Jinja	filters,	see	the	List	of	Built-	in	Filters	in	the	Jinja	documentation	.	
TIP:	Ifyou	add	a""select""	or	a""reject""	filter	to	avalue	box,	additional	fields	will	appear	at	the	bottom	of	
the	middle	pane	when	you	add	one	of	those	filters.	Depending	on	the	filter	you	chose,	type	the	
value	you	want	to	include	or	reject	in	the	Name	field	that	appears,	and	select	String	or	Number	as	
needed.	Click	[Save	]to	finish	configuring	the	filter.	
26.	When	you	are	done	adding	values	from	the	left-	hand	pane	and	adding	filters	from	the	middle	pane	to	the	
various	value	boxes,	clickÂ 	[Check	OutputÂ Data	].The	data	for	the	Jinja2	template	you	just	created	appears	
in	the	[Output	Data	]tab,	while	the	actual	code	for	the	template	appears	in	the	[Jinja	Template	]tab.	
27.	As	needed,	edit	any	other	data	on	the	Transform	Wizard	pane,	and	then	click	[Save	Template	].Â 	
28.	Close	the	Transform	Wizard	pane.	The	Application	page	appears	again.	
NOTE:	You	can	add	more	than	one	Transform	operator	to	the	same	application.	You	can	also	add	
one	or	more	Condition	operators	to	the	same	application.	For	more	information,	see	
Creating	an	Application	with	aCondition	Operator.	
29.	On	theÂ 	Application	detail	page,	add	any	additional	steps	and	operators	to	the	new	application,	and	then	
click	[Save	].Â The	application	is	added	to	the	Applications	page.	
96 

97
E d i ti n g	a	P o w e r F l o w	A p p l i c a ti o n	
On	the	Applications	page	in	the	PowerFlow	user	interface,	you	can	edit	an	existing	application	and	its	steps.Â You	
can	also	add	and	remove	steps	from	that	application.	
NOTE:	You	cannot	overwrite	applications	where	ScienceLogic,	Inc.	is	listed	as	the	Author,	but	you	can	edit	a	
ScienceLogic	application	and	save	itwith	adifferent	name.	
To	edit	an	application:	
1.	From	the	Applications	page,	select	the	application	that	you	want	to	edit.	The	Application	detail	page	
appears.	
2.	Click	the	[Open	Editor	]Â button	(	).	The	PowerFlow	builder	interface	appears,	including	the	Steps	
Registry	pane,	which	contains	alist	of	all	of	the	available	steps:	
Editing	aPowerFlow	Application 

Editing	aPowerFlow	Application	
3.	On	the	Steps	Registry	pane,	you	can	search	for	astep	or	filter	the	list	of	steps	to	help	you	find	the	step	you	
need:	
l	Click	the	[Search	Steps	]tab	(	)Â to	search	the	entire	list	of	steps	from	the	Search	Steps	Registry	field.	
l	Click	the	[GroupÂ Steps	]Â tab	(	)to	group	the	steps	by	Synchronization	PowerPack	,by	atag,	or	to	
show	all	of	the	steps	in	one	list.	
TIP:	Click	the	[Actions	]button	(	)on	astep	in	the	Steps	Registry	pane	to	view	more	information	
about	that	step,	including	the	stepÂ ID,	the	Synchronization	PowerPack	for	that	step,	the	version,	and	
creator	of	the	step.	You	can	also	click	[Edit	Step	Code	]to	edit	the	code	for	that	step.	
4.	To	create	astep,	click	[Create	a	Step	](	)on	the	StepsÂ Registry	pane,	type	afile	name,	and	edit	the	step	
code	for	the	new	step.	
5.	Click	the	step	or	steps	you	want	to	add	from	the	Steps	Registry	pane	and	drag	them	to	the	main	viewing	
pane	of	the	PowerFlow	Builder	page	to	add	them	to	the	application.	Add	any	relevant	information	to	the	
Configuration	pane	for	that	step,	and	click	[Save	]to	close	the	Configuration	pane.	
6.	To	adjust	the	position	of	any	step	in	the	application,	click	the	step	you	want	to	move	and	drag	itto	its	new	
location.	
7.	To	redirect	the	arrow	between	steps,	click	the	arrow	and	drag	itto	reposition	it.	
8.	To	edit	the	configuration	for	astep,	click	the	gear	icon	(	)on	the	step	and	update	the	fields	as	needed	on	
the	Configuration	pane.	
TIP:	You	can	also	right-	click	the	gear	icon	(	)on	astep	to	edit	the	step	(on	the	Configuration	pane	or	
to	delete	the	step.	
9.	To	remove	astep,	click	the	step	to	select	itand	press	the	[Delete	]key	on	your	keyboard.	
10.	To	save	the	changes	you	made	to	the	application,	click	[Save	].Â You	can	also	click	the	Save	as	option	to	save	
the	application	with	anew	name.	
11.	To	stop	editing	and	close	the	Search	Steps	Registry	pane,	click	the	[CloseÂ Editor	]button	(	).	
98 

99
C r e a ti n g	a	S te p	
On	the	Applications	page,	you	can	create	new	steps	using	Python	that	you	can	add	to	new	or	existing	PowerFlow	
applications.
NOTE:	All	Python	step	code	should	be	Python	3.7	or	later.	
To	create	astep:	
1.	From	the	Applications	page	(	),	click	the	down	arrow	next	to	[Create	Application	]and	select	Create	
Step	.A	CreateÂ Step	window	appears.	
TIP:	You	can	also	create	astep	from	an	Application	page	by	clicking	the	[Open	Editor	]button	(	)and	then	
clicking	[Create	Step	](	)on	the	Steps	Registry	pane.	
2.	In	the	File	Name	field,	type	aname	for	the	step.	The	name	cannot	include	spaces,	and	PascalCase,	such	as	
""CacheRead"",	is	recommended.Â Also,	the	file	name	must	match	the	class	name	in	the	Python	code.	
3.	In	the	Edit	step	code	text	box,	add	or	update	the	Python	code	for	the	step	as	needed,	including	the	metadata	
information	for	the	new	step	in	the	def	__init_	_(self):	section.	For	more	information,	see	the	""Creating	
aStep""Â chapter	in	the	PowerFlow	for	Developers	Manual	.	
4.	Click	[Save	].The	step	is	added	to	the	Steps	Registry	pane.	
A l i g n i n g	a	C o n f i g u r a ti o n	O b j e c t	w i th	a n	A p p l i c a ti o n	
Before	you	can	run	aÂ PowerFlow	application,	you	must	align	the	application	with	aconfiguration	object	from	the	
Configurations	page.	A	configuration	object	defines	global	variables,	such	as	endpoints	and	credentials,	that	
can	be	used	by	multiple	steps	and	applications.	Each	variable	in	aconfiguration	object	is	set	up	as	aname	and	
value	pair.	You	can	also	encrypt	avariable	to	protect	sensitive	data	like	apassword.	
You	can	""align""	the	configuration	object	you	want	to	use	with	an	application	from	the	Configuration	pane	for	that	
application.
To	align	aconfiguration	object	with	an	application:	
1.	From	the	Applications	page	(	),	select	the	application	that	you	want	to	align	with	aconfiguration	object.	
The	Application	page	for	that	application	appears.	
Creating	aStep 

Aligning	aConfiguration	Object	with	an	Application	
2.	Click	[Configure	](	).	The	Configuration	pane	opens	on	the	right	side	of	the	Application	page:	
TIP:	To	view	apop-	up	description	of	afield	on	the	Configuration	pane	for	an	application,	hover	over	
the	label	name	for	that	field.	
3.	Select	aconfiguration	from	the	Configuration	drop-	down	to	""align""	to	this	application.	This	step	is	required	
for	all	applications.	
TIP:	You	can	select	none	from	the	Configuration	drop-	down	to	clear	or	""un-	align""Â the	selected	
configuration	object	from	an	application.	Also,	ifyou	did	not	select	aconfiguration	object	when	
editing	fields	on	the	Configuration	pane,	the	previously	set	configuration	object	will	remain	
aligned	(if	there	was	apreviously	set	configuration	object).	
4.	Click	[Show	JSON	Configs	]to	view	the	JSON	configuration	data	for	the	configuration	object.	Click	[Hide	
JSON	Editor	]again	to	view	the	fields	instead.	
5.	As	needed,	edit	the	other	configuration	values	for	the	application.	Press	[Enter	]after	editing	an	item	to	make	
sure	your	changes	are	saved.	
NOTE:	To	prevent	potential	issues	with	security	and	configuration,	any	fields	that	are	encrypted	and	
any	configuration-	specific	fields	containing	apadlock	icon	(	)on	the	Configuration	pane	
cannot	be	edited.	
6.	Click	[Save	]and	wait	for	the	""App	&	Config	modifications	saved""	pop-	up	message	to	appear.Â The	
Configuration	pane	automatically	closes	after	this	message	appears.	
100 

101
R u n n i n g	a	P o w e r F l o w	A p p l i c a ti o n	
You	can	run	an	application	directly	from	the	Applications	page	(the	list	view)	or	from	an	Application	page	(the	
detail	view).	Ifyou	run	the	application	from	the	Application	page,	you	have	the	following	additional	options:	
l	Run	.Executes	the	application	normally,	with	alog	level	of	1.	This	is	the	default,	and	itis	the	same	as	the	Run	
Now	option	from	the	Applications	page.	
l	Debug	Run	.Executes	the	application	in	Debug	Mode	with	alog	level	of	10.	
l	Custom	Run	.Executes	the	application	using	alogging	level	that	you	specify	(Error,	Warning,	Info,	or	
Debug).Â You	can	also	add	any	customer	parameters	that	you	might	want	to	use	to	test	specific	features	in	the	
application.	
To	run	an	application:	
1.	From	the	Applications	page	(	),	click	the	[Actions	]button	(	)for	the	application	you	want	to	run	and	
selectÂ 	Run	Now	.	
TIP:	You	can	also	select	an	application	from	the	Applications	page	and	click	[Run	](	)from	the	
Application	page.	Ifyou	hover	over	the	[Run	]button,	you	can	selectÂ 	Debug	Run	or	Custom	Run	.	
2.	As	the	application	runs,	the	color	of	the	border	around	each	step	represents	whether	itis	running,	is	
successful,	or	has	failed:	
Running	aPowerFlow	Application 

Viewing	Previous	Runs	of	anÂ Application	with	the	Timeline	
Step	Color	Icon	State	
Blue	Running	
Green	Successful	
Red	Failed	
Yellow	Warning	
NOTE:	Pop-	up	status	messages	also	appear	in	the	bottom	left-	hand	corner	of	the	Application	page	to	
update	you	on	the	progress	of	the	application	run.	
3.	Ifastep	triggers	achild	application,	abranch	icon	(	)appears	in	the	upper	right-	hand	corner	of	the	step.	
Double-	click	the	branch	icon	to	open	the	child	application.	Click	the	branch	icon	once	to	display	the	
triggered	application's	run	ID	as	alink	in	apop-	up	window.	Ifno	run	IDÂ is	present,	the	branch	icon	displays	
""NONE"".	
V i e w i n g	P r e v i o u s	R u n s	o f	a n Â A p p l i c a ti o n	w i th	th e	T i m e l i n e	
The	Application	page	for	aselected	application	contains	a[Timeline	]button	(	)that	displays	ahistory	of	
previous	runs	of	that	application.	
Also,	you	can	click	[Replay	](	)to	run	that	application	again,	such	as	when	an	application	failed.	
102 

103
To	view	and	filter	the	Timeline:	
1.	From	an	Application	page,	click	[Timeline	](	).	The	Timeline	displays	above	the	steps	for	that	
application:	
2.	The	default	view	for	the	Timeline	shows	the	last	two	hours	of	runs	for	that	application.	The	image	above	
shows
the	last	four	hours	of	runs.	Use	the	left	arrow	icon	(	)and	the	right	arrow	icon	(	)to	move	through	the	
Timeline	in	15-	minute	increments:	
NOTE:	The	Timeline	displays	colored	dots	at	aspecific	time	that	represent	the	last	time	this	application	
was	run.	A	green	icon	means	arun	was	successful,	and	ared	icon	means	that	arun	failed.Â 	
Viewing	Previous	Runs	of	anÂ Application	with	the	Timeline 

Viewing	Previous	Runs	of	anÂ Application	with	the	Timeline	
3.	You	can	hover	over	or	click	an	icon	for	arun	on	the	Timeline	to	view	apop-	up	window	that	displays	the	run	
ID	and	the	configuration	and	queue	used	for	that	run:	
TIP:	Click	the	link	for	the	run	ID	or	click	View	Run	to	open	the	Application	page	for	that	specific	run	of	
the	application.	The	run	ID	also	displays	in	the	StepÂ Log	pane	for	the	""triggering""	step	for	that	
application,	and	italso	appears	at	the	end	of	the	URL	for	that	application.	On	that	page,	you	can	
select	astep	and	open	the	Step	Log	to	view	any	issues.	
4.	Click	the	Filter	icon	(	)to	filter	or	search	the	list	of	previous	runs	for	this	application.	AÂ 	Filter	Â window	
appears:	
104 

105	
5.	Edit	the	following	fields	on	the	Filter	window	as	needed:	
l	Date	.The	date	for	the	history	of	previous	runs	you	want	to	view.	Click	the	field	to	open	apop-	up	
calendar.	
l	Start	Time	.The	starting	time	for	the	history,	using	local	time	instead	of	UTC	time.	Click	the	field	to	
open	apop-	up	time	selector.	
l	Window	Size	.The	length	of	the	history,	in	hours.	The	default	history	view	for	the	Timeline	is	two	
hours.	
l	Run	State	.Select	the	type	of	previous	runs	you	want	to	view.Â Your	options	include	all	,success	,failure	,	
and	pending	.The	default	is	all	.	
l	Configuration	.Select	aconfiguration	file	to	filter	for	application	runs	using	that	configuration	only.	
l	Queue	.Type	aqueue	name	to	filter	for	application	runs	that	use	that	queue.	
l	UTC	Time	.Select	UTC	ifyou	do	not	want	to	use	local	time.	The	Schedule	feature	uses	UTC	time.	
6.	Click	[Search	]to	run	the	filter	or	search.	
TIP:	Ifthe	Timeline	is	open	and	you	want	to	close	it,	click	the	[Timeline	]button	(	).	
S c h e d u l i n g	a	P o w e r F l o w	A p p l i c a ti o n	
You	can	create	one	or	more	schedules	for	asingle	application	in	the	PowerFlow	user	interface.	When	creating	
each	schedule,	you	can	specify	the	queue	and	the	configuration	file	for	that	application.	
Scheduling	aPowerFlow	Application 

Scheduling	aPowerFlow	Application	
To	schedule	an	application:	
1.	On	the	Applications	page	(	),	click	the	[Schedule	]button	for	the	application	you	want	to	schedule.	The	
Schedule	window	appears,	displaying	any	existing	schedules	for	that	application:	
NOTE:	Ifyou	set	up	aschedule	using	acron	expression,	the	details	of	that	schedule	display	in	amore	
readable	format	in	this	list.	For	example,	ifyou	set	up	acron	expression	of	*/4	****,the	
schedule	on	this	window	includes	the	cron	expression	along	with	an	explanation	of	that	
expression:Â ""	Every	0,	4,	8,	12,	16,	20,	24,	28,	32,	36,	40,	44,	48,	52,	and	56th	minute	past	
every	hour	"".	
106 

107	
2.	Select	aschedule	from	the	list	to	view	the	details	for	that	schedule.	
3.	Click	the	+	icon	to	create	aschedule.	A	blank	Schedule	window	appears:	
4.	In	the	Schedule	window,	complete	the	following	fields:	
l	Schedule	Name	.Type	aname	for	the	schedule.	
l	Switch	to	.Use	this	toggle	to	switch	between	acron	expression	and	setting	the	frequency	in	seconds.	
o	Cron	expression	.Select	this	option	to	schedule	the	application	using	acron	expression.Â If	you	
select	this	option,	you	can	create	complicated	schedules	based	on	minutes,	hours,	the	day	of	the	
month,	the	month,	and	the	day	of	the	week.Â As	you	update	the	cron	expression,	the	Schedule	
window	displays	the	results	of	the	expression	in	more	readable	language,	such	as	Expression:	
""Every	0and	30th	minute	past	every	hour	on	the	1and	31st	of	every	month""	,based	on	*/30	**	
/30	**.	
o	Frequency	in	seconds	.Type	the	number	of	seconds	per	interval	that	you	want	to	run	the	
application.	
l	Custom	Parameters	.Type	any	JSON	parameters	you	want	to	use	for	this	schedule,	such	as	
information	about	aconfiguration	file	or	mappings.	
5.	Click	[Save	Schedule	].The	schedule	is	added	to	the	list	of	schedules	on	the	initial	Schedule	window.	Also,	
on	the	Applications	page,	the	word	""Scheduled""	appears	in	the	Scheduled	column	for	this	application,	and	
the	[Schedule	]button	contains	acheck	mark:	
Scheduling	aPowerFlow	Application 

Viewing	PowerFlowÂ System	Diagnostics	
NOTE:	After	you	create	aschedule,	itcontinues	to	run	until	you	delete	it.Â Also,	you	cannot	edit	an	existing	
schedule,	but	you	can	delete	itand	create	asimilar	schedule	ifneeded.	
To	view	or	delete	an	existing	schedule:	
1.	On	the	Applications	page,	click	the	[Schedule	]button	for	the	application	that	contains	aschedule	you	want	
to	delete.	The	Schedule	window	appears.	
2.	Click	the	down	arrow	icon	(	)to	view	the	details	of	an	existing	schedule:	
3.	To	delete	the	selected	schedule,	click	[Delete	].Â The	schedule	is	removed.	
V i e w i n g	P o w e r F l o w	Â S y s te m	D i a g n o s ti c s	
The	""PowerFlow	System	Diagnostics""	application	lets	you	view	platform	diagnostics	for	the	PowerFlow	system	.You	
can	use	the	information	displayed	in	these	diagnostics	to	help	you	troubleshoot	issues	with	the	different	tools	used	
by	PowerFlow	.	
NOTE:	Older	versions	of	this	application	are	named	""Integration	ServiceÂ SystemÂ Diagnostics"".	
To	view	the	diagnostics	report:	
1.	From	theÂ 	Applications	page,	select	the	""PowerFlow	System	Diagnostics""	application.	TheÂ 	Application	page	
appears.	
108 

109	
2.	Click	[Configure	](	).	The	Configuration	pane	appears:	
3.	Complete	the	following	fields:	
l	Configuration	.Select	aconfiguration	to	align	with	this	application.	The	""IS	-System	Diagnostic	
Configuration	Example""	configuration	object	contains	the	structure	needed	for	this	application,	and	
you	can	use	that	configuration	object	as	atemplate.	Be	sure	to	update	the	configuration	object	with	
values	for	is_	swarm_	manager	,is_	username	,and	is_	password	.	
l	device_	sync_	app	.Â Specify	the	name	of	the	Incident	Creation	application	that	contains	the	relevant	
device	mappings.	
l	incident_	create_	app	.Specify	the	name	of	the	Device	Sync	application	that	contains	information	
about	the	incidents	that	have	been	created.	
l	api_	alias	.Specify	the	alias	to	reference	the	APIÂ internally	to	make	calls.	
4.	Click	[Save	]and	wait	for	the	""App	&	Config	modifications	saved""	pop-	up	message	to	appear.Â The	
Configuration	pane	automatically	closes	after	this	message	appears.	
Viewing	PowerFlowÂ System	Diagnostics 

Backing	up	Data	
5.	On	the	Application	page,	click	[Run	](	).	The	application	generates	areport	that	you	can	access	on	the	
Reports	page	(	):	
This	diagnostic	report	displays	overall	PowerFlow	settings,	such	as	the	PowerFlow	version,	Docker	version,	
kernel	version,	hostname,	cluster	settings,	scheduled	applications,	CPU	and	memory	statistics,	installation	
date,	and	cache	information.	
B a c k i n g	u p	D a ta	
You	can	use	PowerFlow	to	back	up	and	recover	data	in	the	Couchbase	database.	
This	option	uses	the	""PowerFlow	Backup""	application	in	the	PowerFlow	user	interface	to	create	abackup	file	and	
send	that	file	using	secure	copy	protocol	(SCP)	to	adestination	system.	You	can	then	use	the	""PowerFlow	Restore""	
application	to	get	abackup	file	from	the	remote	system	and	restore	its	content.	
110 

111
C r e a t i n g	a	B a c k u p	
To	create	abackup:	
1.	To	add	the	relevant	configuration	information,	go	to	the	Configurations	page	(	),	click	the	[Actions	]	
button	(	)for	the	""IS	-System	Backup	Configuration	Example"",	and	select	Edit	.The	Configuration	pane	for	
that	configuration	object	appears:	
Backing	up	Data 

Backing	up	Data	
2.	Click	[Copy	as	]and	provide	values	for	the	following	fields:	
l	backup_	destination	.The	location	where	the	backup	file	is	created.	
l	remote_	host	.The	hostname	for	the	remote	location	where	you	want	to	send	the	backup	file	via	SCP	
from	the	backup_	destination	location.	
l	remote_	user	.The	user	login	for	the	remote	location.	
l	remote_	password	.The	user	password	for	the	remote	location.	Encrypt	this	value.	
l	remote_	destination	.The	remote	location	where	the	application	will	send	the	backup	file.	
3.	Click	[Save	].	
4.	Go	to	the	Applications	page	and	select	the	""PowerFlow	Backup""	application.	The	Application	page	
appears.	
5.	Click	[Configure	](	).	The	Configuration	pane	appears:	
112 

113	
6.	In	the	configuration	file,	provide	values	for	the	following	fields:	
l	Configuration	.Select	aconfiguration	object	you	created	in	steps	1-	3.	
l	cluster_	node	.Specify	the	node	from	which	you	want	to	make	the	backup	(for	asingle-	node	backup	
only).	
l	single_	node	.Select	this	option	ifyou	want	to	back	up	from	asingle	node	in	the	cluster.	Specify	the	
node	in	the	cluster_	node	field.	
l	bucket	.Select	which	bucket	in	Couchbase	you	want	to	back	up.	Your	options	include	all	buckets,	
content-	only	buckets,	or	logs.	The	default	is	all	.	
l	document_	key	:Select	whether	you	want	to	back	up	all	record	or	CI	and	device	cache	records.	
l	data_	only	.Select	this	option	ifyou	only	want	to	restore	bucket	data.	
l	compress	.Select	this	option	to	compress	backups	using	Gzip.	
l	include_	syncpacks	.Select	this	option	ifyou	want	to	back	up	the	Synchronization	PowerPacks	on	the	
PowerFlow	system	.	
l	installed_	only	.Select	this	option	ifyou	want	to	back	up	only	Synchronization	PowerPacks	that	have	
been	installed.	Ifyou	do	not	select	this	option,	the	application	will	also	back	upÂ 	Synchronization	
PowerPacks	that	have	been	uploaded	to	the	PowerFlow	system	.	
NOTE:	The	options	you	select	affect	the	name	of	the	backup	file	that	this	application	generates.	
For	example,	is_	couchbase_	backup-	data_	only-	2019-	04-	01T185527Z.tar	is	a	
uncompressed	data	only	backup,	while	is_	couchbase_	backup-	data_	only-	cache-	
logs-	couchbase.isnet-	2019-	04-	01T185937Z.tar.gz	is	acompressed	data-	only	
backup	of	the	CI	and	device	cache	from	the	couchbase.isnet	node	in	acluster.	
7.	Click	[Save	]and	wait	for	the	""App	&	Config	modifications	saved""	pop-	up	message	to	appear.Â The	
Configuration	pane	automatically	closes	after	this	message	appears.	
8.	On	the	Application	page,	click	[Run	Now	](	).	When	the	application	completes,	afile	named	""is_	
couchbase_	backup-	<date>	.tar""	is	added	to	the	remote	server	in	the	specified	remote	backup	destination.	
9.	To	ensure	that	the	backup	was	created,	select	the	""Create	ISÂ Backup""	step	and	open	the	Step	Log	section.	
10.	Look	for	entries	related	to	backup	and	make	anote	of	the	of	the	backup	file	name,	which	you	will	need	when	
you	run	the	""PowerFlow	Restore""	application:	
Backing	up	Data 

Backing	up	Data	
TIP:	You	can	schedule	the	""PowerFlow	Backup""	application	to	run	on	aregular	basis,	or	you	can	run	the	
application	as	needed.Â To	schedule	the	application,	click	the	[Schedule	]button	for	the	""PowerFlow	
Backup""	application	on	theÂ 	Applications	page.	For	more	information,	see	Scheduling	a	PowerFlow	
Application	.	
R e s t or i n g	a	B a c k u p	
After	you	have	created	abackup	using	the	""PowerFlow	Backup""	application	in	the	PowerFlow	user	interface,	you	
can	use	the	""PowerFlow	Restore""	application	to	restore	that	file.	
NOTE:	Do	not	restore	the	PowerFlow	backup	to	asystem	that	uses	adifferent	encryption	key.	
114 

115
To	restore	abackup:	
1.	In	the	PowerFlow	user	interface,	go	to	the	Applications	page	and	select	the	PowerFlow	Restore	
application.	The	Application	page	appears.	
2.	Click	[Configure	](	).	The	Configuration	pane	appears:	
3.	In	the	Configuration	pane,	provide	values	for	the	following	fields:	
l	Configuration	.Select	the	same	configuration	object	you	aligned	with	the	""PowerFlow	Backup""	
application.	Required.	
l	remote_	destination	.Type	the	name	of	the	backup	file	created	by	the	""PowerFlow	Backup""	
application.	
l	data_	only	.Select	this	option	ifyou	only	want	to	restore	bucket	data.	
l	include_	syncpacks	.Select	this	option	ifyou	want	to	restore	the	Synchronization	PowerPacks	you	
backed	up	with	the	""PowerFlow	Backup""	application.	
l	force_	syncpack_	upload	.Select	this	option	ifyou	want	to	force	upload	Synchronization	PowerPacks	if	
the	files	already	exist	in	the	PowerFlow	system	.	
Backing	up	Data 

Backing	up	Data	
4.	Click	[Save	]and	wait	for	the	""App	&	Config	modifications	saved""	pop-	up	message	to	appear.Â The	
Configuration	pane	automatically	closes	after	this	message	appears.	
5.	On	the	Application	page,	click	[Run	Now	](	).	
6.	To	ensure	that	the	backup	was	restored,	click	to	open	the	Step	Log	section	and	look	for	entries	related	to	
restoring	the	backup:	
NOTE:	After	running	the	""PowerFlow	Restore""	application,	the	""PowerFlow	Backup""	application	might	display	
as	""Run	status	pending"".	This	issue	occurs	because	at	the	time	of	the	last	backup	from	Couchbase,	the	
logs	for	the	""PowerFlow	Backup""	application	showed	apending	state.	This	message	is	addressed	
during	the	next	run,	and	itdoes	not	cause	any	issues	with	the	backup	or	restore	processes.	
116 

117
V i e w i n g	a	R e p o r t	f o r	a	P o w e r F l o w	A p p l i c a ti o n	
In	the	PowerFlow	user	interface,	the	Reports	page	(	)contains	alist	of	reports	associated	with	applications.	Ifan	
application	has	the	reporting	feature	enabled	and	the	application	supports	reports,	PowerFlow	will	generate	a	
report	each	time	you	run	the	application.	
An	individual	report	displays	data	only	from	the	most	recent	run	of	the	application;	areport	is	not	an	aggregation	of	
all	previous	runs.	
You	can	search	for	aspecific	report	by	typing	the	name	of	that	report	in	the	Search	field	at	the	top	of	the	Reports	
page.Â The	user	interface	filters	the	list	as	you	type.	
NOTE	:Not	all	applications	generate	reports.	Currently,	only	the	""PowerFlow	System	Diagnostics""	applications	
support	the	generation	of	reports.	
Viewing	aReport	for	aPowerFlow	Application 

Viewing	aReport	for	aPowerFlow	Application	
To	view	details	for	an	application	report:	
1.	On	the	Reports	page	(	),	click	the	name	of	the	application	to	expand	the	list	of	reports	for	that	application.	
TIP:	Click	the	arrow	buttons	to	scroll	forward	and	back	through	the	list	of	reports.	
2.	Click	areport	name	in	the	Report	ID	column.	The	ReportÂ Details	page	appears:	
TIP:	The	Status	field	in	areport	displays	the	current	state	of	the	synced	item,	which	can	include	
New	,Â Removed	,Updated	,or	Unchanged	.	
3.	To	view	the	detail	page	for	the	application	on	the	Applications	page,	click	the	Application	Name	link.	
TIP:	From	the	detail	page	for	the	application,	click	[Reports	](	)to	return	to	the	Reports	page.	
4.	To	delete	areport,	open	the	report	and	click	[Delete	].Â ClickÂ 	[OK	]to	delete	the	report.	
118 

Chapte r	
6	
Managing	Configuration	Objects	
O v e r v i e w
This	chapter	describes	how	to	use	the	Configurations	page	(	)to	create	or	use	aconfiguration	object	that	
contains	aset	of	variables	that	all	steps	and	applications	can	use.	
This	chapter	covers	the	following	topics:	
What	is	a	Configuration	Object?	120	
Creating	a	Configuration	Object	122	
Editing	a	Configuration	Object	125
119 

120
W h a t	i s	a	C o n f i g u r a ti o n	O b j e c t?	
A	configuration	object	is	astand-	alone	JSON	file	that	lives	on	the	PowerFlow	system	.A	configuration	object	
supplies	the	login	credentials	and	other	global	variables	that	can	be	used	by	all	steps	and	applications	in	
PowerFlow	.Configuration	objects	allow	the	same	application	to	be	deployed	in	multiple	PowerFlow	instances,	
with	different	configurations.	
Configuration	objects	can	map	variables	from	SL1	to	athird-	party	platform.	For	instance,	SL1	has	device	classes	
and	various	third-	party	platforms	have	CI	classes;	the	configuration	object	would	map	these	two	variables.	
You	can	create	and	edit	configuration	objects	on	the	Configurations	page	of	the	PowerFlow	user	interface.	After	
you	create	the	configuration	object,	itappears	in	the	Configuration	drop-	down	on	the	Configuration	pane	of	
the	Applications	page.	Before	you	can	run	an	application,	you	must	select	aconfiguration	object	and	""align""	that	
configuration	object	with	the	application.	
TIP:	You	can	select	none	from	the	Configuration	drop-	down	to	clear	or	""un-	align""Â the	selected	configuration	
object	from	an	application.	Also,	ifyou	did	not	select	aconfiguration	object	when	editing	fields	on	the	
Configuration	pane,	the	previously	set	configuration	object	will	remain	aligned	(if	there	was	apreviously	
set	configuration	object).	
You	can	include	the	config.	prefix	with	avariable	to	tell	PowerFlow	to	use	aconfiguration	file	to	resolve	the	
variable.	Ifyou	want	to	re-	use	the	same	settings	between	applications,	such	as	hostname	and	credentials,	define	
configuration	variables.	
What	is	aConfiguration	Object? 

What	is	aConfiguration	Object?	
The	Configurations	page	(	)displays	alist	of	available	configurations.	From	this	page	you	can	create	and	edit	
configuration	objects:	
You	can	search	for	aspecific	configuration	object	by	typing	the	name	of	that	configuration	in	the	Search	field	at	the	
top	of	the	Configurations	page.Â The	user	interface	filters	the	list	as	you	type.	Click	the	[Actions	]button	(	)for	a	
configuration	object	to	edit	or	delete	that	object.	
TIP:	Click	the	down	arrow	icon	(	)for	aconfiguration	object	to	see	which	applications	are	currently	
""aligned""	with	that	configuration	object.	
121 

122
C r e a ti n g	a	C o n f i g u r a ti o n	O b j e c t	
When	creating	or	editing	aconfiguration	object	on	the	Configurations	page,	the	name-	value	pairs	in	the	
Configuration	Data	section	display	in	fields	by	default	instead	of	ablock	of	JSON	code.	For	more	complex	
configuration	objects,	you	can	click	[Toggle	JSON	Editor	]to	switch	between	text	fields	and	JSON	code	for	the	
configuration	data.	
To	create	anew	configuration	object:	
1.	From	the	Configurations	page	(	),	click	[Create	Configuration	].The	Create	Configuration	pane	
appears:	
TIP:	Instead	of	creating	acompletely	new	configuration	object,	you	can	also	edit	an	existing	
configuration	object	that	has	some	of	the	configuration	data	that	you	want	to	use	and	click	[Copy	
as	]from	the	Configuration	pane	to	create	acopy	of	that	configuration	object.	
Creating	aConfiguration	Object 

Creating	aConfiguration	Object	
2.	Complete	the	following	fields:	
l	Friendly	Name	.Name	of	the	configuration	object	that	will	display	in	the	user	interface.	
l	Description	.A	brief	description	of	the	configuration	object.	
l	Author	.User	or	organization	that	created	the	configuration	object.	
l	Version	.Version	of	the	configuration	object.	
l	Configuration	Data	Values	.To	add	configuration	values	in	the	form	of	name-	value	pairs,	click	
[AddÂ Value	].Â Complete	the	Name	and	Value	fields,	and	select	Encrypted	ifneeded.	
TIP:	Click	[Toggle	JSON	Editor	]to	view	the	JSON	configuration	data	for	the	configuration	
object	at	the	bottom	of	the	pane.	Click	[Toggle	JSON	Editor	]again	to	view	the	
Configuration	Data	Values	section	with	the	[Add	Value	]button	instead.	
123 

124	
3.	In	the	Configuration	Data	section,	include	the	required	block	of	code	to	ensure	that	the	applications	
aligned	to	this	configuration	object	do	not	fail:	
{	
""encrypted"":	false,	
""name"":	""sl1_	db_	host	"",	
""value"":	""${	config.sl1_	host	}""	
}	
NOTE:	Ifyou	are	using	SL1	with	an	External	Database	(SL1	Extended	architecture	or	acloud-	based	
architecture),	update	the	""value""	of	that	block	of	code	to	be	the	host	of	your	database.	This	field	
accepts	IP	addresses.	For	example:	""value"":	""db.sciencelogic.com""	.Ifyou	are	not	using	the	
SL1	Extended	architecture	or	acloud-	based	architecture,	you	do	not	need	to	make	any	changes	to	
the	block	of	code	other	than	pasting	the	code	into	the	configuration	object.	
4.	Click	[Toggle	JSON	Editor	]to	view	the	JSON	configuration	data.	In	the	Configuration	Data	field,	update	
the	default	variable	definitions	to	match	your	PowerFlow	configuration.	For	example,	you	could	add	the	
following	JSON	code	to	the	Configuration	Data	field:	
[	
{	
""encrypted"":	false,	
""name"":	""sl1_	db_	host"",	
""value"":	""10.2.11.42""	
},
{	
""encrypted"":	false,	
""name"":	""em7_	user"",	
""value"":	""em7admin""	
},
{	
""encrypted"":	true,	
""name"":	""em7_	password"",	
""value"":	""+dqGJe1NwTyvdaO2EizTWjJ2uj2C1wzBzgNqVhpdTHA=""	
}	
]	
5.	When	creating	anew	configuration	variable,	note	the	syntax:	
l	The	configuration	file	is	surrounded	by	square	brackets.	
l	Each	variable	definition	is	surrounded	by	curly	braces.	
l	Each	key	name	is	surrounded	by	double-	quotes	and	followed	by	acolon,	while	each	value	is	
surrounded	by	double-	quotes	and	followed	by	acomma.	
l	Each	key:value	pair	in	the	definition	is	separated	with	acomma	after	the	closing	curly	brace.	The	last	
key:value	pair	should	not	include	acomma.	
Creating	aConfiguration	Object 

Editing	aConfiguration	Object	
6.	To	create	aconfiguration	variable,	define	the	following	keys:	
l	encrypted	.Specifies	whether	the	value	will	appear	in	plain	text	or	encrypted	in	this	JSON	file.	Ifyou	
set	this	to	""true"",	when	the	value	is	uploaded,	PowerFlow	encrypts	the	value	of	the	variable.	The	plain	
text	value	cannot	be	retrieved	again	by	an	end	user.	The	encryption	key	is	unique	to	each	PowerFlow	
system	.The	value	is	followed	by	acomma.	
l	name	.Specifies	the	name	of	the	configuration	file,	without	the	JSON	suffix.	This	value	appears	in	the	
user	interface.	The	value	is	surrounded	by	double-	quotes	and	followed	by	acomma.	
l	value	.Specifies	the	value	to	assign	to	the	variable.	The	value	is	surrounded	by	double-	quotes	and	
followed	by	acomma.	
7.	Click	[Save	],or	click	[Copy	as	]to	save	the	configuration	object	as	anew	configuration	object	with	adifferent	
name.	
8.	Align	this	configuration	object	with	the	applications	that	you	want	to	run	by	clicking	the	Configure	button	
from	the	detail	page	for	each	application	and	selecting	this	configuration	object	from	the	Configuration	
drop-	down.	
NOTE	:In	astep,	you	can	include	the	config.	prefix	with	avariable	to	tell	the	PowerFlow	system	to	look	in	a	
configuration	object	to	resolve	the	variable.	
E d i ti n g	a	C o n f i g u r a ti o n	O b j e c t	
To	edit	an	existing	configuration	object:	
1.	Go	to	the	Configurations	page.	
125 

126	
2.	Click	the	[Actions	]button	(	)for	the	configuration	object	you	want	to	edit	and	select	Edit	.The	
Configuration	pane	appears:	
3.	Edit	the	values	in	the	following	fields	as	needed:	
l	Description	.A	brief	description	of	the	configuration	object.	
l	Version	.Version	of	the	configuration	object.	
l	Configuration	Data	Values	.To	add	configuration	values	in	the	form	of	name-	value	pairs,	click	
[AddÂ Value	].Â Complete	the	Name	and	Value	fields,	and	select	Encrypted	ifneeded.	
TIP:	Click	[Toggle	JSON	Editor	]to	view	the	JSON	configuration	data	for	the	configuration	object	at	the	
bottom	of	the	pane.	Click	[Toggle	JSON	Editor	]again	to	view	the	Configuration	Data	Values	section	
with	the	[Add	Value	]button	instead.	
4.	To	make	acopy	of	this	configuration	object,	click	[Copy	As	]and	update	the	relevant	fields	in	the	Create	
Configuration	pane.	
5.	Click	[Save	]to	save	your	changes.	
Editing	aConfiguration	Object 

Chapte r	
7	
Viewing	Logs	in	SL1	PowerFlow	
O v e r v i e w
This	chapter	describes	the	different	types	of	logging	available	in	PowerFlow	.	
This	chapter	covers	the	following	topics:	
Logging	Data	in	PowerFlow	128	
Logging	Configuration	129	
PowerFlow	Log	Files	129	
Working	with	Log	Files	130	
Viewing	the	Step	Logs	and	Step	Data	for	a	PowerFlow	Application	132	
Removing	Logs	on	a	Regular	Schedule	133
127 

128
L o g g i n g	D a ta	i n	P o w e r F l o w	
PowerFlow	allows	you	to	view	log	data	locally,	remotely,	or	through	Docker.	
L oc a l	L og g i n g	
PowerFlow	writes	logs	to	files	on	ahost	system	directory.	Each	of	the	main	components,	such	as	the	process	
manager	or	Celery	workers,	and	each	application	that	is	run	on	the	platform,	generates	alog	file.	The	application	
log	files	use	the	application	name	for	easy	consumption	and	location.	
In	aclustered	environment,	the	logs	must	be	written	to	the	same	volume	or	disk	being	used	for	persistent	storage.	
This	ensures	that	all	logs	are	gathered	from	all	hosts	in	the	cluster	onto	asingle	disk,	and	that	each	application	log	
can	contain	information	from	separately	located,	disparate	workers.	
You	can	also	implement	log	features	such	as	rolling,	standard	out,	level,	and	location	setting,	and	you	can	
configure	these	features	with	their	corresponding	environment	variable	or	setting	in	aconfiguration	file.	
NOTE:	Although	itis	possible	to	write	logs	to	file	on	the	host	for	persistent	debug	logging,	as	abest	practice,	
ScienceLogic	recommends	that	you	utilize	alogging	driver	and	write	out	the	container	logs	
somewhere	else.	
TIP:	You	can	use	the	""Timed	Removal""	application	in	the	PowerFlow	user	interface	to	remove	logs	from	
Couchbase	on	aregular	schedule.	On	the	Configuration	pane	for	that	application,	specify	the	number	
of	days	before	the	next	log	removal.	
R e m ot e	L og g i n g	
Ifyou	use	your	own	existing	logging	server,	such	as	Syslog,	Splunk,	or	Logstash,	PowerFlow	can	route	its	logs	to	a	
customer-	specified	location.	To	do	so,	attach	your	service,	such	as	logspout,	to	the	microservice	stack	and	
configure	your	service	to	route	all	logs	to	the	server	of	your	choice.	
CAUTION:	Although	PowerFlow	supports	logging	to	these	remote	systems,	ScienceLogic	does	not	officially	
own	or	support	the	configuration	of	the	remote	logging	locations.	Use	the	logging	to	aremote	
system	feature	at	your	own	discretion.	
V i e w i n g	L og s	i n	D oc k e r	
You	can	use	the	Docker	command	line	to	view	the	logs	of	any	current	running	service	in	the	PowerFlow	cluster.	To	
view	the	logs	of	any	service,	run	the	following	command:	
docker	service	logs	-f	iservices_	<service_	name	>	
Logging	Data	in	PowerFlow 

Logging	Configuration	
Some	common	examples	include	the	following:	
docker	service	logs	âf	iservices_	couchbase	
docker	service	logs	âf	iservices_	steprunner	
docker	service	logs	âf	iservices_	contentapi	
NOTE:	Application	logs	are	stored	on	the	central	database	as	well	as	on	all	of	the	Docker	hosts	in	aclustered	
environment.	These	logs	are	stored	at	/var/log/iservices	for	both	single-	node	or	clustered	
environments.	However,	the	logs	on	each	Docker	host	only	relate	to	the	services	running	on	that	host.	
For	this	reason,	using	the	Docker	service	logs	is	the	best	way	to	get	logs	from	all	hosts	at	once.	
L o g g i n g	C o n f i g u r a ti o n	
The	following	table	describes	the	variables	and	configuration	settings	related	to	logging	in	PowerFlow	:	
Environment	Variable/Config	Setting	Description	Default	Setting	
logdir	The	directory	to	which	logs	will	
be	written.	
/var/log/iservices	
stdoutlog	Whether	logs	should	be	written	
to	standard	output	(stdout).	
True	
loglevel	Log	level	setting	for	PowerFlow	
application	modules.	
debug/info	(varies	between	
development	and	product	
environments)	
celery_	log_	level	The	log	level	for	Celery-	related	
components	and	modules.	
debug/info	(varies	between	
development	and	product	
environments)	
P o w e r F l o w	L o g	F i l e s	
In	PowerFlow	version	2.1.0	and	later,	additional	logging	options	were	added	for	gui	,api	,and	rabbitmq	services.	
The	logs	from	aservice	are	available	in	the	/var/log/iservices	directory	on	which	that	particular	service	is	
running.
To	aggregate	logs	for	the	entire	cluster,	ScienceLogic	recommends	that	you	use	atool	like	Docker	
Syslog:Â 	https://docs.docker.com/config/containers/logging/syslog/	.	
129 

130
L og s	f or	t h e	g u i	S e r v i c e	
By	default	all	nginx	logs	are	written	to	stdout.	Although	not	enabled	by	default,	you	can	also	choose	to	write	access	
and	error	logs	to	file	in	addition	to	stdout.	
To	log	to	apersistent	file,	simply	mount	afile	to	/host/directory:/var/log/nginx	,which	will	by	default	log	both	
access	and	error	logs.	
For	pypiserver	logs,	the	logs	are	not	persisted	to	disk	by	default.	You	can	choose	to	persist	pypiserver	logs	by	setting	
the	log_	to_	file	environment	variable	to	true.	
Ifyou	choose	to	persist	logs,	you	should	mount	ahost	volume	to	/var/log/devpi	to	access	logs	from	ahos.	For	
example:Â 	/var/log/iservices/devpi:/var/log/devpi	.	
L og s	f or	t h e	a p i	S e r v i c e	
By	default	all	log	messages	related	to	PowerFlow	are	written	out	to	/var/log/iservices/contentapi	.	
PowerFlow	-specific	logging	is	controlled	by	existing	settings	listed	in	Logging	Configuration	.Although	not	
enabled	by	default,	you	can	also	choose	to	write	nginx	access	and	error	logs	to	file	in	addition	to	stdout.	
To	log	to	apersistent	file,	simply	mount	afile	to	/file/location/host:/var/log/nginx/nginx_	error.log	or	
/host/directory:/var/log/nginx	depending	ifyou	want	access	or	error	logs.	
L og s	f or	t h e	r a b b i t m q	S e r v i c e	
Logs	for	the	rabbitmq	service	were	not	previously	persisted,	but	with	PowerFlow	2.1.0,	all	rabbitmq	service	logs	
are	written	to	stdout	by	default.	You	can	choose	write	to	alogfile	(stdout	or	logfile,	not	both).	
To	write	to	the	logfile:	
1.	Add	the	following	environment	variable	for	the	rabbitmq	service:	
RABBITMQ_	LOGS:	""/var/log/rabbitmq/rabbit.log""	
2.	Mount	avolume:	/var/log/iservices/rabbit:/var/log/rabbitmq	
The	retention	policy	of	these	logs	is	10	MB,	for	atotal	of	five	maximum	logs	written	to	file.	
W o r k i n g	w i th	L o g	F i l e s	
Use	the	following	procedures	to	help	you	locate	and	understand	the	contents	of	the	various	log	files	related	to	
PowerFlow	.	
A c c e s s i n g	D oc k e r Â L og	F i l e s	
The	Docker	log	files	contain	information	logged	by	all	containers	participating	in	PowerFlow	.The	information	
below	is	also	available	in	the	PowerPacks	listed	above.	
Working	with	Log	Files 

Working	with	Log	Files	
To	access	Docker	log	files:	
1.	Use	SSH	to	connect	to	the	PowerFlow	instance.	
2.	Run	the	following	Docker	command:	
docker	service	ls	
3.	Note	the	Docker	service	name,	which	you	will	use	for	the	<service_	name>	value	in	the	next	step.	
4.	Run	the	following	Docker	command:	
docker	service	logs	-f	<service_	name>	
A c c e s s i n g	L oc a l	F i l e	S y s t e m	L og s	
The	local	file	system	logs	display	the	same	information	as	the	Docker	log	files.	These	log	files	include	debug	
information	for	all	of	the	PowerFlow	applications	and	all	of	the	Celery	worker	nodes.	
To	access	local	file	system	logs:	
1.	Use	SSH	to	connect	to	the	PowerFlow	instance.	
2.	Navigate	to	the	/var/log/iservices	directory	to	view	the	log	files.	
U n d e r s t a n d i n g	t h e	C on t e n t s	of	L og	F i l e s	
The	pattern	of	deciphering	log	messages	applies	to	both	Docker	logs	and	local	log	files,	as	these	logs	display	the	
same	information.	
The	following	is	an	example	of	amessage	in	aDocker	log	or	alocal	file	system	log:	
""2018-	11-	05	19:02:28,312"",""FLOW"",""12"",""device_	sync_	sciencelogic_	to_	servicenow"",""ipaas_	
logger"",""142"",""stop	Query	and	Cache	ServiceNow	CIs|41.4114570618""	
You	can	parse	this	data	in	the	following	manner:	
'YYYY-	MM-	DD'	'HH-	MM-	SS,ss'	'log-	level'	'process_	id'	'is_	app_	name'	'file'	'lineOfCode'	
'message'
To	extract	the	runtime	for	each	individual	task,	use	regex	to	match	on	alog	line.	For	instance,	in	the	above	
example,	there	is	the	following	sub-	string:	
""stop	Query	and	Cache	ServiceNow	CIs|41.4114570618""	
Use	regex	to	parse	the	string	above:	
""stop	â¦â¦	|	â¦""	
where:	
l	Everything	after	the	|is	the	time	taken	for	execution.	
l	The	string	between	""stop""	and	|represents	the	step	that	was	executed.	
In	the	example	message,	the	""Query	and	Cache	ServiceNow	CIs""	step	took	around	41	seconds	to	run.	
131 

132
V i e w i n g	th e	S te p	L o g s	a n d	S te p	D a ta	f o r	a	P o w e r F l o w	
A p p l i c a ti o n
The	Step	Log	tab	on	an	Application	page	displays	the	time,	the	type	of	log,	and	the	log	messages	from	astep	that	
you	selected	in	the	main	pane.	All	times	that	are	displayed	in	the	Message	column	of	this	pane	are	in	seconds,	
such	as	""stop	Query	and	Cache	ServiceNow	CI	List|	5.644190788269043	"".	
TIP:	You	can	view	log	data	for	astep	on	the	StepÂ Log	tab	while	the	Configuration	pane	for	that	step	is	open.	
To	view	logs	for	the	steps	of	an	application:	
1.	From	the	[Applications	]tab,	select	an	application.	The	Application	page	appears.	
2.	Select	astep	in	the	application.	
Viewing	the	Step	Logs	and	Step	Data	for	aPowerFlow	Application 

Removing	Logs	on	aRegular	Schedule	
3.	Click	the	[Step	Log	]tab	in	the	bottom	left-	hand	corner	of	the	screen.	The	[Step	Log	]tab	appears	at	the	
bottom	of	the	page,	and	itdisplays	log	messages	for	the	selected	step:	
TIP:	For	longer	log	messages,	click	the	down	arrow	icon	(	)in	the	Message	column	of	the	[Step	
Log	]tab	to	open	the	message.	Also,	you	can	triple-	click	alog	message	to	highlight	the	entire	text	
block	so	you	can	easily	copy	the	log	message	to	another	text	viewer.	
4.	Click	the	[Step	Data	]tab	to	display	the	JSON	data	that	was	generated	by	the	selected	step.	
5.	Click	the	[Step	Log	]tab	to	close	the	tab.	
6.	To	generate	more	detailed	logs	when	you	run	this	application,	hover	over	[Run	](	)and	select	Debug	Run	.	
TIP:	Log	information	for	astep	is	saved	for	the	duration	of	the	result_	expires	setting	in	the	PowerFlow	system	.	
The	result_	expires	setting	is	defined	in	the	opt/iservices/scripts/docker-	compose.yml	file.	The	
default	value	for	log	expiration	is	7	days.	This	environment	variable	is	set	in	seconds.	
R e m o v i n g	L o g s	o n	a	R e g u l a r	S c h e d u l e	
The	""Timed	Removal""	application	in	the	PowerFlow	user	interface	lets	you	remove	logs	from	Couchbase	on	a	
regular	schedule.	
To	schedule	the	removal	of	logs:	
1.	In	the	PowerFlow	user	interface,	go	to	the	[Applications	]tab	and	select	the	""Timed	Removal""	application.	
133 

134	
2.	Click	the	[Configure	]button	(	).	The	Configuration	pane	appears:	
3.	Complete	the	following	fields:	
l	Configuration	.Select	the	relevant	configuration	object	to	align	with	this	application.	Required.	
l	time_	in_	days	.Specify	how	often	you	want	to	remove	the	logs	from	Couchbase.	The	default	is	7	days.	
Required.	
4.	Click	the	[Save	]button	and	close	the	Configuration	pane.	
5.	Click	theÂ 	[Run	Now	]button	(	)to	run	the	""Timed	Removal""Â application.	
Removing	Logs	on	aRegular	Schedule 

Chapte r	
8	
Using	SL1	to	Monitor	SL1	PowerFlow	
O v e r v i e w
This	chapter	describes	the	various	ScienceLogic	PowerPacks	that	you	can	use	to	monitor	the	components	of	the	
PowerFlow	system	.This	chapter	also	describes	the	suggested	settings,	metrics,	and	situations	for	healthy	SL1	and	
PowerFlow	systems.	
Use	the	following	menu	options	to	navigate	the	SL1	user	interface:	
l	To	view	apop-	out	list	of	menu	options,	click	the	menu	iconÂ 	(	).	
l	To	view	apage	containing	all	of	the	menu	options,	click	the	Advanced	menu	icon	(	).	
This	chapter	covers	the	following	topics:	
Monitoring	PowerFlow	136	
Configuring	the	Docker	PowerPack	137	
Configuring	the	SL1	PowerFlow	PowerPack	139	
Configuring	the	Couchbase	PowerPack	142	
Configuring	the	RabbitMQ	PowerPack	144	
Stability	of	the	PowerFlow	Platform	147
135 

136
M o n i to r i n g	P o w e r F l o w	
You	can	use	anumber	of	ScienceLogic	PowerPacks	to	help	you	monitor	the	health	of	your	PowerFlow	system.Â This	
section	describes	those	PowerPacks	and	additional	resources	and	procedures	you	can	use	to	monitor	the	
components	of	PowerFlow	.	
TIP:	You	can	also	use	the	Dashboard	page	(	)in	the	PowerFlow	user	interface	to	monitor	the	status	of	the	
various	tasks,	workers,	and	applications	that	are	running	on	your	PowerFlow	system.	You	can	use	this	
information	to	quickly	determine	ifyour	PowerFlow	instance	is	performing	as	expected.	
You	can	download	the	following	PowerPacks	from	the	ScienceLogic	CustomerÂ Portal	to	help	you	monitor	your	
PowerFlow	system:	
l	Linux	Base	Pack	PowerPack	:This	PowerPack	monitors	your	Linux-	basedÂ 	PowerFlow	server	with	SSHÂ 	(the	
PowerFlow	ISO	is	built	on	top	of	an	Oracle	Linux	Operating	System).	This	PowerPack	provides	key	
performance	indicators	about	how	your	PowerFlow	server	is	performing.	The	only	configuration	you	need	to	
do	with	this	PowerPack	is	to	install	the	latest	version	of	it.	
l	Docker	PowerPack	:This	PowerPack	monitors	the	various	Docker	containers,	services,	and	Swarm	that	
manage	the	PowerFlow	containers.	This	PowerPack	also	monitors	PowerFlow	when	itis	configured	for	High	
Availability.	Use	version	103	or	later	of	the	Docker	PowerPack	to	monitor	PowerFlow	services	in	SL1	.For	
more	information,	see	Configuring	the	Docker	PowerPack	.	
l	SL1	PowerFlow	PowerPack	.This	PowerPack	monitors	the	status	of	the	applications	in	your	PowerFlow	
system.	Based	on	the	events	generated	by	this	PowerPack	,you	can	diagnose	why	applications	failed	on	
PowerFlow	.For	more	information,	see	Configuring	the	SL1	PowerFlow	PowerPack	.	
l	Couchbase	PowerPack	:Â This	PowerPack	monitors	the	Couchbase	database	that	PowerFlow	uses	for	storing	
the	cache	and	various	configuration	and	application	data.	This	data	provides	insight	into	the	health	of	the	
databases	and	the	Couchbase	servers.	For	more	information,	see	Configuring	the	Couchbase	
PowerPack	.	
l	AMQP:	RabbitMQ	PowerPack	.This	PowerPack	monitors	RabbitMQ	configuration	data	and	performance	
metrics	using	Dynamic	Applications.	You	can	use	this	PowerPack	to	monitor	the	RabbitMQ	service	used	by	
PowerFlow	.For	more	information,	see	Configuring	the	RabbitMQ	PowerPack	.	
You	can	use	each	of	the	PowerPacks	listed	above	to	monitor	different	aspects	of	PowerFlow	.Be	sure	to	download	
and	install	the	latest	version	of	each	PowerPack.	
The	following	sub-	topics	describe	the	configuration	steps	you	need	to	take	for	each	PowerPack.	For	best	results,	
complete	these	configuration	steps	in	the	given	order	to	set	up	monitoring	of	PowerFlow	within	SL1	.	
Monitoring	PowerFlow 

Configuring	the	Docker	PowerPack	
C o n f i g u r i n g	th e	D o c k e r	P o w e r P a c k	
The	Docker	PowerPack	monitors	the	various	Docker	containers,	services,	and	Swarm	that	manage	the	PowerFlow	
containers.	This	PowerPack	also	monitors	PowerFlow	when	itis	configured	for	High	Availability.	Use	version	103	
or	later	of	the	Docker	PowerPack	to	monitor	PowerFlow	services	in	SL1	.	
To	configure	the	Docker	PowerPack	to	monitor	PowerFlow	:	
1.	Make	sure	that	you	have	already	installed	theÂ 	Linux	Base	Pack	PowerPack.	
2.	In	SL1	,go	to	the	Credential	Management	page	(System	>	Manage	>	Credentials)	and	click	the	wrench	
icon	(	)for	the	example	Docker	Basic	-Dev	ssh	credential.	The	Credential	Editor	modal	page	appears.	
3.	Complete	the	following	fields,	and	keep	the	other	fields	at	their	default	settings:	
l	Credential	Name	.Type	anew	name	for	the	credential.	
l	Hostname/IP	.Type	the	hostname	or	IP	address	for	the	PowerFlow	instance,	or	type	""%D"".	
l	Username	.Type	the	username	for	the	PowerFlow	instance.	
l	Password	.Type	the	password	for	the	PowerFlow	instance.	
4.	Click	[Save	As	]and	close	the	Credential	Editor	modal	page.	
5.	On	the	Devices	page,	click	[Add	Devices	]to	discover	yourÂ 	PowerFlow	server	using	the	new	Docker	SSH	
new	credential.	
TIP:	Use	the	Unguided	NetworkÂ Discovery	option	and	search	for	the	new	Docker	credential	on	the	Choose	
credentials	page	of	the	Discovery	wizard.	Also,	be	sure	to	select	Discover	Non-	SNMP	and	Model	
Devices	in	the	Advanced	options	section.	For	more	information,	see	the	Discovery	and	
Credentials	manual	.	
After	the	discovery	is	complete,	the	Docker	and	Linux	Dynamic	Applications	will	automatically	align	to	your	
discovered	Docker	Host	for	your	PowerFlow	.SL1	creates	anew	Device	record	for	the	PowerFlow	server	and	
new	Device	Component	records	for	Docker	containers.	
6.	Go	to	the	Devices	page	and	select	the	new	device	representing	your	PowerFlow	server.	
137 

138	
7.	Go	to	the	[Collections	]tab	of	the	DeviceÂ Investigator	page	for	the	new	device	and	make	sure	that	all	of	the	
Docker	and	Linux	Dynamic	Applications	have	automatically	aligned.	This	process	usually	takes	afew	
minutes.	A	group	of	Docker	and	Linux	Dynamic	Applications	should	now	appear	on	the	[Collections	]tab:	
Configuring	the	Docker	PowerPack 

Configuring	the	SL1	PowerFlow	PowerPack	
8.	To	view	your	newly	discovered	device	components,	navigate	to	the	Device	Components	page	(Devices	>	
Device	Components).	Ifyou	do	not	see	your	newly	discovered	Docker	Host,	wait	for	the	dynamic	applications	
on	the	Docker	host	to	finish	modeling	out	its	component	devices.	A	Docker	Swarm	virtual	root	device	will	also	
be	discovered.	After	discovery	finishes,	you	should	see	the	following	devices	representing	your	PowerFlow	
system	on	the	Device	Components	page:	
NOTE:	Ifthe	Docker	Swarm	root	device	is	modeled	with	adifferent	device	class,	click	the	wrench	icon	(	)	
for	the	Docker	Swarm	root	device,	click	the	[Actions	]button	on	the	Device	Properties	window	and	
select	Device	Class	.From	the	Device	Class	window,	select	ScienceLogic	|Integration	Service	as	the	
Device	Class	and	click	[Apply	].Save	your	changes.	
C o n f i g u r i n g	th e	S L 1	P o w e r F l o w	P o w e r P a c k	
The	SL1	PowerFlow	Â PowerPack	monitors	the	status	of	the	applications	in	your	PowerFlow	system.	Based	on	the	
events	generated	by	this	PowerPack	,you	can	diagnose	why	applications	failed	in	PowerFlow	.	
To	configure	SL1	to	monitor	PowerFlow	,you	must	first	create	aSOAP/XML	credential.	This	credential	allows	the	
Dynamic	Applications	in	the	PowerFlow	PowerPack	to	communicate	with	PowerFlow	.	
139 

140
In	addition,	before	you	can	run	the	Dynamic	Applications	in	the	PowerFlow	PowerPack	,you	must	manually	align	
the	Dynamic	Application	from	this	PowerPack	to	your	PowerFlow	device	in	SL1	.These	steps	are	covered	in	detail	
below.
C on f i g u r i n g	t h e	P ow e r P a c k	
To	configure	the	PowerFlow	PowerPack:	
1.	In	SL1	,make	sure	that	you	have	already	installed	the	Linux	Base	PowerPack,	the	Docker	PowerPack,	and	the	
PowerFlow	PowerPack	on	your	SL1	system.	
2.	In	SL1	,navigate	to	the	Credential	Management	page	(System	>Â Manage	>Â Credentials)	and	click	the	
wrench	icon	(	)for	the	IS	-Example	credential.	The	Credential	Editor	modal	page	appears.	
3.	Complete	the	following	fields,	and	keep	the	other	fields	at	their	default	settings:	
l	Profile	Name	.Type	aname	for	the	credential.	
l	URL	.Type	the	URL	for	your	PowerFlow	system.	
l	HTTP	Auth	User	.Type	the	PowerFlow	administrator	username.	
l	HTTP	Auth	Password	.Type	the	PowerFlow	administrator	password	
l	Timeout	(seconds)	.Type	""20"".	
l	Embed	Value	[%1]	.Type	""False"".	
4.	Click	the	[Save	As	]button	and	close	the	Credential	Editor	modal	page.	You	will	use	this	new	credential	to	
manually	align	the	following	DynamicÂ Applications:	
l	REST:	Performance	Metrics	Monitor	
l	REST:	Performance	Metrics	Monitor	(Couchbase)	
l	REST:	Performance	Metrics	Monitor	(Integration	Service)	
l	REST:	Performance	Metrics	Monitor	(ServiceNow)	
5.	Go	to	the	Devices	page,	select	the	device	representing	your	PowerFlow	server,	and	click	the	[Collections	]	
tab.	
6.	Click	[Edit	],click	[AlignÂ DynamicÂ App	],and	select	Choose	Dynamic	Application	.The	Choose	Dynamic	
Application	window	appears.	
7.	In	the	Search	field,	type	the	name	of	the	first	of	the	PowerFlow	Dynamic	Applications.	Select	the	Dynamic	
Application	and	click	[Select	].	
8.	Select	Choose	Dynamic	Application	.The	Choose	Credential	window	appears.	
9.	In	the	Search	field,	type	the	name	of	the	credential	you	created	in	steps	2-	4,	select	the	new	credential,	and	
click	[Select	].The	Align	Dynamic	Application	window	appears.	
10.	Click	[Align	Dynamic	App	].The	Dynamic	Application	is	added	to	the	[Collections	]tab.	
11.	Repeat	steps	6-	10	for	each	remaining	Dynamic	Application	for	this	PowerPack,	and	click	[Save	]when	you	
are	done	aligning	Dynamic	Applications	.	
Configuring	the	SL1	PowerFlow	PowerPack 

Configuring	the	SL1	PowerFlow	PowerPack	
E v e n t s	G e n e r a t e d	b y	t h e	P ow e r P a c k	
The	""ScienceLogic:	Integration	Service	Queue	Configuration""	Dynamic	Application	generates	aMajor	event	in	
SL1	ifan	application	fails	in	PowerFlow	:	
The	related	Event	Policy	includes	the	name	of	the	application,	the	Task	ID,	and	the	traceback	of	the	failure.	You	
can	use	the	application	name	to	identify	the	application	that	failed	in	PowerFlow	.You	can	use	the	Task	ID	to	
determine	the	exact	execution	of	the	application	that	failed,	which	you	can	then	use	for	debugging	purposes.	
To	view	more	information	about	the	execution	of	an	application	in	PowerFlow	,navigate	to	the	relevant	page	in	
PowerFlow	by	formatting	the	URL	in	the	following	manner:	
https://	<PowerFlow	_hostname>	/integrations/	<application_	name>	?runid=	<task_	id>	
For	example:
https://192.0.2.0/integrations/sync_	credentials?runid=c7e157ae-	5644-	4161-	a241-	
59516feeadec	
141 

142
C o n f i g u r i n g	th e	C o u c h b a s e	P o w e r P a c k	
Couchbase	stores	all	cache	and	configuration	data	on	PowerFlow	.Monitoring	the	performance	of	your	PowerFlow	
is	critical	in	ensuring	the	health	of	your	PowerFlow	instance.	
After	you	install	the	Couchbase	PowerPack	in	SL1	,create	anew	Couchbase	SOAP/XML	credential.	Using	that	
credential,	you	need	to	manually	align	the	""Couchbase	Component	Count""	and	""Couchbase	Pool	Discovery""	
Dynamic	Applications	with	theÂ DockerÂ Swarm	root	device.	These	steps	are	covered	in	detail	below.	
To	configure	the	Couchbase	PowerPack:	
1.	In	SL1	,navigate	to	the	Credential	Management	page	(System	>Â Manage	>Â Credentials)	and	click	the	
wrench	icon	(	)for	the	Couchbase	Sample	credential.	The	Credential	Editor	modal	page	appears.	
2.	Complete	the	following	fields,	and	keep	the	other	fields	at	their	default	settings:	
l	Profile	Name	.Type	anew	descriptive	name	for	the	credential.	
l	URL	.Type	the	full	URL	for	PowerFlow	.Ensure	that	the	port	8091	is	appended	to	the	hostname.Â For	
PowerFlow	version	1.8.2	and	later,	use	https	for	this	URL.	
l	HTTPÂ Auth	User	.Type	the	username	for	your	PowerFlow	instance.	
l	HTTPÂ Auth	Password	.Type	the	password	for	your	PowerFlow	instance.	
NOTE:	For	aclustered	PowerFlow	environment,	point	the	Couchbase	credentials	at	the	load	
balancer	for	PowerFlow	.The	example	above	is	for	asingle	node	deployment.	
3.	Click	[Save	As	]and	close	the	Credential	Editor	modal	page.	
4.	Go	to	the	Device	Components	page	(Devices	>	Device	Components)	and	expand	the	DockerÂ Swarm	root	
device	by	clicking	the	+	icon.	
5.	Click	the	wrench	icon	(	)for	the	âStack	|Docker	Stackâ	component	device	(iservices)	and	click	the	
[Collections	]tab.	
Configuring	the	Couchbase	PowerPack 

Configuring	the	Couchbase	PowerPack	
6.	Align	the	""Couchbase:	Pool	Discovery""	Dynamic	Application	by	clicking	the	[Actions	]button	and	selecting	
Add	Dynamic	Application	.The	Dynamic	Application	Alignment	modal	appears:	
7.	Select	the	""Couchbase:	Pool	Discovery""	Dynamic	Application	and	select	the	Couchbase	credential	that	you	
created	in	steps	1-	3.	Click	[Save	].	
8.	Click	the	[Actions	]button,	select	Add	Dynamic	Application	and	align	the	""Couchbase:	Component	Count""	
Dynamic	Application	and	the	Couchbase	credential.	Click	[Save	].	
9.	Select	the	""Couchbase:	Component	Count""	Dynamic	Application	and	select	the	IS	credential	that	you	
created	in	steps	2-	3.	Click	[Save	].SL1	models	out	your	Couchbase	components	and	provides	you	with	
additional	information	about	the	usage	of	the	Couchbase	service.	
143 

144
10.	Navigate	to	the	Device	Components	page	(Devices	>	Device	Components)	to	see	the	Couchbase	
components:	
C o n f i g u r i n g	th e	R a b b i tM Q	P o w e r P a c k	
You	can	monitor	the	RabbitMQ	service	with	the	AMQP:	RabbitMQ	PowerPack	.This	PowerPack	monitors	
RabbitMQ	configuration	data	and	performance	metrics	using	Dynamic	Applications,	and	the	PowerPack	creates	a	
major	event	in	SL1	for	any	applications	in	PowerFlow	that	are	in	aFailed	state.	
After	you	install	the	RabbitMQ	PowerPack	in	SL1	,create	anew	SOAP/XML	credential.	Using	that	credential,	you	
need	to	manually	align	the	following	Dynamic	Applications	:	
l	ScienceLogic:	Integration	Service	Queue	Configuration	
l	AMQP:	RabbitMQ	Configuration	
l	AMQP:	RabbitMQ	Performance	
To	configure	the	RabbitMQ	Â PowerPack:	
1.	In	SL1	,navigate	to	the	Credential	Management	page	(System	>Â Manage	>Â Credentials)	and	click	the	
wrench	icon	(	)for	the	IS	Sample	credential.	The	Credential	Editor	modal	page	appears.	
Configuring	the	RabbitMQ	PowerPack 

Configuring	the	RabbitMQ	PowerPack	
2.	Complete	the	following	fields,	and	keep	the	other	fields	at	their	default	settings:	
l	Profile	Name	.Type	anew	descriptive	name	for	the	credential.	
l	URL	.Type	the	full	URL	for	your	PowerFlow	.For	PowerFlow	version	1.8.2	and	later,	use	https	for	this	
URL.	
l	HTTPÂ Auth	User	.Type	the	username	for	your	PowerFlow	instance.	
l	HTTPÂ Auth	Password	.Type	the	password	for	your	PowerFlow	instance.	
NOTE:	For	aclustered	PowerFlow	environment,	point	the	credentials	at	the	load	balancer	for	
the	PowerFlow	system	.The	example	above	is	for	asingle	node	deployment.	
3.	Click	[Save	As	]and	close	the	Credential	Editor	modal	page.	
4.	Go	to	the	Device	Components	page	(Devices	>	Device	Components)	and	click	the	wrench	icon	(	)for	
the	Docker	Swarm	root	device.	
5.	Click	the	[Collections	]tab	on	the	Device	Properties	window.	
6.	Align	the	""ScienceLogic:	Integration	Service	Queue	Configuration""	Dynamic	Application	by	clicking	the	
[Actions	]and	selecting	Add	Dynamic	Application	.The	Dynamic	Application	Alignment	modal	appears.	
145 

146	
7.	Select	the	""ScienceLogic:	Integration	Service	Queue	Configuration""	Dynamic	Application	and	select	the	
credential	that	you	created	in	step	2.	Click	[Save	].This	Dynamic	Application	queries	PowerFlow	every	15	
minutes	by	default	to	retrieve	information	about	any	failed	integrations,	which	generates	aMajor	event	in	SL1	
(the	events	auto-	expire	after	90	minutes):	
TIP:	The	events	generated	by	this	Dynamic	Application	include	the	Integration	ID	,which	you	can	use	
to	find	the	relevant	application	on	your	PowerFlow	instance.	Copy	the	name	in	the	event	message	
and	navigate	to	https://<	PowerFlow	>/integrations/<integration_	ID>	.	
8.	To	view	more	information	about	your	failed	applications,	navigate	to	the	[Configurations	]tab	for	the	device	
and	click	the	report	for	the	""ScienceLogic:	Integration	Service	Queue	Configuration""	Dynamic	Application	.	
This	configuration	report	shows	you	more	information	about	the	failed	integrations	on	your	Integration	Service	
instance.	For	example,	you	can	use	the	Last	Run	ID	field	to	find	the	exact	logs	for	aspecific	execution	of	the	
application.	To	do	this,	copy	the	Integration	ID	and	the	Last	Run	ID	and	navigate	to	
https://<	PowerFlow	>/integrations/<integration_	ID>?runid=<last_	run_	id>	
9.	Align	the	""AMQP:	RabbitMQ	Configuration""	and	""AMQP:	RabbitMQ	Performance""	Dynamic	Applications	
using	the	same	process	as	steps	6-	7.	
Configuring	the	RabbitMQ	PowerPack 

Stability	of	the	PowerFlow	Platform	
S ta b i l i ty	o f	th e	P o w e r F l o w	P l a tf o r m	
This	topic	defines	what	ahealthy	SL1	system	and	ahealthy	PowerFlow	system	look	like,	based	on	the	following	
settings,	metrics,	and	situations.	
W h a t	m a k e s	u p	a	h e a l t h y	S L 1	s y s t e m ?	
To	ensure	the	stability	of	your	SL1	system	,review	the	following	settings	in	your	SL1	environment:	
l	The	SL1	system	has	been	patched	to	aversion	that	has	been	released	by	ScienceLogic	within	the	last	12	
months.	ScienceLogic	issues	asoftware	update	at	least	quarterly.	Itis	important	for	the	security	and	stability	of	
the	system	that	customers	regularly	consume	these	software	updates.	
l	The	user	interface	and	API	response	times	for	standard	requests	are	within	five	seconds:	
o	Response	time	for	aspecific	user	interface	request.	
o	Response	time	for	aspecific	API	request.	
l	At	least	20%	of	local	storage	is	free	and	available	for	new	data.	Free	space	is	acombination	of	unused	
available	space	within	InnoDB	datafiles	and	filesystem	area	into	which	those	files	can	grow	
l	The	central	system	is	keeping	up	with	all	collection	processing:	
o	Performance	data	stored	and	available	centrally	within	three	minutes	of	collection	
o	Event	data	stored	and	available	centrally	within	30	seconds	of	collection	
o	Run	book	automations	are	completing	normally	
l	Collection	is	completing	normally.	Collection	tasks	are	completing	without	early	termination	(sigterm).	
l	All	periodic	maintenance	tasks	are	completing	successfully:	
o	Successfully	completing	daily	maintenance	(pruning)	on	schedule	
o	Successfully	completing	backup	on	schedule	
l	High	Availability	and	Disaster	Recovery	are	synchronized	(where	used):	
o	Replication	synchronized	(except	when	halted	/recovering	from	DR	backup).	
o	Configuration	matches	between	nodes.	
W h a t	m a k e s	u p	a	h e a l t h y	P ow e r F l ow	s y s t e m ?	
To	ensure	the	stability	of	the	PowerFlow	system	,review	the	following	settings	in	your	environment:	
l	The	settings	from	the	previous	list	are	being	met	in	your	SL1	system.	
l	You	are	running	asupported	version	of	PowerFlow	.	
l	The	memory	and	CPU	percentage	of	the	host	remains	less	than	80%	on	core	nodes.	
l	Task	workloads	can	be	accepted	by	the	API	and	placed	onto	the	queues	for	execution.	
147 

148	
l	The	PowerFlow	API	is	responding	to	POST	calls	to	run	applications	within	the	default	timeout	of	30	seconds.	
For	standard	applications	triggers,	this	is	usually	sub-	second.	
l	The	PowerFlow	Scheduler	is	configured	correctly.	For	example,	there	are	no	tasks	accidentally	set	to	run	
every	minute	or	every	second.	
l	Task	workloads	are	actively	being	pulled	from	queues	for	execution	by	workers.	Workers	are	actively	
processing	tasks,	and	not	just	leaving	items	in	queue.	
l	Worker	nodes	are	all	up	and	available	to	process	tasks.	
l	Couchbase	does	not	frequently	read	documents	from	disk.Â You	can	check	this	value	with	the	âDisk	Fetches	
per	secondâ	metric	in	the	Couchbase	user	interface.	
l	The	Couchbase	Memory	Data	service	memory	usage	is	not	using	all	allocated	memory,	forcing	data	writes	to	
disk.	You	can	check	this	value	with	the	""Data	service	memory	allocation""	metric	in	the	main	Couchbase	
dashboard.	
l	Container	services	are	not	restarting.	
l	The	RabbitMQ	memory	usage	is	not	more	than	2-	3	GB	per	10.000	messages	in	queues.	The	memory	
usage	might	be	alittle	larger	ifyou	are	running	considerably	larger	tasks.	
l	RabbitMQ	mirrors	are	synchronized.	
l	RabbitMQ	is	only	mirroring	the	dedicated	queues,	not	temporary	or	TTL	queues.	
l	All	Couchbase	indexes	are	populated	on	all	Couchbase	nodes.	
l	The	Couchbase	nodes	are	fully	rebalanced	and	distributed.	
l	The	Docker	Swarm	cluster	has	at	least	three	active	managers	in	aHigh	Availability	cluster.	
l	For	any	Swarm	node	that	is	also	aswarm	manager,	and	that	node	is	running	PowerFlow	services	:	
l	At	least	one	CPU	with	4	GB	of	memory	is	available	on	the	host	to	actively	manage	the	swarm	cluster.	
l	Any	PowerFlow	services	running	on	this	host	are	not	able	to	consume	all	of	the	available	resources,	
causing	cluster	operations	to	fail.	
Some	of	the	following	PowerFlow	settings	might	vary,	based	on	your	configuration:	
l	The	number	of	applications	sitting	in	queue	is	manageable.	A	large	number	of	applications	sitting	in	queue	
could	indicate	either	alarge	spike	in	workload,	or	no	workers	are	processing.	
l	The	number	of	failed	tasks	is	manageable.	A	large	number	of	failed	tasks	could	be	caused	by	ServiceNow	
timeouts,	expected	failure	conditions,	and	other	situations.	
l	ServiceNow	is	not	overloaded	with	custom	table	transformations	that	cause	long	delays	when	PowerFlow	is	
communicating	with	ServiceNow.	
Stability	of	the	PowerFlow	Platform 

Chapte r	
9	
Using	the	pfctl	Command-	line	Utility	
O v e r v i e w
This	chapter	describes	how	to	use	the	pfctl	command-	line	utility	to	run	automatic	cluster	healthcheck	and	
autoheal	actions	that	will	verify	the	configuration	of	your	PowerFlow	cluster	or	asingle	PowerFlow	node.	The	pfctl	
utility	also	includes	an	autocluster	action	that	performs	multiple	administrator-	level	actions	on	either	the	node	or	
the	cluster.	You	can	use	this	action	to	automate	the	configuration	of	athree-	node	cluster.	
NOTE:	The	pfctl	command-	line	utility	was	called	iservicecontrol	in	previous	release	of	SL1	PowerFlow	.You	
can	use	either	""iservicecontrol""	or	""pfctl""	in	commands,	but	""iservicecontrol""	will	eventually	be	
deprecated	in	favor	of	""pfctl"".	
NOTE:	The	different	actions	available	with	the	pfctl	utility	should	replace	the	need	to	configure	and	run	the	
different	PowerPacks	described	in	Using	SL1	to	Monitor	PowerFlow	.	
This	chapter	covers	the	following	topics:	
What	is	the	pfctl	Utility?	150	
healthcheck	and	autoheal	150	
autocluster	153	
upgrade	154	
open_	firewall_	ports	155
149 

150
W h a t	i s	th e	p f c tl	U ti l i ty ?	
The	pfctl	command-	line	utility	included	in	PowerFlow	contains	automatic	cluster	healthcheck	and	autoheal	
actions	that	will	verify	the	configuration	of	your	cluster	or	single	node.Â The	utility	also	includes	an	autocluster	
action	that	performs	multiple	administrator-	level	actions	on	either	the	node	or	the	cluster.	
The	pfctl	utility	is	included	in	PowerFlow	version	2.0.0	or	later.	Ifyou	are	using	an	older	version	of	PowerFlow	,	
contactÂ 	ScienceLogic	Support	for	the	latest	version	of	the	utility.	
NOTE:	You	can	use	key-	based	authentication	instead	of	username	and	password	authentication	for	the	pfctl	
command-	line	utility.	
For	alist	of	all	of	the	actions	you	can	run	on	asingle	node,	SSH	to	the	PowerFlow	server	and	run	the	following	
command:	
pfctl	node-	action	--help	
For	alist	of	all	of	the	actions	you	can	run	on	aclustered	system,	run	the	following	command:	
pfctl	cluster-	action	--help	
See	the	following	topics	for	more	information	on	the	following	pfctl	actions:	
l	healthcheck	and	autoheal	
l	autocluster	
l	upgrade	
l	open_	firewall_	ports	
h e a l th c h e c k	a n d	a u to h e a l	
The	pfctl	command-	line	utility	performs	multiple	administrator-	level	actions	in	aclustered	PowerFlow	
environment.	The	pfctl	utility	contains	automatic	cluster	healthcheck	and	autoheal	capabilities	that	you	can	use	
to	prevent	issues	with	your	PowerFlow	environment:	
l	TheÂ 	healthcheck	Â action	executes	various	commandsÂ to	verify	configurations,	proxies,	internal	connectivity,	
queue	cluster,	database	cluster,	indexes,	NTP	settings,	Docker	versions	on	all	clusters,	and	more.Â Any	
previously	reported	troubleshooting	issues	are	addressed	with	the	healthcheck	action.	
l	TheÂ 	autoheal	Â action	automatically	takes	corrective	action	on	your	cluster.	
After	deploying	any	clustersÂ in	aPowerFlow	system,	or	ifyou	are	troubleshooting	an	existing	cluster,	you	should	first	
runÂ the	healthcheck	Â action	to	generate	immediate	diagnostics	of	the	entire	cluster	and	all	services	and	containers	
associated	with	the	cluster.	Ifthe	healthcheck	Â action	finds	any	issues,	you	can	run	the	autoheal	action	to	attempt	
to	address	those	issues.	
What	is	the	pfctl	Utility? 

healthcheck	and	autoheal	
h e a l t h c h e c k
The	following	commandsÂ show	the	formatting	for	ahealthcheck	action	for	asingle	node	,followed	by	an	example:	
pfctl	--host	<host>Â <username>	:<password>	node-	action	--action	healthcheck	
pfctl	--host	10.2.11.222	isadmin:isadmin222	node-	action	--action	healthcheck	
The	following	commandsÂ show	the	formatting	for	ahealthcheck	action	for	aclustered	environment	,followed	by	
an	example:
pfctl	--host	<host>Â <username>	:<password>	--host	<host>Â <username>	:<password>	--host	
<host>Â <username>	:<password>	cluster-	action	--action	healthcheck	
pfctl	--host	10.2.11.222	isadmin:isadmin222	--host	10.2.11.232	isadmin:isadmin232	--	
host	10.2.11.244	isadmin:isadminpassÂ cluster-	action	--action	healthcheck	
a u t oh e a l
The	following	commandsÂ show	the	formatting	for	an	autoheal	action	for	asingle	node	,followed	by	an	example:	
pfctl	--host	<host>Â <username>	:<password>	node-	action	--action	autoheal	
pfctl	--host	10.2.11.222	isadmin:isadmin222	node-	action	--action	autoheal	
The	following	commandsÂ show	the	formatting	for	an	autoheal	action	for	aclustered	environment	,followed	by	an	
example:
pfctl	--host	<host>Â <username>	:<password>	--host	<host>Â <username>	:<password>	--host	
<host>Â <username>	:<password>	cluster-	action	--action	autoheal	
pfctl	--host	10.2.11.222	isadmin:isadmin222	--host	10.2.11.232	isadmin:isadmin232	--	
host	10.2.11.244	isadmin:isadminpass	cluster-	action	--action	autoheal	
E x a m p l e	O u t p u t	
The	following	section	lists	example	healthcheck	output:	
verify	db	host	for	cluster	10.2.11.222...........	[OK]	
check	dex	connectivity	10.2.11.222...............	[OK]	
check	rabbit	cluster	count	10.2.11.222...........	[OK]	
check	rabbit	cluster	alarms	10.2.11.222..........	[OK]	
verify	cmd	in	container	10.2.11.222..............	[OK]	
verify	cmd	in	container	10.2.11.222..............	[OK]	
verify	cmd	in	container	10.2.11.222..............	[OK]	
verify	cmd	in	container	10.2.11.222..............	[OK]	
verify	cmd	in	container	10.2.11.222..............	[OK]	
verify	cmd	in	container	10.2.11.222..............	[OK]	
verify	cmd	in	container	10.2.11.232..............	[Skipped	-	iservices_	steprunner	not	
found	on	10.2.11.232]	
verify	cmd	in	container	10.2.11.232..............	[OK]	
151 

152	
verify	cmd	in	container	10.2.11.232..............	[OK]	
verify	cmd	in	container	10.2.11.232..............	[Skipped	-	iservices_	contentapi	not	
found	on	10.2.11.232]	
verify	cmd	in	container	10.2.11.232..............	[OK]	
verify	cmd	in	container	10.2.11.232..............	[Skipped	-	iservices_	steprunner	not	
found	on	10.2.11.232]	
verify	cmd	in	container	10.2.11.232..............	[OK]	
verify	cmd	in	container	10.2.11.232..............	[OK]	
verify	cmd	in	container	10.2.11.232..............	[Skipped	-	iservices_	contentapi	not	
found	on	10.2.11.232]	
verify	cmd	in	container	10.2.11.232..............	[OK]	
verify	cmd	in	container	10.2.11.244..............	[OK]	
verify	cmd	in	container	10.2.11.244..............	[OK]	
verify	cmd	in	container	10.2.11.244..............	[OK]	
verify	cmd	in	container	10.2.11.244..............	[OK]	
verify	cmd	in	container	10.2.11.244..............	[OK]	
verify	cmd	in	container	10.2.11.244..............	[OK]	
get	file	hash	10.2.11.222........................	[OK]	
get	file	hash	10.2.11.232........................	[OK]	
/etc/iservices/isconfig.yml	does	not	match	between	10.2.11.222	and	10.2.11.232	
get	file	hash	10.2.11.222........................	[OK]	
get	file	hash	10.2.11.232........................	[OK]	
get	file	hash	10.2.11.244........................	[OK]	
/opt/iservices/scripts/docker-	compose.yml	does	not	match	between	10.2.11.222	and	
10.2.11.244
get	file	hash	10.2.11.222........................	[OK]	
get	file	hash	10.2.11.232........................	[OK]	
get	file	hash	10.2.11.244........................	[OK]	
get	file	hash	10.2.11.222........................	[OK]	
get	file	hash	10.2.11.232........................	[OK]	
get	file	hash	10.2.11.244........................	[OK]	
get	file	hash	10.2.11.222........................	[OK]	
get	file	hash	10.2.11.232........................	[OK]	
get	file	hash	10.2.11.244........................	[OK]	
check	cpu	10.2.11.222............................	[OK]	
check	disk	10.2.11.222...........................	[OK]	
check	memory	10.2.11.222.........................	[Failed]	
check	cpu	10.2.11.232............................	[OK]	
check	disk	10.2.11.232...........................	[OK]	
check	memory	10.2.11.232.........................	[OK]	
check	cpu	10.2.11.244............................	[OK]	
check	disk	10.2.11.244...........................	[OK]	
check	memory	10.2.11.244.........................	[OK]	
Utilization	warnings	in	the	cluster:	
{'10.2.11.222':	['There	is	less	than	2000mb	memory	available']}	
verify	ntp	sync	10.2.11.222......................	[OK]	
verify	ntp	sync	10.2.11.232......................	[OK]	
verify	ntp	sync	10.2.11.244......................	[OK]	
check	replica	count	logs	10.2.11.222.............	[OK]	
check	replica	count	content	10.2.11.222..........	[Failed]	
Identified	missing	replicas	on	some	buckets:	['Replica	count	for	bucket:	content	is	
not	the	expected	2']	
verify	pingable	addr	10.2.11.222.................	[OK]	
verify	pingable	addr	10.2.11.232.................	[OK]	
verify	pingable	addr	10.2.11.244.................	[OK]	
get	exited	container	count	10.2.11.222...........	[OK]	
healthcheck	and	autoheal 

autocluster	
get	exited	container	count	10.2.11.232...........	[OK]	
get	exited	container	count	10.2.11.244...........	[OK]	
6	exited	(stale)	containers	found	cluster-	wide	
verify	node	indexes	10.2.11.222..................	[Failed]	
Some	nodes	are	missing	required	indexes.	Here	are	the	nodes	with	the	missing	indeces:	
Missing	the	following	indexes:	{'couchbase.isnet':	['idx_	casbin'],	'couchbase-	work	-	
er2.isnet':
['idx_	content_	configuration']}	
U s i n g	p f c t l	h e a l t h c h e c k	on	t h e	d oc k e r -	c om p os e	f i l e	
You	can	also	validate	the	docker-	compose	file	with	the	pfctl	healthcheck	action.	The	action	will	show	a	
message	ifpypiserver	or	dexserver	services	are	not	configured	properly	in	the	docker-	compose	file.	You	can	fix	
these	settings	manually	or	with	the	pfctl	autoheal	action,	which	corrects	the	docker-	compose	file	and	copies	itto	
all	the	nodes	in	the	clustered	environment.	
When	using	version	1.3.0	or	later	of	the	pfctl	command-	line	utility,	the	autocluster	action	validates	and	fixes	the	
pypiserver	and	dexserver	services	definitions	in	the	docker-	compose	file.	
The	healthcheck	action	in	the	pfctl	command-	line	utility	for	PowerFlow	clusters	will	check	the	Docker	version	
for	each	cluster	to	ensure	that	the	Docker	version	is	the	same	in	all	the	hosts.	
a u to c l u s te r
You	can	use	the	pfctl	command-	line	utility	to	perform	multiple	administrator-	level	actions	on	yourÂ 	PowerFlow	
cluster.	You	can	use	the	autocluster	action	with	the	pfctl	command	to	automate	the	configuration	of	athree-	node	
cluster.
NOTE:	Ifyou	are	using	another	cluster	configuration,	the	deployment	process	should	be	manual,	because	the	
pfctl	utility	only	supports	the	automated	configuration	of	athree-	node	cluster.	
WARNING:	The	autocluster	action	will	completely	reset	and	remove	all	data	from	the	system.	When	you	run	
this	action,	you	will	get	aprompt	verifying	that	you	want	run	the	action	and	delete	all	data.	
To	automate	the	configuration	of	athree-	node	cluster,	run	the	following	command:	
pfctl	--host	<is_	host1>	<username>	:<password>	--host	<is_	host2>	
<username>	:<password>	--host	<is_	host3>	<username>	:<password>	autocluster	
For	example:	
pfctl	--host	192.11.1.1	isadmin:passw0rd	--host	192.11.1.2	isadmin:passw0rd	--host	
192.11.1.3	isadmin:passw0rd	autocluster	
153 

154
Running	this	command	will	configure	your	PowerFlow	three-	node	cluster	without	any	additional	manual	steps	
required.
WARNING:	Ifthe	isadmin	(host)	password	contains	aspecial	character,	such	as	an	""@""	or	""#""	symbol,	the	
password	must	be	escaped	in	the	iservicecontrol	autocluster	command	by	adding	single	
quotes,	such	as	'user:password'	.For	example:	iservicecontrol	--host	10.10.10.100	
'isadmin:testing@is'	--host	10.10.10.102	'isadmin:testing@is'	--host	
10.10.10.105	'isadmin:testing@is'	autocluster	
u p g r a d e
This	topic	explains	how	to	use	the	upgrade	action	in	the	pfctl	utility	to	upgrade	aclustered	environment	from	
PowerFlow	version	1.8.x	to	2.1.0.	The	upgrade	action	is	only	for	upgrading	acluster	from	a1.8.x	installation,	
and	should	not	be	run	otherwise.	
NOTE:	The	pfctl	utility	cannot	be	installed	on	a1.8.4	PowerFlow	system	because	the	utility	is	not	compatible	
with	Python	2.7.Â If	you	want	to	use	the	utility	on	a1.8.4	PowerFlow	system,	you	will	need	to	run	the	
is_	upgrade_	to_	v2.sh	on	any	of	the	cluster	nodes.	This	script	will	update	the	nodes	to	Python	3.x,	
and	then	you	can	download	and	install	the	pfctl	utility	on	the	upgraded	node,	and	upgrade	of	the	
other	nodes	from	that	node.	
NOTE:	Ifyou	have	an	environment	that	has	Python	3.6	or	later	available,	the	pfctl	package	can	be	installed	
in	that	system	(a	local	environment,	virtual	machine,	or	another	PowerFlow	system).	That	
environment	must	have	anÂ SSH	connection	to	the	PowerFlow	nodes	that	you	want	to	upgrade.Â Also,	
be	aware	that	only	certain	actions,	like	the	upgrade	action,	can	be	run	from	an	external	system,	but	
most	of	the	actions	that	the	pfctl	use	will	only	run	inside	the	cluster	nodes.	
To	run	the	upgrade	action	in	aclustered	environment:	
upgrade 

open_	firewall_	ports	
1.	Back	up	your	PowerFlow	data.	For	more	information,	see	Backing	up	Data	.	
2.	Run	the	upgrade	action	with	the	pfctl	utility:	
NOTE:	You	will	need	to	know	ifthe	upgrade	process	will	be	done	offline	or	online,	and	you	will	need	
the	url	of	the	PowerFlow	RPM	file	or	the	local	path	of	the	PowerFlow	RPM	or	ISO	file.	
For	an	offline	upgrade,	run	the	following	command	on	the	PowerFlow	instance:	
pfctl	--host	<swarm-	node1-	ip>	<is-	username>	:<is-	password>	--host	<swarm-	node2-	ip>	
<is-	username>	:<is-	password>	node-	action	--action	upgrade	--upgrade_	args	offline	
<PowerFlow-	iso-	local-	path>	
where	<PowerFlow-	iso-	local-	path>	is	alocal	path	such	as	/home/isadmin/sl1-	powerflow-	
2.2.0.iso	.	
For	an	online	upgrade,	run	the	following	command	on	the	PowerFlow	instance:	
pfctl	--host	<swarm-	node1-	ip>	<is-	username>:<is-	password>	--host	<swarm-	node2-	ip>	
<is-	username>	:<is-	password>	node-	action	--action	upgrade	--upgrade_	args	online	
<PowerFlow-	rpm-	url-	or-	local-	path>	
3.	Manually	open	the	firewall	ports	you	need,	or	use	the	pfctl	open-	firewall-	ports	action.	
4.	Synchronize	NTP	in	all	the	nodes.	For	more	information,	see	Preparing	the	PowerFlow	System	for	High	
Availability	.	
5.	Manually	verify	that	NTP	is	synchronized	in	all	the	nodes	manually,	or	use	the	pfctl	node	action	to	verify	that.	
6.	To	deploy	PowerFlow	,run	the	autocluster	action	using	the	pfctl	utility	ifyour	clustered	environment	contains	
three	nodes.	
NOTE:	Ifyou	are	using	another	cluster	configuration,	the	deployment	process	should	be	manual,	
because	the	pfctl	utility	only	supports	the	automated	configuration	of	athree-	node	cluster.	
7.	Run	the	healthcheck	cluster	action	using	the	pfctl	utility	to	check	that	the	cluster	environment	was	deployed	
correctly.	
8.	Run	the	autoheal	cluster	action	using	the	pfctl	utility	to	fix	any	errors	found	during	the	healthcheck	action	
run.	You	should	only	run	the	autoheal	action	ifthe	healthcheck	action	found	errors.	
o p e n _	f i r e w a l l _	p o r ts	
To	open	firewall	ports	for	asingle	node,	SSH	to	the	PowerFlow	server	and	run	the	following	command:	
pfctl	--host	<is_	host>	isadmin:	<password>	node-	action	--action	open_	firewall_	ports	
155 

156	
TIP:	Many	of	the	other	pfctl	actions	use	the	same	format	as	the	open_	firewall_	ports	action,	above.	The	only	
change	you	need	to	make	for	those	commands	is	to	replace	the	name	of	the	action	at	the	end	of	the	
command.	For	example:	pfctl	--host	<is_	host>	isadmin:<password>	node-	action	--	
action	pull_	latest_	images	
open_	firewall_	ports 

Chapte r	
10	
Troubleshooting	SL1	PowerFlow	
O v e r v i e w
This	chapter	includes	troubleshooting	resources,	procedures,	and	frequently	asked	questions	related	to	working	
with	SL1	PowerFlow	.	
This	chapter	covers	the	following	topics:	
Initial	Troubleshooting	Steps	159	
SL1	PowerFlow	159	
ServiceNow	159	
Resources	for	Troubleshooting	159	
Useful	PowerFlow	Ports	159	
pfctl	healthcheck	and	autoheal	160	
Helpful	Docker	Commands	160	
Helpful	Couchbase	Commands	162	
Useful	APIÂ Commands	163	
Diagnosis	Tools	164	
Identifying	Why	a	Service	or	Container	Failed	165	
Step	1:	Obtain	the	ID	of	the	failed	container	for	the	service	165	
Step	2:	Check	for	any	error	messages	or	logs	indicating	an	error	165	
Step	3:	Check	for	out	of	memory	events	166	
Troubleshooting	aCloud	Deployment	of	PowerFlow	166	
Identifying	Why	a	PowerFlow	Application	Failed	167	
Determining	Where	an	Application	Failed	167
157 

158	
Retrieving	Additional	Debug	Information	(Debug	Mode)	167	
Frequently	Asked	Questions	169	
What	is	the	first	thing	Ishould	do	when	Ihave	an	issue	with	PowerFlow?	169	
Can	the	steprunner_	syncpacks	service	can	be	limited	to	just	workers?	169	
What	is	the	difference	between	the	steprunner_	syncpacks	and	the	steprunner	services?	169	
What	is	the	minimal	image	required	for	workers?	169	
Ifthe	GUI	server	is	constrained	to	use	only	the	manager	nodes,	do	the	worker	nodes	need	to	have	their	
isconfig.yml	file	updated	with	the	correct	HOST	value?	170	
Can	Iunload	unwanted	images	from	aworker	node?	170	
IfIdedicated	workers	to	one	SL1	stack,	how	are	jobs	configured	to	run	only	on	those	workers?	170	
Approximately	how	much	data	is	sent	between	distributed	PowerFlow	nodes?	170	
How	can	Ioptimize	workers,	queues,	and	tasks?	170	
Why	do	IÂ get	a""Connection	refused""Â error	when	trying	to	communicate	with	Couchbase?	173	
Why	do	Ihave	client-	side	timeouts	when	communicating	with	Couchbase?	173	
What	causes	aTask	Soft	Timeout?	174	
How	do	Iaddress	an	""Error	when	connecting	to	DBÂ Host""	message	when	access	is	denied	to	user	""root""?	174	
How	do	Iremove	aschedule	that	does	not	have	aname?	174	
How	do	Iidentify	and	fix	adeadlocked	state?	175	
Why	are	incident	numbers	not	populated	in	SL1	on	Incident	creation	in	ServiceNow?	178	
Why	am	Inot	getting	any	Incidents	after	disabling	the	firewall?	178	
Why	are	Incidents	not	getting	created	in	ServiceNow?	178	
What	ifthe	PowerFlow	user	interface	is	down	and	Incidents	are	not	being	generated	in	ServiceNow?	178	
What	ifmy	Incident	does	not	have	aCI?	178	
How	can	Ipoint	the	""latest""	container	to	my	latest	available	images	forÂ PowerFlow?	179	
Why	does	the	latest	tag	not	exist	after	the	initial	ISO	installation?	179	
How	do	Irestore	an	offline	backup	of	my	PowerFlow	system?	179	
What	do	IÂ do	ifÂ I	get	aCode	500	Error	when	Itry	to	access	the	PowerFlow	user	interface?	180	
What	should	IÂ do	ifIÂ get	a500	Error?	181	
What	are	some	common	examples	of	using	the	iscli	tool?	181	
How	do	Iview	aspecific	run	of	an	application	in	PowerFlow?	182	
Why	am	Igetting	an	""ordinal	not	in	range""	step	error?	182	
How	do	IÂ clear	abacklog	of	Celery	tasks	in	Flower?	182	
Why	does	traffic	from	specific	subnets	not	get	aresponse	from	the	PowerFlow?	183 

Initial	Troubleshooting	Steps	
I n i ti a l	T r o u b l e s h o o ti n g	S te p s	
PowerFlow	acts	as	amiddle	server	between	data	platforms.	For	this	reason,	the	first	steps	should	always	be	to	
ensure	that	there	are	no	issues	with	the	data	platforms	with	which	PowerFlow	is	talking.	There	might	be	additional	
configurations	or	actions	enabled	on	ServiceNow	or	SL1	that	result	in	unexpected	behavior.	For	detailed	
information	about	how	to	perform	the	steps	below,	see	Resources	for	Troubleshooting	.	
S L 1	P ow e r F l ow	
1.	Run	docker	service	ls	on	the	PowerFlow	server.	
2.	Note	the	Docker	container	version,	and	verify	that	the	Docker	services	are	running.	
3.	Ifacertain	service	is	failing,	make	anote	the	service	name	and	version.	
4.	Ifacertain	service	is	failing,	run	docker	service	ps	<service_	name>	to	see	the	historical	state	of	the	
service	and	make	anote	of	this	information.	For	example:	docker	service	ps	iservices_	contentapi	.	
5.	Make	anote	of	any	logs	impacting	the	service	by	running	docker	service	logs	<service_	name>	.For	
example:	docker	service	logs	iservices_	couchbase	.	
S e r v i c e N ow	
1.	Make	anote	of	the	ServiceNow	version	and	Synchronization	PowerPack	version,	ifapplicable.	
2.	Make	anote	of	whether	the	user	is	running	an	update	set	or	aversion	of	the	Certified/Scoped	application,	if	
relevant.	
3.	Make	anote	of	the	ServiceNow	application	that	is	failing	on	PowerFlow	.	
4.	Make	anote	of	what	step	is	failing	in	the	application,	try	running	the	application	in	debug	mode,	and	capture	
any	traceback	or	error	messages	that	occur	in	the	step	log.	
R e s o u r c e s	f o r	T r o u b l e s h o o ti n g	
This	section	contains	port	information	for	PowerFlow	and	troubleshooting	commands	for	Docker,	Couchbase,	and	
the	PowerFlow	API.	
U s e f u l	P ow e r F l ow	P or t s	
l	http://	<IP	ofÂ 	PowerFlow	>	:8081	.Provides	access	to	Docker	Visualizer,	avisualizer	for	Docker	Swarm.	
l	https://	<IP	ofÂ 	PowerFlow	>	:8091	.Provides	access	to	Couchbase,	aNoSQL	database	for	storage	and	
data	retrieval.	
l	https://	<IP	ofÂ 	PowerFlow	>	:15672	.Provides	access	to	the	RabbitMQ	Dashboard,	which	you	can	use	to	
monitor	the	service	that	distributes	tasks	to	be	executed	by	PowerFlow	workers.	Use	guest/guest	for	the	login.	
l	https://	<IP	ofÂ 	PowerFlow	>	/flower	.Provides	access	to	Flower,	atool	for	monitoring	and	administrating	
Celery	clusters.	
159 

160	
NOTE:	For	version	2.0.0	and	later	of	PowerFlow	,port	5556	must	be	open	for	both	PowerFlow	and	the	
client.	
p f c t l	h e a l t h c h e c k	a n d	a u t oh e a l	
The	PowerFlow	includes	acommand-	line	utility	called	pfctl	that	performs	multiple	administrator-	level	actions	in	a	
clustered	PowerFlow	environment.	The	pfctl	utility	contains	automatic	cluster	healthcheck	and	autoheal	
capabilities	that	you	can	use	to	prevent	issues	with	your	PowerFlow	environment.	
For	more	information,	see	Using	the	pfctl	Command-	line	Utility	.	
H e l p f u l	D oc k e r	C om m a n d s	
PowerFlow	is	aset	of	services	that	are	containerized	using	Docker.	For	more	information	about	Docker,	see	the	
Docker	tutorial	.	
Use	the	following	Docker	commands	for	troubleshooting	and	diagnosing	issues	with	PowerFlow	:	
V i e w i n g	C o n t a i n e r	V e r s i o n s	a n d	S t a t u s	
To	view	the	PowerFlow	version,	SSH	to	your	PowerFlow	instance	and	run	the	following	command:	
docker	service	ls	
In	the	results,	you	can	see	the	container	ID,	name,	mode,	status	(see	the	replicas	column),	and	version	(see	the	
image	column)	for	all	the	services	that	make	up	PowerFlow	:	
R e s t a r t i n g	a	S e r v i c e	
Run	the	following	command	to	restart	asingle	service:	
docker	service	update	--force	<service_	name>	
S t o p p i n g	a l l	P o w e r F l o w	Â S e r v i c e s	
Run	the	following	command	to	stop	all	PowerFlow	services:	
docker	stack	rm	iservices	
R e s t a r t i n g	D o c k e r	
Run	the	following	command	to	restart	Docker:	
Resources	for	Troubleshooting 

Resources	for	Troubleshooting	
systemctl	restart	docker	
NOTE:	Restarting	Docker	does	not	clear	the	queue.	
V i e w i n g	L o g s	f o r	a	S p e c i f i c Â S e r v i c e	
You	can	use	the	Docker	command	line	to	view	the	logs	of	any	current	running	service	in	the	PowerFlow	cluster.	To	
view	the	logs	of	any	service,	run	the	following	command:	
docker	service	logs	-f	iservices_	<service_	name	>	
Some	common	examples	include	the	following:	
docker	service	logs	âf	iservices_	couchbase	
docker	service	logs	âf	iservices_	steprunner	
docker	service	logs	âf	iservices_	contentapi	
NOTE:	Application	logs	are	stored	on	the	central	database	as	well	as	on	all	of	the	Docker	hosts	in	aclustered	
environment.	These	logs	are	stored	at	/var/log/iservices	for	both	single-	node	or	clustered	
environments.	However,	the	logs	on	each	Docker	host	only	relate	to	the	services	running	on	that	host.	
For	this	reason,	using	the	Docker	service	logs	is	the	best	way	to	get	logs	from	all	hosts	at	once.	
C l e a r i n g	R a b b i t M Q	V o l u m e	
RabbitMQ	is	aservice	that	distributes	tasks	to	be	executed	by	PowerFlow	workers.	This	section	covers	how	to	
handle	potential	issues	with	RabbitMQ.	
The	following	error	message	might	appear	ifyou	try	to	run	aPowerFlow	application	via	the	API:	
Internal	error	occurred:	Traceback	(most	recent	call	last):\n	File	\"./content_	
api.py\",	line	199,	in	kickoff_	application\n	task_	status	=	...	line	623,	in	_on_	
close\n	(class_	id,	method_	id),	ConnectionError)\nInternalError:	Connection.open:	(541)	
INTERNAL_	ERROR	-	access	to	vhost	'/'	refused	for	user	'guest':	vhost	'/'	is	down	
First,	verify	that	your	services	are	up.	Ifthere	is	an	issue	with	your	RabbitMQ	volume,	you	can	clear	the	volume	with	
the	following	commands:	
docker	service	rm	iservices_	rabbitmq	
docker	volume	rm	iservices_	rabbitdb	
Ifyou	get	amessage	stating	that	the	volume	is	in	use,	run	the	following	command:	
docker	rm	<id	of	container	using	volume>	
Re-	deploy	PowerFlow	by	running	the	following	command:	
docker	stack	deploy	-c	/opt/iservices/scripts/docker-	compose.yml	iservices	
161 

162	
NOTE:	Restarting	Docker	does	not	clear	the	queue,	because	the	queue	is	persistent.	However,	clearing	the	
queue	with	the	commands	above	might	result	in	data	loss	due	to	the	tasks	being	removed	from	the	
queue.	
V i e w i n g	t h e	P r o c e s s	S t a t u s	o f	A l l	S e r v i c e s	
Run	the	following	command:	
docker	ps	
D e p l o y i n g	S e r v i c e s	f r o m	a	D e f i n e d	D o c k e r	C o m p o s e	F i l e	
Run	the	following	command:	
docker	stack	deploy	-c	<compose-	file>	iservices	
D y n a m i c a l l y	S c a l i n g	f o r	M o r e	W o r k e r s	
Run	the	following	command:	
docker	service	scale	iservices_	steprunner=10	
C o m p l e t e l y	R e m o v i n g	S e r v i c e s	f r o m	R u n n i n g	
Run	the	following	command:	
docker	stack	rm	iservices	
H e l p f u l	C ou c h b a s e	C om m a n d s	
C h e c k i n g	t h e	C o u c h b a s e	C a c h e	t o	E n s u r e	a n	S L 1	D e v i c e	I D	i s	L i n k e d	t o	a	
S e r v i c e N o w Â S y s	I D	
You	can	determine	how	an	SL1	device	links	to	aServiceNow	CI	record	by	using	the	respective	device	and	sys	IDs.	
You	can	retrieve	these	IDs	from	the	PowerFlow	Couchbase	service.	
First,	locate	the	correlation	ID	with	the	following	Couchbase	query:	
select	meta	().id	from	logs	where	meta	().id	like	""lookup%""	
This	query	returns	results	similar	to	the	following:	
[	
{	
""id"":	""lookup-	ScienceLogicRegion+DEV+16""	
},
{	
""id"":	""lookup-	ScienceLogicRegion+DEV+17""	
}	
]	
Resources	for	Troubleshooting 

Resources	for	Troubleshooting	
After	you	locate	the	correlation	ID,	run	the	following	query:	
select	cache_	data	from	logs	where	meta	().id	=	""lookup-	ScienceLogicRegion+DEV+16""	
This	query	returns	the	following	results:	
[	
{	
""cache_	data"":	{	
""company"":	""d6406d3bdbc72300c40bdec0cf9619c2"",	
""domain"":	null,	
""snow_	ci"":	""u_	cmdb_	ci_	aws_	service"",	
""sys_	id"":	""0c018f14dbd36300f3ac70adbf9619f7""	
}	
}	
]	
A c c e s s i n g	C o u c h b a s e	w i t h	t h e	C o m m a n d -	l i n e	I n t e r f a c e	
Ifyou	don't	have	access	to	port	8091	on	your	PowerFlow	instance,	you	can	connect	to	the	Couchbase	container	by	
using	the	command-	line	interface	(CLI).	
To	access	Couchbase	by	using	the	CLI,	run	the	following	commands:	
docker	exec	-it	<container_	id>	/bin/bash	
cbq	-u	<username>	-p	<password>	-e	""https://	<localhost>	:8091""	
U s e f u l	A P I Â C om m a n d s	
G e t t i n g Â 	P o w e r F l o w	A p p l i c a t i o n s	f r o m	t h e	P o w e r F l o w	A P I	
You	can	use	the	API	or	cURL	to	retrieve	the	application	code,	which	is	useful	when	you	are	troubleshooting	
potential	code-	related	issues.	You	cannot	access	these	API	endpoints	with	abrowser,	but	you	can	request	these	
API	endpoints	by	using	an	application	such	as	Postman:	
https://<	PowerFlow	>/api/v1/applications/<	application_	name	>	
Ifyou	do	not	have	access	to	Postman,	you	can	use	cURL	to	get	the	same	information.	
curl	-iku	<username	>:<	password	>	-H	""Accept:	application/json""	-H	""Content-	Type:	
application/json""	-X	GET	https://<	PowerFlow	>/api/v1/applications/<	application_	name	>	
C r e a t i n g	a n d	R e t r i e v i n g	S c h e d u l e s	w i t h	t h e	P o w e r F l o w	A P I	
You	can	define	and	retrieve	schedules	using	the	PowerFlow	API.	You	can	define	all	of	these	schedules	in	the	
PowerFlow	user	interface	as	well.	
To	create	aschedule	via	the	API,	POST	the	following	payload	to	the	API	endpoint:	
https://<	PowerFlow	>/api/v1/schedule	
{	
163 

164	
""application_	id"":	""APP_	ID"",	
""entry_	id"":	""SCHEDULE_	NAME"",	
""params"":	{""a"":""B""},	
""schedule"":	{	
""schedule_	info"":	{	
""day_	of_	month"":	""*"",	
""day_	of_	week"":	""*"",	
""hour"":	""*"",	
""minute"":	""*"",	
""month_	of_	year"":	""*""	
},
""schedule_	type"":	""crontab""	
},
""total_	runs"":	0	
}
You	can	also	specify	the	schedule	to	run	on	afrequency	in	seconds	by	replacing	the	schedule	portion	with	the	
following:
""schedule"":	{	
""schedule_	info"":	{	
""run_	every"":	FREQUENCY_	IN_	SECONDS	
},
""schedule_	type"":	""frequency""	
}	
D i a g n os i s	T ool s	
Multiple	diagnosis	tools	exist	to	assist	in	troubleshooting	issues	with	the	PowerFlow	platform:	
l	Docker	PowerPack	.This	PowerPack	monitors	your	Linux-	basedÂ 	PowerFlow	server	with	SSHÂ 	(the	PowerFlow	
ISO	is	built	on	top	of	an	Oracle	Linux	Operating	System).	This	PowerPack	provides	key	performance	
indicators	about	how	your	PowerFlow	server	is	performing.	For	more	information	on	the	Docker	PowerPack	
and	other	PowerPacks	that	you	can	use	to	monitor	PowerFlow	,see	the	""Using	SL1	to	Monitor	SL1	
PowerFlow	""chapter	in	the	SL1	PowerFlow	Platform	manual	.	
l	Flower.	This	web	interface	tool	can	be	found	at	the	/flower	endpoint.	Itprovides	adashboard	displaying	the	
number	of	tasks	in	various	states	as	well	as	an	overview	of	the	state	of	each	worker.	This	tool	shows	the	
current	number	of	active,	processed,	failed,	succeeded,	and	retried	tasks	on	the	PowerFlow	platform.	This	
tool	also	shows	detailed	information	about	each	of	the	tasks	that	have	been	executed	on	the	platform.	This	
data	includes	the	UUID,	the	state,	the	arguments	that	were	passed	to	it,	as	well	as	the	worker	and	the	time	of	
execution.	Flower	also	provides	aperformance	chart	that	shows	the	number	of	tasks	running	on	each	
individual	worker.	
l	Debug	Mode	.All	applications	can	be	run	in	""debug""	mode	via	the	PowerFlow	API.	Running	applications	in	
debug	mode	may	slow	down	the	platform,	but	they	will	result	in	much	more	detailed	logging	information	that	
is	helpful	for	troubleshooting	issues.	For	more	information	on	running	applications	in	Debug	Mode,	see	
Retrieving	Additional	Debug	Information	.	
l	Application	Logs	.All	applications	generate	alog	file	specific	to	that	application.	These	log	files	can	be	found	
at	/var/log/iservices	and	each	log	file	will	match	the	ID	of	the	application.	These	log	files	combine	all	the	log	
messages	of	all	previous	runs	of	an	application	up	to	acertain	point.	These	log	files	roll	over	and	will	get	
auto-	cleared	after	acertain	point.	
Resources	for	Troubleshooting 

Identifying	Why	a	Service	or	Container	Failed	
l	Step	Logs	.Step	logs	display	the	log	output	for	aspecific	step	in	the	application.	These	step	logs	can	be	
accessed	via	the	PowerFlow	user	interface	by	clicking	on	astep	in	an	application	and	bringing	up	the	Step	
Log	tab.	These	step	logs	display	just	the	log	output	for	the	latest	run	of	that	step.	
l	Service	Logs	.Each	Docker	service	has	its	own	log.	These	can	be	accessed	via	SSH	by	running	the	following	
command:	
docker	service	logs	-f	<service_	name>	
I d e n ti f y i n g	W h y	a	S e r v i c e	o r	C o n ta i n e r	F a i l e d	
This	section	outlines	the	troubleshooting	steps	necessary	to	determine	the	underlying	root	cause	of	why	aservice	or	
container	was	restarted.	For	this	section,	we	use	the	iservices_	redis	service	as	an	example.	
S t e p	1 :	O b t a i n	t h e	I D	of	t h e	f a i l e d	c on t a i n e r	f or	t h e	s e r v i c e	
Run	the	following	command	for	the	service	that	failed	previously:	
docker	service	ps	--no-	trunc	<servicename>	
For	example:	
docker	service	ps	--no-	trunc	iservices_	redis	
From	the	command	result	above,	we	see	that	one	container	with	the	id	3s7s86n45skf	had	failed	previously	while	
running	on	node	is-	scale-	03	,with	the	error	""non-	zero	exit"",	and	another	container	was	restarted	in	its	place.	
At	this	point,	we	can	ask	the	following	questions:	
l	When	you	run	docker	service	ps	--no-	trunc	,is	the	error	something	obvious?	Does	the	error	say	that	it	
cannot	mount	avolume,	or	that	the	image	is	not	found?	Ifso,	that's	most	likely	the	root	cause	of	the	issue	and	
what	needs	to	be	addressed	
l	Did	the	node	on	which	that	container	was	running	go	down?	Or	is	that	node	still	up?	
l	Are	the	other	services	running	on	that	node	running	fine?Â Was	only	this	single	service	affected?	
l	Ifother	services	are	running	fine	on	that	same	node,	itis	probably	aproblem	with	the	service	itself.	Ifall	
services	on	that	node	are	not	functional,	itcould	mean	anode	failure.	
At	this	point,	we	should	be	confident	that	the	cause	of	the	issue	is	not	adeploy	configuration	issue,	itis	not	an	entire	
node	failure,	and	the	problem	exists	within	the	service	itself.	Continue	to	Step	2	ifthis	is	the	case.	
S t e p	2 :	C h e c k	f or	a n y	e r r or	m e s s a g e s	or	l og s	i n d i c a t i n g	a n	e r r or	
Using	the	id	obtained	from	step	1	we	can	collect	the	logs	from	the	failed	container	with	the	following	commands:	
docker	service	logs	<failed-	id>	
165 

166
For	example:	
docker	service	logs	3s7s86n45skf	
Search	the	service	logs	for	any	explicit	errors	or	warning	messages	that	might	indicate	why	the	failure	occurred.	
Usually,	you	can	find	the	error	message	in	those	logs,	but	ifthe	container	ran	out	of	memory,	itmay	not	be	seen	
here.	Continue	to	Step	3	ifthe	logs	provide	nothing	fruitful.	
S t e p	3 :	C h e c k	f or	ou t	of	m e m or y	e v e n t s	
Ifthere	were	no	errors	in	the	logs,	or	anywhere	else	that	can	be	seen,	apossible	cause	for	acontainer	restart	could	
be	that	the	system	ran	out	of	memory.	
Perform	the	following	steps	to	identify	ifthis	is	the	case:	
1.	Log	in	to	the	node	where	the	container	failed	in	our	example.Â As	seen	in	step	1,	the	container	failed	on	is-	
scale-	03	.	
2.	From	the	node	where	the	container	failed,	run	the	following	command:	
journalctl	-k	|	grep	-i	-e	memory	-e	oom	
3.	Check	the	result	for	any	out	of	memory	events	that	caused	the	container	to	stop.	Such	an	event	typically	looks	
like	the	following:
is-	scale-	03	kernel:	Out	of	memory:	Kill	process	5946	(redis-	server)	score	575	
or	sacrifice	child	
T r ou b l e s h oot i n g	a	C l ou d	D e p l oy m e n t	of	P ow e r F l ow	
After	completing	the	AWS	setup	instructions,	ifnone	of	the	services	start	and	you	see	the	following	error	during	
troubleshooting,	the	problem	is	that	you	need	to	restart	Docker	after	installing	the	RPM	installation.	
sudo	docker	service	ps	iservices_	couchbase	--no-	trunc	
""error	creating	external	connectivity	network:	Failed	to	Setup	IP	tables:	Unable	to	
enable	SKIP	DNAT	rule:	(iptables	failed:	iptables	--wait	-t	nat	-I	DOCKER	-i	docker_	
gwbridge	-j	RETURN:	iptables:	No	chain/target/match	by	that	name.""	
Identifying	Why	a	Service	or	Container	Failed 

Identifying	Why	aPowerFlow	Application	Failed	
I d e n ti f y i n g	W h y	a	P o w e r F l o w	A p p l i c a ti o n	F a i l e d	
D e t e r m i n i n g	W h e r e	a n	A p p l i c a t i on	F a i l e d	
IfaPowerFlow	application	fails,	afailure	icon	(	)appears	under	that	application	on	the	Application	detail	
page.
To	determine	where	the	application	is	failing:	
1.	Open	the	application	and	locate	which	step	is	failing.	A	failed	step	is	highlighted	in	red	in	the	image	above.	
2.	Select	the	step	and	click	the	StepÂ Log	tab	to	view	the	logs	for	that	step.	
3.	Review	the	error	message	to	determine	the	next	steps.	
R e t r i e v i n g	A d d i t i on a l	D e b u g	I n f or m a t i on	( D e b u g	M od e )	
The	logs	in	PowerFlow	use	the	following	loglevel	settings,	from	most	verbose	to	least	verbose:	
l	10	.DebugÂ Mode.	
l	20	.Informational.	
l	30	.Warning.	This	is	the	default	settings	ifyou	do	not	specify	aloglevel.	
l	40	.Error.	
167 

168	
WARNING:	Ifyou	run	applications	with	""loglevel"":	10,	those	applications	will	take	longer	to	run	because	of	
increased	I/O	requirements.	Enabling	debug	logging	using	the	following	process	is	the	only	
recommended	method.	ScienceLogic	does	not	recommend	setting	""loglevel"":	10	for	the	whole	
stack	with	the	docker-	compose	file.	
To	run	an	application	in	Debug	Mode,	POST	the	following	to	the	API	endpoint:	
https://<	PowerFlow	>/api/v1/applications/run	
Request	body:	
{	
""name"":	""<application_	name>"",	
""params"":	{	
""loglevel"":	10	
}	
}
After	running	the	application	in	Debug	Mode,	go	back	to	the	PowerFlow	user	interface	and	review	the	step	logs	to	
see	detailed	debug	output	for	each	step	in	the	application.	When	run	in	Debug	Mode,	the	step	log	output	shows	
additional	debug	statements	such	as	""Saved	data	for	next	step"",	which	displays	the	data	being	sent	from	one	step	to	
the	next.	
This	information	is	especially	helpful	when	trying	to	understand	why	an	application	or	step	failed:	
You	can	also	run	an	application	in	debug	using	curl	via	SSH:	
Identifying	Why	aPowerFlow	Application	Failed 

Frequently	Asked	Questions	
1.	SSH	to	the	PowerFlow	instance.	
2.	Run	the	following	command:	
curl	-v	-k	-u	isadmin:em7admin	-X	POST	""https://	<your_	
hostname>	/api/v1/applications/run""	-H	'Content-	Type:	application/json'	-H	'cache-	
control:	no-	cache'	-d	'{""name"":	""interface_	sync_	sciencelogic_	to_	
servicenow"",""params"":	{""loglevel"":	10}}'	
F r e q u e n tl y	A s k e d	Q u e s ti o n s	
This	section	contains	aset	of	frequently	asked	questionsÂ 	(FAQs)	and	the	answers	to	address	those	situations.	
TIP:	For	additional	troubleshooting	information	specific	to	multi-	tenant	environments,	see	Common	
Problems,	Symptoms,	and	Solutions	in	the	PowerFlow	for	Multi-	tenant	Environments	appendix.	
W h a t	i s	t h e	f i r s t	t h i n g	I	s h ou l d	d o	w h e n	I	h a v e	a n	i s s u e	w i t h	
P ow e r F l ow	?	
Ensure	that	all	the	services	are	up	and	running	by	running	the	following	command:	
docker	service	ls	
C a n	t h e	s t e p r u n n e r _	s y n c p a c k s	s e r v i c e	c a n	b e	l i m i t e d	t o	j u s t	w or k e r s ?	
The	syncpack_	steprunners	are	responsible	for	keeping	all	Synchronization	PowerPack	virtual	environments	in	sync	
on	all	nodes.	As	you	install	or	upgrade	Synchronization	PowerPacks	,these	steprunners	consistently	maintain	the	
virtual	environment	changes	throughout	the	cluster	without	network	storage.	Disabling	these	steprunners	will	not	
affect	the	system	operationally	unless	you	need	to	update,	modify,	or	install	new	Synchronization	PowerPacks	onto	
the	nodes	of	asystem.	
W h a t	i s	t h e	d i f f e r e n c e	b e t w e e n	t h e	s t e p r u n n e r _	s y n c p a c k s	a n d	t h e	
s t e p r u n n e r	s e r v i c e s ?	
The	syncpack_	steprunners	accept	no	other	tasks,	beside	create	and	update	tasks	for	the	virtual	environment.	
W h a t	i s	t h e	m i n i m a l	i m a g e	r e q u i r e d	f or	w or k e r s ?	
At	aminimum,	worker	nodes	that	will	only	be	used	to	process	tasks	will	need	just	the	worker	image.	
169 

170
I f	t h e	G U I	s e r v e r	i s	c on s t r a i n e d	t o	u s e	on l y	t h e	m a n a g e r	n od e s ,	d o	t h e	
w or k e r	n od e s	n e e d	t o	h a v e	t h e i r	i s c on f i g . y m l	f i l e	u p d a t e d	w i t h	t h e	
c or r e c t	H O S T	v a l u e ? Â Â Â Â 	
The	isconfig.yml	file	is	applied	to	all	nodes	in	the	cluster	when	the	stack	is	deployed.	The	config	used	is	created	
from	the	config	on	the	system	where	you	actually	run	Docker	stack	deploy	and	applied	to	all	other	nodes.	In	other	
words,	only	the	managers	from	which	you	are	deploying	the	stack	need	to	have	the	isconfig.yml	value	correct.	
C a n	I	u n l oa d	u n w a n t e d	i m a g e s	f r om	a	w or k e r	n od e ?	
Removing	unwanted	images	is	part	of	typical	Docker	operations,	and	ituses	the	following	command:	docker	
image	rm	<image	name>	
For	more	information,	see	https://docs.docker.com/engine/reference/commandline/image_	rm/.	
I f	I	d e d i c a t e d	w or k e r s	t o	on e	S L 1	s t a c k ,	h ow	a r e	j ob s	c on f i g u r e d	t o	
r u n	on l y	on	t h os e	w or k e r s ?	
You	create	aworker	service	with	the	user_	queues	variable	set,	with	alist	of	queues	that	you	want	that	worker	to	
process	only.	The	queues	are	auto-	created.	That	worker	service	can	also	be	pinned	to	run	on	specific	nodes.	You	
can	tell	an	integration	to	run	on	aspecific	queue	at	runtime,	on	aschedule,	or	in	the	application	configuration.	The	
queue	will	be	processed	by	the	workers	that	you	previously	defined.	Ifno	workers	are	listening	to	the	queue,	the	
task	will	not	process.	
A p p r ox i m a t e l y	h ow	m u c h	d a t a	i s	s e n t	b e t w e e n	d i s t r i b u t e d	P ow e r F l ow	
n od e s ?
The	amount	of	data	sent	between	distributed	PowerFlow	nodes	largely	depends	on	the	applications	that	are	
currently	running,	the	size	of	the	syncs	for	those	applications,	and	how	much	of	the	cache	those	syncs	use.	In	
general,	only	configuration	files,	cache,	and	some	logs	are	stored	or	replicated	between	database	nodes,	while	
the	queue	service	mirrors	messages	between	its	nodes	as	well.	As	aresult,	not	alot	of	information	is	being	
replicating	at	any	given	time,	as	only	necessary	cluster	and	vital	data	are	replicated.	
H ow	c a n	I	op t i m i z e	w or k e r s ,	q u e u e s ,	a n d	t a s k s ?	
The	PowerFlow	system	uses	Celery	to	spawn	and	manage	worker	processes	and	queues.	You	can	define	
environment	variables	to	optimize	these	worker	processes.	
You	can	use	the	following	environment	variables	to	optimize	worker	processes:	
l	task_	soft_	time_	limit	.Enforces	global	timeout	for	all	tasks	in	the	PowerFlow	system	.Ifatask	exceeds	the	
specified	timeout	limit,	the	PowerFlow	system	terminates	the	task	so	that	the	next	task	in	the	queue	can	be	
processed.	Possible	values	are:	
o	Integer	that	specifies	the	time	in	seconds.	
o	Default	value	is	""3600""	(1	hour).	
Frequently	Asked	Questions 

Frequently	Asked	Questions	
l	optimization	.Determines	how	tasks	are	distributed	to	worker	processes.	Possible	values	are:	
o	-Ofair	.Celery	distributes	atask	only	to	the	worker	process	that	is	available	for	work.	
o	""""(double-	quotation	mark,	space,	double-	quotation	mark).	Distributes	and	queues	all	tasks	to	all	
available	workers.	Although	this	increases	performance,	tasks	in	queues	might	be	delayed	waiting	for	
long-	running	tasks	to	complete.	
o	Default	value	is	-Ofair	.	
l	task_	acks_	late	.Specifies	that	ifaworker	process	crashes	while	executing	atask,	Celery	will	redistribute	the	
task	to	another	worker	process.	Possible	values	are:	
o	True	.Enables	the	environment	variable.	
o	False	.disabled	the	environment	variable.	
o	Default	value	is	""False""	
NOTE	:Because	many	applications	run	at	regular	intervals	or	are	scheduled,	the	PowerFlow	system	re-	
executes	tasks	even	ifthe	task_	acks_	late	environment	variable	is	disabled.	in	the	event	of	aworker	
crash,	ifyou	want	to	ensure	that	tasks	are	completed,	you	can	enable	the	task_	acks_	late	variable.	
However,	be	aware	that	iftasks	are	not	idempotent,	the	task_	acks_	late	variable	can	cause	
unpredictable	results.	
To	define	these	environment	variables:	
1.	Either	go	to	the	console	of	the	PowerFlow	system	or	use	SSH	to	access	the	server.	
2.	Log	in	as	isadmin	with	the	appropriate	password.	
3.	Use	atext	editor	like	vi	to	edit	the	file	/opt/iservices/scripts/docker-	compose.yml	.	
4.	You	can	define	the	environment	variables	for	one	or	more	worker	processes.	The	docker-	compose.yml	file	
contains	definitions	for	worker	processes.	For	example,	you	might	see	something	like	this:	
services:	
steprunner	:	
image:	sciencelogic/is-	worker:latest	
environment:
LOGLEVEL:	10	
celery_	log_	level:	10	
logdir:	/var/log/iservices	
broker_	url:	'pyamqp://guest@rabbit//'	
result_	backend:	'redis://redis:6379/0'	
db_	host:	'couchbase,localhost'	
secrets:	
-	is_	pass	
-	encryption_	key	
deploy:
replicas:	2	
networks:	
-	isnet	
depends_	on:	
-	redis	
-	rabbitmq	
-	couchbase	
171 

172	
volumes:	
-	""/var/log/iservices:/var/log/iservices""	
-	""statedb:/var/run/celery/states/""	
steprunner_	1:	
image:	:	sciencelogic/is-	worker:latest	
environment	:	
LOGLEVEL:	10	
celery_	log_	level:	10	
task_	soft_	time_	limit:	30	
optimization:	""""	
task_	acks_	late:	'False'	
logdir:	/var/log/iservices	
broker_	url:	'pyamqp://guest@rabbit//'	
result_	backend:	'redis://redis:6379/0'	
db_	host:	'couchbase,localhost'	
secrets:	
-	is_	pass	
-	encryption_	key	
deploy:
replicas:	2	
networks:	
-	isnet	
depends_	on:	
-	redis	
-	rabbitmq	
-	couchbase	
volumes:	
-	""/var/log/iservices:/var/log/iservices""	
-	""statedb:/var/run/celery/states/""	
steprunner_	2:	
image:	:	sciencelogic/is-	worker:latest	
environment	:	
LOGLEVEL:	10	
celery_	log_	level:	10	
task_	soft_	time_	limit:	30	
optimization:	'-	Ofair'	
task_	acks_	late:	'False'	
logdir:	/var/log/iservices	
broker_	url:	'pyamqp://guest@rabbit//'	
result_	backend:	'redis://redis:6379/0'	
db_	host:	'couchbase,localhost'	
secrets:	
-	is_	pass	
-	encryption_	key	
deploy:
replicas:	2	
networks:	
-	isnet	
depends_	on:	
-	redis	
-	rabbitmq	
-	couchbase	
volumes:	
-	""/var/log/iservices:/var/log/iservices""	
-	""statedb:/var/run/celery/states/""	
Frequently	Asked	Questions 

Frequently	Asked	Questions	
5.	The	services	with	names	that	start	with	""steprunner""	are	the	workers	for	the	PowerFlow	system	.To	define	the	
optimization	variables,	enter	the	variables	and	values	in	the	definition	of	the	worker,	under	the	environment	
section.	See	the	example	in	step	#3	to	see	the	syntax	for	environment	variables.	
6.	After	you	have	updated	the	docker-	compose	file,	you	can	update	and	re-	deploy	the	PowerFlow	system	to	
pick	up	your	changes	to	the	file.	To	do	this,	execute	the	following	command:	
docker	stack	deploy	-c	/opt/iservices/scripts/docker-	compose.yml	is4	
W h y	d o	I Â g e t	a	"" C on n e c t i on	r e f u s e d "" Â e r r or	w h e n	t r y i n g	t o	
c om m u n i c a t e	w i t h	C ou c h b a s e ?	
Ifyou	get	a""Connection	refused	to	Couchbase:8091""	error	when	you	are	trying	to	communicate	with	Couchbase,	
check	the	firewalld	service	by	running	the	following	command:	
systemctl	status	firewalld	
Firewalld	is	responsible	for	all	of	the	internal	communications	between	the	various	Docker	services	on	the	Docker	
Swarm.	Iffirewalld	is	not	active,	there	will	be	no	communications	between	the	services,	and	you	might	see	an	
error	like	""Connection	refused	to	Couchbase:8091"".	
To	start	the	firewalld	service,	run	the	following	command:	
systemctl	start	firewalld	
W h y	d o	I	h a v e	c l i e n t -	s i d e	t i m e ou t s	w h e n	c om m u n i c a t i n g	w i t h	
C ou c h b a s e ?
Ifyou	are	running	an	intensive	application,	or	ifyou	are	running	in	Debug	Mode,	you	might	see	the	following	stack	
trace	error:	
(generated,	catch	TimeoutError):	<RC=0x17	[Client-	Side	timeout	exceeded	for	operation.	
Inspect	network	conditions	or	increase	the	timeout],	HTTP	Request	failed.	Examine	
'objextra'	for	full	result,	Results=1,	C	Source=	(src/http.c,144),	
OBJ=ViewResult<rc=0x17	[Client-	Side	timeout	exceeded	for	operation.	Inspect	network	
conditions	or	increase	the	timeout],	value=None,	http_	status=0,	tracing_	context=0,	
tracing_	output=None>,	Tracing	Output=	{"":nokey:0"":	null}>	
This	error	occurs	when	there	is	too	much	load	going	into	the	Couchbase	database.	Ifyou're	running	with	Debug	
Mode,	that	mode	creates	alarge	number	of	extra	log	messages	in	the	database,	which	can	contributeto	this	error.	
To	work	around	this	issue,	increase	the	timeout	being	used	by	setting	the	db_	host	environment	variable	in	the	
steprunner	service:	
db_	host:	'couchbase.isnet,localhost?n1ql_	timeout=100000000.00'	
Ifyou	increase	the	timeout,	the	timeout	errors	should	go	away.	
173 

174	
NOTE:	Increasing	the	timeout	might	not	always	be	the	correct	action.	Ifyou	are	using	an	especially	large	
system,	you	might	want	to	allocate	additional	resources	to	the	Couchbase	services,	including	more	
memory	for	indexing	and	search.	Ifyou	are	encountering	timeouts	in	anon-	temporary	fashion,	such	
as	only	running	Debug	Mode	for	an	applicationto	determine	what	went	wrong,	you	might	want	to	add	
more	resources	instead	of	increasing	the	timeout.	
W h a t	c a u s e s	a	T a s k	S of t	T i m e ou t ?	
The	following	error	might	occur	when	you	see	along-	running	task	fail:	
raise	SoftTimeLimitExceeded	()	SoftTimeLimitExceeded:	SoftTimeLimitExceeded	()	
This	error	message	means	that	the	default	timeout	for	atask	on	your	PowerFlow	system	is	too	low.	By	default	the	
task	timeout,	which	is	set	by	the	environment	variable	task_	soft_	time_	limit	,is	set	to	3600	seconds	(1	hour).	
Ifyou	intend	to	have	tasks	executing	for	longer	than	an	hour	at	atime,	you	can	increase	this	setting	by	changing	the	
task_	soft_	time_	limit	environment	variable	in	your	steprunners.	Note	that	the	value	is	set	in	seconds.	
H ow	d o	I	a d d r e s s	a n	"" E r r or	w h e n	c on n e c t i n g	t o	D B Â H os t ""	m e s s a g e	
w h e n	a c c e s s	i s	d e n i e d	t o	u s e r	"" r oot "" ?	
In	this	situation,	you	get	an	error	similar	to	the	following:	
Error	when	connecting	to	DB	Host...	(1045,	""Access	denied	for	user	
'root'@'10.86.21.224'	(using	password:	NO)""	
This	issue	occurs	when	the	encryption_	key	file	from	one	PowerFlow	system	does	not	match	the	encryption_	key	
file	on	another	system,	such	as	when	you	take	data	from	aproduction	PowerFlow	system	to	atest	PowerFlow	
system.
The	encryption	key	must	be	identical	between	two	PowerFlow	systems	ifyou	plan	to	migrate	from	one	to	another.	
The	encryption	key	must	be	identical	between	High	Availability	or	Disaster	Recovery	systems	as	well.	
To	address	this	issue:	
1.	Copy	the	encryption_	key	file	from	the	/etc/iservices/	folder	on	the	production	system	to	the	/etc/iservices/	
folder	on	the	test	system.	
2.	Re-	upload	all	applications	and	configurations	from	the	production	system	to	the	test	system.	
3.	Remove	and	redeploy	the	stack	on	the	test	PowerFlow	after	copying	the	key	by	running	the	following	
command	on	the	test	system:	
docker	stack	deploy	-c	/opt/iservices/scripts/docker-	compose.yml	iservices	
H ow	d o	I	r e m ov e	a	s c h e d u l e	t h a t	d oe s	n ot	h a v e	a	n a m e ?	
Ifyou	encounter	aschedule	in	PowerFlow	that	was	created	without	aname,	PowerFlow	cannot	update	or	delete	
that	schedule	using	the	API.	
Frequently	Asked	Questions 

Frequently	Asked	Questions	
NOTE:	This	issue	only	affects	versions	of	PowerFlow	prior	to	version	1.8.2.	
To	address	this	issue,	you	will	need	to	delete	all	schedules	on	PowerFlow	,which	involves	going	into	Couchbase	
and	deleting	afile.	
WARNING:	Exercise	extreme	caution	when	performing	this	procedure,	as	deleting	the	wrong	file	will	cause	
your	PowerFlow	instance	to	stop	working.	
To	delete	all	schedules,	including	aschedule	without	aname:	
1.	Log	in	to	the	Couchbase	management	interface	at	https://	<localhost>	:8091	.	
2.	Navigate	to	the	[Buckets	]tab.	
3.	Click	the	Documents	link	on	the	content	bucket:	
4.	On	the	content	>	documents	page,	extend	the	results	to	show	100	records	per	page	
5.	Search	for	""schedule"".	You	will	see	afile	similar	to	the	following	example:	
6.	Delete	the	file.	
WARNING:	Deleting	this	file	deletes	all	schedules	on	your	PowerFlow	instance.	
H ow	d o	I	i d e n t i f y	a n d	f i x	a	d e a d l oc k e d	s t a t e ?	
IfPowerFlow	appears	to	be	running,	but	itis	not	processing	any	new	applications	or	tasks,	then	PowerFlow	could	
be	in	adeadlocked	state.	
175 

176
A	deadlocked	state	occurs	when	one	or	more	applications	include	steps	that	are	either	ordered	improperly	or	that	
contain	syntax	errors.	In	this	situation,	tasks	are	waiting	on	subsequent	tasks	to	finish	executing,	but	the	worker	pool	
is	exhausted.	As	aresult,	PowerFlow	is	not	able	to	execute	those	subsequent	tasks.	
To	identify	adeadlocked	state	with	aPowerFlow	system	:	
1.	Navigate	to	the	Celery	Flower	interface	for	PowerFlow	by	typing	the	URL	or	IP	address	for	yourÂ 	PowerFlow	
and	adding	/flower	at	the	end	of	the	URL,	such	as	https://192.0.2.0/flower/	.	
2.	Click	the	[Dashboard	]tab	for	Flower.	A	list	of	workers	appears:	
3.	Review	the	number	of	active	or	running	tasks	for	all	workers.	Ifall	workers	have	the	maximum	number	of	
tasks,	and	no	new	tasks	are	being	consumed,	then	you	might	have	adeadlock	state.	
Frequently	Asked	Questions 

Frequently	Asked	Questions	
To	fix	adeadlocked	state	in	aPowerFlow	system	,perform	one	of	the	following	steps:	
1.	Go	to	the	console	of	the	PowerFlow	system	or	use	SSH	to	access	the	server.	
2.	Log	in	as	isadmin	with	the	appropriate	password.	
3.	Increase	the	number	of	workers	by	either:	
A.	Running	the	following	command	at	the	shell	prompt:	
docker	service	scale	iservices_	steprunner=	x	
where	xis	the	number	of	workers	
B.	Using	atext	editor	like	vi	to	edit	the	file	/opt/iservices/scripts/docker-	compose.yml	.In	the	
environment:	section	at	the	top	of	the	file,	add	the	following:	
worker_	threads:	number_	greater_	than_	3	
where	number_	greater_	than_	3is	an	integer	greater	than	3.	
4.	After	you	have	updated	the	docker-	compose	file,	you	can	update	and	re-	deploy	the	PowerFlow	system	to	
pick	up	the	changes	in	the	docker-	compose.yml	file	To	do	this,	execute	the	following	at	the	shell	prompt:	
docker	stack	deploy	-c	/opt/iservices/scripts/docker-	compose.yml	is4	
The	PowerFlow	system	should	now	include	additional	workers.	
5.	Navigate	to	the	Celery	Flower	interface	for	PowerFlow	by	typing	the	URL	or	IP	address	for	PowerFlow	and	
adding	/flower	at	the	end	of	the	URL,	such	as	https://192.0.2.0/flower/	.	
6.	Click	the	[Dashboard	]tab	for	Flower.	A	list	of	workers	appears:	
7.	Review	the	number	of	active	or	running	tasks	for	all	workers.	
177 

178
W h y	a r e	i n c i d e n t	n u m b e r s	n ot	p op u l a t e d	i n	S L 1	on	I n c i d e n t	c r e a t i on	
i n	S e r v i c e N ow ?	
Ifan	incident	exists	in	ServiceNow,	but	incident	data	is	not	getting	back	to	SL1	,and	the	""Sync	ServiceNow	Incident	
State	to	SL1	Event""	application	fails	on	the	""Get	Incident""	step	(with	a404	error)	and	eventually	times	out,	the	issue	
might	be	because	the	ServiceNow	API	is	overloaded.	
W h y	a m	I	n ot	g e t t i n g	a n y	I n c i d e n t s	a f t e r	d i s a b l i n g	t h e	f i r e w a l l ?	
Ifyou	disabled	the	firewall	to	enable	SNMP	monitoring	on	the	PowerFlow	,but	were	not	able	to	connect,	you	should	
add	the	additional	rule	you	need.	
W h y	a r e	I n c i d e n t s	n ot	g e t t i n g	c r e a t e d	i n	S e r v i c e N ow ?	
To	troubleshoot	why	Incidents	are	note	getting	created	in	ServiceNow:	
1.	In	SL1	,go	to	the	[Events	Console	](classic	user	interface)	or	the	Events	page	(new	user	interface)	and	
locate	the	event	that	was	created.	
2.	Click	the	View	Notification	Log	mailbox	icon	(	)for	that	event.	The	Event	ActionsÂ Log	window	appears.	
3.	On	the	Event	ActionsÂ Log	window,	verify	that	the	Run	Book	Action	was	triggered,	and	that	the	RunÂ Book	
Action	successfully	posted	to	the	PowerFlow	.	
4.	In	the	PowerFlow	user	interface,	get	the	associated	application	ID,	such	as	isapp-	24f2f1-	23etc	,for	that	run	of	
the	""Create	or	Update	ServiceNow	Incident	from	SL1	Event""	to	see	where	the	application	failed.	
5.	Look	at	the	logs	for	that	run	of	the	PowerFlow	application.	
W h a t	i f	t h e	P ow e r F l ow	u s e r	i n t e r f a c e	i s	d ow n	a n d	I n c i d e n t s	a r e	n ot	
b e i n g	g e n e r a t e d	i n	S e r v i c e N ow ?	
Ifthe	PowerFlow	user	interface	is	unresponsive	and	Incidents	are	not	being	generated	in	ServiceNow,	achange	to	
the	firewall	rules	during	deployment	might	have	impacted	the	ingress	network	for	Docker.	
To	address	this	issue,	restart	Docker	after	any	firewall	or	network	configuration	change	by	running	the	following	
command:
systemctl	restart	docker	
W h a t	i f	m y	I n c i d e n t	d oe s	n ot	h a v e	a	C I ?	
For	an	Incident	with	an	active	event:	
1.	In	SL1	,go	to	the	[Events	Console	](classic	user	interface)	or	the	Events	page	(new	user	interface)	and	
locate	the	event	that	was	created.	
2.	Click	the	View	Notification	Log	mailbox	icon	(	)for	that	event.	The	Event	ActionsÂ Log	window	appears.	
3.	On	the	Event	ActionsÂ Log	window,	locate	the	PowerFlow	application	run	ID.	
Frequently	Asked	Questions 

Frequently	Asked	Questions	
4.	In	the	PowerFlow	user	interface,	open	the	application	that	used	that	run	and	review	the	Step	Log.	
5.	Confirm	that	the	device	class	was	mapped	in	the	""Sync	Devices	from	SL1	to	ServiceNow""	application.	
6.	Confirm	that	the	""Sync	Devices	from	SL1	to	ServiceNow""	application	is	running	at	least	every	24	hours,	and	
that	the	""Sync	Devices	from	SL1	to	ServiceNow""	application	has	run	within	24	hours	of	that	event	sync	run.	
H ow	c a n	I	p oi n t	t h e	"" l a t e s t ""	c on t a i n e r	t o	m y	l a t e s t	a v a i l a b l e	i m a g e s	
f or Â 	P ow e r F l ow	?	
Ifyou	force-	upgraded	an	RPM	on	top	of	an	existing	PowerFlow	RPM	of	the	same	version	(such	as	version	1.8.0	
force-	installed	on	1.8.0),	and	you	have	custom	worker	types	pointing	to	specific	images,	the	latest	tag	gets	created	
incorrectly.
To	address	this	issue:	
1.	Modify	the	docker-	compose.yml	file	and	update	all	SL1	images	to	point	to	the	correct	version	that	you	
expect	to	be	latest.	
2.	Change	any	custom	worker	or	custom	services	using	SL1	containers	to	point	to:	latest	.	
3.	Re-	install	the	RPM	of	the	same	version	via	force.	
W h y	d oe s	t h e	l a t e s t	t a g	n ot	e x i s t	a f t e r	t h e	i n i t i a l	I S O	i n s t a l l a t i on ?	
This	situation	only	affects	users	with	custom	services	that	point	to	the	latest	tag.	To	work	around	this	issue,	run	the	
tag	latest	script	manually	after	running	the	./pull_	start_	iservices.sh	command:	
python	/opt/iservices/scripts/system_	updates/tag_	latest.py	
/opt/iservices/scripts/docker-	compose.yml	
H ow	d o	I	r e s t or e	a n	of f l i n e	b a c k u p	of	m y	P ow e r F l ow	s y s t e m ?	
To	completely	restore	aPowerFlow	system	from	an	existing	backup	using	afresh	PowerFlow	installation,	copy	the	
following	files	and	make	sure	that	they	match	what	existed	on	the	previous	PowerFlow	system:	
l	/etc/iservices/encryption_	key	.This	file	ensures	that	the	restored	data	can	be	decrypted	as	itwas	on	the	
previous	system.	
l	/etc/iservices/is_	pass	.Â Use	this	file	ifyou	wish	to	re-	use	the	same	password	from	the	old	system.	
l	/etc/iservices/isconfig.yml	.Â This	file	contains	authentication	related	settings,	ifused,	and	itsets	the	
hostname	to	the	load	balancer,	ifit's	aclustered	environment.	
l	/opt/iservices/scripts/docker-	compose-	override.yml	.This	file	contains	your	PowerFlow	container	
versions	and	environment	settings.	
l	/opt/iservices/scripts/docker-	compose.yml	.This	file	contains	your	PowerFlow	container	versions	and	
environment	settings.	
179 

180
The	swarm	cluster	should	have	the	same	amount	of	nodes	and	node	labels	applied	as	in	the	previous	system	to	
ensure	identically	matching	Docker	environments.	All	nodes	in	the	cluster	must	have	the	PowerFlow	images	in	
your	docker-	compose.yml	file	loaded	and	available	(use	[docker	image	ls]).	
After	the	PowerFlow	system	is	started,	you	can	run	the	""PowerFlow	Restore""	application	to	restore	all	previously	
used	applications,	cache,	and	config	data.	For	more	information,	see	Backing	up	Data	.	
NOTE:	Ifthe	workers/api	are	now	running	on	completely	new	nodes	after	the	restore,	you	will	have	to	re-	
install	theÂ 	Synchronization	PowerPacks	from	the	PowerFlow	user	interface	to	install	the	environments	
on	the	new	nodes.	Applications	from	those	Synchronization	PowerPacks	will	already	be	configured	
with	the	restored	settings.	
NOTE:	This	process	is	not	considered	a""Passive	Disaster	Recovery	failover	scenario.	Active/Passive	
relationships	between	PowerFlow	clusters	is	not	supported.	
W h a t	d o	I Â d o	i f Â I	g e t	a	C od e	5 0 0	E r r or	w h e n	I	t r y	t o	a c c e s s	t h e	
P ow e r F l ow	u s e r	i n t e r f a c e ?	
To	address	this	issue:	
1.	SSH	to	your	PowerFlow	instance.	
2.	Check	your	Docker	services	with	the	following	command:	
docker	service	ls	
3.	Ensure	that	all	of	your	services	are	up	and	running:	
4.	Ifall	of	your	services	are	up	and	running,	but	you	are	still	getting	Code	500	Errors,	navigate	to	the	
Couchbase	management	portal	of	your	PowerFlow	server	at	port	8091	over	HTTPS.	
Frequently	Asked	Questions 

Frequently	Asked	Questions	
5.	In	the	Couchbase	portal,	navigate	to	the	[Indexes	]tab	and	verify	that	all	of	your	indexes	are	in	aready	state:	
6.	Wait	until	all	status	entries	are	ready	and	all	build	progress	entries	are	100%	,and	then	navigate	back	to	
your	PowerFlow	user	interface.	
7.	Verify	that	the	Code	500	Error	no	longer	exists.	
8.	Ifan	index	is	stuck	in	anon-	ready	state,	find	the	index	name,	copy	that	value	and	execute	the	following	
command	in	the	Couchbase	Query	Editor:	
BUILD	INDEX	ON	content	(INDEX_	NAME_	HERE	)	
W h a t	s h ou l d	I Â d o	i f	I Â g e t	a	5 0 0	E r r or ?	
A	500	Internal	Server	Error	will	always	have	some	kind	of	stack	trace	in	the	contentapi	logs.Â Run	the	following	
command	to	find	the	cause	of	the	500	Error:	
docker	service	logs	-t	iservices_	contentapi	
A	502	or	504	Error	might	mean	that	the	user	interface	cannot	reach	an	API	container	on	anode,	or	the	API	
container	cannot	reach	adatabase	node	in	the	cluster.	To	address	this	issue:	
l	For	acluster,	make	sure	the	cluster	is	healthy,	and	all	database	nodes	are	balanced.	
l	For	acluster,	make	sure	that	the	firewall	ports	are	open	on	all	nodes.	
l	Run	the	following	commands	to	check	the	logs	for	502	or	504	Errors:	
docker	service	logs	-t	iservices_	gui	
docker	service	logs	-t	iservices_	contentapi	
The	logs	will	specify	which	container	caused	atimeout	when	trying	to	reach	that	container.	
W h a t	a r e	s om e	c om m on	e x a m p l e s	of	u s i n g	t h e	i s c l i	t ool ?	
The	PowerFlow	system	includes	acommand	line	utility	called	the	iscli	tool.	You	can	use	the	iscli	tool	to	upload	
components	such	as	steps,	configurations,	and	applications	from	the	local	file	system	onto	PowerFlow	.	
For	more	information	on	how	to	use	this	tool,	SSH	to	your	PowerFlow	instance	and	type	the	following	command:	
181 

182	
iscli	--help	
You	can	use	the	iscli	tool	to	add	drop	files	or	additional	content	onto	the	PowerFlow	.You	can	also	use	the	utility	to	
upload	content	to	aremote	host.	Examples	of	common	syntax	include	the	following:	
iscli	-usf	<STEP_	FILE.PY>	-U	isadmin	-p	em7admin	
iscli	-uaf	<APPLICATION_	FILE.JSON>	-U	isadmin	-p	em7admin	
iscli	-ucf	<CONFIG_	FILE.JSON>	-U	isadmin	-p	em7admin	
iscli	-usf	<STEP_	FILE.PY>	-U	isadmin	-p	em7admin	-H	<IS_	HOST>	
NOTE:	The	password	for	the	iscli	tool	should	be	the	same	password	as	the	PowerFlow	Administrator	(isadmin	)	
user	password.	For	more	information,	see	Changing	the	PowerFlow	Password	.	
H ow	d o	I	v i e w	a	s p e c i f i c	r u n	of	a n	a p p l i c a t i on	i n	P ow e r F l ow	?	
To	view	the	log	results	of	aprevious	execution	or	run	of	an	application	in	the	PowerFlow	:	
1.	Use	Postman	or	another	API	tool	to	locate	the	appID	and	the	name	of	the	application.	
2.	In	the	PowerFlow	,update	the	PowerFlow	URL	with	the	appID,	in	the	following	format:	
https://	<PowerFlow	>/integrations/	<application_	name>	?runid=	<App_	ID>	
For	example:	
https://	<PowerFlow	>/integrations/CreateServiceNowCI?runid=isapp-	d8d1afad-	74f8-	42d4-	
b3ed-	4a2ebcaef751	
W h y	a m	I	g e t t i n g	a n	"" or d i n a l	n ot	i n	r a n g e ""	s t e p	e r r or ?	
Ifyou	get	an	""ordinal	not	in	range""	error,	check	your	CI	Class	Mappings	to	make	sure	the	mappings	do	not	contain	
international	or	""special""	characters.	
For	example:	
AWS	|	Availability	Zone	-	SÃ£o	Paulo	
Ifyou	find	aclass	mapping	with	aspecial	character	like	the	above	example,	remove	the	class	mapping,	or	rename	
the	device	class	in	SL1	to	not	include	the	special	characters.	Then	you	can	sync	the	CI	classes	again.	
H ow	d o	I Â c l e a r	a	b a c k l og	of	C e l e r y	t a s k s	i n	F l ow e r ?	
To	clear	abacklog	of	Celery	tasks:	
1.	docker	exec	into	abash	shell	in	aworker	process.	For	example:	
docker	exec	-it	e448db31aaec	/bin/bash	
where	e448db31aaec	is	the	container	ID	of	the	is-	worker	process	on	your	system	
Frequently	Asked	Questions 

Frequently	Asked	Questions	
2.	Run	the	Python	interpreter.	
3.	Run	the	following	commands:	
from	ipaascommon.celeryapp	import	app	
app.control.purge	()	
W h y	d oe s	t r a f f i c	f r om	s p e c i f i c	s u b n e t s	n ot	g e t	a	r e s p on s e	f r om	t h e	
P ow e r F l ow	?	
In	this	situation,	you	can	see	traffic	going	into	the	host	and	into	the	Docker	network,	the	traffic	is	not	being	routed	
back	out.Â Responses	were	lost	in	the	Docker	ingress	network,	and	the	client	timed	out.	
To	address	this	issue:	
1.	Remove	the	Docker	service	by	running	the	following	command:	
docker	stack	rm	iservices	
2.	Remove	the	default	ingress	network:	
docker	network	rm	ingress	
3.	Add	anewly	addressed	ingress	network:	
docker	network	create	--driver	overlay	--ingress	--subnet=172.16.0.0/16	--	
gateway=172.16.0.1	ingress	
4.	Redeploy	PowerFlow	:	
docker	stack	deploy	-c	docker-	compose.yml	iservices	
Ifthe	containers	have	an	exposed	port	and	you	find	the	following	error	in	the	logs,	you	might	need	to	remove	
/var/lib/docker/network/files/local-	kv.db	:	
error=""failed	to	detect	service	binding	for	container	iservices_	guiâ¦""	
To	address	this	issue:	
1.	Remove	the	Docker	service:	
docker	stack	rm	iservices	
2.	Remove	the	.db	file:	
rm	/var/lib/docker/network/files/local-	kv.db	
3.	Restart	the	docker	daemon	
systemctl	restart	docker	
4.	Redeploy	PowerFlow	:	
docker	stack	deploy	-c	docker-	compose.yml	iservices	
183 

Chapte r	
11	
API	Endpoints	in	SL1	PowerFlow	
O v e r v i e w
SL1	PowerFlow	includes	an	API	that	is	available	after	you	install	the	PowerFlow	system	.	
This	chapter	covers	the	following	topics:	
Interacting	with	the	API	185	
Available	Endpoints	185
184 

185
I n te r a c ti n g	w i th	th e	A P I	
To	view	the	full	documentation	for	theÂ 	PowerFlow	API:	
1.	From	the	PowerFlow	system	,copy	the	/opt/iservices/scripts/swagger.yml	file	to	your	local	computer.	
2.	Open	abrowser	session	and	go	to	editor.swagger.io	.	
3.	In	the	Swagger	Editor,	open	the	File	menu,	select	Import	File	,and	import	the	file	swagger.yml	.The	right	
pane	in	the	Swagger	Editor	displays	the	API	documentation.	
A v a i l a b l e	E n d p o i n ts	
P O S T
/applications	.Add	anew	application	or	overwrite	an	existing	application.	
/applications/	{appName}/run	.Run	asingle	application	by	name	with	saved	or	provided	configurations.	
/applications/run	.Run	asingle	application	by	name.Â For	more	information,	see	Querying	for	the	State	of	a	
PowerFlow	Application	.	
/configurations	.Add	anew	configuration	or	overwrite	an	existing	configuration.	
/license	.Add	license	data	for	this	PowerFlow	system.	
/roles/owner	.Add	anew	owner	assigned	aspecific	role.	
/steps	.Add	anew	step	or	overwrite	an	existing	step.	
/steps/run	.Run	asingle	step	by	name.	
/schedule	.Add	anew	scheduled	PowerFlow	application.	
/syncpacks/	{syncpackName}/install	.Install	aspecific	Synchronization	PowerPack	version	by	name.	
/tasks/	{taskId}/replay	.Replay	aspecific	PowerFlow	application.	Replayed	applications	run	with	the	same	
application	variables,	configuration,	and	queue	as	the	originally	executed	application.	
/tasks/	{taskId}/revoke	.Revoke	or	terminate	aspecific	task	or	application.	By	default,	this	command	will	not	
terminate	the	current	running	task.	Ifan	application	ID	is	provided,	all	tasks	associated	with	that	application	are	
revoked.	
Interacting	with	the	API 

Available	Endpoints	
Q u e r y i n g	f o r	t h e	S t a t e	o f	a	P o w e r F l o w	A p p l i c a t i o n	
When	triggering	PowerFlow	application	from	the	applications/run	endpoint,	you	can	query	for	the	state	of	that	
application	in	two	ways:	
1.	Asynchronously	.When	you	POST	arun	of	aPowerFlow	application	to	/applications/run	,the	response	is	
an	integration	status	with	aTask	ID,	such	as:	isap-	23233-	df2f24-	etc	.At	any	time,	you	can	query	for	the	
current	state	of	that	task	from	the	endpoint	/api/v1/tasks/isap-	23233-	df2f24-	etc	.The	response	includes	
all	of	the	steps	run	by	the	application,	along	with	the	status	of	the	steps,	and	URL	links	to	additional	info,	such	
as	logs	for	each	step.	
2.	Synchronously	.When	you	POST	arun	of	an	application,	you	can	tell	PowerFlow	to	wait	responding	until	
the	application	is	complete	by	adding	the	wait	argument.	For	example,	
/api/v1/applications/run?wait=20	will	wait	for	20	seconds	before	responding.	The	maximum	wait	time	
is	30	seconds.	When	the	application	completes,	or	30	seconds	has	passed,	the	API	returns	the	current	status	
of	the	integration	run.	This	process	works	the	same	as	ifyou	had	manually	queried	/api/v1/tasks/isapp-	
w2ef2f2f	.Please	note	that	while	the	API	is	waiting	for	your	application	to	complete,	you	are	holding	on	to	a	
thread.	Ifyou	have	multiple	applications	that	run	for	along	period	of	time,	do	not	use	asynchronous	query	
unless	you	have	no	other	option.	ScienceLogic	recommends	using	an	asynchronous	query	whenever	
possible.	
G E T
/about	.Retrieve	version	information	about	the	packages	used	by	this	PowerFlow	system.	
/applications	.Retrieve	alist	of	all	available	applications	on	this	PowerFlow	system.	
/applications/	{appName}	.Retrieve	aspecific	application.	
/applications/	{appName}/logs	.Retrieve	the	logs	for	the	specified	application.	
/cache/	{cache_	id}	.Retrieve	aspecific	cache	to	gather	information	about	the	user	interface	and	the	PowerFlow	
applications.
/configurations	.Retrieve	alist	of	all	configurations	on	this	PowerFlow	system.	
/configurations/	{configName}	.Retrieve	aspecific	configuration.	
/license	.Retrieve	license	data	for	this	PowerFlow	system.	
/reports	.Retrieve	alist	of	paginated	reports.	
/reports/	{reportId}	.Retrieve	aspecific	report	by	ID.	
/roles	.Retrieve	alist	of	available	roles	on	this	PowerFlow	system.	
/roles/owner	.Retrieve	alist	of	roles	assigned	to	owners	on	this	PowerFlow	system.	
/roles/owner/	{owner}	.Retrieve	the	role	assigned	to	aspecific	owner.	
/schedule	.Retrieve	alist	of	all	scheduled	applications	on	this	PowerFlow	system.	
186 

187
/steps	.Retrieve	alist	of	all	steps	on	this	PowerFlow	system.	
/steps/	{stepName}	.Retrieve	aspecific	step.	
/syncpacks	.Retrieve	alist	of	all	Synchronization	PowerPacks	on	this	PowerFlow	system.	
/syncpacks/	{synpackName}	.Retrieve	the	full	details	about	aspecific	Synchronization	PowerPack	.	
/api/v1/syncpacks?only_	installed=true	.Retrieve	alist	of	only	the	installed	Synchronization	PowerPacks	on	this	
system.
/api/v1/syncpacks?only_	activated=true	.Retrieve	alist	of	only	the	activated	Synchronization	PowerPacks	on	
this	system.	
/tasks/	{taskId}	.Retrieve	aspecific	task.	
R E S T
/tasks	.Terminate	all	running	tasks.	
/tasks/	{taskId}	.Terminate	aspecific	running	task.	
D E L E T E
/applications/	{appName}	.Delete	aPowerFlow	application	by	name.	
/cache/	{cache_	id}	.Delete	acache	entry	by	name.	
/configurations/	{configName}	.Delete	aconfiguration	by	name.	
/license	.Delete	license	data	for	this	PowerFlow	system.	
/roles/owner	.Delete	aspecific	owner	role.	
/schedule	.Delete	ascheduled	PowerFlow	application	by	ID.	
/reports/	{appName}	.Delete	aspecific	report	by	name.	
/reports/	{reportId}	.Delete	aspecific	report	by	report	ID.	
/steps/	{stepName}	.Delete	aspecific	step	by	name.	
/syncpacks/	{spName}	.Delete	aspecific	Synchronization	PowerPack	by	name.	
Available	Endpoints 

Appe ndix	
A	
Configuring	the	PowerFlow	System	for	High	
Availability	
O v e r v i e w
This	appendix	describes	how	to	create	High	Availability	deployments	to	protect	the	data	in	PowerFlow	.	
This	appendix	covers	the	following	topics:	
Types	of	High	Availability	Deployments	for	PowerFlow	189	
Additional	Deployment	Options	198	
Requirements	Overview	199	
Preparing	the	PowerFlow	System	for	High	Availability	204	
Configuring	Clustering	and	High	Availability	204	
Scaling	iservices-	contentapi	213	
Manual	Failover	213	
Additional	Configuration	Information	215	
Known	Issues	218
188 

189
T y p e s	o f	H i g h	A v a i l a b i l i ty	D e p l o y m e n ts	f o r	P o w e r F l o w	
The	following	table	contains	aset	of	ratings	that	depict	the	level	of	resiliency	enabled	by	various	PowerFlow	
deployment	types.	The	higher	the	rating,	the	more	resilient	the	PowerFlow	system,	not	just	from	anode	failure	
perspective,	but	also	from	athroughput	and	load-	balancing	regard.	
Deployment	Type	Resiliency
Rating	
Typical	Audience	
Single-	node	deployment	F	Users	who	want	PowerFlow	running,	but	do	not	care	about	failover.	
Three-	node	cluster	B+	Users	who	want	PowerFlow	running,	and	also	want	support	for	
automatic	failover	for	one-	node	failure.	
3+	node	cluster	with	
separate	workers	(at	least	4	
nodes)	
A-	Users	who	want	automatic	failover	for	one-	node	failure,	and	intend	to	
have	very	CPU-	or	memory-	intensive	tasks	executing	on	the	workers	
constantly.	
3+	node	cluster	with	
separate	workers,	and	
drained	manager	nodes	(at	
least	6	nodes)	
A	Users	who	want	automatic	failover	for	one-	node	failure,	intend	to	
have	very	CPU-	or	memory-	intensive	tasks	executing	on	the	workers,	
and	want	to	completely	mitigate	risks	of	resource	contention	between	
services.	
You	can	start	with	any	deployment	type,	and	at	alater	time	scale	up	to	any	other	deployment	type	as	needed.	For	
example,	ayou	can	start	with	asingle-	node	deployment,	then	at	alater	date	add	three	more	nodes	to	enable	a	
3+	node	cluster	with	separate	workers.	
The	deployments	listed	in	the	table	are	just	the	standards	for	deployment.	For	very	high-	scale	customers,	amore	
advanced	deployment	might	be	necessary.	For	deployment	requirements	like	this,	please	contact	ScienceLogic	
Support.
WARNING:	Ifyou	are	deploying	PowerFlow	without	aload	balancer,	you	can	only	use	the	deployed	IP	
address	as	the	management	user	interface.	Ifyou	use	another	node	to	log	in	to	the	PowerFlow	
system,	you	will	get	an	internal	server	error.	Also,	ifthe	deployed	node	is	down,	you	must	
redeploy	the	system	using	the	IPÂ address	for	another	active	node	to	access	the	management	user	
interface.	
The	standard	deployments	are	listed	below	in	the	following	topics:	
l	Standard	Single-	node	Deployment	(1	Node)	
l	Standard	Three-	node	Cluster	(3	Nodes)	
l	3+	Node	Cluster	with	Separate	Workers	(4	or	More	Nodes)	
l	3+	Node	Cluster	with	Separate	Workers	and	Drained	Manager	Nodes	(6	or	More	Nodes)	
Types	of	High	Availability	Deployments	for	PowerFlow 

Types	of	High	Availability	Deployments	for	PowerFlow	
NOTE:	You	can	use	acommand-	line	utility	called	pfctl	that	performs	multiple	administrator-	level	actions	on	
either	the	node	or	the	cluster.	You	can	use	this	script	to	automate	the	configuration	of	athree-	node	
cluster.	For	more	information,	see	Automating	the	Configuration	of	a	Three-	Node	Cluster	.	
S t a n d a r d	S i n g l e -	n od e	D e p l oy m e n t	( 1	N od e )	
Single-	node	deployment	is	the	standard	deployment	that	comes	along	with	the	ISO	and	RPM	installation.	This	is	
the	default	deployment	ifyou	install	the	ISO	and	run	the	pull_	start_	iservices.sh	script.	
This	deployment	provides	asingle	node	running	the	PowerFlow	system.	Ifthis	node	fails,	the	system	will	not	be	
operational.
R e q u i r e m e n t s
One	node,	8	CPU,	24	GB	memory	minimum,	preferably	34	GB	to	56	GB	memory,	depending	on	workload	
sizes.	For	more	information,	see	System	Requirements	.	
R i s k s
A	single	node	supports	no	data	replication,	no	queue	mirroring,	and	no	failover	capabilities.	
C o n f i g u r a t i o n
This	configuration	is	available	as	adefault	deployment	with	the	docker-	compose	included	in	the	PowerFlow	2.0.0	
or	later	ISO	or	RPM.	
190 

191
S t a n d a r d	T h r e e -	n od e	C l u s t e r	( 3	N od e s )	
The	following	High	Availability	deployment	is	an	example	of	athree-	node	cluster:	
l	Each	node	in	the	Swarm	is	aSwarm	Manager.	
l	All	Swarm	nodes	are	located	within	the	same	data	center.	
The	three-	node	cluster	is	the	most	basic	option	providing	full	High	Availability	and	data	replication	support	among	
three	nodes.	In	this	deployment,	each	of	the	three	nodes	are	running	the	same	services	in	aclustered	
environment,	which	provides	failover	and	data	loss	prevention	capabilities.	This	deployment	option	will	satisfy	most	
High	Availability	needs,	but	itdoes	not	mitigate	risks	with	the	potential	for	worker	operations	to	affect	and	degrade	
the	database	and	queue	services,	because	all	services	are	running	on	the	same	nodes.	
This	deployment	provides:	
l	Automatic	failover	for	one	out	of	three	node	failure	:Ifone	node	in	the	cluster	fails,	automatic	failover	
occurs,	and	the	PowerFlow	system	will	continue	to	be	operational	running	on	two	out	of	three	of	the	nodes.	
l	Full	data	replication	between	all	three	nodes	.All	nodes	have	acopy	of	the	same	data	replicated	across	
all	three	nodes.	Ifone	or	two	nodes	fail,	you	will	not	experience	data	loss	in	the	database	or	in	the	queues.	
l	Full	queue	mirroring	between	all	three	nodes	.All	nodes	have	amirror	of	the	queues	defined	in	the	
PowerFlow	environment.	Ifone	or	two	nodes	fail,	the	system	still	retains	messages	in	queues	using	the	
autoheal	policy	by	default.	For	more	information	about	autoheal	behavior	in	RabbitMQ,	see	The	RabbitMQ	
Split-	brain	Handling	Strategy	.	
R e q u i r e m e n t s
Three	nodes,	8	CPU,	24	GB	memory	minimum,	preferably	34	GB	to	56	GB	memory,	depending	on	workload	
sizes.	For	more	information,	see	System	Requirements	.	
Types	of	High	Availability	Deployments	for	PowerFlow 

Types	of	High	Availability	Deployments	for	PowerFlow	
R i s k s
When	only	three	nodes	are	allocated	used	for	High	Availability,	the	following	risks	are	present:	
l	Over-	utilization	of	nodes	causing	clustering	issues	.In	athree	node	cluster,	worker	containers,	and	
Docker	Swarm	Managers	are	running	on	the	same	node	as	the	database	and	queue	services.	As	aresult,	if	
the	node	is	not	provisioned	correctly,	there	could	be	some	resource	contention.	Ifanode	reaches	100%	
CPU,	Docker	Swarm	cluster	operations	might	fail,	causing	anode	to	completely	restart,	and	causing	a	
failover	or	other	unexpected	behavior.	
l	Over-	utilization	of	workers	nodes	causing	database	or	queue	issues	.Since	all	services	are	sharing	
the	same	nodes	in	this	configuration,	ifworker	operations	become	extremely	CPU-	or	memory-	intensive,	the	
system	might	try	to	use	resources	needed	from	the	database	or	queue.	Ifthis	happens,	you	might	encounter	
failures	when	querying	the	database	or	using	the	queues.	
M i t i g a t i n g	R i s k s	
The	above	risks	can	be	mitigated	by	ensuring	that	the	node	is	deployed	with	adequate	CPU	and	memory	for	the	
workloads	that	you	plan	to	run	on	the	node.	Memory	limits	are	placed	on	containers	by	default.	Ifneeded,	you	
could	also	add	CPU	limits	to	worker	containers	to	further	prevent	resource	contention.	
C o n f i g u r a t i o n
PowerFlow	uses	adocker-	compose-	override.yml	file	to	persistently	store	user-	specific	configurations	for	
containers,	such	as	proxy	settings,	replica	settings,	additional	node	settings,	and	deploy	constraints.	The	user-	
specific	changes	are	kept	in	this	file	so	that	they	can	be	re-	applied	when	the	/opt/iservices/docker-	
compose.yml	file	is	completely	replaced	on	an	RPM	upgrade,	ensuring	that	no	user-	specific	configurations	are	
lost.
Below	is	an	example	docker-	compose-	override.yml	file	for	PowerFlow	2.0.0	or	later:	
services:	
contentapi:	
environment:	
db_	host:	""couchbase.isnet,couchbase-	worker.isnet,couchbase-	worker2.isnet""	
couchbase:	
deploy:	
placement:	
constraints:	
-	""node.hostname	==	<Swarm	node	hostname1>""	
environment:	
db_	host:	couchbase.isnet	
hostname:	couchbase.isnet	
networks:	
isnet:	
aliases:	
-	couchbase	
-	couchbase.isnet	
couchbase-	worker:	
container_	name:	couchbase-	worker	
depends_	on:	
-	couchbase	
deploy:	
192 

193	
placement:	
constraints:	
-	""node.hostname	==	<Swarm	node	hostname2>""	
replicas:	0	
environment:	
AUTO_	REBALANCE:	""true""	
TYPE:	WORKER	
db_	host:	couchbase	
hostname:	couchbase-	worker.isnet	
image:	""sciencelogic/is-	couchbase:1.7.0""	
networks:	
isnet:	
aliases:	
-	couchbase-	worker	
-	couchbase-	worker.isnet	
ports:	
-	""8100:8091""	
secrets:	
-	is_	pass	
-	encryption_	key	
volumes:	
-	""/var/data/couchbase:/opt/couchbase/var""	
couchbase-	worker2:	
container_	name:	couchbase-	worker2	
depends_	on:	
-	couchbase	
deploy:	
placement:	
constraints:	
-	""node.hostname	==	<Swarm	node	hostname3>""	
replicas:	0	
environment:	
AUTO_	REBALANCE:	""true""	
TYPE:	WORKER	
db_	host:	couchbase	
hostname:	couchbase-	worker2.isnet	
image:	""sciencelogic/is-	couchbase:1.7.0""	
networks:	
isnet:	
aliases:	
-	couchbase-	worker2	
-	couchbase-	worker2.isnet	
ports:	
-	""8101:8091""	
secrets:	
-	is_	pass	
-	encryption_	key	
volumes:	
-	""/var/data/couchbase:/opt/couchbase/var""	
dexserver:	
deploy:	
replicas:	2	
environment:	
db_	host:	""couchbase.isnet,couchbase-	worker.isnet,couchbase-	worker2.isnet""	
pypiserver:	
deploy:	
Types	of	High	Availability	Deployments	for	PowerFlow 

Types	of	High	Availability	Deployments	for	PowerFlow	
placement:	
constraints:	
-	""node.hostname	==	<Swarm	node	hostname1>""	
rabbitmq:	
deploy:	
placement:	
constraints:	
-	""node.hostname	==	<Swarm	node	hostname1>""	
hostname:	rabbit_	node1.isnet	
image:	""sciencelogic/is-	rabbit:3.7.14-	3""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node1.isnet	
volumes:	
-	""rabbitdb:/var/lib/rabbitmq""	
rabbitmq2:	
deploy:	
placement:	
constraints:	
-	""node.hostname	==	<Swarm	node	hostname2>""	
hostname:	rabbit_	node2.isnet	
image:	""sciencelogic/is-	rabbit:3.7.14-	3""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node2.isnet	
volumes:	
-	""rabbitdb2:/var/lib/rabbitmq""	
rabbitmq3:	
deploy:	
placement:	
constraints:	
-	""node.hostname	==	<Swarm	node	hostname3>""	
hostname:	rabbit_	node3.isnet	
image:	""sciencelogic/is-	rabbit:3.7.14-	3""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node3.isnet	
volumes:	
-	""rabbitdb3:/var/lib/rabbitmq""	
scheduler:	
environment:	
db_	host:	""couchbase.isnet,couchbase-	worker2.isnet,couchbase-	worker.isnet""	
steprunner:	
environment:	
db_	host:	""couchbase.isnet,couchbase-	worker2.isnet,couchbase-	worker.isnet""	
version:	""3.4""	
volumes:	
rabbitdb2:
rabbitdb3:	
194 

195
3 +	N od e	C l u s t e r	w i t h	S e p a r a t e	W or k e r s	( 4	or	M or e	N od e s )	
The	three-	node	cluster	with	separate	workers	is	aslight	variation	of	the	standard	three-	node	cluster.	With	this	
deployment	strategy,	all	worker	operation	load	is	run	by	aseparate	independent	node.	This	is	preferable	over	the	
standard	three-	node	deployment,	because	itcompletely	prevents	worker	operations	from	stealing	resources	from	
the	databases	or	queues.	
Since	steprunner	workload	is	entirely	on	dedicated	servers,	you	have	greater	ability	to	scale	up	to	more	workers,	or	
even	add	additional	nodes	of	workers	to	the	system,	without	affecting	critical	database	or	queue	operations.	
This	deployment	provides	acomplete	separation	of	worker	processing	from	the	database	and	queue	processing,	
which	is	very	helpful	for	users	which	have	very	CPU-	intensive	tasks	that	execute	frequently.	
The	following	High	Availability	deployment	adds	Docker	Swarm	worker	nodes	where	steprunners	can	be	
constrained.	This	lets	you	continue	to	scale	out	new	worker	nodes	as	the	load	increases.	This	also	lets	you	distribute	
steprunners	based	on	workloads.	Core	services	include	ContentAPI,	RabbitMQ,	and	Couchbase.	
You	can	add	drained	Docker	Swarm	Manager	nodes	to	increase	fault	tolerance	of	the	Swarm,	and	to	ensure	that	
the	orchestration	of	the	Swarm	is	not	impeded	by	large	workloads	on	the	core	nodes.	
The	maximum	Couchbase	cluster	with	fully	replicated	nodes	is	four.	Anything	greater	than	four	will	not	have	afull	
replica	set	and	will	auto-	shard	data	across	additional	nodes.	There	is	no	way	as	of	this	version	of	Couchbase	to	set	
the	placement	of	the	replicas.	Redis	replication	and	clustering	is	not	currently	supported	in	this	version	of	
PowerFlow	.	
R e q u i r e m e n t s
Three	nodes,	8	CPU,	24	GB	memory	minimum,	preferably	34	GB	to	56	GB	memory,	depending	on	workload	
sizes.	For	more	information,	see	System	Requirements	.	
One	or	more	worker	node	with	your	choice	of	sizing.	
Types	of	High	Availability	Deployments	for	PowerFlow 

Types	of	High	Availability	Deployments	for	PowerFlow	
W o r k e r	N o d e	S i z i n g	
Worker	nodes	can	be	sized	to	any	CPU	or	memory	constraints,	though	the	greater	the	memory	and	CPU,	more	
workers	the	node	can	run.	The	minimum	size	of	aworker	node	is	2	CPU,	4	GB	memory.	
R i s k s
Core	Node	over-	utilization	could	cause	Swarm	clustering	problems.	Because	the	Swarms	are	the	same	
nodes	as	the	core	managers,	there	is	apossibility	for	heavily	loaded	databases	and	queues	to	contend	with	the	
Swarm	hosts	for	resources.	In	this	case	the	Swarm	may	restart	itself	and	the	services	running	on	that	node.	This	is	
not	as	likely	to	occur	with	workers	running	on	their	own	dedicated	nodes.	
M i t i g a t i n g	R i s k s	
The	above	risks	can	easily	be	mitigated	by	ensuring	the	node	is	deployed	with	adequate	CPU	and	memory	for	the	
workloads	itis	expected	to	run.	Additionally,	you	can	apply	CPU	and	memory	limits	to	the	database	or	queue	
containers	so	that	there	will	always	be	enough	resources	allocated	to	the	host	to	prevent	this	scenario.	For	more	
information,	see	Configuring	Additional	Elements	of	PowerFlow	.	
C o n f i g u r a t i o n
Using	this	configuration	consists	of:	
l	Joining	the	standard	three-	node	Swarm	cluster	with	one	or	more	nodes	as	aSwarm	worker.	
l	Labeling	each	additional	""worker""	node	with	aSwarm	label	""worker"".	For	more	information,	see	Creating	a	
Node	Label	.	
l	In	addition	to	the	standard	three-	node	deployment,	steprunners	should	be	updated	to	run	on	adedicated	
node	in	the	docker-	compose-	override	file:	
steprunner3:	
deploy:	
placement:	
constraints:
-	node.labels.types	==	worker	
3 +	N od e	C l u s t e r	w i t h	S e p a r a t e	W or k e r s	a n d	D r a i n e d	M a n a g e r	N od e s	
( 6	or	M or e	N od e s )	
This	deployment	option	is	the	most	robust	of	the	one-	node	auto-	failover	deployments,	and	completely	mitigates	
known	risks	for	resource	contention	in	clusters.	
This	configuration	provides	everything	that	the	3+	node	cluster	with	dedicated	workers	provides,	with	the	addition	
of	drained	Swarm	Managers.	The	drained	Swarm	Managers	mitigate	the	risk	of	database	or	queue	processing	
causing	contention	of	resources	for	the	Swarm	clustering	operations	at	the	host	level.	
196 

197
This	deployment	should	only	be	used	for	large	deployments	of	PowerFlow	.This	deployment	separates	out	all	the	
core	services	onto	their	own	dedicated	worker	node	and	lets	you	distribute	steprunners	based	on	workloads:	
You	can	add	drained	Docker	Swarm	Manager	nodes	to	increase	fault	tolerance	of	the	Swarm,	and	to	ensure	that	
the	orchestration	of	the	Swarm	is	not	impeded	by	large	workloads	on	the	core	nodes.	
The	maximum	Couchbase	cluster	with	fully	replicated	nodes	is	four.	Anything	greater	than	four	will	not	have	afull	
replica	set	and	will	auto-	shard	data	across	additional	nodes.	There	is	no	way	as	of	this	version	of	Couchbase	to	set	
the	placement	of	the	replicas.	Redis	replication	and	clustering	is	not	currently	supported	in	this	version	of	
PowerFlow	.	
R e q u i r e m e n t s
Three	nodes,	8	CPU,	24	GB	memory	minimum,	preferably	34	GB	to	56	GB	memory,	depending	on	workload	
sizes.	For	more	information,	see	System	Requirements	.	
Also,	three	nodes,	2	CPU,	4	GB	memory	for	the	Swarm	Manager.	
R i s k s
None.
C o n f i g u r a t i o n
Use	the	same	docker-	compose-	override.yml	file	found	in	Standard	Three-	node	Cluster	(3	Nodes)	.	
Next,	add	the	additional	three	nodes	to	the	cluster	as	managers,	and	drain	them	of	all	services	(see	Using	
Drained	Managers	to	Maintain	Swarm	Health	).	Promote	the	drained	nodes	to	Swarm	Managers,	and	make	
all	other	nodes	workers.	
Types	of	High	Availability	Deployments	for	PowerFlow 

Additional	Deployment	Options	
A d d i ti o n a l	D e p l o y m e n t	O p ti o n s	
The	following	diagrams	show	additional	High	Availability	deployment	architectures	that	are	supported	for	
PowerFlow	.	
C r os s -	D a t a	C e n t e r	S w a r m	C on f i g u r a t i on	
Docker	Swarm	requires	three	data	centers	to	maintain	quorum	of	the	swarm	in	the	event	of	afull	data	center	
outage.	Each	data	center	must	have	alow-	latency	connection	between	the	data	centers.	
NOTE:	Implementing	clustering	across	links	with	alatency	that	is	greater	than	80	ms	is	not	supported,	and	
may	cause	one	or	more	of	the	following	situations:	nodes	dropping	out	of	the	cluster,	or	automatically	
failover,	failed	data	replication,	and	potential	cluster	communication	issues	resulting	in	timeouts	and	
significantly	increased	overhead.	
The	cross-	data	center	configuration	has	the	following	limitation:	the	Redis	service	cannot	be	deployed	in	High	
Availability.	As	aresult,	all	task	results	saved	by	any	steprunner	will	have	to	be	saved	within	that	data	center.	Upon	
afailure	of	that	data	center,	anew	Redis	service	will	be	created,	but	an	application	in	the	middle	of	its	run	would	
have	to	retry.	
198 

199
The	following	High	Availability	deployment	shows	across-	data	center	swarm	configuration:	
A d d i t i on a l	N ot e s	
Tagging	and	constraints	in	the	Docker	compose	file	should	be	used	to	ensure	proper	placement.	Example	
compose	files	are	not	available	at	this	time.	
Configuration	management	solutions	such	as	Ansible	should	be	used	to	update	and	manage	large	swarm	
environments.
For	an	easy	upgrade	of	PowerFlow	,use	Docker	Hub	to	pull	the	latest	images	or	use	an	internal	Docker	registry.	
R e q u i r e m e n ts	O v e r v i e w	
Because	PowerFlow	uses	the	Docker	Swarm	tool	to	maintain	its	cluster	and	automatically	re-	balance	services	
across	nodes,	ScienceLogic	strongly	recommends	that	you	implement	the	following	best	practices	from	Docker,	
Couchbase,	and	RabbitMQ	.The	topics	in	this	section	describe	those	best	practices,	along	with	requirements	and	
frequently	asked	questions.	
Requirements	Overview 

Requirements	Overview	
IMPORTANT	:To	support	automatic	failover	of	the	Couchbase	database	without	manual	intervention,	you	
must	set	up	at	least	three	nodes	for	automatic	failover	of	asingle	node,	five	nodes	for	
automatic	failover	of	two	nodes,	and	so	on.	
NOTE:	For	aclustered	PowerFlow	environment,	you	must	install	the	PowerFlow	RPM	on	every	server	that	you	
plan	to	cluster	the	PowerFlow	.You	can	load	the	Docker	images	for	the	services	onto	each	server	
locally	by	running	/opt/iservices/scripts/pull_	start_	iservices.sh	.Installing	the	RPM	onto	each	
server	ensures	that	the	PowerFlow	containers	and	necessary	data	are	available	on	all	servers	in	the	
cluster.	For	more	information,	see	Installing	PowerFlow	via	RPM	.	
NOTE:	You	can	use	acommand-	line	utility	called	pfctl	that	performs	multiple	administrator-	level	actions	on	
either	the	node	or	the	cluster.	You	can	use	this	script	to	automate	the	configuration	of	athree-	node	
cluster.	For	more	information,	see	Automating	the	Configuration	of	a	Three-	Node	Cluster	.	
D oc k e r	S w a r m	R e q u i r e m e n t s	f or	H i g h	A v a i l a b i l i t y	
After	implementing	Docker	Swarm	High	Availability,	ifanode	goes	down,	all	the	services	on	that	failed	node	can	
be	dynamically	re-	provisioned	and	orchestrated	among	the	other	nodes	in	the	cluster.	High	Availability	for	Swarm	
also	facilitates	network	connections	with	the	various	other	High	Availability	components.	
Docker	Swarm	requires	the	following:	
l	The	cluster	contains	at	least	three	nodes	running	as	managers.	With	three	nodes,	there	can	be	aquorum	
vote	between	managers	when	anode	is	failed	over.	
l	A	load	balancer	with	avirtual	IP	running	in	front	of	all	nodes	in	the	cluster.	The	load	balancer	allows	user	
interface	requests	to	be	distributed	among	each	of	the	hosts	in	the	case	one	of	the	hosts	fails	for	ports	
443:HTTPS,	3141:Devpi	and	5556:Dex.	
An	example	of	why	aload	balancer	is	needed	in	front	of	the	virtual	IP	is	the	ServiceNow	ticketing	workflow.	If	
youâre	only	directing	the	request	to	asingle	node	and	that	node	goes	down,	your	ticketing	will	stop	even	ifthe	other	
PowerFlow	nodes	are	still	up	and	functional.	The	load	balancer	will	account	for	the	downed	node	and	
automatically	route	to	the	other	nodes	in	the	cluster.	
For	more	information,	see	the	Docker	High	Availability	Documentation	.	
What	happens	ifIÂ use	three	nodes	and	two	of	the	nodes	fail?	
Docker	fault	tolerance	is	limited	to	one	failure	in	athree-	node	cluster.	Ifmore	than	one	node	goes	down	in	a	
three-	node	cluster,	automatic	High	Availability	and	failover	cannot	be	guaranteed,	and	manual	intervention	may	
be	required.	Adding	more	nodes	is	the	only	way	to	increase	the	fault	tolerance.	
In	the	event	of	atwo	out	of	three	failure,	after	you	perform	manual	failover	actions,	the	PowerFlow	system	will	be	
back	up	and	running.	
For	more	information	about	the	manual	failover	steps,	see	the	Failover	section.	
200 

201
C ou c h b a s e	D a t a b a s e	R e q u i r e m e n t s	f or	H i g h	A v a i l a b i l i t y	
Couchbase	HighÂ Availability	ensures	that	no	application,	configuration,	or	step	data	from	the	PowerFlow	system	
will	be	lost	in	the	event	of	anode	failure.	
To	support	automatic	failover,	Couchbase	requires	at	least	three	nodes	in	the	high	availability	cluster.	
Each	node	will	have	an	independent	and	persistent	storage	volume	that	is	replicated	throughout	the	cluster.	
Alternatively,	shared	storage	can	be	used	instead	of	independent	persistent	volumes.	This	replication	ensures	that	
data	is	replicated	in	all	places,	and	ifasingle	node	goes	down,	no	data	will	be	lost.	
For	more	information,	see	the	Couchbase	documentation	.	
What	ifIhave	three	nodes	and	two	of	them	fail?	
In	the	event	of	afailure	of	two	out	of	three	nodes,	no	data	will	be	lost,	because	the	data	is	being	replicated.	
Ifmultiple	Couchbase	data	nodes	go	down	at	the	same	time,	automatic	failover	might	not	occur	(not	even	nodes	
for	quorum	to	failover).	You	will	then	need	to	perform	manual	failover	steps.	After	you	perform	these	manual	
actions,	the	PowerFlow	system	will	be	operational	again.	For	more	information	about	the	manual	failover	steps,	
see	the	Failover	section.	
R a b b i t M Q	C l u s t e r i n g	a n d	P e r s i s t e n c e	f or	H i g h	A v a i l a b i l i t y	
Implementing	RabbitMQ	High	Availability	ensures	that	ifany	integrations	or	tasks	are	waiting	in	the	Rabbit	queue,	
those	tasks	will	not	be	lost	ifanode	containing	the	Rabbit	queue	fails.	
NOTE	:Â You	can	switch	between	both	single-	node	and	cluster	options	at	any	time	during	deployment.	
RabbitMQ	clustering	requires	aDocker	Swarm	configuration	with	multiple	nodes.	For	more	information,	see	
Configuring	Docker	Swarm	.	
As	abest	practice	for	security,	enable	the	user	interface	only	temporarily	during	cluster	configuration.	The	default	
user	interface	login	is	guest/guest	.	
R a b b i t M Q	O p t i o n	1 :	P e r s i s t i n g	Q u e u e	t o	D i s k	o n	a	S i n g l e	N o d e	( D e f a u l t	C o n	-	
f i g u r a t i o n )
With	this	configuration,	the	PowerFlow	queue	runs	on	asingle	node,	and	the	queue	is	persisted	on	disk.	As	a	
result,	ifthe	PowerFlow	stack	is	removed	and	re-	deployed,	no	messages	are	lost	during	the	downtime.	Any	
messages	that	exist	in	the	queue	before	the	stack	is	stopped	continue	to	exist	after	the	stack	is	re-	deployed.	
Potential	Risks	and	Mitigations	
Because	the	queue	runs	on	asingle	node,	ifthat	node	fails,	then	the	queue	and	its	related	data	might	be	lost.	
You	can	mitigate	data	loss	by	persisting	the	queues	on	your	choice	of	network	shared	storage,	so	that	ifthe	queue	
fails	on	one	node,	the	queue	and	its	messages	can	be	brought	back	up	on	another	node.	
Requirements	Overview 

Requirements	Overview	
Requirements/Setup	(Enabled	by	Default)	
l	You	must	define	astatic	hostname	for	the	RabbitMQ	host	in	the	docker-	compose	file.	The	default	is	rabbit_	
node1.isnet	.	
l	You	must	mount	avolume	to	/var/lib/rabbitmq	in	the	docker-	compose	file.	
Example	docker-	compose	Definition	
rabbitmq:	
image:	sciencelogic/is-	rabbit:3.7.7-	1	
hostname:	rabbit_	node1.isnet	
volumes:	
-	""rabbitdb:/var/lib/rabbitmq""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node1.isnet	
R a b b i t M Q	O p t i o n	2 :	C l u s t e r i n g	N o d e s	w i t h	P e r s i s t e n t	Q u e u e s	o n	E a c h Â N o d e	
This	configuration	lets	multiple	nodes	join	aRabbitMQ	cluster.	When	you	include	multiple	nodes	int	he	RabbitMQ	
cluster,	all	queue	data,	messages,	and	other	necessary	information	is	automatically	replicated	and	persisted	on	all	
nodes	in	the	cluster.	Ifany	node	fails,	then	the	remaining	nodes	in	the	cluster	continue	maintaining	and	processing	
the	queue.	
Because	the	RabbitMQ	cluster	includes	disk-	persisted	queues,	ifall	nodes	in	the	Rabbit	cluster	fail,	or	ifthe	service	
is	removed	entirely,	then	no	data	loss	should	occur.	Upon	restart,	the	nodes	will	resume	with	the	same	cluster	
configuration	and	with	the	previously	saved	data.	
Ifyou	include	multiple	nodes	in	aRabbitMQ	cluster,	PowerFlow	automatically	applies	an	HA	policy	of	all-	node	
replication,	with	retroactive	queue	synchronization	disabled.	For	more	information,	refer	to	the	RabbitMQ	
documentation.
Potential	Risks	and	Mitigations	
Ifyou	create	aDocker	Swarm	cluster	with	only	two	nodes,	the	cluster	might	stop	functioning	ifasingle	node	
fails.Â To	prevent	this	situation,	include	at	least	three	nodes	in	each	cluster.	
Requirements/Setup
For	aDocker	Swarm	configuration	with	multiple	independent	nodes:	
l	Both	RabbitMQ	services	must	be	""pinned""	to	each	of	the	two	nodes.	See	the	Example	Compose	Definition	
below.	
l	You	must	add	anew	RabbitMQ	service	to	the	docker-	compose.yml	file.	This	new	service	should	have	a	
hostname	and	alias	following	the	designated	pattern.	The	designated	pattern	is:	rabbit_	nodex.isnet	,where	x	
is	the	node	number.	This	configuration	supports	up	to	20	clustered	nodes	by	default.	
l	After	you	update	the	docker-	compose.yml	file,	the	nodes	will	auto-	cluster	when	you	perform	adeployment.	
202 

203
Example	docker-	compose	Definition	of	Two	Clustered	Rabbit	Services	
rabbitmq:	
image:	sciencelogic/is-	rabbit:3.7.7-	1	
hostname:	rabbit_	node1.isnet	
volumes:	
-	""rabbitdb:/var/lib/rabbitmq""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node1.isnet	
deploy:	
placement:	
constraints:	
-	node.hostname	==	node-	number-	1.domain	
rabbitmq2:	
image:	sciencelogic/is-	rabbit:3.7.7-	1	
hostname:	rabbit_	node2.isnet	
volumes:	
-	""rabbitdb:/var/lib/rabbitmq""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node2.isnet	
deploy:	
placement:	
constraints:	
-	node.hostname	==	node-	number-	2.domain	
C h e c k i n g	t h e	S t a t u s	o f	a	R a b b i t M Q	C l u s t e r	
This	section	contains	commands	and	additional	resources	for	administering	your	clusters.	
To	check	the	status	of	your	clustered	RabbitMQ	environment:	
1.	Run	docker	ps	and	locate	the	iservices_	rabbit	container.	
2.	Run	the	following	command	on	the	RabbitMQ	container:	
docker	exec	-it	[container_	id]	/bin/bash	
You	can	run	the	following	commands	for	more	information:	
l	rabbitmqctl	cluster_	status	.Returns	information	about	the	current	cluster	status,	including	nodes	in	the	
cluster,	and	failed	nodes.	
l	rabbitmqctl	list_	policies	.Returns	information	about	current	policies.	Ensure	that	the	ha-	all	policy	is	
automatically	set	for	your	cluster.	
For	additional	cluster-	related	administrative	commands,	see	the	RabbitMQ	Cluster	Management	documentation	
page	.	
Requirements	Overview 

Preparing	the	PowerFlow	System	for	High	Availability	
P r e p a r i n g	th e	P o w e r F l o w	S y s te m	f o r	H i g h	A v a i l a b i l i ty	
You	need	to	prepare	your	PowerFlow	system	in	the	following	ways	before	configuring	the	High	Availability	solution:	
1.	Make	sure	that	your	PowerFlow	system	has	been	updated	with	yum	upgrade	.	
2.	Run	the	following	commands	to	open	up	the	proper	firewall	ports	for	Docker	Swarm	on	each	swarm	node:	
firewall-	cmd	--add-	port=2376/tcp	--permanent	
firewall-	cmd	--add-	port=2377/tcp	--permanent	
firewall-	cmd	--add-	port=7946/tcp	--permanent	
firewall-	cmd	--add-	port=7946/udp	--permanent	
firewall-	cmd	--add-	port=4789/udp	--permanent	
firewall-	cmd	--add-	protocol=esp	--permanent	
NOTE:	Ifyour	system	is	fully	yum-	updated,	you	only	have	to	run	the	following	commands:	
firewall-	cmd	--add-	service	docker-	swarm	--permanent	
firewall-	cmd	--reload	
TIP:	To	view	alist	of	all	ports,	run	the	following	command:	firewall-	cmd	--list-	all	
3.	Make	sure	that	the	/etc/iservices/is_	pass	and	/etc/iservices/encryption_	key	are	identical	on	all	clustered	
nodes.	
4.	Make	sure	that	NTP	is	properly	configured	on	all	nodes:	
l	Edit	the	/etc/chrony.conf	file	to	add	NTP	servers.	Ifyou	want	to	use	the	pool.ntp.org	NTP	servers,	
remove	the	.ol.	from	the	domain	names.	
l	Enable	chronyd	by	running	the	following	commands:	
systemctl	start	chronyd	
systemctl	enable	chronyd	
timedatectl	#ensure	ntp	is	enabled	is	yes	and	ntp	sync	is	yes	
C o n f i g u r i n g	C l u s te r i n g	a n d	H i g h	A v a i l a b i l i ty	
This	section	describes	how	to	configure	clustering	and	High	Availability	with	Docker	Swarm	and	the	Couchbase	
database,	using	three	or	more	nodes.	
NOTE	:This	topic	assumes	you	are	using	PowerFlow	ISOs	for	each	node,	which	includes	an	initial	Docker	
Swarm	node	configuration.	The	use	of	the	PowerFlow	ISO	is	not	required,	however.	You	could	
instead	deploy	another	node	(without	using	the	PowerFlow	ISO)	and	configure	aLinux	operating	
system	based	on	Red	Hat.	You	could	then	add	that	system	to	the	swarm.	
204 

205
A u t om a t i n g	t h e	C on f i g u r a t i on	of	a	T h r e e -	N od e	C l u s t e r	
You	can	use	the	pfctl	command-	line	utility	to	perform	multiple	administrator-	level	actions	on	yourÂ 	PowerFlow	
cluster.	You	can	use	the	autocluster	action	with	the	pfctl	command	to	automate	the	configuration	of	athree-	node	
cluster.
NOTE:	Ifyou	are	using	another	cluster	configuration,	the	deployment	process	should	be	manual,	because	the	
pfctl	utility	only	supports	the	automated	configuration	of	athree-	node	cluster.	
WARNING:	The	autocluster	action	will	completely	reset	and	remove	all	data	from	the	system.	When	you	run	
this	action,	you	will	get	aprompt	verifying	that	you	want	run	the	action	and	delete	all	data.	
To	automate	the	configuration	of	athree-	node	cluster,	run	the	following	command:	
pfctl	--host	<is_	host1>	<username>	:<password>	--host	<is_	host2>	
<username>	:<password>	--host	<is_	host3>	<username>	:<password>	autocluster	
For	example:	
pfctl	--host	192.11.1.1	isadmin:passw0rd	--host	192.11.1.2	isadmin:passw0rd	--host	
192.11.1.3	isadmin:passw0rd	autocluster	
Running	this	command	will	configure	your	PowerFlow	three-	node	cluster	without	any	additional	manual	steps	
required.
WARNING:	Ifthe	isadmin	(host)	password	contains	aspecial	character,	such	as	an	""@""	or	""#""	symbol,	the	
password	must	be	escaped	in	the	iservicecontrol	autocluster	command	by	adding	single	
quotes,	such	as	'user:password'	.For	example:	iservicecontrol	--host	10.10.10.100	
'isadmin:testing@is'	--host	10.10.10.102	'isadmin:testing@is'	--host	
10.10.10.105	'isadmin:testing@is'	autocluster	
TIP:	For	more	information	about	other	actions	you	can	perform	with	the	pfctl	utility,	see	Using	the	pfctl	
Command-	line	Utility	.	
Configuring	Clustering	and	High	Availability 

Configuring	Clustering	and	High	Availability	
C on f i g u r i n g	D oc k e r	S w a r m	
To	configure	Docker	Swarm	for	clustering	(three	or	more	nodes)	and	High	Availability:	
NOTE:	Two-	Node	High	Availability	is	not	possible	because	Docker	Swarm	requires	an	odd	number	of	nodes	
(3+)	for	quorum	and	consensus.	
1.	Ifyou	do	not	already	have	PowerFlow	running	in	your	environment,	install	PowerFlow	on	asingle	node.	
Doing	this	creates	asingle-	node	Docker	Swarm	manager.	
2.	Ensure	that	NTP	is	configured	on	all	swarm	nodes.	For	more	information,	see	Preparing	PowerFlow	
System	for	High	Availability	.	
3.	SSH	to	the	Docker	Swarm	manager	(leader)	and	run	the	following	command	to	retrieve	the	join	token.	Make	
note	of	the	token,	because	you	will	need	itto	join	anode	to	the	swarm	in	step	4,	below:	
docker	swarm	join-	token	manager	
4.	Run	the	following	commands	on	each	Docker	Swarm	node	that	you	want	to	join	to	the	cluster:	
docker	swarm	init	
docker	swarm	join	--token	<join	token>	<swarm	manager	ip>	:<port>	
where	<join	token>	is	the	value	from	step	3.	For	example:	
docker	swarm	join	--token	SWMTKN-	1-	
5e8skxby61cthkfkv6gzhhil89v0og2m7lx014tvvv42n7m0rz-	an0fdam5zj0v7d471co57d09h	
10.7.3.21:2377	
5.	Run	the	following	command	to	verify	that	the	nodes	have	been	added:	
docker	node	ls	
6.	Ifyou	are	using	local	images	and	not	connecting	to	DockerÂ Hub,	load	docker	images	on	the	other	swarm	
nodes:	
for	i	in	$(ls	-1	/opt/iservices/images/);	do	docker	load	-i	
/opt/iservices/images/$i;	done	
206 

207
C on f i g u r i n g	t h e	C ou c h b a s e	D a t a b a s e	
To	add	aCouchbase	worker	node:	
1.	In	the	docker-	compose-	override.yml	file,	add	the	following	line	to	constrain	the	Couchbase	container	to	a	
single	Docker	Swarm	node	at	the	bottom	of	the	couchbase	section:	
deploy:
...
hostname:	couchbase.isnet	
deploy:	
placement:	
constraints:	
-	node.hostname	==	<name	of	Docker	Swarm	node>	
networks:	
isnet:	
aliases:	
-	couchbase	
-	couchbase.isnet	
environment:	
db_	host:	couchbase.isnet	
Configuring	Clustering	and	High	Availability 

Configuring	Clustering	and	High	Availability	
2.	Add	the	couchbase-	worker	and	couchbase-	worker2	section.	deploy	>	replicas	on	the	workers	should	be	set	
to	0:	
couchbase-	worker:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:feature-	INT-	1208-	HA-	IS-	
Services
container_	name:	couchbase-	worker.isnet	
volumes:	
-	""/var/data/couchbase:/opt/couchbase/var""	
deploy:	
placement:	
constraints:	
-	node.hostname	==	<name	of	Docker	Swarm	node>	
networks:	
isnet:	
aliases:	
-	couchbase-	worker	
-	couchbase-	worker.isnet	
hostname:	couchbase-	worker.isnet	
ports:	
-	""8095:8091""	
secrets:	
-	is_	pass	
-	encryption_	key	
ulimits:	
nofile:	80000	
core:	100000000	
memlock:	100000000	
environment:	
TYPE:	'WORKER'	
AUTO_	REBALANCE:	'true'	
db_	host:	'couchbase'	
depends_	on:	
-	couchbase	
NOTE	:Â This	deployment	makes	the	Couchbase	worker	user	interface	available	on	port	8095	of	the	Docker	
Swarm	stack.	Ifthe	master	node	goes	down,	or	ifthe	primary	Couchbase	user	interface	is	not	
available	on	port	8091,	you	can	still	access	the	secondary	Couchbase	user	interface	through	port	
8095.	
3.	Add	couchbase-	worker	to	the	db_	host	setting	for	contentapi:	
contentapi:
...	
environment:	
...
db_	host:	'couchbase,couchbase-	worker,couchbase-	worker2'	
4.	All	db_	host	variables	in	docker-	compose	should	be	in	the	following	format:	
db_	host:	'couchbase,couchbase-	worker,couchbase-	worker2'	
208 

209	
5.	Ifyou	are	using	the	override	file,	run	the	/opt/iservices/compose_	override.sh	script	to	validate	and	update	
the	docker-	compose.yml	file	with	your	changes.	
6.	Deploy	the	stack	with	only	the	Couchbase	node	by	editing	the	replicas	on	couchbase-	worker	to	1	and	
running	the	following	command:	
docker	stack	deploy	-c	<location	of	compose	file	>	iservices	
7.	After	the	two-	node	Couchbase	cluster	has	been	successfully	deployed	and	the	secondary	indexes	are	
successfully	added,	edit	the	replicas	on	couchbase-	worker2	to	1	and	run	the	following	command:	
docker	stack	deploy	-c	<location	of	compose	file	>	iservices	
8.	Set	the	replicas	in	the	docker-	compose-	override.yml	file	as	well.	
9.	After	the	second	worker	is	added,	set	the	number	of	replicas	to	""2""	on	each	bucket	(content	and	logs)	in	the	
Couchbase	Administrator	user	interface	and	click	[Save	Changes	]:	
10.	Rebalance	the	cluster	by	navigating	to	the	Servers	section	of	the	Couchbase	Administrator	user	interface	and	
clicking	the	Rebalance	button:	
Configuring	Clustering	and	High	Availability 

Configuring	Clustering	and	High	Availability	
C o d e	E x a m p l e :	d o c k e r -	c o m p o s e -	o v e r r i d e . y m l	
PowerFlow	uses	adocker-	compose-	override.yml	file	to	persistently	store	user-	specific	configurations	for	
containers,	such	as	proxy	settings,	replica	settings,	additional	node	settings,	and	deploy	constraints.	The	user-	
specific	changes	are	kept	in	this	file	so	that	they	can	be	re-	applied	when	the	/opt/iservices/docker-	
compose.yml	file	is	completely	replaced	on	an	RPM	upgrade,	ensuring	that	no	user-	specific	configurations	are	
lost.
Ifyou	are	running	PowerFlow	in	acluster,	these	files	should	always	be	the	same	between	all	manager	nodes.	With	
this	in	place,	ifany	manager	node	dies,	you	can	re-	deploy	with	the	same	settings	from	any	other	manager	node.	
The	following	section	includes	acomplete	example	of	the	/opt/iservices/scripts/docker-	compose-	
override.yml	file	for	athree-	node	Couchbase	and	RabbitMQ	clustered	deployment:	
NOTE	:Ifshared	volumes	are	available	in	the	cluster,	the	deploy	placement	can	be	omitted	and	removed.	
version:	'3.2'	
services:	
steprunner:	
environment:	
db_	host:	couchbase.isnet,couchbase-	worker2.isnet,couchbase-	worker.isnet	
scheduler:	
environment:	
db_	host:	couchbase.isnet,couchbase-	worker2.isnet,couchbase-	worker.isnet	
couchbase:	
environment:	
db_	host:	'couchbase.isnet'	
deploy:	
placement:	
constraints:	
-	node.hostname	==	<swarm	node	hostname>	
networks:	
isnet:	
aliases:	
-	couchbase	
-	couchbase.isnet	
hostname:	couchbase.isnet	
couchbase-	worker:	
image:	sciencelogic/is-	couchbase:1.7.0	
container_	name:	couchbase-	worker	
volumes:	
-	""/var/data/couchbase:/opt/couchbase/var""	
ports:	
-	""8100:8091""	
deploy:	
replicas:	0	
placement:	
constraints:	
-	node.hostname	==	<swarm	node	hostname>	
210 

211	
networks:	
isnet:	
aliases:	
-	couchbase-	worker	
-	couchbase-	worker.isnet	
hostname:	couchbase-	worker.isnet	
secrets:	
-	is_	pass	
-	encryption_	key	
environment:	
TYPE:	'WORKER'	
AUTO_	REBALANCE:	'true'	
db_	host:	'couchbase'	
depends_	on:	
-	couchbase	
couchbase-	worker2:	
image:	sciencelogic/is-	couchbase:1.7.0	
container_	name:	couchbase-	worker2	
ports:	
-	""8101:8091""	
volumes:	
-	""/var/data/couchbase:/opt/couchbase/var""	
deploy:	
replicas:	0	
placement:	
constraints:	
-	node.hostname	==	<swarm	node	hostname>	
networks:	
isnet:	
aliases:	
-	couchbase-	worker2	
-	couchbase-	worker2.isnet	
hostname:	couchbase-	worker2.isnet	
secrets:	
-	is_	pass	
-	encryption_	key	
environment:	
TYPE:	'WORKER'	
AUTO_	REBALANCE:	'true'	
db_	host:	'couchbase'	
depends_	on:	
-	couchbase	
rabbitmq:	
image:	sciencelogic/is-	rabbit:3.7.7-	1	
hostname:	rabbit_	node1.isnet	
volumes:	
-	""rabbitdb:/var/lib/rabbitmq""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node1.isnet	
deploy:	
placement:	
constraints:	
Configuring	Clustering	and	High	Availability 

Configuring	Clustering	and	High	Availability	
-	node.hostname	==	<swarm	node	hostname>	
rabbitmq2:	
image:	sciencelogic/is-	rabbit:3.7.7-	1	
hostname:	rabbit_	node2.isnet	
volumes:	
-	""rabbitdb2:/var/lib/rabbitmq""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node2.isnet	
deploy:	
placement:	
constraints:	
-	node.hostname	==	<swarm	node	hostname>	
rabbitmq3:	
image:	sciencelogic/is-	rabbit:3.7.7-	1	
hostname:	rabbit_	node3.isnet	
volumes:	
-	""rabbitdb3:/var/lib/rabbitmq""	
networks:	
isnet:	
aliases:	
-	rabbit	
-	rabbit_	node3.isnet	
deploy:	
placement:	
constraints:	
-	node.hostname	==	<swarm	node	hostname>	
contentapi:	
environment:	
db_	host:	'couchbase.isnet,couchbase-	worker.isnet,couchbase-	worker2.isnet'	
pypiserver:	
image:	sciencelogic/is-	pypi:4.8.0-	1	
hostname:	devpi	
container_	name:	devpi	
volumes:	
-	""devpi:/data""	
networks:	
isnet:	
aliases:	
-	pypiserver	
secrets:	
-	is_	pass	
dexserver:	
image:	scr.sl1.io/is-	dex:2.18.0-	1	
ports:	
-	""5556:5556""	
-	""5558:5558""	
command:	[""serve"",	""/dexConfiguration.yaml""]	
networks:	
isnet:	
aliases:	
212 

213	
-	dexserver	
configs:	
-	source:	dex_	config	
target:	/dexConfiguration.yaml	
volumes:	
rabbitdb2:
rabbitdb3:
devpi:	
configs:	
dex_	config:	
file:	/etc/iservices/dexConfiguration.yaml	
S c a l i n g	i s e r v i c e s -	c o n te n ta p i	
To	scale	out	the	iservices-	contentapi	to	distribute	the	service	across	the	three	nodes,	run	the	following	command:	
docker	service	scale	iservices-	contentapi=3	
M a n u a l	F a i l o v e r	
Ifyou	have	acluster	with	three	or	more	nodes	that	is	not	configured	with	automatic	failover,	you	must	perform	the	
following	manual	failover	steps.	
NOTE	:Ifyou	can	access	the	Couchbase	Administrator	user	interface	(https://	<IP	ofÂ 	PowerFlow	>	:8091	)	
on	the	node	that	is	still	running,	you	can	simply	click	the	[Failover	]button	in	the	Couchbase	
Administrator	user	interface	instead	of	manually	running	the	couchbase-	cli	commands	below.	
NOTE	:In	athree-	node	cluster,	asingle	failed	node	will	be	automatically	removed,	you	will	still	need	to	
perform	are-	balance.	
To	initiate	amanual	failover:	
1.	Log	in	to	the	Docker	Swarm	node	where	the	node	that	is	running	resides.	
2.	Remove	any	failed	managers	from	the	cluster	by	running	the	following	Docker	commands:	
docker	swarm	init	--force-	new-	cluster	
docker	node	rm	<failed	node	id>	
Scaling	iservices-	contentapi 

Manual	Failover	
3.	Run	the	following	command	identify	the	Container	ID	of	the	running	Couchbase	container:	
docker	ps	
4.	Connect	to	the	Docker	container:	
docker	exec	-i	-t	<container	id>	/bin/bash	
5.	Identify	the	failed	node	by	running	the	following	commands:	
couchbase-	cli	server-	list	-c	couchbase	-u	isadmin	-p	<password>	
couchbase-	cli	server-	list	-c	couchbase-	worker	-u	isadmin	-p	<password>	
6.	One	of	the	previous	commands	will	show	afailed	node.	Copy	the	IP	address	and	port	number	of	the	failed	
node	for	step	7.	
7.	Use	the	currently	running	cluster	and	theÂ failed	node's	IP	address	and	port	to	run	the	following	command	to	
failover:	
couchbase-	cli	failover	-c	<couchbase|couchbase-	worker>	-u	isadmin	-p	<password>	-	
-server-	failover	<ip:port>	--force	
For	example,	ifthe	functioning	node	is	couchbase-	worker	,and	the	IP:Port	of	the	failed	service	is	
10.0.0.4:4379	,then	the	command	would	be:	
couchbase-	cli	failover	-c	couchbase-	worker	-u	isadmin	-p	<password>	--server-	
failover	10.0.0.4:4379	--force	
8.	Rebalance	the	cluster	using	the	functioning	container	name:	
couchbase-	cli	rebalance	-c	<cluster|cluster-	worker>	-u	isadmin	-p	<password>	
9.	In	the	unlikely	event	that	afailover	occurs	and	no	queries	can	be	performed,	validate	that	the	indexes	exist,	
and	ifnot,	rebuild	them.	To	rebuild	the	primary	indexes,	run	the	following	commands:	
cbq	-u	isadmin	
CREATE	PRIMARY	INDEX	ON	content;	
CREATE	PRIMARY	INDEX	ON	logs;	
To	recover	aDocker	Swarm	node:	
1.	Re-	deploy	the	node.	
2.	Add	anew	manager	node	to	the	swarm	stack.	
To	restore	the	failed	Couchbase	node:	
1.	Log	in	to	the	node	where	the	failed	Couchbase	cluster	node	was	pinned.	
2.	Run	one	of	the	following	commands,	depending	on	the	Couchbase	node	being	recovered:	
l	docker	service	scale	iservices_	couchbase=0	
l	docker	service	scale	iservices_	couchbase-	worker=0	
214 

215	
3.	Ifthe	Docker	Swarm	node	was	restored	and	not	rebuilt,	remove	files	from	the	old	container:	
rm	-rf	/var/data/couchbase/*	
docker	service	scale	iservices_	couchbase	scale	1	
A	new	node	is	added	to	Couchbase	that	will	automatically	re-	balance	the	cluster	after	itis	added.	
A d d i ti o n a l	C o n f i g u r a ti o n	I n f o r m a ti o n	
O p t i m i z a t i on Â S e t t i n g s	t o	I m p r ov e	P e r f or m a n c e	of	L a r g e -	S c a l e Â C l u s t e r s	
In	large-	scale	clusters,	one	of	the	root	causes	of	abnormal	memory	and	CPU	usage	is	from	inter-	worker	
communication	overhead,	or	overly	""chatty""	workers,	and	their	event	queues.	You	can	completely	disable	inter-	
worker	eventing	to	significantly	reduce	overhead	on	the	queuing	system	and	prevent	the	symptoms	associated	with	
abnormal	memory	usage.	
Also,	to	improve	the	performance	of	large-	scale	clusters	by	default,	the	following	optimization	settings	were	added	
to	the	docker-	compose.yml	file	for	all	workers	in	version	2.0.1	of	the	PowerFlow	platform:	
steprunner-	<worker-	x>:	
environment:	
additional_	worker_	args:	""-	-max-	tasks-	per-	child	1	--without-	gossip	--without-	
mingle""
In	addition	to	the	default	optimization	settings	above,	you	can	further	reduce	system	overhead	by	setting	the	--	
without-	heartbeat	environment	variable	in	additional_	worker_	args	.Please	note	that	this	setting	
will	reduce	the	memory	and	CPU	utilization	of	the	system,	but	itwill	come	at	the	cost	of	preventing	the	Flower	
service	from	getting	an	accurate	depiction	of	current	worker	states.	
Ifyou	want	to	disable	these	new	configuration	settings,	set	the	environment	variable	""disable_	default_	
optimizations""	to	""True""	for	all	workers.	
NOTE:	Workers	will	continue	to	generate	events	for	consumption	from	monitoring	tools	like	Flower	even	with	
the	new	default	configuration	settings.	In	some	extremely	large	clusters,	you	might	want	to	completely	
disable	eventing	of	workers	completely,	especially	ifFlower	is	not	in	use.Â To	completely	disable	
worker	eventing,	set	the	environment	variable	""disable_	events""	to	""True""	.	
For	more	information,	see	https://docs.celeryproject.org/en/latest/reference/celery.bin.worker.html#cmdoption-	
celery-	worker-	without-	gossip	
Additional	suggestions	for	improving	performance	in	large-	scale	clusters:	
l	Assess	the	impact	of	using	Flower	before	keeping	itenabled	for	along	period	of	time.	Running	Flower	can	
cause	increased	overhead	on	the	RabbitMQ	nodes,	but	the	overhead	is	not	significant	initially.	However,	the	
overhead	generated	by	Flower	will	continue	to	increase	as	more	workers	are	added	to	the	stack,	and	those	
workers	send	events	to	Flower.	
Additional	Configuration	Information 

Additional	Configuration	Information	
l	ScienceLogic	recommends	that	you	monitor	memory	and	queue	utilization	before	and	after	running	Flower	
with	your	current	environment	size	to	determine	whether	the	extra	overhead	provided	is	worth	the	task	
information	itprovides.	
l	Ifasystem	event	causes	workers	to	restart,	itis	possible	that	all	workers	constantly	restarting	at	the	same	time,	
every	0	seconds	will	generate	increased	load	on	the	system,	making	itdifficult	for	other	services	to	start	up.	To	
prevent	this,	itis	recommended	to	add	arestart_	delay	to	workers	to	prevent	a""rush""	of	hundreds	of	workers	
trying	to	re-	connect	over	the	network	all	at	once.	For	example:	
steprunner-	<worker-	x>:	
deploy	
restart_	policy:	
delay:	30s	
E x p os i n g	A d d i t i on a l	C ou c h b a s e	C l u s t e r	N od e	M a n a g e m e n t	I n t e r f a c e s	
ov e r	T L S	
The	is_	gui	container	acts	as	areverse	proxy	to	the	internal	services	and	their	individual	management	interfaces.	
This	container	configured	in	/etc/nginx/conf.d/default.conf	.	
To	expose	the	management	interfaces	of	additional	Couchbase	nodes	within	acluster:	
1.	Copy	the	configuration	from	the	gui	container:	
docker	cp	<container	id>:/etc/nginx/conf.d/default.conf	./	
2.	Edit	the	configuration	to	include	the	desired	services:	
server	{	
listen	8092	ssl;	
server_	name	couchbase-	worker;	
location	=	/	{	
return	301	https://$host:8092/ui/index.html;	
}
location	/	{	
resolver	127.0.0.11	valid=5s;	
set	$upstream	couchbase-	worker.isnet;	
proxy_	pass	http://$upstream:8092$request_	uri;	
proxy_	pass_	header	Server;	
proxy_	pass_	header	Cache-	Control;	
proxy_	pass_	header	Content-	Length;	
proxy_	pass_	header	Connection;	
proxy_	pass_	header	Pragma;	
proxy_	pass_	header	ns-	server-	ui;	
proxy_	pass_	header	invalid-	auth-	response;	
}	
ssl_	certificate	/etc/iservices/is_	cert.pem;	
ssl_	certificate_	key	/etc/iservices/is_	key.pem;	
ssl_	protocols	TLSv1.2;	
ssl_	ciphers	'ECDHE-	ECDSA-	AES256-	GCM-	SHA384:ECDHE-	RSA-	AES256-	GCM-	
216 

217	
SHA384:ECDHE-	ECDSA-	CHACHA20-	POLY1305:ECDHE-	RSA-	CHACHA20-	POLY1305:ECDHE-	ECDSA-	AES128-	
GCM-	SHA256:ECDHE-	RSA-	AES128-	GCM-	SHA256:ECDHE-	ECDSA-	AES256-	SHA384:ECDHE-	RSA-	AES256-	
SHA384:ECDHE-	ECDSA-	AES128-	SHA256:ECDHE-	RSA-	AES128-	SHA256';	
ssl_	prefer_	server_	ciphers	on;	
ssl_	session_	cache	shared:SSL:20m;	
ssl_	session_	timeout	180m;	
add_	header	Strict-	Transport-	Security	""max-	age=31536000""	always;	
}
3.	Create	the	following	Dockerfile:	
FROM	sciencelogic/is_	gui	
COPY	./default.conf	/etc/nginx/conf.d/default.conf	
4.	Build	the	container	with	the	new	configurations:	
docker	build	-t	<customer>/is_	gui:<is	version>-	1	-f	Dockerfile	.	
5.	Add	the	image	name	to	the	is_	gui	section	in	the	docker-	compose-	override.yml	file,	and	do	aDocker	
stack	deploy	to	enable	the	new	is_	gui	container.	
H A P r ox y	C on f i g u r a t i on	( O p t i on a l )	
CAUTION:	As	aconvenience,	ScienceLogic	provides	an	example	configuration	for	the	HAProxy	load	
balancer	below.	Please	note	that	itis	your	responsibility	to	configure	the	load	balancer.	
ScienceLogic	cannot	be	held	responsible	for	any	deployments	that	deviate	from	the	example	
HAProxy	load	balancer	configuration.	
The	following	example	configuration	describes	using	HAProxy	as	aload	balancer:	
defaults	
mode	http	
log	global	
option	httplog	
option	dontlognull	
option	http-	server-	close	
option	redispatch	
retries	3	
timeout	http-	request	1m	
timeout	queue	1m	
timeout	connect	1m	
timeout	client	1m	
timeout	server	1m	
timeout	http-	keep-	alive	10s	
timeout	check	10s	
maxconn	6000	
frontend	http_	front	
bind	*:80	
bind	*:443	
option	tcplog	
mode	tcp	
tcp-	request	inspect-	delay	5s	
Additional	Configuration	Information 

Known	Issues	
default_	backend	http_	back	
frontend	dex_	front	
bind	*:5556	
option	tcplog	
mode	tcp	
tcp-	request	inspect-	delay	5s	
default_	backend	dex_	back	
frontend	devpi_	front	
bind	*:3141	
option	tcplog	
mode	tcp	
tcp-	request	inspect-	delay	5s	
default_	backend	devpi_	back	
backend	http_	back	
mode	tcp	
balance	roundrobin	
server	master1	<docker	swarm	node	1	ip>:443	check	
server	master2	<docker	swarm	node	2	ip>:443	check	
server	master3	<docker	swarm	node	3	ip>:443	check	
backend	dex_	back	
mode	tcp	
balance	roundrobin	
server	master1	<docker	swarm	node	1	ip>:5556	check	
server	master2	<docker	swarm	node	2	ip>:5556	check	
server	master3	<docker	swarm	node	3	ip>:5556	check	
backend	devpi_	back	
mode	tcp	
balance	roundrobin	
server	master1	<docker	swarm	node	1	ip>:3141	check	
server	master2	<docker	swarm	node	2	ip>:3141	check	
server	master3	<docker	swarm	node	3	ip>:3141	check	
K n o w n	I s s u e s	
The	following	section	describes	the	known	issues	you	might	encounter	with	the	High	Availability	solution	and	how	
to	address	those	issues.	
D oc k e r	N e t w or k	A l i a s	i s	i n c or r e c t	
Ifyou	experience	issues	with	the	iservices_	contentapi	container,	the	Alias	IP	might	be	incorrect.	
To	address	this	issue,	run	the	followingÂ commands	on	the	relevant	node:	
docker	service	scale	iservices_	contentapi=0	
docker	service	scale	iservices_	contentapi=1	(or	another	number	as	needed)	
218 

219	
NOTE:	This	issue	was	addressed	in	the	docker-	ce	18.06	version	of	Docker,	which	is	required	for	version	
1.8.0	of	PowerFlow	.	
D oc k e r	c on t a i n e r	on	l a s t	s w a r m	n od e	c a n n ot	c om m u n i c a t e	w i t h	ot h e r	
s w a r m	n od e s	
This	is	an	issue	with	the	Encapsulating	Security	Payload	(ESP)	protocol	not	being	enabled	in	firewalld.	You	can	
enable	the	ESPÂ protocol	with	the	firewalld	docker-	swarm	script.	
To	address	this	issue,	add	the	following	firewall	rule	to	each	node:	
firewall-	cmd	--add-	protocol=esp	--permanant	
firewall-	cmd	--reload	
C ou c h b a s e	s e r v i c e	d oe s	n ot	s t a r t ,	r e m a i n s	a t	n c	-z	l oc a l h os t	
To	address	this	issue,	stop	the	container	where	this	is	happening	and	remove	its	persistent	volume:	
rm	-rf	/var/data/couchbase	
C ou c h b a s e -	w or k e r	f a i l s	t o	c on n e c t	t o	m a s t e r	
A	connection	failure	might	happen	afew	times	when	astack	is	freshly	deployed.	You	can	ignore	these	messages,	
and	the	worker	should	eventually	connect	to	the	master.	
C ou c h b a s e	r e b a l a n c e	f a i l s	w i t h	"" R e b a l a n c e	e x i t e d ""	e r r or	
In	this	situation,	you	received	the	following	error:	
Rebalance	exited	with	reason	{service_	rebalance_	failed,index,	
{linked_	process_	died,<12807.821.0>,	
{no_	connection,""index-	service_	api""}	
}}
Ifthe	Couchbase	rebalance	fails	on	the	initial	rejoin	of	afailed	node	into	acluster,	you	should	check	the	index	
states	and	wait	until	the	indexes	are	no	longer	in	awarmup	state.	After	the	indexes	are	created	on	that	node,	the	
rebalance	should	succeed.	
W h e n	s e t t i n g	u p	a	t h r e e -	n od e	H i g h	A v a i l a b i l i t y	C ou c h b a s e	c l u s t e r ,	t h e	
s e c on d	n od e	d oe s	n ot	a p p e a r	
In	this	situation,	ifyou	have	cloned	any	of	the	nodes,	the	nodes	might	think	that	there	is	asplit-	brain	condition.	
To	address	this	issue,	delete	the	Couchbase	data	on	the	newly	added	nodes	by	running	the	following	command	
on	each	node:	
rm	-rf	/var/data/couchbase/*	
Known	Issues 

Known	Issues	
T h e Â 	P ow e r F l ow	u s e r	i n t e r f a c e	f a i l s	t o	s t a r t	a f t e r	a	m a n u a l	f a i l ov e r	of	
t h e	s w a r m	n od e	
To	address	this	issue,	run	the	followingÂ commands	on	the	relevant	node:	
docker	stack	rm	iservices	
systemctl	restart	docker	
docker	stack	deploy	-c	docker-	compose.yml	iservices	
T h e Â 	P ow e r F l ow	u s e r	i n t e r f a c e	r e t u r n s	5 0 4	e r r or s	
Ensure	that	your	PowerFlow	systems	have	been	updated	with	yum	upgrade	.	
N T P	s h ou l d	b e	u s e d ,	a n d	a l l	n od e	t i m e s	s h ou l d	b e	i n	s y n c	
Ifall	nodes	time	are	not	in	sync,	you	might	experience	issues	with	the	iservices_	steprunners.	
The	following	is	an	example	of	aDocker	Swarm	error	caused	by	the	time	not	being	in	sync:	
Error	response	from	daemon:	certificate	(1	-	2v4umws4pxag6kbxaelwfl3vf)	not	valid	
before	Fri,	30	Nov	2018	13:47:00	UTC,	and	it	is	currently	Fri,	30	Nov	2018	06:41:24	
UTC:	x509:	certificate	has	expired	or	is	not	yet	valid	
For	more	information,	see	Preparing	the	PowerFlow	System	for	High	Availability	.	
E x a m p l e	L og s	f r om	F l ow e r	
iservices_	flower.1.jg6glaf298d2@is-	scale-	05	|	[W	181023	20:17:40	state:113]	
Substantial	drift	from	celery@1ee384863e37	may	mean	clocks	are	out	of	sync.	Current	
drift	is	iservices_	flower.1.jg6glaf298d2@is-	scale-	05	|	18	seconds.	[orig:	2018-	10-	23	
20:17:40.090473	recv:	2018-	10-	23	20:17:58.486666]	
220 

Appe ndix	
B	
Configuring	the	PowerFlow	Â System	for	Multi-	
tenant	Environments	
O v e r v i e w
This	appendix	describes	the	best	practices	and	troubleshooting	solutions	for	deploying	PowerFlow	in	amulti-	tenant	
environment	that	supports	multiple	customers	in	ahighly	available	fashion.	This	section	also	covers	how	to	perform	
an	upgrade	of	PowerFlow	with	minimal	downtime.	
This	appendix	covers	the	following	topics:	
Quick	Start	Checklist	for	Deployment	222	
Deployment	222	
Advanced	RabbitMQ	Administration	and	Maintenance	226	
Creating	a	Custom	Configuration	Object	227	
Failure	Scenarios	231	
Examples	and	Reference	236	
Test	Cases	241	
Backup	Considerations	243	
Resiliency	Considerations	244	
Additional	Sizing	Considerations	246	
Node	Placement	Considerations	247	
Common	Problems,	Symptoms,	and	Solutions	248	
Common	Resolution	Explanations	255	
PowerFlow	Multi-	tenant	Upgrade	Process	259
221 

222
Q u i c k	S ta r t	C h e c k l i s t	f o r	D e p l o y m e n t	
1.	Deploy	and	cluster	the	initial	High	Availability	stack.	Label	these	nodes	as	""core"".	
2.	Create	the	PowerFlow	configuration	for	the	newÂ 	PowerFlow	systems.	This	configuration	information	includes	
the	SL1	IP	address,	the	ServiceNow	user	and	domain,	and	other	related	information.	
3.	Deploy	and	cluster	the	worker	node	or	nodes	for	the	customer.	
4.	Label	the	worker	node	or	nodes	specifically	for	the	customer.	
5.	Update	the	docker-	compose.yml	file	on	acore	node:	
l	Add	two	steprunner	services	for	each	customer,	one	for	real-	time	eventing,	and	one	for	backlogged	
events,	labeled	based	on	the	organization	name:	acme	and	acme-	catchups	.	
l	Update	the	new	steprunner	hostnames	to	indicate	who	the	steprunner	works	for.	
l	Update	the	new	steprunner	deploy	constraints	to	deploy	only	to	the	designated	labels.	
l	Update	the	new	steprunner	user_	queues	environment	variable	to	only	listen	on	the	desired	queues.	
6.	Schedule	the	required	integrations	for	this	customer	:	
l	Device	Sync	daily,	ifdesired	
l	Correlation	queue	manager	running	on	the	catchup	queue	
7.	Modify	the	Run	Book	Automations	in	SL1	to	trigger	the	integration	to	run	on	the	queue	for	this	customer:	
l	Modify	the	IS_	PASSTHROUGH	dictionary	with	""queue""	setting.	
l	Specify	the	configuration	to	use	in	PowerFlow	for	this	SL1	instance.	
D e p l o y m e n t
The	following	sections	describe	how	to	deploy	PowerFlow	in	amulti-	tenant	environment.	After	the	initial	High	
Availability	(HA)	core	services	are	deployed,	the	multi-	tenant	environment	differs	in	the	deployment	and	
placement	of	workers	and	use	of	custom	queues.	
C or e	S e r v i c e	N od e s	
For	amulti-	tenant	deployment,	ScienceLogic	recommends	that	you	dedicate	at	least	three	nodes	to	the	core	
PowerFlow	services.	These	core	PowerFlow	services	are	shared	by	all	workers	and	customers.	As	aresult,	itis	
essential	that	these	services	are	clustered	to	handle	failovers.	
Because	these	core	services	are	critical,	ScienceLogic	recommends	that	you	initially	allocate	afairly	large	amount	
of	resources	to	these	services.	Allocating	more	resources	than	necessary	to	these	nodes	allows	you	to	further	scale	
workers	in	the	future.	Ifthese	nodes	become	overly	taxed,	you	can	add	another	node	dedicated	to	the	core	
services	in	the	cluster.	
Quick	Start	Checklist	for	Deployment 

Deployment
These	core	services	nodes	are	dedicated	to	the	following	services:	
l	API	
l	UI	
l	RabbitMQ	
l	Couchbase	
l	Redis	
Itis	critical	to	monitor	these	core	service	nodes,	and	to	always	make	sure	these	nodes	have	enough	resources	for	
new	customers	and	workers	as	they	are	on-	boarded.	
To	ensure	proper	failover	and	persistence	of	volumes	and	cluster	information,	the	core	services	must	be	pinned	to	
each	of	the	nodes.	For	more	information,	see	Configuring	Core	Service	Nodes	,below.	
R e q u i r e m e n t s	
Three	nodes	(or	more	for	additional	failover	support)	with	six	CPUs	and	56	GB	memory	each	
C o n f i g u r i n g	C o r e	S e r v i c e	N o d e s	
l	Install	the	PowerFlow	RPM	on	your	core	three	nodes.	
l	See	the	High	Availability	section	for	information	about	how	to	join	the	cluster	as	amanager,	and	copy	the	
/etc/iservices/encryption_	key	and	/etc/iservices/is_	pass	file	from	acore	service	node	to	the	new	worker	
node	(same	location	and	permissions).	
l	Create	a	label	on	the	node	and	label	these	nodes	as	""core	node"".	
l	See	the	Configuring	Clustering	and	HighÂ Availability	section	for	details	on	clustering	Couchbase	and	
RabbitMQ,	and	an	example	compose	file	of	this	setup.	
l	Update	the	contentapi,	UI,	and	redis	services	so	that	those	services	are	only	ever	deployed	onto	the	core	
nodes.	
C r i t i c a l	E l e m e n t s	t o	M o n i t o r	o n	C o r e	N o d e s	
l	Memory	utilization:	Warnings	at	80%	
l	CPU	utilization:	Warnings	at	80%	
l	RabbitMQ	queue	sizes	(can	also	be	monitored	from	the	Flower	API,	or	the	PowerFlow	user	interface)	
W or k e r	S e r v i c e	N od e s	
Separate	from	the	core	services	are	the	worker	services	.These	worker	services	are	intended	to	be	deployed	on	
nodes	separate	from	the	core	services,	and	other	workers,	and	these	worker	services	aim	to	provide	processing	
only	for	specified	dedicated	queues.	Separating	the	VMs	or	modes	where	worker	services	are	deployed	will	ensure	
that	one	customer's	workload,	no	matter	how	heavy	itgets,	will	not	negatively	affect	the	other	core	services,	or	
other	customer	workloads.	
223 

224
R e q u i r e m e n t s
The	resources	allocated	to	the	worker	nodes	depends	on	the	worker	sizing	chosen,	the	more	resources	provided	to	
aworker,	the	faster	their	throughput.	Below	is	abrief	guideline	for	sizing.	Please	note	that	even	ifyou	exceed	the	
number	of	event	syncs	per	minute,	events	will	be	queued	up,	so	the	sizing	does	not	have	to	be	exact.	The	below	
sizing	just	provides	asuggested	guideline.	
E v e n t	S y n c	T h r o u g h p u t	N o d e	S i z i n g	
CPU	Memory	Worker	count	Time	to	sync	aqueue	full	of	10,000	events	Events	Synced	per	second	
2	16	GB	6	90	minutes	1.3	
4	32	GB	12	46	minutes	3.6	
8	54	GB	25	16.5	minutes	10.1	
T e s t	E n v i r o n m e n t	a n d	S c e n a r i o	
1.	Each	Event	Sync	consists	of	PowerFlow	workers	reading	from	the	pre-	populated	queue	of	10000	events.	The	
sync	interprets,	transforms,	and	then	POSTS	the	new	event	as	acorrelated	ServiceNow	incident	into	
ServiceNow.	This	process	goes	on	to	then	query	ServiceNow	for	the	new	sysID	generated	for	the	incident,	
transforms	it,	and	then	POSTs	itback	to	SL1	as	an	external	ticket	to	complete	the	process.	
l	Tests	were	performed	on	anode	of	workers	only.	
l	Tests	were	performed	with	a2.6	GHz	virtualized	CPU	in	avCenter	VM.	Both	SL1	and	ServiceNow	were	
responding	quickly	when	doing	so.	
l	Tests	were	performed	with	apre-	populated	queue	of	10000	events.	
l	Tests	were	performed	with	the	current	deployed	version	of	Cisco	custom	integration.	Data	will	again	be	
gathered	for	the	next	version	when	itis	completed	by	Pro	Services.	
l	Each	event	on	the	queue	consisted	of	asingle	correlated	event.	
C o n f i g u r i n g	t h e	W o r k e r	N o d e	
l	Install	the	PowerFlow	RPM	on	the	new	node.	
l	See	the	High	Availability	section	for	information	about	how	to	join	the	cluster	as	amanager	or	worker,	and	
copy	the	/etc/iservices/encryption_	key	and	/etc/iservices/is_	pass	file	from	acore	service	node	to	the	
new	worker	node	(same	location	and	permissions).	
l	By	default,	the	worker	will	listen	on	and	accept	work	from	the	default	queue,	which	is	used	primarily	by	the	
user	interface,	and	any	integration	run	without	acustom	queue.	
l	To	configure	this	worker	to	run	customer-	specific	workloads	with	custom	queues,	see	Onboarding	a	
Customer	.	
l	Modify	the	docker-	compose.yml	on	acore	service	node	accordingly.	
Deployment 

Deployment	
l	Ifyou	just	want	the	node	to	accept	default	work,	the	only	change	necessary	is	to	increase	worker	count	using	
the	table	provided	in	the	requirements	section	
l	Ifyou	want	the	node	to	be	customer	specific,	be	sure	to	add	the	proper	labels	and	setup	custom	queues	for	
the	worker	in	the	docker-	compose	when	deploying.	This	information	is	contained	in	the	Onboarding	a	
customer	section.	
I n i t i a l	W o r k e r	N o d e	D e p l o y m e n t	S e t t i n g s	
Itis	required	that	there	is	always	at	least	one	worker	instance	listening	on	the	default	queue	for	proper	functionality.	
The	default	worker	can	run	in	any	node.	
W o r k e r	F a i l o v e r	C o n s i d e r a t i o n s	a n d	A d d i t i o n a l	S i z i n g	
When	deploying	anew	worker,	especially	ifitis	going	to	be	acustom	queue	dedicated	worker,	itis	wise	to	
consider	deploying	an	extra	worker	listening	on	the	same	queues.	Ifyou	have	on	asingle	worker	node	listening	to	
adedicated	customer	queue,	there	is	potential	for	that	queue	processing	to	stop	completely	ifthat	single	node	
worker	fails.	
For	this	reason,	ScienceLogic	recommends	that	for	each	customer	dedicated	worker	you	deploy,	you	deploy	a	
second	one	as	well.	This	way	there	are	two	nodes	listening	to	the	customer	dedicated	queue,	and	ifone	node	fails,	
the	other	node	will	continue	processing	from	the	queue	with	no	interruptions.	
When	deciding	on	worker	sizing,	it's	important	to	take	this	into	consideration.	For	example,	ifyou	have	acustomer	
that	requires	afour-	CPU	node	for	optimal	throughput,	an	option	would	be	to	deploy	two	nodes	with	two	CPUs,	so	
that	there	is	failover	ifone	node	fails.	
l	How	to	know	when	more	resources	are	necessary	
l	Extra	worker	nodes	ready	for	additional	load	or	failover	
K n o w i n g	W h e n	M o r e	R e s o u r c e s	a r e	N e c e s s a r y	f o r	a	W o r k e r	
Monitoring	the	memory,	CPU	and	pending	integrations	in	queue	can	give	you	an	indication	of	whether	more	
resources	are	needed	for	the	worker.	Generally,	when	queue	times	start	to	build	up,	and	tickets	are	not	synced	
over	in	an	acceptable	time	frame,	more	workers	for	task	processing	are	required.	
Although	more	workers	will	process	more	tasks,	they	will	be	unable	to	do	so	ifthe	memory	or	CPU	required	by	the	
additional	workers	is	not	present.	When	adding	additional	workers,	itis	important	to	watch	the	memory	or	CPU	
utilization,	so	long	as	the	utilization	is	under	75%,	itshould	be	okay	to	add	another	worker.	Ifutilization	is	
consistently	over	80%,	then	you	should	add	more	resources	to	the	system	before	addling	additional	workers.	
K e e p i n g	a	W o r k e r	N o d e	o n	S t a n d b y	f o r	E x c e s s	L o a d	D i s t r i b u t i o n	
Even	ifyou	have	multiple	workers	dedicated	to	asingle	customer,	there	are	still	scenarios	in	which	aparticular	
customer	queue	spikes	in	load,	and	you'd	like	an	immediate	increase	in	throughput	to	handle	this	load.	In	this	
scenario	you	don't	have	the	time	to	deploy	anew	IS	node	and	configure	itto	distribute	the	load	for	greater	
throughput,	as	you	need	increased	load	immediately.	
225 

226
This	can	be	handled	by	having	anode	on	standby.	This	node	has	the	same	IS	RPM	version	installed,	and	sits	idle	in	
the	stack	(or	is	turned	off	completely).	When	aspike	happens,	and	you	need	more	resources	to	distribute	the	load,	
you	can	then	apply	the	label	to	the	corresponding	to	the	customer	who's	queues	spiked.	After	setting	the	label	on	
the	standby	node,	you	can	scale	up	the	worker	count	for	that	particular	customer.	Now,	with	the	stand-	alone	node	
labeled	for	work	for	that	customer,	additional	worker	instances	will	be	distributed	to	and	started	on	the	standby	
node.
When	the	spike	has	completed,	you	can	return	the	node	to	standby	by	reversing	the	above	process.	Decrease	the	
worker	count	to	what	itwas	earlier,	and	then	remove	the	customer	specific	label	from	the	node.	
C r i t i c a l	E l e m e n t s	t o	M o n i t o r	i n	a	S t e p r u n n e r	
l	Memory	utilization:	Warnings	at	80%	
l	CPU	utilization:	Warnings	at	80%	
l	Successful,	failed,	active	tasks	executed	by	steprunner	(retrievable	from	Flower	API	or	PowerPack	)	
l	Pending	tasks	in	queue	for	the	worker	(retrievable	by	Flower	API	or	PowerPack	)	
l	Integrations	in	queue	(similar	information	here	as	in	pending	tasks	in	queue,	but	this	is	retrievable	from	the	
PowerFlow	API).	
A d v a n c e d	R a b b i tM Q	A d m i n i s tr a ti o n	a n d	M a i n te n a n c e	
This	section	describes	how	multi-	tenant	deployments	can	use	separate	virtual	hosts	and	users	for	each	tenant.	
U s i n g	a n	E x t e r n a l	R a b b i t M Q	I n s t a n c e	
In	certain	scenarios,	you	might	not	want	to	use	the	default	RabbitMQ	queue	that	is	prepackaged	with	PowerFlow	.	
For	example,	you	might	already	have	aRabbitMQ	production	cluster	available	that	you	just	want	to	connect	with	
PowerFlow	.You	can	do	this	by	defining	anew	virtual	host	in	RabbitMQ,	and	then	you	configure	the	PowerFlow	
broker	URL	for	contentapi,	steprunner,	scheduler	services	so	that	they	point	to	the	new	virtual	host.	
Any	use	of	an	external	RabbitMQ	server	will	not	be	officially	supported	by	ScienceLogic	ifthere	are	issues	in	the	
external	RabbitMQ	instance.	
S e t t i n g	a	U s e r	ot h e r	t h a n	G u e s t	f or	Q u e u e	C on n e c t i on s	
By	default	RabbitMQ	contains	the	default	credentials	guest/guest	.PowerFlow	uses	these	credentials	by	default	
when	communicating	with	RabbitMQ	in	the	swarm	cluster.	All	communication	is	encrypted	and	secured	within	the	
overlay	Docker	network.	
To	add	another	user,	or	to	change	the	user	that	PowerFlow	uses	when	communicating	with	the	queues:	
1.	Create	anew	user	in	RabbitMQ	that	has	full	permissions	to	avirtual	host.	For	more	information,	see	the	
RabbitMQ	documentation	.	
Advanced	RabbitMQ	Administration	and	Maintenance 

Creating	aCustom	Configuration	Object	
2.	Update	the	broker_	url	environment	variable	with	the	new	credentials	in	the	docker-	compose	file	and	then	
re-	deploy.	
C on f i g u r i n g	t h e	B r ok e r	( Q u e u e )	U R L	
When	using	an	external	RabbitMQ	system,	or	ifyou	are	using	credentials	other	than	guest/guest	to	authenticate,	
you	need	to	update	the	broker_	url	environment	variable	in	the	contentapi,	steprunner,	and	scheduler	services.	
You	can	do	this	by	modifying	the	environment	section	of	the	services	in	docker-	compose	and	changing	broker_	
url	.The	following	line	is	an	example:	
broker_	url:	'pyamqp://username:password@rabbitmq-	hostname/v-	host'	
C r e a ti n g	a	C u s to m	C o n f i g u r a ti o n	O b j e c t	
When	anew	SL1	system	is	to	be	onboarded	into	PowerFlow	,by	default	their	integrations	are	executed	on	the	
default	queue.	In	large	multi-	tenant	environments,	ScienceLogic	recommends	separate	queues	for	each	
customer.	Ifdesired,	each	customer	can	also	have	specific	queues.	
C r e a t e	t h e	C on f i g u r a t i on	O b j e c t	
The	first	step	to	setting	up	anew	PowerFlow	system	is	to	create	aconfiguration	object	with	variables	that	will	satisfy	
all	PowerFlow	applications.	The	values	of	these	should	be	specific	to	the	new	system	(such	as	SL1	IP	address,	
username,	password).	
See	the	example	configuration	for	atemplate	you	can	fill	out	for	new	system.	
Because	integrations	might	update	their	variable	names	from	EM7	to	SL1	in	the	future,	ScienceLogic	recommends	
to	cover	variables	for	both	em7_	and	sl1_	.The	example	configuration	contains	this	information.	
L a b e l	t h e	W or k e r	N od e	S p e c i f i c	t o	t h e	C u s t om e r	
For	an	example	label,	ifyou	want	aworker	node	to	be	dedicated	to	acustomer	called	""acme"",	you	could	create	a	
node	label	called	""customer""	and	make	the	value	of	the	label	""acme"".	Setting	this	label	now	makes	iteasier	to	
cluster	in	additional	workers	and	distribute	load	dynamically	in	the	future.	
C r e a t i n g	a	N o d e	L a b e l	
This	topic	outlines	creating	alabel	for	anode.	Labels	provide	the	ability	to	deploy	aservice	to	specific	nodes	
(determined	by	labels)	and	to	categorize	the	nodes	for	the	work	they	will	be	performing.	Take	the	following	actions	
to	set	anode	label:	
#	get	the	list	of	nodes	available	in	this	cluster	(must	run	from	a	manager	node)	
docker	node	ls	
#	example	of	adding	a	label	to	a	docker	swarm	node	
docker	node	update	--label-	add	customer=acme	<node	id>	
227 

228
P l a c i n g	a	S e r v i c e	o n	a	L a b e l e d	N o d e	
After	you	create	anode	label,	refer	to	the	example	below	for	updating	your	docker-	compose-	override.yml	file	
and	ensuring	the	desired	services	deploy	to	the	matching	labeled	nodes:	
#	example	of	placing	workers	on	a	specific	labeled	node	
steprunner-	acme:	
...
deploy:	
placement:	
constraints:
-	node.labels.customer	==	acme	
resources:	
limits:	
memory:	1.5G	
replicas:	15	
...	
D e d i c a t i n g	Q u e u e s	P e r	I n t e g r a t i on	or	C u s t om e r	
Dedicating	aseparate	queue	for	each	customer	is	beneficial	in	that	work	and	events	created	from	one	system	will	
not	affect	or	slow	down	work	created	from	another	system,	provided	the	multi-	tenant	system	has	enough	resources	
allocated.	In	the	example	below,	we	created	two	new	queues	in	addition	to	the	default	queue,	and	allocated	
workers	to	it.	Both	of	these	worker	services	use	separate	queues	as	described	below,	but	run	on	the	same	labeled	
worker	node.	
Example	Queues	to	Deploy:	
l	acmequeue	.The	queue	we	use	to	sync	events	specific	from	acustomer	called	""acme"".	Only	events	syncs	
and	other	integrations	for	""acme""	will	run	on	this	queue.	
l	acmequeue-	catchup	.The	queue	where	any	old	events	that	should	have	synced	over	already	(but	failed	
due	to	PowerFlow	not	being	available	or	other	reason)	will	run.	Running	these	catchup	integrations	on	a	
separate	queue	ensures	that	real-	time	event	syncing	isn't	delayed	in	favor	of	an	older	event	catching	up.	
A d d	W o r k e r s	f o r	t h e	N e w	Q u e u e s	
First,	define	additional	workers	in	our	stack	that	are	responsible	for	handling	the	new	queues.	All	modifications	are	
made	in	docker-	compose-	override.yml	:	
1.	Copy	an	existing	steprunner	service	definition.	
2.	Change	the	steprunner	service	name	to	something	unique	for	the	stack:	
l	steprunner-	acmequeue	
l	steprunner-	acmequeue-	catchup	
3.	Modify	the	""replicas""	value	to	specify	how	many	workers	should	be	listening	to	this	queue:	
l	steprunner-	acmequeue	will	get	15	workers	as	it's	expecting	avery	heavy	load	
l	steprunner-	acmequeue-	catchup	will	get	3	workers	as	it's	not	often	that	this	will	run	
Creating	aCustom	Configuration	Object 

Creating	aCustom	Configuration	Object	
4.	Add	anew	environment	variable	labeled	""user_	queues"".	This	environment	variable	tells	the	worker	what	
queues	to	listen	to:	
l	steprunner-	acmequeue	will	set	user_	queues=	""acmequeue""	
l	steprunner-	acmequeue-	catchup	will	set	user_	queues=""acmequeue-	catchup""	
5.	To	ensure	that	these	workers	can	be	easily	identified	for	the	queue	to	which	they	are	assigned,	modify	the	
hostname	setting	:	
l	Hostname:	""acmequeue-	{{.Task.ID}}""	
l	Hostname	""acmequeue-	catchup-	{{.Task.ID}}""	
6.	After	the	changes	have	been	made,	run	/opt/iservices/script/compose-	override.sh	to	validate	that	the	
syntax	is	correct.	
7.	When	you	are	ready	to	deploy,	re-	run	the	docker	stack	deploy	with	the	new	compose	file.	
After	these	changes	have	been	made,	your	docker-	compose	entries	for	the	new	steprunners	should	look	similar	to	
the	following:	
steprunner-	acme-	catchup:	
image:	sciencelogic/is-	worker:latest	
depends_	on:	
-	couchbase	
-	rabbitmq	
-	redis	
hostname:	""acme-	catchup-	{{.Task.ID}}""	
deploy:	
placement:	
constraints:	
-	node.labels.customer	==	acme	
resources:	
limits:	
memory:	2G	
replicas:	3	
environment:	
user_	queues:	'acmequeue-	catchup'	
..
..
..	
steprunner-	acme:	
image:	sciencelogic/is-	worker:latest	
depends_	on:	
-	couchbase	
-	rabbitmq	
-	redis	
hostname:	""acmequeue-	{{.Task.ID}}""	
deploy:	
placement:	
constraints:	
-	node.labels.customer	==	acme	
resources:	
limits:	
memory:	2G	
replicas:	15	
environment:	
user_	queues:	'acmequeue'	
..	
229 

230	
..
..	
Once	deployed	via	docker	stack	deploy,	you	should	see	the	new	workers	in	Flower,	as	in	the	following	image:	
You	can	verify	the	queues	being	listened	to	by	looking	at	the	""broker""	section	of	Flower,	or	by	clicking	into	aworker	
and	clicking	the	[Queues	]tab:	
C r e a t e	A p p l i c a t i on	S c h e d u l e s	a n d	A u t om a t i on	S e t t i n g s	t o	U t i l i z e	
S e p a r a t e	Q u e u e s	
After	the	workers	have	been	configured	with	specific	queue	assignments,	schedule	your	PowerFlow	applications	to	
run	on	those	queues,	and	configure	Run	Book	Automations	(RBAs)	to	place	the	applications	on	those	queues.	
Creating	aCustom	Configuration	Object 

Failure	Scenarios	
S c h e d u l i n g	a n	A p p l i c a t i o n	w i t h	a	S p e c i f i c	Q u e u e	a n d	C o n f i g u r a t i o n	
To	run	an	application	on	aspecific	queue	using	aconfiguration	for	aparticular	system,	you	can	use	the	""params""	
override	available	in	the	scheduler.	Below	is	an	example	of	the	scheduled	integration	which	utilizes	the	
acmecqueue-	catchup	queue:	
In	the	example	above,	the	cisco_	correlation_	queue_	manager	is	scheduled	to	run	every	300	seconds,	using	
the	acme	configuration,	and	will	run	on	the	acmequeue	.You	can	have	any	number	of	scheduled	runs	per	
application.	Ifwe	were	to	add	additional	customers,	we	would	add	anew	schedule	entry	with	differing	
configurations,	and	queues	for	each.	
C o n f i g u r i n g	A u t o m a t i o n s	t o	U t i l i z e	a	S p e c i f i c	Q u e u e	a n d	C o n f i g u r a t i o n	
The	last	step	to	ensuring	integrations	for	your	newly	onboarded	SL1	system	is	to	update	the	RunÂ Book	Automations	
in	SL1	to	provide	the	configuration	and	queue	to	use	when	the	RunÂ Book	Automation	triggers	an	event.	
Modify	the	Event-	correlation	policy	with	the	following	changes:	
1.	IS4_	PASSTHROUGH=	{""queue"":""acmequeue""}	
2.	CONFIG_	OVERRIDE=	'acme-	scale-	config'	
F a i l u r e	S c e n a r i o s	
This	topic	cover	how	PowerFlow	handles	situations	where	certain	services	fail.	
W or k e r	C on t a i n e r s	
In	case	of	failure,	when	can	the	worker	containers	be	expected	to	restart?	
231 

232	
l	The	worker	containers	have	astrict	memory	limit	of	2	GB.	These	containers	may	be	restarted	ifthe	service	
requests	more	memory	than	the	2	GB	limit.	
l	The	restart	is	done	by	aSIGKILL	of	the	OOM_	Killer	on	the	Linux	system.	
What	happens	when	a	worker	container	fails?	
l	Worker	containers	are	ephemeral,	and	simply	execute	the	tasks	allotted	to	them.	There	is	no	impact	to	a	
worker	instance	restarting.	
What	processing	is	affected	when	service	is	down?	
l	The	task_	reject_	on_	worker_	lost	environment	variable	dictates	whether	the	task	being	executed	at	the	time	
the	worker	was	restarted	will	be	re-	queued	for	execution	by	another	worker.	(default	false)	
l	For	more	information	about	Celery,	the	task-	processing	framework	used	by	PowerFlow	):	
http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-	reject-	on-	worker-	lost	
What	data	can	be	lost?	
l	Workers	contain	no	persistent	data,	so	there	is	no	data	to	lose,	other	than	the	data	from	the	current	task	that	is	
being	executed	on	that	worker	when	its	shut	down	(if	there	is	one)	
l	Any	PowerFlow	application	that	fails	can	be	replayed	(and	re-	executed	by	the	workers)	on	demand	with	the	
/api/v1/tasks/<task-	id>/replay	endpoint.	
A P I
When	can	the	API	be	expected	to	restart?	
l	The	API	also	has	adefault	memory	limit.	As	with	the	steprunners	(worker	containers),	ifthe	memory	limit	is	
reached,	the	API	is	restarted	by	aSIGKILL	of	the	OOM_	Killer	on	the	Linux	system	to	prevent	amemory	leak.	
What	happens	when	itfails?	
l	On	the	clustered	system,	there	are	always	three	contentapi	services,	so	ifone	of	the	API	containers	fails,	API	
requests	will	still	be	routed	to	the	functioning	containers	through	the	internal	load	balancer.	
What	processing	is	affected	when	service	is	down?	
l	Ifnone	of	the	API	services	are	up	and	running,	any	Run	Book	Automation	calls	to	sync	an	incident	through	
PowerFlow	results	in	an	error.	These	failed	event	syncs	are	then	placed	in	adatabase	table	in	SL1	which	
theÂ 	PowerFlow	queries	on	aschedule	every	few	minutes.	The	next	time	that	scheduled	integration	runs,	the	
integration	recognizes	the	events	that	failed	to	send	to	PowerFlow	,and	the	integration	will	process	them	so	
that	the	events	sync.	
l	Even	ifthe	API	is	down,	the	events	that	were	generated	while	itwas	down	will	be	synced	by	the	scheduled	
application.	PowerFlow	will	reach	out	to	SL1	for	those	items	that	SL1	failed	to	post	to	the	PowerFlow	.	
What	data	can	be	lost?	
l	The	API	contains	no	persistent	data,	so	there	is	no	risk	of	data	loss.	
Failure	Scenarios 

Failure	Scenarios	
C ou c h b a s e
Ifacore	service	node	running	Couchbase	fails,	the	database	should	continue	to	work	normally	and	continue	
processing	events,	as	long	as	asuitable	number	of	clustered	nodes	are	still	up	and	running.	Three	core	service	
nodes	provides	automatic	failover	handling	of	one	node,	five	core	service	nodes	provides	automatic	failover	
handling	of	two	nodes,	and	so	on.	See	the	High	Availability	section	for	more	information.	
Ifthere	are	enough	clustered	core	nodes	still	running,	the	failover	will	occur	with	no	interruptions,	and	the	failing	
node	can	be	added	back	at	any	time	with	no	interruptions.	
NOTE	:For	optimal	performance	and	data	distribution	after	rejoining	acluster,	you	can	click	the	[Re-	balance	]	
button	from	the	Couchbase	user	interface,	ifneeded.	
Ifthere	are	not	enough	clustered	core	nodes	still	running,	then	you	will	manually	have	to	fail	over	the	Couchbase	
Server.	In	this	scenario,	since	automatic	failover	could	not	be	performed	(due	to	too	few	nodes	available),	there	
will	be	disruption	in	event	processing.	For	more	information,	see	the	Manual	Failover	section	.	
In	case	of	failure,	when	can	Couchbase	be	expected	to	restart?	
l	In	ideal	conditions,	the	Couchbase	database	should	not	restart,	although	Couchbase	might	be	restarted	
when	the	node	itis	running	on	is	over-	provisioned.	For	more	information,	see	the	known	issue	.	
What	happens	when	itfails?	
l	Each	Couchbase	node	in	the	cluster	contains	afully	replicated	set	of	data.	Ifany	single	node	fails,	automatic	
failover	will	occur	after	the	designated	time	(120	seconds	by	default).	Automatic	failover,	processing,	and	
queries	to	the	database	will	continue	without	issue.	
l	Ifthe	database	simply	is	restarted	and	not	down	for	along	period	of	time	(120	seconds),	then	the	system	will	
not	automatically	fail	over,	and	the	cluster	will	still	be	maintained.	
l	Iftwo	out	of	three	of	the	database	nodes	fail	for	aperiod	of	time,	processing	may	be	paused	until	auser	takes	
manual	failover	action.	These	manual	actions	are	documented	in	the	Manual	Failover	section	.	
What	processing	is	affected	when	service	is	down?	
l	In	the	event	of	an	automatic	failover	(1/3	node	failure),	no	processing	will	be	affected	and	queries	to	the	
database	will	still	be	functional.	
l	In	the	event	of	alarge	failure	(2/3	node	failure)	automatic	failover	will	not	happen,	and	manual	intervention	
may	be	needed	to	so	you	can	query	the	database	again.	
What	data	can	be	lost?	
l	Every	Couchbase	node	has	full	data	replication	between	each	of	the	three	nodes.	In	the	event	of	afailure	of	
any	of	the	nodes,	no	data	is	lost,	as	areplicated	copy	exists	across	the	cluster.	
233 

234
R a b b i t M Q
RabbitMQ	clustered	among	all	core	service	nodes	provides	full	mirroring	to	each	node.	So	long	as	there	is	at	least	
one	node	available	running	RabbitMQ,	the	queues	should	exist	and	be	reachable.	This	means	that	amultiple	
node	failure	will	have	no	effect	on	the	RabbitMQ	services,	and	itshould	continue	to	operate	normally.	
In	case	of	failure,	when	can	RabbitMQ	be	expected	to	restart?	
l	Similar	to	the	Couchbase	database,	in	asmooth-	running	system,	RabbitMQ	should	never	really	restart.	
l	As	with	other	containers,	RabbitMQ	might	be	restarted	when	the	node	itis	running	on	is	over-	provisioned.	
For	more	information,	see	the	known	issue	.	
What	happens	when	RabbitMQ	fails?	
l	All	RabbitMQ	nodes	in	the	cluster	mirror	the	other	queues	and	completely	replicate	the	data	between	them.	
The	data	is	also	persisted.	
l	In	the	event	of	any	RabbitMQ	node	failure,	the	other	nodes	in	the	cluster	will	take	over	responsibility	for	
processing	its	queues.	
l	Ifall	RabbitMQ	nodes	are	restarted,	their	messages	are	persisted	to	disk,	so	any	tasks	or	messages	sitting	in	
queue	at	the	time	of	the	failure	are	not	lost,	and	are	reloaded	once	the	system	comes	back	up.	
l	In	the	event	of	anetwork	partition	(""split-	brain""	state	)RabbitMQ	will	follow	the	configured	partition	handling	
strategy	(default	autoheal).	
l	For	more	information,	see	https://www.rabbitmq.com/partitions.html#automatic-	handling	.	
What	processing	is	affected	when	service	is	down?	
l	When	this	service	is	down	completely	(no	nodes	running),	the	API	may	fail	to	place	event	sync	tasks	onto	the	
queue.	As	such,	any	Run	Book	Automation	calls	to	sync	an	incident	through	PowerFlow	will	result	in	an	error.	
l	These	failed	event	syncs	are	then	placed	in	adatabase	table	in	SL1	which	PowerFlow	queries	on	aschedule	
every	few	minutes.	The	next	time	that	scheduled	integration	runs,	the	integration	recognizes	the	events	that	
failed	to	send	to	PowerFlow	,and	the	integration	will	process	them	so	that	the	events	sync.	
What	data	can	be	lost?	
l	All	data	is	replicated	between	nodes,	so	there	is	little	risk	of	data	loss.	
l	The	only	time	there	may	be	loss	of	tasks	in	queues	is	ifthere	is	anetwork	partition,	also	called	a""split-	brain""	
state	.	
P ow e r F l ow	U s e r Â I n t e r f a c e	
In	case	of	failure,	when	can	the	user	interface	be	expected	to	restart?	
l	The	PowerFlow	user	interface	(GUI)	should	never	be	seen	as	restarted	unless	auser	forcefully	restarted	the	
interface.	
l	The	PowerFlow	user	interface	might	be	restarted	when	the	node	itis	running	on	is	over-	provisioned.	For	
more	information,	see	the	known	issue	.	
Failure	Scenarios 

Failure	Scenarios	
What	happens	when	itfails?	
l	The	GUI	service	provides	the	proxy	routing	throughout	the	stack,	so	ifthe	GUI	service	is	not	available,	Run	
Book	Automation	POSTS	to	PowerFlow	will	fail.	However,	as	with	an	API	failure,	ifthe	Run	Book	Actions	can	
not	POST	to	PowerFlow	,those	events	will	be	placed	in	adatabase	table	in	SL1	that	PowerFlow	queries	on	a	
schedule	every	few	minutes.	The	next	time	that	scheduled	integration	runs,	the	integration	recognizes	the	
events	that	failed	to	send	to	PowerFlow	,and	the	integration	will	process	them	so	that	the	events	sync.	
l	When	the	GUI	service	is	down	and	SL1	cannot	POST	to	it,	the	syncing	of	the	events	might	be	slightly	
delayed,	as	the	events	will	be	pulled	in	and	created	with	the	next	run	of	the	scheduled	integration.	
What	data	can	be	lost?	
l	The	PowerFlow	user	interface	persists	no	data,	so	there	is	no	risk	of	any	data	loss.	
R e d i s
Ifthe	Redis	service	fails,	itwill	automatically	be	restarted,	and	will	be	available	again	in	afew	minutes.	The	impact	
of	this	happening,	is	that	task	processing	in	PowerFlow	is	delayed	slightly,	as	the	worker	services	pause	themselves	
and	wait	for	the	Redis	service	to	become	available	again.	
Consistent	Redis	failures	
Consistent	failures	and	restarts	in	Redis	typically	indicate	your	system	has	too	little	memory,	or	the	Redis	service	
memory	limit	is	set	too	low,	or	not	low	at	all.	By	default,	PowerFlow	version	1.8.1	and	later	ships	with	adefault	
memory	limit	of	8	GB	to	ensure	that	the	Redis	service	only	ever	uses	8	GB	of	memory,	and	itejects	entries	ifitis	
going	to	go	over	that	limit.	This	limit	is	typically	sufficient,	though	ifyou	have	enough	workers	running	large	enough	
integrations	to	overfill	the	memory,	you	may	need	to	increase	the	limit.	
Before	increasing	Redis	memory	limit,	be	sure	that	there	is	suitable	memory	available	to	the	system.	
K n ow n	I s s u e	f or	G r ou p s	of	C on t a i n e r s	
Ifyou	see	multiple	containers	restarting	at	the	same	time	on	the	same	node,	itindicates	an	over-	provisioning	of	
resources	on	that	node.	This	only	occurs	on	Swarm	manager	nodes,	as	the	nodes	are	not	only	responsible	for	the	
services	they	are	running,	but	also	for	maintaining	the	Swarm	cluster	and	communicating	with	other	manager	
nodes.
Ifresources	become	over-	provisioned	on	one	of	those	manager	nodes	(as	they	were	with	the	core	nodes	when	we	
saw	the	failure),	the	Swarm	manager	will	not	be	able	to	perform	its	duties	and	may	cause	adocker	restart	on	that	
particular	node.	This	failure	is	indicated	by	âcontext	deadline	exceededâ,	and	âheartbeat	failuresâ	in	the	journalctl	
âno-	page	|grep	docker	|grep	err	logs.	
This	is	one	of	the	reasons	why	docker	recommends	running	âmanager-	onlyâ	nodes,	in	which	the	manager	nodes	
are	only	responsible	for	maintaining	the	Swarm,	and	not	responsible	for	running	other	services.	Ifany	nodes	that	
are	running	PowerFlow	services	are	also	aSwarm	manager,	make	sure	that	the	nodes	are	not	over-	provisioned,	
otherwise	the	containers	on	that	node	may	restart.	For	this	reason,	ScienceLogic	recommends	monitoring	and	
placing	thresholds	at	80%	utilization.	
235 

236
To	combat	the	risk	of	over-	provisioning	affecting	the	docker	Swarm	manager,	apply	resource	constraints	on	the	
services	for	the	nodes	that	are	also	Swarm	managers,	so	that	docker	operations	always	have	some	extra	memory	
or	CPU	on	the	host	to	do	what	they	need	to	do.	Alternatively,	you	can	only	use	drained	nodes,	which	are	not	
running	any	services,	as	Swarm	managers,	and	not	apply	any	extra	constraints.	
For	more	information	about	Swarm	management,	see	https://docs.docker.com/engine/Swarm/admin_	guide/	.	
E xa m p l e s	a n d	R e f e r e n c e	
E x a m p l e	of	a Â 	P ow e r F l ow	Â C on f i g u r a t i on	O b j e c t	
[	
{	
""encrypted"":	false,	
""name"":	""em7_	host"",	
""value"":	""<ip	address>	""	
},
{	
""encrypted"":	false,	
""name"":	""sl1_	host"",	
""value"":	""${config.em7_	host}""	
},
{	
""encrypted"":	false,	
""name"":	""sl1_	id"",	
""value"":	""${config.em7_	id}""	
},
{	
""encrypted"":	false,	
""name"":	""sl1_	db_	port"",	
""value"":	7706	
},
{	
""encrypted"":	false,	
""name"":	""snow_	host"",	
""value"":	""<arecord>	.service-	now.com""	
},
{	
""encrypted"":	true,	
""name"":	""em7_	password"",	
""value"":	""<password>""	
},
{	
""encrypted"":	false,	
""name"":	""sl1_	user"",	
""value"":	""${config.em7_	user}""	
},
{	
""encrypted"":	false,	
""name"":	""sl1_	password"",	
""value"":	""${config.em7_	password}""	
},
{	
""encrypted"":	false,	
""name"":	""sl1_	db_	user"",	
""value"":	""${config.em7_	db_	user}""	
},	
Examples	and	Reference 

Examples	and	Reference	
{	
""encrypted"":	false,	
""name"":	""sl1_	db_	password"",	
""value"":	""${config.em7_	db_	password}""	
},
{	
""encrypted"":	false,	
""name"":	""em7_	user"",	
""value"":	""<username>	""	
},
{	
""encrypted"":	false,	
""name"":	""em7_	db_	user"",	
""value"":	""root""	
},
{	
""encrypted"":	false,	
""name"":	""em7_	db_	password"",	
""value"":	""<password>	""	
},
{	
""encrypted"":	false,	
""name"":	""snow_	user"",	
""value"":	""<username>""	
},
{	
""encrypted"":	true,	
""name"":	""snow_	password"",	
""value"":	""<password>	""	
},
{	
""encrypted"":	false,	
""name"":	""Domain_	Credentials"",	
""value"":	{	
""c9818d2c4a36231201624433851894bb"":	{	
""password"":	""3m7Admin!"",	
""user"":	""is4DomainUser2""	
}	
}	
},
{	
""name"":	""region"",	
""value"":	""ACMEScaleStack""	
},
{	
""encrypted"":	false,	
""name"":	""em7_	id"",	
""value"":	""${config.region}""	
},
{	
""encrypted"":	false,	
""name"":	""generate_	report"",	
""value"":	""true""	
}	
]	
E x a m p l e	of	a	S c h e d u l e	C on f i g u r a t i on	
[	
{	
237 

238	
""application_	id"":	""device_	sync_	sciencelogic_	to_	servicenow"",	
""entry_	id"":	""dsync	every	13	hrs	acme"",	
""last_	run"":	null,	
""params"":	{	
""configuration"":	""acme-	scale-	config"",	
""mappings"":	{	
""cmbd_	ci_	ip_	router"":	[	
""Cisco	Systems	|	12410	GSR"",	
""Cisco	Systems	|	AIR-	AP1141N"",	
""Cisco	Systems	|	AP	1200-	IOS"",	
""Cisco	Systems	|	Catalyst	5505""	
],
""cmdb_	ci_	esx_	resource_	pool"":	[	
""VMware	|	Resource	Pool""	
],
""cmdb_	ci_	esx_	server"":	[	
""VMware	|	ESXi	5.1	w/HR"",	
""VMware	|	Host	Server"",	
""VMware	|	ESX	(i)	4.0"",	
""VMware	|	ESX	(i)	w/HR"",	
""VMware	|	ESX	(i)	4.0	w/HR"",	
""VMware	|	ESX	(i)"",	
""VMware	|	ESX	(i)	4.1	w/HR"",	
""VMware	|	ESXi	5.1	w/HR"",	
""VMware	|	ESXi	5.0	w/HR"",	
""VMware	|	ESX	(i)	4.1"",	
""VMware	|	ESXi	5.1"",	
""VMware	|	ESXi	5.0""	
],
""cmdb_	ci_	linux_	server"":	[	
""ScienceLogic,	Inc.	|	EM7	Message	Collector"",	
""ScienceLogic,	Inc.	|	EM7	Customer	Portal"",	
""ScienceLogic,	Inc.	|	EM7	All-	In-	One"",	
""ScienceLogic,	Inc.	|	EM7	Integration	Server"",	
""ScienceLogic,	Inc.	|	EM7	Admin	Portal"",	
""ScienceLogic,	Inc.	|	EM7	Database"",	
""ScienceLogic,	Inc.	|	OEM"",	
""ScienceLogic,	Inc.	|	EM7	Data	Collector"",	
""NET-	SNMP	|	Linux"",	
""RHEL	|	Redhat	5.5"",	
""Virtual	Device	|	Content	Verification""	
],	
""cmdb_	ci_	vcenter"":	[	
""VMware	|	vCenter"",	
""Virtual	Device	|	Windows	Services""	
],	
""cmdb_	ci_	vcenter_	cluster"":	[	
""VMware	|	Cluster""	
],	
""cmdb_	ci_	vcenter_	datacenter"":	[	
""VMware	|	Datacenter""	
],	
""cmdb_	ci_	vcenter_	datastore"":	[	
""VMware	|	Datastore"",	
""VMware	|	Datastore	Cluser""	
],	
""cmdb_	ci_	vcenter_	dv_	port_	group"":	[	
""VMware	|	Distributed	Virtual	Portgroup""	
],	
""cmdb_	ci_	vcenter_	dvs"":	[	
""VMware	|	Distributed	Virtual	Switch""	
Examples	and	Reference 

Examples	and	Reference
],	
""cmdb_	ci_	vcenter_	folder"":	[	
""VMware	|	Folder""	
],	
""cmdb_	ci_	vcenter_	network"":	[	
""VMware	|	Network""	
],	
""cmdb_	ci_	vmware_	instance"":	[	
""VMware	|	Virtual	Machine""	
]	
},
""queue"":	""acmequeue"",	
""region"":	""ACMEScaleStack""	
},
""schedule"":	{	
""schedule_	info"":	{	
""run_	every"":	47200	
},
""schedule_	type"":	""frequency""	
},
""total_	runs"":	0	
},
{	
""application_	id"":	""device_	sync_	sciencelogic_	to_	servicenow"",	
""entry_	id"":	""dsync	every	12	hrs	on	.223"",	
""last_	run"":	null,	
""params"":	{	
""configuration"":	""em7-	host-	223"",	
""mappings"":	{	
""cmdb_	ci_	esx_	resource_	pool"":	[	
""VMware	|	Resource	Pool""	
],
""cmdb_	ci_	esx_	server"":	[	
""VMware	|	ESXi	5.1	w/HR"",	
""VMware	|	Host	Server"",	
""VMware	|	ESX	(i)	4.0"",	
""VMware	|	ESX	(i)	w/HR"",	
""VMware	|	ESX	(i)	4.0	w/HR"",	
""VMware	|	ESX	(i)"",	
""VMware	|	ESX	(i)	4.1	w/HR"",	
""VMware	|	ESXi	5.1	w/HR"",	
""VMware	|	ESXi	5.0	w/HR"",	
""VMware	|	ESX	(i)	4.1"",	
""VMware	|	ESXi	5.1"",	
""VMware	|	ESXi	5.0""	
],
""cmdb_	ci_	linux_	server"":	[	
""ScienceLogic,	Inc.	|	EM7	Message	Collector"",	
""ScienceLogic,	Inc.	|	EM7	Customer	Portal"",	
""ScienceLogic,	Inc.	|	EM7	All-	In-	One"",	
""ScienceLogic,	Inc.	|	EM7	Integration	Server"",	
""ScienceLogic,	Inc.	|	EM7	Admin	Portal"",	
""ScienceLogic,	Inc.	|	EM7	Database"",	
""ScienceLogic,	Inc.	|	OEM"",	
""ScienceLogic,	Inc.	|	EM7	Data	Collector"",	
""NET-	SNMP	|	Linux"",	
""RHEL	|	Redhat	5.5"",	
""Virtual	Device	|	Content	Verification""	
],
""cmdb_	ci_	vcenter"":	[	
""VMware	|	vCenter"",	
239 

240	
""Virtual	Device	|	Windows	Services""	
],
""cmdb_	ci_	vcenter_	cluster"":	[	
""VMware	|	Cluster""	
],
""cmdb_	ci_	vcenter_	datacenter"":	[	
""VMware	|	Datacenter""	
],
""cmdb_	ci_	vcenter_	datastore"":	[	
""VMware	|	Datastore"",	
""VMware	|	Datastore	Cluser""	
],
""cmdb_	ci_	vcenter_	dv_	port_	group"":	[	
""VMware	|	Distributed	Virtual	Portgroup""	
],
""cmdb_	ci_	vcenter_	dvs"":	[	
""VMware	|	Distributed	Virtual	Switch""	
],
""cmdb_	ci_	vcenter_	folder"":	[	
""VMware	|	Folder""	
],
""cmdb_	ci_	vcenter_	network"":	[	
""VMware	|	Network""	
],
""cmdb_	ci_	vmware_	instance"":	[	
""VMware	|	Virtual	Machine""	
]	
}	
},
""schedule"":	{	
""schedule_	info"":	{	
""run_	every"":	43200	
},
""schedule_	type"":	""frequency""	
},
""total_	runs"":	0	
},
{	
""application_	id"":	""cisco_	correlation_	queue_	manager"",	
""entry_	id"":	""acme	catchup	events"",	
""last_	run"":	{	
""href"":	""/api/v1/tasks/isapp-	a20d5e08-	a802-	4437-	92ef-	32d643c6b777"",	
""start_	time"":	1544474203	
},
""params"":	{	
""configuration"":	""acme-	scale-	config"",	
""queue"":	""acmequeue-	catchup""	
},
""schedule"":	{	
""schedule_	info"":	{	
""run_	every"":	300	
},
""schedule_	type"":	""frequency""	
},
""total_	runs"":	33	
},
{	
""application_	id"":	""cisco_	incident_	state_	sync"",	
""entry_	id"":	""incident	sync	every	5	mins	on	.223"",	
""last_	run"":	{	
""href"":	""/api/v1/tasks/isapp-	52b19097-	e0bf-	450b-	948c-	487aff33fc3b"",	
Examples	and	Reference 

Test	Cases	
""start_	time"":	1544474203	
},
""params"":	{	
""configuration"":	""em7-	host-	223""	
},
""schedule"":	{	
""schedule_	info"":	{	
""run_	every"":	300	
},
""schedule_	type"":	""frequency""	
},
""total_	runs"":	2815	
},
{	
""application_	id"":	""cisco_	incident_	state_	sync"",	
""entry_	id"":	""incident	sync	every	5	mins	acme"",	
""last_	run"":	{	
""href"":	""/api/v1/tasks/isapp-	dde1dba5-	2343-	4026-	8801-	35a02e4e57a1"",	
""start_	time"":	1544474202	
},	
""params"":	{	
""configuration"":	""acme-	scale-	config"",	
""queue"":	""acmequeue""	
},
""schedule"":	{	
""schedule_	info"":	{	
""run_	every"":	300	
},
""schedule_	type"":	""frequency""	
},
""total_	runs"":	1587	
},
{	
""application_	id"":	""cisco_	correlation_	queue_	manager"",	
""entry_	id"":	""qmanager	.223"",	
""last_	run"":	{	
""href"":	""/api/v1/tasks/isapp-	cb7cc2e5-	eab1-	474a-	907a-	055f26dbc36d"",	
""start_	time"":	1544474203	
},
""params"":	{	
""configuration"":	""em7-	host-	223""	
},
""schedule"":	{	
""schedule_	info"":	{	
""run_	every"":	300	
},
""schedule_	type"":	""frequency""	
},
""total_	runs"":	1589	
}	
]	
T e s t	C a s e s	
L oa d	T h r ou g h p u t	T e s t	C a s e s	
Event	throughput	testing	with	PowerFlow	only:	
241 

242
The	following	test	cases	can	be	attempted	with	any	number	of	dedicated	customer	queues.	The	expectation	is	that	
each	customer	queue	will	be	filled	with	10,000	events,	and	then	you	can	time	how	long	ittakes	to	process	
through	all	10,000	events	in	each	queue.	
1.	Disable	any	steprunners.	
2.	Trigger	10,000	events	through	SL1	,and	let	them	build	up	in	the	PowerFlow	queue.	
3.	After	all	10,000	events	are	waiting	in	queue,	enable	the	steprunners	to	begin	processing.	
4.	Time	the	throughput	of	all	event	processing	to	get	an	estimate	of	how	many	events	per	second	and	per	
minute	that	PowerFlow	will	handle.	
5.	The	results	from	the	ScienceLogic	test	system	are	listed	in	the	sizing	section	of	worker	nodes	.	
Event	throughput	testing	with	SL1	triggering	PowerFlow	:	
This	test	is	executed	in	the	same	manner	as	the	event	throughput	test	described	above,	but	in	this	scenario	you	
never	disable	the	steprunners,	and	you	let	the	events	process	through	PowerFlow	as	they	are	alerted	to	by	SL1	.	
1.	Steprunners	are	running.	
2.	Trigger	10,000	events	through	SL1	,and	let	the	steprunners	handle	the	events	as	they	come	in.	
3.	Time	the	throughput	of	all	event	processing	to	get	an	estimate	of	how	many	events	per	second	and	per	
minute	that	PowerFlow	will	handle.	
The	difference	between	the	timing	of	this	test	and	the	previous	test	can	show	how	much	of	adelay	the	SL1	is	taking	
to	alert	PowerFlow	about	an	event,	and	subsequently	sync	it.	
F a i l u r e	T e s t	C a s e s	
1.	Validate	that	bringing	one	of	the	core	nodes	down	does	not	impact	the	overall	functionality	of	the	PowerFlow	
system.	Also,	validate	that	bringing	the	core	node	back	up	rejoins	the	cluster	and	the	system	continues	to	be	
operational.	
2.	Validate	that	bringing	down	adedicated	worker	node	only	affects	that	specific	workers	processing.	Also	
validate	that	adding	anew	""standby""	node	is	able	to	pick	up	the	worker	where	the	previous	failed	worker	left	
off.	
3.	Validate	that	when	the	Redis	service	fails	on	any	node,	itis	redistributed	and	is	functional	on	another	node.	
4.	Validate	that	when	aPowerFlow	application	fails,	you	can	view	the	failure	in	the	PowerFlow	Timeline.	
5.	Validate	that	you	can	query	for	and	filter	only	for	failing	tasks	for	aspecific	customer.	
Separated	queue	test	scenarios	
1.	Validate	that	scheduling	two	runs	of	the	same	application	with	differing	configurations	and	queues	works	as	
expected:	
l	Each	scheduled	run	should	be	placed	on	the	designated	queue	and	configuration	for	that	schedule.	
l	The	runs,	their	queues,	and	configurations	should	be	viewable	from	the	PowerFlow	Timeline,	or	can	
be	queried	from	the	log's	endpoint.	
Test	Cases 

Backup	Considerations	
2.	Validate	that	each	SL1	triggering	event	is	correctly	sending	the	appropriate	queue	and	configuration	that	the	
event	sync	should	be	run	on:	
l	This	data	should	be	viewable	from	the	PowerFlow	Timeline.	
l	The	queue	and	configuration	should	be	correctly	recognized	by	PowerFlow	and	executed	by	the	
corresponding	worker.	
3.	Validate	the	behavior	of	anode	left	""on	standby""	waiting	for	alabel	to	start	picking	up	work.	As	soon	as	alabel	
is	assigned	and	workers	are	scaled,	that	node	should	begin	processing	the	designated	work.	
B a c k u p	C o n s i d e r a ti o n s	
This	section	covers	the	items	you	should	back	up	in	your	PowerFlow	system,	and	how	to	restore	backups.	For	more	
information,	see	Backing	up	Data	.	
W h a t	t o	B a c k	U p	
When	taking	backups	of	the	PowerFlow	environment,	collect	the	following	information	from	the	host	level	of	your	
primary	manager	node	(this	is	the	node	from	which	you	control	the	stack):	
Files	in	/opt/iservices/scripts:	
l	/opt/iservices/scripts/docker-	compose.yml	
l	/opt/iservices/scripts/docker-	compose-	override.yml	
All	files	in	/etc/iservices/	
l	/etc/iservices/is_	pass	
l	/etc/iservices/encryption_	key	
In	addition	to	the	above	files,	make	sure	you	are	storing	Couchbase	dumps	somewhere	by	using	the	cbbackup	
command,	or	the	""PowerFlow	Backup""	application.	
F a l l	B a c k	a n d	R e s t or e	t o	a	D i s a s t e r	R e c ov e r y	( P a s s i v e )	S y s t e m	
You	should	do	adata-	only	restore	if:	
l	The	system	you're	restoring	to	is	adifferent	configuration	or	cluster	setup	than	the	system	you	made	the	
backup	on	
l	The	backup	system	has	all	the	indexes	added	and	already	up	to	date	
You	should	do	afull	restore	if:	
l	The	deployment	where	the	backup	was	made	is	identical	to	the	deployment	where	itis	being	restored	(same	
amount	of	nodes,	etc)	
l	There	are	no	indexes	defined	on	the	system	you're	backing	up	
Once	failed	over,	be	sure	to	disable	the	""PowerFlow	Backup""	application	from	being	scheduled.	
243 

244
R e s i l i e n c y	C o n s i d e r a ti o n s	
T h e	R a b b i t M Q	S p l i t -	b r a i n	H a n d l i n g	S t r a t e g y	( S L 1	D e f a u l t	S e t	t o	
A u t oh e a l )
Ifmultiple	RabbitMQ	cluster	nodes	are	lost	at	once,	the	cluster	might	enter	a""Network	Partition""	or	""Split-	brain""	
state.	In	this	state,	the	queues	will	become	paused	ifthere	is	no	auto-	handling	policy	applied.	The	cluster	will	
remain	paused	until	auser	takes	manual	action.	To	ensure	that	the	cluster	knows	how	to	handle	this	scenario	as	
the	user	would	want,	and	not	pause	waiting	for	manual	intervention,	itis	essential	to	set	apartition	handling	policy.	
For	more	information	on	RabbitMQ	Network	partition	(split-	brain)	state,	how	itcan	occur,	and	what	happens,	see:	
http://www.rabbitmq.com/partitions.html	.	
By	default,	ScienceLogic	sets	the	partition	policy	to	autoheal	in	favor	of	continued	service	ifany	nodes	go	down.	
However,	depending	on	the	environment,	you	might	wish	to	change	this	setting.	
For	more	information	about	the	automatic	split-	brain	handling	strategies	that	RabbitMQ	provides,	see:	
http://www.rabbitmq.com/partitions.html#automatic-	handling	.	
autoheal	is	the	default	setting	set	by	SL1	,and	as	such,	queues	should	always	be	available,	though	ifmultiple	
nodes	fail,	some	messages	may	be	lost.	
NOTE:	Ifyou	are	using	pause_	minority	mode	and	a""split-	brain""	scenario	occurs	for	RabbitMQ	in	asingle	
cluster,	when	the	split-	brain	situation	is	resolved,	new	messages	that	are	queued	will	be	mirrored	
(replicated	between	all	nodes	once	again).	
S c i e n c e L og i c	P ol i c y	R e c om m e n d a t i on	
ScienceLogic's	recommendations	for	applying	changes	to	the	default	policy	include	the	following:	
l	Ifyou	care	more	about	continuity	of	service	in	adata	center	outage,	with	queues	always	available,	even	if	
the	system	doesn't	retain	some	messages	from	afailed	data	center,	use	autoheal	.This	is	the	SL1	default	
setting.	
l	Ifyou	care	more	about	retaining	message	data	in	adata	center	outage,	with	queues	that	might	not	be	
available	until	the	nodes	are	back,	but	will	recover	themselves	once	nodes	are	back	online	to	ensure	that	no	
messages	are	lost,	use	pause_	minority	.	
l	Ifyou	prefer	not	to	have	RabbitMQ	handle	this	scenario	automatically	,and	you	want	to	manually	
recover	your	queues	and	data,	where	queues	will	be	paused	and	unusable	until	manual	intervention	is	made	
to	determine	where	to	fallback,	use	ignore	.	
C h a n g i n g	t h e	R a b b i t M Q	D e f a u l t	S p l i t -	b r a i n	H a n d l i n g	P ol i c y	
The	best	way	to	change	the	SL1	default	split-	brain	strategy	is	to	make	acopy	of	the	RabbitMQ	config	file	from	a	
running	rabbit	system,	add	your	change,	and	then	mount	that	config	back	into	the	appropriate	place	to	apply	your	
overrides.	
Resiliency	Considerations 

Resiliency	Considerations	
1.	Copy	the	config	file	from	acurrently	running	container:	
docker	cp	<container-	id>	:/etc/rabbitmq/rabbitmq.conf	/destination/on/host	
2.	Modify	the	config	file:	
change	cluster_	partition_	handling	value	
3.	Update	your	docker-	compose.yml	file	to	mount	that	file	over	the	config	for	all	rabbitmq	nodes:	
mount	""[/path/to/config]/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf""	
U s i n g	D r a i n e d	M a n a g e r s	t o	M a i n t a i n	S w a r m Â H e a l t h	
To	maintain	Swarm	health,	ScienceLogic	recommends	that	you	deploy	some	swarm	managers	that	do	not	take	
any	of	the	workload	of	the	application.	The	only	purpose	for	these	managers	is	to	maintain	the	health	of	the	swarm.	
Separating	these	workloads	ensures	that	aspike	in	application	activity	will	not	affect	the	swarm	clustering	
management	services.	
ScienceLogic	recommends	that	these	systems	have	2	CPU	and	4	GB	of	memory.	
To	deploy	adrained	manager	node:	
1.	Add	your	new	manager	node	into	the	swarm.	
2.	Drain	itwith	the	following	command:	
docker	node	update	--availability	drain	<node>	
Draining	the	node	ensures	that	no	containers	will	be	deployed	to	it.	
For	more	information,	see	https://docs.docker.com/engine/swarm/admin_	guide/	.	
U p d a t i n g	t h e	P ow e r F l ow	C l u s t e r	w i t h	L i t t l e	t o	N o	D ow n t i m e	
There	are	two	potential	update	workflows	for	updating	the	PowerFlow	cluster.	The	first	workflow	involves	using	a	
Docker	registry	that	is	connectable	to	swarm	nodes	on	the	network.	The	second	workflow	requires	manually	
copying	the	PowerFlow	RPM	or	containers	to	each	individual	node.	
U p d a t i n g	O f f l i n e	( N o	C o n n e c t i o n	t o	a	D o c k e r	R e g i s t r y )	
1.	Copy	the	PowerFlow	RPM	over	to	all	swarm	nodes.	
2.	Only	install	the	RPM	on	all	nodes.	Do	not	stack	deploy.	This	RPM	installation	automatically	extracts	the	latest	
PowerFlow	containers,	making	them	available	to	each	node	in	the	cluster.	
3.	From	the	primary	manager	node,	make	sure	your	docker-	compose	file	has	been	updated,	and	is	now	
using	the	appropriate	version	tag:	either	latest	for	the	latest	version	on	the	system,	or	1.x.x	.	
4.	Ifall	swarm	nodes	have	the	RPM	installed,	the	container	images	should	be	runnable	and	the	stack	should	
update	itself.	Ifthe	RPM	was	missed	installing	on	any	of	the	nodes,	itmay	not	have	the	required	images,	and	
as	aresult,	services	might	not	deploy	to	that	node.	
245 

246
U p d a t i n g	O n l i n e	( A l l	N o d e s	H a v e	a	C o n n e c t i o n	t o	a	D o c k e r	R e g i s t r y )	
1.	Install	the	PowerFlow	RPM	only	onto	the	master	node.	
2.	Make	sure	the	RPM	doesn't	contain	any	host-	level	changes,	such	as	Docker	daemon	configuration	updates.	If	
there	are	host	level	updates,	you	might	want	to	make	that	update	on	other	nodes	in	the	cluster	
3.	Populate	your	Docker	registry	with	the	latest	PowerFlow	images.	
4.	From	the	primary	manager	node,	make	sure	your	docker-	compose	file	has	been	updated,	and	is	now	
using	the	appropriate	version	tag:	either	latest	for	the	latest	version	on	the	system,	or	1.x.x	.	
5.	Docker	stack	deploy	the	services.	Because	all	nodes	have	access	to	the	same	Docker	registry,	which	has	the	
designated	images,	all	nodes	will	download	the	images	automatically	and	update	with	the	latest	versions	as	
defined	by	the	docker-	compose	file.	
A d d i ti o n a l	S i z i n g	C o n s i d e r a ti o n s	
This	section	covers	the	sizing	considerations	for	the	Couchbase,Â RabbitMQ,	Redis,	contentapi,	and	GUI	services.	
S i z i n g	f o r	C o u c h b a s e	S e r v i c e s	
The	initial	sizing	provided	for	Couchbase	nodes	in	the	multi-	tenant	cluster	for	6	CPUs	and	56	GB	memory	should	
be	more	than	enough	to	handle	multiple	customer	event	syncing	workloads.	
ScienceLogic	recommends	monitoring	the	CPU	percentage	and	Memory	Utilization	percentage	of	the	Couchbase	
nodes	to	understand	when	agood	time	to	increase	resources	is,	such	as	when	Memory	and	CPU	are	consistently	
above	80%.	
S i z i n g	f o r	R a b b i t M Q	S e r v i c e s	
The	only	special	considerations	for	RabbitMQ	sizing	is	how	many	events	you	will	plan	for	in	the	queue	at	once.	
Every	10,000	events	populated	in	the	PowerFlow	queue	will	consume	approximately	1.5	GB	of	memory.	
NOTE:	This	memory	usage	is	drained	as	soon	as	the	events	leave	the	queue.	
S i z i n g	f o r	R e d i s	S e r v i c e s	
The	initial	sizing	deployment	for	redis	should	be	sufficient	for	multiple	customer	event	syncing.	
The	only	time	memory	might	need	to	be	increased	to	redis	is	ifyou	are	attempting	to	view	logs	from	aprevious	run,	
and	the	logs	are	not	available.	A	lack	of	run	logs	from	arecently	run	integration	indicates	that	the	redis	cache	does	
not	have	enough	room	to	store	all	the	step	and	log	data	from	recently	executed	runs.	
S i z i n g	f o r	c o n t e n t a p i	S e r v i c e s	
The	contentapi	services	sizing	should	remain	limited	at	2	GB	memory,	as	is	set	by	default.
Additional	Sizing	Considerations 

Node	Placement	Considerations	
Ifyou	notice	timeouts,	or	500s	when	there	is	alarge	load	going	through	the	PowerFlow	system,	you	may	want	to	
increase	the	number	of	contentapi	replicas.	
For	more	information,	see	placement	considerations	,and	ensure	the	API	replicas	are	deployed	in	the	same	
location	as	the	redis	instance.	
S i z i n g	f o r	t h e	G U I	S e r v i c e	
The	GUI	service	should	not	need	to	be	scaled	up	at	all,	as	itmerely	acts	as	an	ingress	proxy	to	the	rest	of	the	
PowerFlow	services.	
S i z i n g	f o r	W o r k e r s : Â S c h e d u l e r ,	S t e p r u n n e r ,	F l o w e r	
Refer	to	the	worker	sizing	charts	provided	by	ScienceLogic	for	the	recommended	steprunner	sizes.	
Flower	and	Scheduler	do	not	need	to	be	scaled	up	at	all.	
N o d e	P l a c e m e n t	C o n s i d e r a ti o n s	
P r e v e n t i n g	a	K n ow n	I s s u e :	P l a c e	c on t e n t a p i	a n d	R e d i s	s e r v i c e s	i n	t h e	
S a m e	P h y s i c a l	L oc a t i on	
An	issue	exists	where	ifthere	latency	exists	between	the	contentapi	and	redis,	the	Applications	page	may	not	load.	
This	issue	is	caused	by	the	API	making	too	many	calls	before	returning.	The	added	latency	for	each	individual	call	
can	cause	the	overall	endpoint	to	take	longer	to	load	than	the	designated	timeout	window	of	thirty	seconds.	
The	only	impact	of	this	issue	is	the	Applications	page	won't	load.	There	is	no	operational	impact	on	the	integrations	
as	awhole,	even	ifworkers	are	in	separate	geos	than	redis.	
There	is	also	no	risk	to	High	AvailabilityÂ 	(HA)	by	placing	the	API	andRredis	services	on	the	same	geo.	Iffor	whatever	
reason	that	geo	drops	out,	the	containers	will	be	restarted	automatically	in	the	other	location.	
247 

248
C o m m o n	P r o b l e m s ,	S y m p to m s ,	a n d	S o l u ti o n s	
Tool	Issue	Symptoms	Cause	Solution	
Docker
Visualizer	
Docker
Visualizer
shows	some	
services	as	
""undefined"".	
When	viewing	the	Docker	Visualizer	
user	interface,	some	services	are	
displayed	as	""undefined"",	and	states	
aren't	accurate.	
Impact	:	
Cannot	use	Visualizer	to	get	the	current	
state	of	the	stack.	
Failing	docker	stack	
deployment:
https://github.com/
dockersamples/docker-
swarm-
visualizer/issues/110	
Ensure	your	
stack	is	healthy,	
and	services	
are	deployed	
correctly.	Ifno	
services	are	
failing	and	
things	are	still	
showing	as	
undefined,
elect	a	new	
swarm	leader	.	
To	prevent:	
Ensure	your	
configuration	is	
valid	before	
deploying.	
RabbitMQ	RabbitMQ
queues
encountered
anode	failure	
and	are	in	a	
""Network
partition
state""	(split-	
brain
scenario).	
The	workers	are	able	to	connect	to	the	
queue,	and	there	are	messages	on	the	
queue,	but	the	messages	are	not	being	
distributed	to	the	workers.	
Log	in	to	the	RabbitMQ	admin	user	
interface,	which	displays	amessage	
similar	to	""RabbitMQ	experienced	a	
network	partition	and	the	cluster	is	
paused"".
Impact	:	
The	RabbitMQ	cluster	is	paused	and	
waiting	for	user	intervention	to	clean	
the	split-	brain	state.	
Multi-	node	failure	
occurred,	and	rabbit	
wasn't	able	to	
determine	who	the	new	
master	should	be.	This	
also	will	only	occur	if	
there	is	NO	partition	
handling	policy	in	place	
(see	the	resiliency	
section	for	more	
information)
Note	:ScienceLogic	
sets	the	autoheal	policy	
by	default	
Handle	the	
split-	brain	
partition	state	
and
resynchronize
your
RabbitMQ
queues	.	
Note	:This	is	
enabled	by	
default.
To	prevent:	
Set	apartition	
handling	policy.	
See	the	
Resiliency
section	for	more	
information.	
Common	Problems,	Symptoms,	and	Solutions 

Common	Problems,	Symptoms,	and	Solutions	
Tool	Issue	Symptoms	Cause	Solution	
RabbitMQ,
continued	
Execing	into	the	RabbitMQ	container	
and	running	rabbitmqcli	cluster-	
status	shows	nodes	in	apartition	state	
like	the	following:	
[{nodes,
[{disc,
['rabbit@rabbit_
node1.isnet','rabbit@rabbit_
node2.isnet',
'rabbit@rabbit_
node3.isnet','rabbit@rabbit_
node4.isnet',
'rabbit@rabbit_
node5.isnet','rabbit@rabbit_
node6.isnet']}]},
{running_	nodes,	
['rabbit@rabbit_
node4.isnet']},
{cluster_
name,<<""rabbit@rabbit_
node1"">>},
{partitions,
[{'rabbit@rabbit_
node4.isnet',
['rabbit@rabbit_
node1.isnet','rabbit@rabbit_
node2.isnet',
'rabbit@rabbit_
node3.isnet','rabbit@rabbit_
node5.isnet',
'rabbit@rabbit_
node6.isnet']}]},
{alarms,	[{'rabbit@rabbit_	
node4.isnet',	[]}]}]	
PowerFlow
steprunners
and
RabbitMQ	
Workers
constantly
restarting,	no	
real	error	
message.	
Workers	of	aparticular	queue	are	not	
stable	and	constantly	restart.	
Impact	:	
One	queue's	workers	will	not	be	
processing.	
Multi-	node	failure	in	
RabbitMQ,	when	it	
loses	majority	and	can	
not	failover.	
Queues	go	out	of	sync	
because	of	broken	
swarm.	
Recreate
queues	for	the	
particular
worker.
Resynchronize
queues	.	
To	prevent	:	
Deploy	enough	
nodes	to	ensure	
quorum	for	
failover.	
249 

250
Tool	Issue	Symptoms	Cause	Solution	
Couchbase	Couchbase
node	is	
unable	to	
restart	due	to	
indexer	error.	
This	issue	can	be	monitored	in	the	
Couchbase	logs:	
Service	'indexer'	exited	
with	status	134.	Restarting.	
Messages:
sync.runtime_	Semacquire	
(0xc4236dd33c)	
Impact	:	
One	couchbase	node	becomes	
corrupt.	
Memory	is	removed	
from	the	database	while	
itis	in	operation	
(memory	must	be	
dedicated	to	the	VM	
running	Couchbase).	
The	Couchbase	node	
encounters	afailure,	
which	causes	the	
corruption.	
Ensure	that	the	
memory
allocated	to	
your	database	
nodes	is	
dedicated	and	
not	shared	
among	other	
VMs.
To	prevent:	
Ensure	that	the	
memory
allocated	to	
your	database	
nodes	is	
dedicated	and	
not	shared	
among	other	
VMs.	
Couchbase	Couchbase	is	
unable	to	
rebalance.	
Couchbase	nodes	will	not	rebalance,	
usually	with	an	error	saying	""exited	by	
janitor"".
Impact	:	
Couchbase	nodes	cannot	rebalance	
and	provide	even	replication.	
Network	issues:	missing	
firewall	rules	or	blocked	
ports.
The	Docker	swarm	
network	is	stale	because	
of	astack	failure.	
Validate	that	all	
firewall	rules	
are	in	place,	
and	that	no	
external
firewalls	are	
blocking	ports.	
Reset	the	
Docker	swarm	
network	status	
by	electing	a	
new	swarm	
leader.
To	prevent:	
Validate	the	
firewall	rules	
before
deployment.
Use	drained	
managers	to	
maintain
swarm	
Common	Problems,	Symptoms,	and	Solutions 

Common	Problems,	Symptoms,	and	Solutions	
Tool	Issue	Symptoms	Cause	Solution	
PowerFlow
steprunners
to
Couchbase	
Steprunners
unable	to	
communicate
to
Couchbase	
Steprunners	unable	to	communicate	to	
Couchbase	database,	with	errors	like	
""client	side	timeout"",	or	""connection	
reset	by	peer"".	
Impact	:	
Steprunners	cannot	access	the	
database.	
Missing	Environment	
variables	in	compose:	
Check	the	db_	host	
setting	for	the	
steprunner	and	make	
sure	they	specify	all	
Couchbase	hosts	
available	.	
Validate	couchbase	
settings,	ensure	that	the	
proper	aliases,	
hostname,	and	
environment	variables	
are	set.	
Stale	docker	network.	
Validate	the	
deployment
configuration
and	network	
settings	of	your	
docker-
compose	.	
Redeploy	with	
valid	settings.	
In	the	event	of	a	
swarm	failure,	
or	stale	swarm	
network,	reset	
the	Docker	
swarm	network	
status	by	
electing	a	new	
swarm	leader.	
To	prevent:	
Validate
hostnames,
aliases,	and	
environment
settings	before	
deployment.
Use	drained	
managers	to	
maintain
swarm	
251 

252
Tool	Issue	Symptoms	Cause	Solution	
Flower	Worker
display	in	
flower	is	not	
organized
and	hard	to	
read,	and	it	
shows	many	
old	workers	in	
an	offline	
state.	
Flower	shows	all	containers	that	
previously	existed,	even	ifthey	failed,	
cluttering	the	dashboard.	
Impact	:	
Flower	dashboard	is	not	organized	and	
hard	to	read.	
Flower	running	for	a	
long	time	while	workers	
are	restarted	or	coming	
up/coming	down,	
maintaining	the	history	
of	all	the	old	workers.	
Another	possibility	is	a	
known	issue	in	task	
processing	due	to	the	-	
-max-	tasks-	per-	
child	setting.	At	high	
CPU	workloads,	the	
max-	tasks-	per-	
child	setting	causes	
workers	to	exit	
prematurely.	
Restart	the	
flower	service	
by	running	the	
following
command:
docker
service
update	--	
force
iservices_
flower
You	can	also	
remove	the	--	
max-	tasks-	
per-	child	
setting	in	the	
steprunners.	
Common	Problems,	Symptoms,	and	Solutions 

Common	Problems,	Symptoms,	and	Solutions	
Tool	Issue	Symptoms	Cause	Solution	
All
containers
on	a	
particular
node	
All	containers	
on	a	
particular
node	do	not	
deploy.	
Services	are	not	deploying	to	a	
particular	node,	but	instead	they	are	
getting	moved	to	other	nodes.	
Impact	:	
The	node	is	not	running	anything.	
One	of	the	following	
situations	could	cause	
this	issue:	
Invalid	label	
deployment
configuration.
The	node	does	not	have	
the	containers	you	are	
telling	itto	deploy.	
The	node	is	missing	a	
required	directory	to	
mount	into	the	
container.	
Make	sure	the	
node	that	you	
are	deploying	to	
is	labeled	
correctly,	and	
that	the	services	
you	expect	to	be	
deployed	there	
are	properly	
constrained	to	
that	system.	
Go	through	the	
troubleshooting
steps	of	""When	
adocker
service	doesn't	
deploy""	to	
check	that	the	
service	is	not	
missing	a	
requirement	on	
the	host.	
Check	the	node	
status	for	errors:	
docker	node	
ls
To	prevent:	
Validate	your	
configuration
before
deploying.	
253 

254
Tool	Issue	Symptoms	Cause	Solution	
All
containers
on	a	
particular
node	
All	containers	
on	a	
particular
node
periodically
restart	at	the	
same	time.	
All	containers	on	aparticular	node	
restart	at	the	same	time.	
The	system	logs	indicate	an	error	like:	
âerror=""rpc	error:	code	=	
DeadlineExceeded	desc	=	
context	deadline	exceeded""	
Impact	:	
All	containers	restart	on	anode.	
This	issue	only	occurs	in	
single-	node	
deployments	when	the	
only	manager	allocates	
too	many	resources	to	
its	containers,	and	the	
containers	all	restart	
since	the	swarm	drops.	
The	manager	node	gets	
overloaded	by	container	
workloads	and	is	not	
able	to	handle	swarm	
management,	and	the	
swarm	loses	quorum.	
Use	some	
drained
manager	nodes	
for	swarm	
management	to	
separate	the	
workloads.
To	prevent	:	
Use	drained	
managers	to	
maintain
swarm	.	
General
Docker
service	
Docker
service	does	
not	deploy.	
Replicas
remain	at	
0/3	
Docker	service	does	not	deploy.	There	are	avariety	of	
reasons	for	this	issue,	
and	you	can	reveal	
most	causes	by	
checking	the	service	
logs	to	address	the	
issue.	
Identify	the	
cause	of	the	
service	not	
deploying	.	
PowerFlow
user
interface	
The	Timeline	
or	the	
Applications
page	do	not	
appear	in	the	
user
interface.	
The	Timeline	is	not	showing	accurate	
information,	or	the	Applications	page	
is	not	rendering.	
One	of	the	following	
situations	could	cause	
these	issues:	
Indexes	do	not	exist	on	a	
particular	Couchbase	
node.
Latency	between	the	
API	and	the	redis	
service	is	too	great	for	
the	API	to	collect	all	the	
data	itneeds	before	the	
30-	second	timeout	is	
reached.
The	indexer	can't	keep	
up	to	alarge	number	of	
requests,	and	
Couchbase	requires	
additional	resources	to	
service	the	requests.	
Solutions:
Verify	that	
indexes	exist.	
Place	the	API	
and	redis	
containers	in	
the	same	
geography	so	
there	is	little	
latency.	This	
issue	will	be	
fixed	in	afuture	
IS	release	
Increase	the	
amount	of	
memory
allocated	to	the	
Couchbase
indexer	service.	
Common	Problems,	Symptoms,	and	Solutions 

Common	Resolution	Explanations	
C o m m o n	R e s o l u ti o n	E xp l a n a ti o n s	
This	section	contains	aset	of	solutions	and	explanations	for	avariety	of	issues.	
E l e c t	a	N e w	S w a r m	L e a d e r	
Sometimes	when	managers	lose	connection	to	each	other,	either	through	latency	or	aworkload	spike,	there	are	
instances	when	the	swarm	needs	to	be	reset	or	refreshed.	By	electing	anew	leader,	you	can	effectively	force	the	
swarm	to	redo	service	discovery	and	refresh	the	metadata	for	the	swarm.	This	procedure	is	highly	preferred	over	
removing	and	re-	deploying	the	whole	stack.	
To	elect	anew	swarm	leader:	
1.	Make	sure	there	at	least	three	swarm	managers	in	your	stack.	
2.	To	identify	which	node	is	the	current	leader,	run	the	following	command:	
docker	node	ls	
3.	Demote	the	current	leader	with	the	following	command:	
docker	node	demote	<node>	
4.	Wait	until	anew	node	is	elected	leader:	
docker	node	ls	
5.	After	anew	node	is	elected	leader,	promote	the	old	node	back	to	swarm	leader:	
docker	node	promote	<node>	
R e c r e a t e	R a b b i t M Q	Q u e u e s	a n d	E x c h a n g e s	
NOTE:	Ifyou	do	not	want	to	retain	any	messages	in	the	queue,	the	following	procedure	is	the	best	method	for	
recreating	the	queues.	Ifyou	do	have	data	that	you	want	to	retain,	you	can	resynchronize	RabbitMQ	
queues	.	
To	recreate	RabbitMQ	queues:	
1.	Identify	the	queue	or	queues	you	need	to	delete:	
l	Ifdefault	workers	are	restarting,	you	need	to	delete	queues	celery	and	priority.high.	
l	Ifacustom	worker	cannot	connect	to	the	queue,	simply	delete	that	worker's	queue.	
2.	Delete	the	queue	and	exchange	through	the	RabbitMQ	admin	console:	
255 

256	
l	Log	in	to	the	RabbitMQ	admin	console	and	go	to	the	[Queues	]tab.	
l	Find	the	queue	you	want	to	delete	and	click	itfor	more	details.	
l	Scroll	down	and	click	the	[Delete	Queue	]button.	
l	Go	to	the	[Exchanges	]tab	and	delete	the	exchange	with	the	same	name	as	the	queue	you	just	
deleted.	
3.	Delete	the	queue	and	exchange	through	the	command	line	interface:	
l	exec	into	arabbitmq	container	
l	Delete	the	queue	needed:	
rabbitmqadmin	delete	queue	name=	name_	of_	queue	
l	Delete	the	exchange	needed:	
rabbitmqadmin	delete	exchange	name=	name_	of_	queue	
After	you	delete	the	queues,	the	queues	will	be	recreated	the	next	time	aworker	connects.	
R e s y n c h r on i z e	R a b b i t M Q	Q u e u e s	
Ifyour	RabbitMQ	cluster	ends	up	in	a""split-	brain""	or	partitioned	state,	you	might	need	to	manually	decide	which	
node	should	become	the	master.	For	more	information,	see	
http://www.rabbitmq.com/partitions.html#recovering	.	
To	resynchronize	RabbitMQ	queues:	
1.	Identify	which	node	you	want	to	be	the	master.	In	most	cases,	the	master	is	the	node	with	the	most	messages	
in	its	queue.	
2.	After	you	have	identified	which	node	should	be	master,	scale	down	all	other	RabbitMQ	services:	
docker	service	scale	iservices_	rabbitmqx=x0	
3.	After	all	other	RabbitMQ	services	except	the	master	have	been	scaled	down,	wait	afew	seconds,	and	then	
scale	the	other	RabbitMQ	services	back	to	1.	Bringing	all	nodes	but	your	new	master	down	and	back	up	
again	forces	all	nodes	to	sync	to	the	state	of	the	master	that	you	chose.	
I d e n t i f y	t h e	C a u s e	of	a	S e r v i c e	n ot	D e p l oy i n g	
Step	1:	Obtain	the	ID	of	the	failed	container	for	the	service	
Run	the	following	command	for	the	service	that	failed	previously:	
docker	service	ps	--no-	trunc	<servicename>	
For	example:	
docker	service	ps	--no-	trunc	iservices_	redis	
Common	Resolution	Explanations 

Common	Resolution	Explanations	
From	the	command	result	above,	we	see	that	one	container	with	the	ID	3s7s86n45skf	failed	previously	running	
on	node	is-	scale-	03	(non-	zero	exit)	and	another	container	was	restarted	in	its	place.	
At	this	point,	you	can	ask	the	following	questions:	
l	Is	the	error	when	using	docker	service	ps	--no-	trunc	something	obvious?	Does	the	error	say	that	it	
cannot	mount	avolume,	or	that	the	image	was	not	found?	Ifso,	that	is	most	likely	the	root	cause	of	the	issue	
and	needs	to	be	addressed.	
l	Did	the	node	on	which	that	container	was	running	go	down?	Or	is	that	node	still	up?	
l	Are	the	other	services	running	on	that	node	running	fine,	and	was	only	this	service	affected?	Ifother	services	
are	running	fine	on	that	same	node,	itis	probably	aproblem	with	the	service	itself.	Ifall	services	on	that	node	
are	not	functional,	itcould	mean	anode	failure.	
At	this	point,	the	cause	of	the	issue	is	not	adeploy	configuration	issue,	and	itis	not	an	entire	node	failure.	The	
problem	exists	within	the	service	itself.	Continue	to	Step	2	ifthis	is	the	case.	
Step	2:	Check	for	any	interesting	error	messages	or	logs	indicating	an	error	
Using	the	ID	obtained	in	Step	1,	collect	the	logs	from	the	failed	container	with	the	following	command:	
docker	service	logs	<failed-	id>	
For	example:	
docker	service	logs	3s7s86n45skf	
Review	the	service	logs	for	any	explicit	errors	or	warning	messages	that	might	indicate	why	the	failure	occurred.	
R e p a i r	C ou c h b a s e	I n d e x e s	
Index	stuck	in	âcreatedâ	(not	ready)	state	
This	situation	usually	occurs	when	anode	starts	creating	an	index,	but	another	index	creation	was	performed	at	the	
same	time	by	another	node.	After	the	index	is	created,	you	can	run	asimple	query	to	build	the	index	which	will	
change	itfrom	created	to	âreadyâ:	
BUILD	index	on	'content'	('idx_	content_	content_	type_	config_	a3f867db_	7430_	4c4b_	b1b6_	
138f06109edb')	using	GSI	
Deleting	an	index	
Ifyou	encounter	duplicate	indexes,	such	as	asituation	where	indexes	were	manually	created	more	than	once,	you	
can	delete	an	index:	
DROP	index	content.idx_	content_	content_	type_	config_	d8a45ead_	4bbb_	4952_	b0b0_	
2fe227702260
Recreating	all	indexes	on	a	particular	node	
257 

258
To	recreate	all	indexes	on	aparticular	Couchbase	node,	exec	into	the	couchbase	container	and	run	the	following	
command:
Initialize_	couchbase	-s	
NOTE:	Running	this	command	recreates	all	indexes,	even	ifthe	indexes	already	exist.	
A d d	a	B r ok e n	C ou c h b a s e	N od e	B a c k	i n t o	t h e	C l u s t e r	
To	remove	aCouchbase	node	and	re-	add	itto	the	cluster:	
1.	Stop	the	node	in	Docker.	
2.	In	the	Couchbase	user	interface,	you	should	see	the	node	go	down,	failover	manually,	or	wait	the	
appropriate	time	until	itautomatically	fails	over.	
3.	Clean	the	Couchbase	data	directory	on	the	necessary	host	by	running	the	following	command:	
rm	-rf	/var/data/couchbase/*	
4.	Restart	the	Couchbase	node	and	watch	itget	added	back	into	the	cluster.	
5.	Click	the	Rebalance	button	to	replicate	data	evenly	across	nodes.	
R e s t or e	C ou c h b a s e	M a n u a l l y	
Backup
1.	Exec	into	the	Couchbase	container:	
cbbackup	http://couchbase.isnet:8091	/opt/couchbase/var/backup	-u	[user]	-p	
[password]	-x	data_	only=1	
2.	Exit	the	Couchbase	shell	and	then	copy	the	backup	file	in	/var/data/couchbase/backup	to	asafe	
location,	such	as	/home/isadmin	.	
Delete	Couchbase	
rm	-f	/var/data/couchbase/*	
Restore
1.	Copy	the	backup	file	into	/var/data/couchbase/backup	.	
2.	Execute	into	the	Couchbase	container.	
3.	Run	the	following	command	to	restore	the	content:	
cbrestore	/opt/couchbase/var/backup	http://couchbase.isnet:8091	-b	content	-u	
<user>	-p	<password>	
Common	Resolution	Explanations 

PowerFlow	Multi-	tenant	Upgrade	Process	
4.	Run	the	following	command	to	restore	the	logs:	
cbrestore	/opt/couchbase/var/backup	http://couchbase.isnet:8091	-b	logs	-u	<user>	
-p	<password>	
P o w e r F l o w	M u l ti -	te n a n t	U p g r a d e	P r o c e s s	
This	section	describes	how	to	upgrade	PowerFlow	in	amulti-	tenant	environment	with	as	little	downtime	as	
possible.
P e r f or m	E n v i r on m e n t	C h e c k s	B e f or e	U p g r a d i n g	
Validate	Cluster	states	
l	Validate	that	all	Couchbase	nodes	in	the	cluster	are	replicated	and	fully	balanced.	
l	Validate	that	the	RabbitMQ	nodes	are	all	clustered	and	queues	have	ha-	v1-	all	policy	applied.	
l	Validate	that	the	RabbitMQ	nodes	do	not	have	alarge	number	of	messages	backed	up	in	queue.	
Validate	Backups	exist	
l	Ensure	that	you	have	abackup	of	the	database	before	upgrading.	
l	Ensure	that	you	have	acopy	of	your	most	recently	deployed	docker-	compose	file.	Ifall	user-	specific	changes	
are	only	populated	in	docker-	compose-	override,	this	is	not	necessary,	but	you	might	want	abackup	copy.	
l	Make	sure	that	each	node	in	Couchbase	is	fully	replicated,	and	no	re-	balancing	is	necessary.	
Clean	out	old	container	images	ifdesired	
Before	upgrading	to	the	latest	version	of	PowerFlow	,check	the	local	file	system	and	see	ifthere	are	any	older	
versions	taking	up	space	that	you	might	want	to	remove.	These	containers	exist	both	locally	on	the	fs	and	the	
internal	docker	registry.	To	view	any	old	container	versions,	check	the	/opt/iservices/images	directory.	
ScienceLogic	recommends	that	you	keep	at	aminimum	the	last	version	of	containers,	so	you	can	downgrade	if	
necessary.
Cleaning	out	images	is	not	mandatory,	but	itis	just	ameans	of	clearing	out	additional	space	on	the	system	if	
necessary.
To	remove	old	images:	
1.	Delete	any	unwanted	versions	in	/opt/iservices/images	.	
2.	Identify	any	unwanted	images	known	to	Docker	with	docker	images.	
3.	Remove	the	images	with	the	ID	docker	rmi	<id>	.	
P r e p a r e	t h e	S y s t e m s	
Install	the	new	RPM	
259 

260
The	first	step	of	upgrading	is	to	install	the	new	RPM	on	all	systems	in	the	stack.	Doing	so	will	ensure	that	the	new	
containers	are	populated	onto	the	system	(if	using	that	particular	RPM),	and	any	other	host	settings	are	changed.	
RPM	installation	does	not	pause	any	services	or	affect	the	docker	system	in	any	way,	other	than	using	some	
resources.
PowerFlow	has	two	RPMs,	one	with	containers	and	one	without.	Ifyou	have	populated	an	internal	docker	registry	
with	docker	containers,	you	can	install	the	RPM	without	containers	built	in.	Ifno	internal	docker	repository	is	
present,	you	must	install	the	RPM	which	has	the	containers	built	in	it.	Other	than	the	containers,	there	is	no	
difference	between	the	RPMs.	
For	advanced	users,	installing	the	RPM	can	be	skipped.	However	this	means	that	the	user	is	completely	
responsible	for	maintaining	the	docker-	compose	and	host	level	configurations.	
To	install	the	RPM:	
1.	SSH	into	each	node.	
2.	Ifinstalling	the	RPM	which	contains	the	container	images	built	in,	you	may	want	to	upgrade	each	core	node	
one	by	one,	so	that	the	load	of	extracting	the	images	doesn't	affect	all	core	nodes	at	once	
3.	Run	the	following	command:	
rpm	-Uvh	<new-	rpm-	file>	
Compare	compose	file	changes	and	resolve	differences	
After	the	RPM	is	installed,	you	will	notice	anew	docker-	compose	file	is	placed	at	/etc/iservices/scripts/docker-	
compose.ym	l.As	long	as	your	environment-	specific	changes	exist	solely	in	the	compose-	override	file,	all	user	
changes	and	new	version	updates	will	be	resolved	into	that	new	docker-	compose.yml	file.	
ScienceLogic	recommends	that	you	check	the	differences	between	the	two	docker-	compose	files.	You	should	
validate	that:	
1.	All	environment-	specific	and	custom	user	settings	that	existed	in	the	old	docker-	compose	also	exist	in	the	new	
docker-	compose	file.	
2.	The	image	tags	reference	the	correct	version	in	the	new	docker-	compose.	Ifyou	are	using	an	internal	docker	
registry,	be	sure	these	image	tags	represent	the	images	from	your	internal	registry.	
3.	Make	sure	that	any	new	environment	variables	added	to	services	are	applied	to	replicated	services.	To	
ensure	these	updates	persist	through	the	next	upgrade,	also	make	the	changes	in	docker-	compose-	
override.yml.	In	other	words,	ifyou	added	anew	environment	variable	for	Couchbase,	make	sure	to	apply	
that	variable	to	couchbase-	worker1	and	couchbase-	worker2	as	well.	Ifyou	added	anew	environment	
variable	for	the	default	steprunner,	make	sure	to	set	the	same	environment	variable	on	each	custom	worker	
as	well.	
4.	Ifyou	are	using	the	latest	tag	for	images,	and	you	are	using	aremote	repository	for	downloading,	be	sure	that	
the	latest	tag	refers	to	the	images	in	your	repository.	
5.	The	old	docker-	compose	is	completely	unchanged,	and	itmatches	the	current	deployed	environment.	This	
enables	PowerFlow	to	update	services	independently	without	restarting	other	services.	
6.	After	you	resolve	any	differences	between	the	compose	files	has	been	resolved,	proceed	with	the	upgrade	
using	the	old	docker-	compose.yml	(the	one	that	matches	the	currently	deployed	environment).	
Make	containers	available	to	systems	
PowerFlow	Multi-	tenant	Upgrade	Process 

PowerFlow	Multi-	tenant	Upgrade	Process	
After	you	apply	the	host-	level	updates,	you	should	make	sure	that	the	containers	are	available	to	the	system.	
Ifyou	upgraded	using	the	RPM	with	container	images	included,	the	containers	should	already	be	on	all	of	the	
nodes,	you	can	run	docker	images	to	validate	the	new	containers	are	present.	Ifthis	is	the	case	you	may	skip	to	the	
next	section.	
Ifthe	upgrade	was	performed	using	the	RPM	which	did	not	contain	the	container	images,	ScienceLogic	
recommends	that	you	run	the	following	command	to	make	sure	all	nodes	have	the	latest	images:	
docker-	compose	-f	<new_	docker_	compose_	file>	pull	
This	command	validates	that	the	containers	specified	by	your	compose	file	can	be	pulled	and	reached	from	the	
nodes.	While	not	required,	you	might	to	make	sure	that	the	images	can	be	pulled	before	starting	the	upgrade.	If	
the	images	are	not	pulled	manually,	they	will	automatically	be	pulled	by	Docker	when	the	new	image	is	called	for	
by	the	stack.	
P e r f or m	t h e	U p g r a d e	
To	perform	the	upgrade	on	aclustered	system	with	little	downtime,	PowerFlow	re-	deploys	services	to	the	stack	in	
groups.	To	do	this,	PowerFlow	gradually	makes	the	updates	to	groups	of	services	and	re-	runs	docker	stack	deploy	
for	each	change.	To	ensure	that	no	unintended	services	are	updated,	start	off	using	the	same	docker-	compose	file	
that	was	previously	used	to	deploy.	Reusing	the	same	docker-	compose	file	and	updating	only	sections	at	atime	
ensures	that	only	the	intended	services	to	be	updated	are	affected	at	any	given	time.	
Avoid	putting	all	the	changes	in	asingle	docker-	compose	file,	and	do	anew	docker	stack	deploy	with	all	changes	
at	once.	Ifdowntime	is	not	aconcern,	you	can	update	all	services,	but	updating	services	gradually	allows	you	to	
have	little	or	no	downtime.	
WARNING:	Before	upgrading	any	group	of	services,	be	sure	that	the	docker-	compose	file	you	are	deploying	
from	is	exactly	identical	to	the	currently	deployed	stack	(the	previous	version).	Start	with	the	same	
docker-	compose	file	and	update	itfor	each	group	of	services	as	needed,	
Upgrade	Redis,	Scheduler,	and	Flower	
The	first	group	to	update	includes	Redis,	Scheduler	and	Flower.	Ifdesired,	this	group	can	be	upgraded	along	with	
any	other	group.	
To	update:
1.	Copy	the	service	entries	for	Redis,	Scheduler	and	Flower	from	the	new	compose	file	into	the	old	docker-	
compose	file	(the	file	that	matches	the	currently	deployed	environment).	Copying	these	entries	makes	itso	
that	the	only	changes	in	the	docker-	compose	file	(compared	to	the	deployed	stack)	are	changes	for	Redis,	
Scheduler	and	Flower.	
2.	Run	the	following	command:	
docker	stack	deploy	-c	<old_	compose_	with_	small_	changes>	iservices	
3.	Monitor	the	update,	and	wait	until	all	services	are	up	and	running	before	proceeding.	
261 

262
Example	image	definition	of	this	upgrade	group:	
services:	
contentapi:	
image:	repository.auto.sciencelogic.local:5000/is-	api:1.8.1	
couchbase:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:1.8.1	
couchbase-	worker:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:1.8.1	
flower:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:hotfix-	1.8.3	
gui:	
image:	repository.auto.sciencelogic.local:5000/is-	gui:1.8.1	
rabbitmq:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
rabbitmq2:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
rabbitmq3:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
redis:	
image:	repository.auto.sciencelogic.local:5000/is-	redis:4.0.11-	2	
scheduler:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:hotfix-	1.8.3	
steprunner:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:1.8.1	
couchbase-	worker2:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:1.8.1	
steprunner2:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:1.8.1	
Redis	Version	
As	the	Redis	version	might	not	change	with	every	release	of	PowerFlow	,there	might	not	be	any	changes	needed	in	
the	upgrade	for	Redis.	This	can	be	expected	and	is	not	an	issue.	
Flower	Dashboard	
Due	to	aknown	issue	addressed	in	version	1.8.3	of	PowerFlow	,the	Flower	Dashboard	might	not	display	any	
workers.	Flower	eventually	picks	up	the	new	workers	when	they	are	restarted	in	the	worker	group.	Ifthis	is	a	
concern,	you	can	perform	the	Flower	upgrade	in	the	same	group	as	the	workers.
PowerFlow	Multi-	tenant	Upgrade	Process 

PowerFlow	Multi-	tenant	Upgrade	Process	
U p g r a d e	C o r e	S e r v i c e s	( R a b b i t	a n d	C o u c h b a s e )	
The	next	group	of	services	to	update	together	are	the	RabbitMQ/Couchbase	database	services,	as	well	as	the	
GUI.	Because	the	core	services	are	individually	defined	and	""pinned""	to	specific	nodes,	upgrade	these	two	services	
at	the	same	time,	on	anode-	by-	node	basis.	In	between	each	node	upgrade,	wait	and	validate	that	the	node	
rejoins	the	Couchbase	and	Rabbit	clusters	and	re-	balances	appropriately.	
Because	there	will	always	be	two	out	of	three	nodes	running	these	core	services,	this	group	should	not	cause	any	
downtime	for	the	system.	
Rabbit/Couchbase	Versions	
The	Couchbase	and	RabbitMQ	versions	used	might	not	change	with	every	release	of	PowerFlow	.Ifthere	is	no	
update	or	change	to	be	made	to	the	services,	you	can	ignore	this	section	for	RabbitMQ	or	Couchbase	upgrades,	
or	both.	Assess	the	differences	between	the	old	and	new	docker-	compose	files	to	check	ifthere	is	an	image	or	
environment	change	necessary	for	the	new	version.	Ifnot,	you	can	move	on	to	the	next	section.	
Update	Actions	(assuming	three	core	nodes)	
To	update	first	node	services:	
1.	Update	just	core	node01	by	copying	service	entries	for	couchbase,	rabbitmq1	from	the	new	compose	file	
(compared	and	resolved	as	part	of	above	prepare	steps)	into	the	old	docker-	compose	file.	At	this	point,	the	
compose	file	you	use	to	deploy	should	also	contain	the	updates	for	the	previous	groups	
2.	Before	deploying,	access	the	Couchbase	user	interface,	select	the	first	server	node,	and	click	""failover"".	
Select	graceful	failover.	Manually	failing	over	before	updating	ensures	that	the	system	is	still	operational	
when	the	container	comes	down.	
3.	For	the	failover	command	that	can	be	run	through	the	command-	line	interface	ifthe	user	interface	is	not	
available,	see	the	Manual	Failover	section	.	
4.	Run	the	following	command:	
docker	stack	deploy	-c	<compose_	file>	
5.	Monitor	the	process	to	make	sure	the	service	updates	and	restarts	with	the	new	version.	To	make	sure	that	as	
little	time	as	possible	is	used	when	updating	the	database,	the	database	containers	should	already	be	
available	on	the	core	nodes.	
6.	After	the	node	is	back	up,	go	back	to	the	Couchbase	UI	and	add	the	node	back,	and	rebalance	the	cluster	to	
make	itwhole	again.	
7.	For	more	information	on	how	to	re-	add	the	node	and	rebalance	the	cluster	ifthe	user	interface	is	not	
available,	see	the	Manual	Failover	section	.	
First	node	Couchbase	update	considerations	:	
l	When	updating	the	first	couchbase	node,	be	sure	to	set	the	environment	variable	JOIN_	ON:	""couchbase-	
worker2"",	so	that	the	couchbase	master	knows	to	rejoin	the	workers	after	restarting	
l	Keep	in	mind	by	default,	only	the	primary	Couchbase	node	user	interface	is	exposed.	Because	of	this,	when	
the	first	Couchbase	node	is	restarted,	the	Couchbase	admin	user	interface	will	be	inaccessible.	Ifyou	would	
like	to	have	the	Couchbase	user	interface	available	during	the	upgrade	of	this	node,	ensure	that	at	least	one	
other	Couchbase-	worker	services	port	is	exposed.	
263 

264
Special	GUI	consideration	with	1.8.3	
In	the	upgrade	to	version	1.8.3	of	PowerFlow	,the	Couchbase	and	RabbitMQ	user	interface	ports	will	be	exposed	
through	PowerFlow	user	interface	with	HTTPS.	To	ensure	there	is	no	port	conflict	between	services	and	PowerFlow	
user	interface,	ensure	that	the	Couchbase	and	RabbitMQ	user	interface	port	mappings	are	removed	or	modified	
from	the	default	(8091)	admin	port.	To	avoid	conflicts,	make	sure	the	new	PowerFlow	user	interface	definition	
does	not	conflict	with	the	Couchbase	or	RabbitMQ	definitions.	
l	Modifying	the	port	mapping	(exposing	to	adifferent	port	than	8091)	means	itwill	still	be	exposed	via	HTTP	
through	aport	other	than	8091	until	the	PowerFlow	user	interface	is	upgraded,	at	which	point	that	port	can	
then	be	closed.	
l	Removing	the	port	means	no	port	mapping	will	be	defined	to	8091	for	the	Couchbase	service,	and	the	
Couchbase	user	interface	will	be	inaccessible	until	the	PowerFlow	user	interface	is	updated.	
The	PowerFlow	user	interface	will	not	update	until	all	port	conflicts	are	resolved.	You	can	upgrade	the	PowerFlow	
user	interface	at	any	time	after	this	has	been	done,	but	be	sure	to	first	review	the	Update	the	GUI	topic,	below.	
NOTE:	You	can	manually	remove	port	mappings	from	aservice	with	the	following	command,	though	the	
command	will	restart	the	service:	docker	service	update	--publish-	rm	
published=8091,target=8091	iservices_	couchbase	
Example	docker-	compose	with	images	and	JOIN_	ON	for	updating	the	first	node:	
services:	
contentapi:	
image:	repository.auto.sciencelogic.local:5000/is-	api:1.8.1	
couchbase:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:hotfix-	1.8.3	
environment:	
JOIN_	ON:	""couchbase-	worker2""	
couchbase-	worker:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:1.8.1	
flower:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:hotfix-	1.8.3	
gui:	
image:	repository.auto.sciencelogic.local:5000/is-	gui:hotfix-	1.8.3	
rabbitmq:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
rabbitmq2:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
rabbitmq3:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
redis:	
image:	repository.auto.sciencelogic.local:5000/is-	redis:4.0.11-	2	
scheduler:	
PowerFlow	Multi-	tenant	Upgrade	Process 

PowerFlow	Multi-	tenant	Upgrade	Process	
image:	repository.auto.sciencelogic.local:5000/is-	worker:hotfix-	1.8.3	
steprunner:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:1.8.1	
couchbase-	worker2:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:1.8.1	
steprunner2:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:1.8.1	
Update	second,	and	third	node	services	
To	update	the	second	and	third	node	services,	repeat	the	steps	from	the	first	node	on	each	node	until	all	nodes	are	
re-	clustered	and	available.	Be	sure	to	check	the	service	port	mappings	to	ensure	that	there	are	no	conflicts	(as	
described	above),	and	remove	any	HTTP	ports	ifyou	choose.	
U p d a t e	t h e	G U I	
You	can	update	the	GUI	service	along	with	any	other	group,	but	due	to	the	port	mapping	changes	in	version	1.8.3	
of	PowerFlow	,you	should	update	this	service	after	the	databases	and	RabbitMQ	nodes	have	been	updated,	and	
their	port	mappings	no	longer	conflict.	
Since	the	GUI	service	provides	all	ingress	proxy	routing	to	the	services,	there	might	be	avery	small	window	where	
PowerFlow	might	not	receive	API	requests	as	the	GUI	(proxy)	is	not	running.	This	downtime	is	limited	to	the	time	it	
takes	for	the	GUI	container	to	restart.	
To	update	the	user	interface:	
1.	Make	sure	that	any	conflicting	port	mappings	are	handled	and	addressed.	
2.	Replace	the	docker-	compose	GUI	service	definition	with	the	new	one.	
3.	Re-	deploy	the	docker-	compose	file,	and	validate	that	the	new	GUI	container	is	up	and	running.	
4.	Make	sure	that	the	HTTPS	ports	are	accessible	for	Couchbase/RabbitMG.	
U p d a t e	W o r k e r s	a n d	c o n t e n t a p i	
You	should	update	the	workers	and	contentapi	last.	Because	these	services	use	multiple	replicas	(multiple	
steprunner	or	containerapi	containers	running	per	service),	you	can	rely	on	Docker	to	incrementally	update	each	
replica	of	the	service	individually.	By	default,	when	aservice	is	updated,	itwill	update	one	container	of	the	service	
at	atime,	and	only	after	the	previous	container	is	up	and	stable	will	the	next	container	be	deployed.	
You	can	utilize	additional	Docker	options	in	docker-	compose	to	set	the	behavior	of	how	many	containers	to	update	
at	once,	when	to	bring	down	the	old	container,	and	what	happens	ifacontainer	upgrade	fails.	See	the	update_	
config	and	rollback_	config	options	available	in	Docker	documentation:	
https://docs.docker.com/compose/compose-	file/	.	
Upgrade	testing	was	performed	by	ScienceLogic	using	default	options.	An	example	where	these	settings	are	
helpful	is	to	change	the	parallelism	of	update_	config	so	that	all	worker	containers	of	aservice	update	at	the	same	
time.	
265 

266
The	update	scenario	described	below	takes	extra	precautions	and	only	updates	one	node	of	workers	per	customer	
at	atime.	Ifyou	decide,	you	can	also	safely	update	all	workers	at	once.	
To	update	the	workers	and	contentapi:	
1.	Modify	the	docker-	compose	file,	the	contentapi,	and	""worker_	node1""	services	of	all	customers	to	use	the	new	
service	definition.	
2.	Run	adocker	stack	deploy	of	the	new	compose	file.	Monitor	the	update,	which	should	update	the	API	
container	one	instance	at	atime,	always	leaving	acontainer	available	to	service	requests.	The	process	
updates	the	workers	of	node1	one	container	instance	at	atime	by	default.	
3.	After	workers	are	back	up	and	the	API	is	fully	updated,	modify	the	docker-	compose	file	and	update	the	
second	node's	worker's	service	definitions.	
4.	Monitor	the	upgrade,	and	validate	as	needed.	
Example	docker-	compose	definition	with	one	of	two	worker	nodes	and	contentapi	updated:	
services:	
contentapi:	
image:	repository.auto.sciencelogic.local:5000/is-	api:hotfix-	1.8.3	
deploy:	
replicas:	3	
couchbase:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:hotfix-	1.8.3	
environment:	
JOIN_	ON:	""couchbase-	worker2""	
couchbase-	worker:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:hotfix-	1.8.3	
flower:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:hotfix-	1.8.3	
gui:	
image:	repository.auto.sciencelogic.local:5000/is-	gui:hotfix-	1.8.3	
rabbitmq:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
rabbitmq2:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
rabbitmq3:	
image:	repository.auto.sciencelogic.local:5000/is-	rabbit:3.7.7-	2	
redis:	
image:	repository.auto.sciencelogic.local:5000/is-	redis:4.0.11-	2	
scheduler:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:hotfix-	1.8.3	
steprunner:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:hotfix-	1.8.3	
couchbase-	worker2:	
image:	repository.auto.sciencelogic.local:5000/is-	couchbase:hotfix-	1.8.3	
PowerFlow	Multi-	tenant	Upgrade	Process 

PowerFlow	Multi-	tenant	Upgrade	Process	
steprunner2:	
image:	repository.auto.sciencelogic.local:5000/is-	worker:1.8.1	
267 

Â©	2003	-2020,	ScienceLogic,	Inc.	
All	rights	reserved.	
LIMITATION	OF	LIABILITY	AND	GENERAL	DISCLAIMER	
ALL	INFORMATION	AVAILABLE	IN	THIS	GUIDE	IS	PROVIDED	""AS	IS,""	WITHOUT	WARRANTY	OF	ANY	
KIND,	EITHER	EXPRESS	OR	IMPLIED.	SCIENCELOGICâ¢	AND	ITS	SUPPLIERS	DISCLAIM	ALL	WARRANTIES,	
EXPRESS	OR	IMPLIED,	INCLUDING,	BUT	NOT	LIMITED	TO,	THE	IMPLIED	WARRANTIES	OF	
MERCHANTABILITY,	FITNESS	FOR	APARTICULAR	PURPOSE	OR	NON-	INFRINGEMENT.	
Although	ScienceLogicâ¢	has	attempted	to	provide	accurate	information	on	this	Site,	information	on	this	Site	
may	contain	inadvertent	technical	inaccuracies	or	typographical	errors,	and	ScienceLogicâ¢	assumes	no	
responsibility	for	the	accuracy	of	the	information.	Information	may	be	changed	or	updated	without	notice.	
ScienceLogicâ¢	may	also	make	improvements	and	/or	changes	in	the	products	or	services	described	in	this	
Site	at	any	time	without	notice.	
Copyrights	and	Trademarks	
ScienceLogic,	the	ScienceLogic	logo,	and	EM7	are	trademarks	of	ScienceLogic,	Inc.	in	the	United	States,	
other	countries,	or	both.	
Below	is	alist	of	trademarks	and	service	marks	that	should	be	credited	to	ScienceLogic,	Inc.Â 	The	Â®	and	â¢	
symbols	reflect	the	trademark	registration	status	in	the	U.S.	Patent	and	Trademark	Office	and	may	not	be	
appropriate	for	materials	to	be	distributed	outside	the	United	States.	
l	ScienceLogicâ¢	
l	EM7â¢	and	em7â¢	
l	Simplify	ITâ¢	
l	Dynamic	Applicationâ¢	
l	Relational	Infrastructure	Managementâ¢	
The	absence	of	aproduct	or	service	name,	slogan	or	logo	from	this	list	does	not	constitute	awaiver	of	
ScienceLogicâs	trademark	or	other	intellectual	property	rights	concerning	that	name,	slogan,	or	logo.	
Please	note	that	laws	concerning	use	of	trademarks	or	product	names	vary	by	country.	Always	consult	a	
local	attorney	for	additional	guidance.	
Other
Ifany	provision	of	this	agreement	shall	be	unlawful,	void,	or	for	any	reason	unenforceable,	then	that	
provision	shall	be	deemed	severable	from	this	agreement	and	shall	not	affect	the	validity	and	enforceability	
of	any	remaining	provisions.	This	is	the	entire	agreement	between	the	parties	relating	to	the	matters	
contained	herein.	
In	the	U.S.	and	other	jurisdictions,	trademark	owners	have	aduty	to	police	the	use	of	their	marks.Â 	Therefore,	
ifyou	become	aware	of	any	improper	use	of	ScienceLogic	Trademarks,	including	infringement	or	
counterfeiting	by	third	parties,	report	them	to	Science	Logicâs	legal	department	immediately.Â 	Report	as	much	
detail	as	possible	about	the	misuse,	including	the	name	of	the	party,	contact	information,	and	copies	or	
photographs	of	the	potential	misuse	to:	legal@sciencelogic.com 

800-	SCI-	LOGIC	(1-	800-	724-	5644)	
International:	+1-	703-	354-	1010 

"
Non,"   U.S. and other countries. Other names may be trademarks of their respective owners.
Copyright Â© 2023 Veritas Technologies Corporation. All rights reserved. Veritas, the Veritas Logo, and NetBackup are trademarks or registered trademarks of Veritas Technologies Corporation in the
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	2023-09-14
	
 Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent
 Compatibility List


  Created on September 14, 2023

  Click here for the HTML version of this document. <https://download.veritas.com/resources/content/live/OSVC/100040000/100040093/en_US/nbu_90_db_scl.html>

 

  page 2, Introduction
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	
 Introduction

 This Software Compatibility List (SCL) document describes the supported database and application agents for Veritas NetBackup 9.0 through 9.x
 For all other NetBackup compatibility lists, refer to <http://www.veritas.com/docs/000033647>
 IPV6 and Dual Stack environments are supported from NetBackup 9.0 onwards with few limitations, refer technote for additional information <http://www.veritas.com/docs/100041420>

 Database and application patch support: Major versions of database software have been qualified in Veritas Labs and are listed in the tables below. NetBackup supports
minor versions, service packs, and patches of database software unless otherwise noted, with the exception of Microsoft Exchange, Microsoft SharePoint and Microsoft
SQL Server. 

 Linux support: For Linux distributions shown in the tables below, NetBackup is supported on all ""editions"" and on all vendor GA updates (n.1, n.2, etc.) or service packs (SP1, SP2, etc.)
unless otherwise noted in this document or in the NetBackup OS Compatibility List, <http://www.veritas.com/docs/100032808>

 Database Appliances: 3rd party database hardware appliances are supported if the appliance OS/architecture and database software are listed in the Veritas NetBackup OS
compatibility list <http://www.veritas.com/docs/100032808> and in the Database and Application Agent compatibility list (this document). For example, some Oracle Exadata and
Exalogic models are supported based upon this support statement. Please consult the vendor documentation to identify the embedded software versions.
 End of life information: More information is available about certain NetBackup features, functionality, 3rd-party product integration, Veritas product integration, applications, databases,
and OS platforms that Veritas intends to replace with newer and improved functionality, or in some cases, discontinue without replacement. Please see the SORT widget titled
""NetBackup Future Platform and Feature Plans"" at <https://sort.veritas.com/netbackup.>
 

  page 3, 9.0 - 9.x.x Database and Application Agent Software Compatibility List Updates 
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	
 9.0 - 9.x.x Database and Application Agent Software Compatibility List
 Updates

 
  Update Information

  Description of Change

  Date

  NetBackup Version Start of Support

 NetBackup GA 9.1.0.1
 2021-09-07
 NetBackup 9.1.0.1
 NetBackup GA 9.1
 2021-06-07
 NetBackup 9.1
 NetBackup GA 9.0.0.1
 2021-03-28
 NetBackup 9.0.0.1
 NetBackup GA 9.0
 2021-01-01
 NetBackup 9.0
 

  page 4, Database Agents
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	
 Database Agents

 The sections below contain information that is related to specific database agents and OS platforms.

 SAP
The NetBackup SAP Agent protects SAP Oracle environments by integrating with SAP BR*Tools. SAP BR*Tools may have a separate version from the SAP Kernel. Reference
Technical Solution 51094 for details <http://www.veritas.com/docs/000033132.>
SAP environments based on Microsoft SQL or DB2 databases are supported via the NetBackup Microsoft SQL and DB2 agents respectively.
SAP supports Oracle and MAX-DB base environments.
 

  page 5, Database Agents, Apache Hadoop
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Apache Hadoop


 The supported Hadoop Versions listed below refer to native Apache Software Foundation open source releases of the Apache Hadoop Project.
NetBackup supports Hadoop file systems from Apache Hadoop, Cloudera and Hortonworks Distributions.
Any modifications or derivations to these original distribution releases is not officially supported.
 
  Apache Hadoop - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Hadoop
 3.1.5
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 3.1.4
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 3.1.1
 64
 CentOS 7.0
 x86-64
 64
 9.0
 Hadoop
 3.1.1
 64
 Oracle Linux 7
 x86-64
 64
 9.0
 Hadoop
 3.1.1
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 3.1.0
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 3.0.0
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 3.0.0
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 3.0.0
 64
 Ubuntu 18.04
 x86-64
 64
 9.0
 Hadoop
 2.8.1
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.8.1
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 2.7.4
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.7.4
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 2.7.3
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.7.3
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 2.6.5
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.6.5
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 2.6.4
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.6.4
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 2.6.3
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.6.3
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 2.6.0
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.6.0
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Hadoop
 2.5.2
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Hadoop
 2.5.2
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 

  page 6, Database Agents, Apache Hadoop, Apache Hadoop - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	
 Note the following:
 NetBackup 9.0 does not capture Extended Attributes (xattrs) or Access Control Lists (ACLs) of an object during backup and hence these are not set on the restored files or folders.
 If the Access Control Lists (ACLs) are enabled and set on any of the files or directories that is backed up then, you have to explicitly set the ACLs on the restored data.
 The backup host can be a NetBackup 9.0 client or a 9.0 media server or a 9.0 master server. Hadoop Plugin needs to be installed on the NetBackup client or a media server or a
master server to make it a Backup Host. Veritas recommends that you have media server as a backup host.
 NetBackup supports installation of the backup host in a virtual machine. The guest operating systems that NetBackup supports are same as the above.
 For a Hadoop cluster that uses Kerberos, Ensure that the Kerberos client (krb5_workstation package) is present on all the backup hosts with kdcserver details.
 NetBackup 8.1 does not support Hadoop cluster that uses Delegation token authentication
 Hadoop implementation on an Isilon clusters is not supported.
 For protection of HIVE data residing on Hadoop cluster. Refer: <https://www.veritas.com/support/en_US/article.100044318.html>
 SSL for Hadoop is supported with NetBackup 9.0 release. Please refer NetBackup Hadoop Admin guide for additional Information
 

  page 7, Database Agents, Apache HBase
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Apache HBase


 The supported HBase Versions listed below refer to native Apache Software Foundation open source releases of the Apache HBase Project.
Only software officially released by the Apache HBase Project can be called Apache HBase.
Any modifications or derivations to these original distribution releases is not officially supported.
Please refer official HBase documentation for HDFS and Operating System support.
Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
 
  Apache HBase - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 HBase
 2.2
 64
 Red Hat Enterprise Linux 7 Update 7
 x86-64
 64
 9.0
 HBase
 2.1
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 HBase
 2.0
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 HBase
 2.0
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 HBase
 1.3
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 HBase
 1.3
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 HBase
 1.2
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 HBase
 1.2
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Note the following:
 SSL for HBase is supported with NetBackup 9.0 release. Please refer NetBackup HBase and Hadoop Admin guide for additional Information
 

  page 8, Database Agents, Enterprise Vault
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Enterprise Vault

 Enterprise Vault Compatibility - Reference Article TECH38537: Enterprise Vault Compatibility List:
<http://www.veritas.com/docs/000097605>

 Note: All SQL versions supported by Enterprise Vault are supported in cluster configurations unless specifically noted in the table below.
Support for Enterprise Vault with SQL Server 2017 starts from NetBackup 8.1.2
Enterprise Vault agent does not support Enterprise Vault and SQL Server in VCS environment.
SQL configurations with AlwaysOn Availability Groups (AGs) are not supported at this time.
 
  Enterprise Vault - Supported Configurations

  Database Software

  Version

  DB Bit

  SQL Server Version

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Enterprise Vault
 14.1
 64
 2016, 2017, 2019
 Windows Server 2019
 x86-64
 64
 9.0
 Enterprise Vault
 14.0
 64
 2016, 2019
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 14.0
 64
 2016, 2017, 2019
 Windows Server 2019
 x86-64
 64
 9.0
 Enterprise Vault
 12.5 [1]
 64
 2012
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Enterprise Vault
 12.5 [2]
 64
 2016, 2017, 2019
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.5
 64
 2017, 2019
 Windows Server 2019
 x86-64
 64
 9.0
 Enterprise Vault
 12.4.1
 2017
 Windows Server 2019
 x86-64
 64
 9.0
 Enterprise Vault
 12.4.1
 64
 2017
 Windows Server 2019
 x86-64
 64
 9.0
 Enterprise Vault
 12.4 [3]
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Enterprise Vault
 12.4 [3]
 2012, 2017
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Enterprise Vault
 12.4 [3]
 64
 2012, 2017
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Enterprise Vault
 12.4 [3]
 2016
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.4 [3]
 64
 2016
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.4 [3]
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Enterprise Vault
 12.3.2
 64
 2017
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.3.1
 64
 2012
 Windows Server 2012
 x86-64
 64
 9.0
 Enterprise Vault
 12.3.1
 64
 2014
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Enterprise Vault
 12.3.1
 64
 2016
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.3 [3]
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Enterprise Vault
 12.3 [3]
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Enterprise Vault
 12.3 [3]
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.3 [3]
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.2 [4]
 64
 Windows Server 2012
 x86-64
 64
 9.0
 

  page 9, Database Agents, Enterprise Vault, Enterprise Vault - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 Enterprise Vault - Supported Configurations

  Database Software

  Version

  DB Bit

  SQL Server Version

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Enterprise Vault
 12.2 [4]
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Enterprise Vault
 12.2 [4]
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Enterprise Vault
 12.1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Enterprise Vault
 12.1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Enterprise Vault
 12.0
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Enterprise Vault
 12.0
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 1. Supported on SQL Server 2012 SP4
 2. Supported on SQL Server 2016 SP2
 3. Refer article <http://www.veritas.com/docs/100042794> for supported configuration of Smart Partitions.
 4. Refer article <http://www.veritas.com/docs/100040487> for support of Enterprise Vault 12.2 on this platform.
 

  page 10, Database Agents, HCL Domino
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 HCL Domino

   HCL Domino - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Domino
 12
 64
 AIX 7.3
 POWER
 64
 9.0
 Domino
 12
 64
 AIX 7.2
 POWER
 64
 9.0
 Domino
 12
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 Domino
 12
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 Domino
 12
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Domino
 12
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Domino
 12
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Domino
 12
 64
 Windows Server 2022
 x86-64
 64
 9.0
 Domino
 11
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Domino
 11
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Domino
 11
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Domino
 11
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Domino
 11
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Domino
 8.5
 64
 AIX 7.1
 POWER
 64
 9.0
 

  page 11, Database Agents, IBM DB2
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 IBM DB2

 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.

 Recommend using the latest DB2 FixPaks or these minimum revisions, DB2 V8.1 FixPak 3 or later.
 IBM PureScale environments are not supported.
   IBM DB2 - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 DB2 UDB - EE/DPF
 11.5
 64
 AIX 7.3
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 AIX 7.2
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 AIX 7.1
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 Red Hat Enterprise Linux 7
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 SUSE Linux Enterprise Server 12
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 Windows Server 2016
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 Windows Server 2019
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.5
 64
 Windows Server 2022
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 AIX 7.2
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 AIX 7.1
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 Red Hat Enterprise Linux 7
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 Solaris 11
 SPARC
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 SUSE Linux Enterprise Server 12
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 11.1
 64
 Windows Server 2016
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 10.5
 64
 AIX 7.1
 POWER
 64
 9.0
 

  page 12, Database Agents, IBM DB2, IBM DB2 - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 IBM DB2 - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 DB2 UDB - EE/DPF
 10.5
 64
 HP-UX 11.31
 IA64
 64
 9.0
 DB2 UDB - EE/DPF
 10.5
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 10.5
 64
 Red Hat Enterprise Linux 7
 z/Architecture
 64
 9.0
 DB2 UDB - EE/DPF
 10.5
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 10.5
 64
 SUSE Linux Enterprise Server 12
 z/Architecture
 64
 9.0
 DB2 UDB - EE/DPF
 10.5
 64
 Windows Server 2012
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 10.5
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 10.1
 64
 AIX 7.1
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 10.1
 64
 HP-UX 11.31
 IA64
 64
 9.0
 DB2 UDB - EE/DPF
 10.1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 DB2 UDB - EE/DPF
 9.7
 64
 AIX 7.1
 POWER
 64
 9.0
 DB2 UDB - EE/DPF
 9.7
 64
 HP-UX 11.31
 IA64
 64
 9.0
 

  page 13, Database Agents, IBM Informix
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 IBM Informix

 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
   IBM Informix - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Informix IDS
 14.10
 64
 AIX 7.3
 POWER
 64
 9.0
 Informix IDS
 14.10
 64
 AIX 7.2
 POWER
 64
 9.0
 Informix IDS
 14.10
 64
 HP-UX 11.31
 IA64
 64
 9.0
 Informix IDS
 14.10
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 Informix IDS
 14.10
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Informix IDS
 14.10
 64
 Solaris 11
 SPARC
 64
 9.0
 Informix IDS
 14.10
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 Informix IDS
 14.10
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Informix IDS
 12.1 [1]
 64
 AIX 7.2
 POWER
 64
 9.0
 Informix IDS
 12.1
 64
 AIX 7.1
 POWER
 64
 9.0
 Informix IDS
 12.1
 64
 HP-UX 11.31
 IA64
 64
 9.0
 Informix IDS
 12.1 [2]
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Informix IDS
 12.1
 64
 Solaris 11
 SPARC
 64
 9.0
 Informix IDS
 12.1
 64
 Solaris 11
 x86-64
 64
 9.0
 Informix IDS
 12.1 [2]
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 1. Supports Informix 12.1 FC8 and later versions.
 2. Informix 12.1 FC1 to FC5 and FC7 is supported.
 

  page 14, Database Agents, MariaDB
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 MariaDB


 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
Customers that have a license for the NetBackup for MariaDB agent can download the install package at:
<https://my.veritas.com>
 
  MariaDB - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 MariaDB
 11
 64
 Windows Server 2019
 x86-64
 64
 9.0
 MariaDB
 10
 64
 CentOS 7.0
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 MariaDB
 10
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 MariaDB
 10
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Windows 8.1
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Windows 10
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Windows Server 2016
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Windows Server 2019
 x86-64
 64
 9.0
 MariaDB
 10
 64
 Windows Server 2022
 x86-64
 64
 9.0
 

  page 15, Database Agents, Microsoft Exchange
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Microsoft Exchange


 It is the best practice of NetBackup Quality Engineering to test the latest service packs and updates for Exchange. For any Exchange service pack (SP) supported by NetBackup, each
update rollup (RU) is also supported unless otherwise noted.

Exchange Granular Recovery For Exchange GRT operations, the OS of the granular proxy host must be a version of Windows that is supported for that version of Exchange.
 Example: For Exchange 2013, the OS of the granular proxy host must be Windows 2012, Windows 2012 R2, Windows 2016, Windows 2019.
 Please refer to the Database/Application vendor for specific Operating System requirements.

 
  Microsoft Exchange - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Exchange
 2019 CU13
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU13
 64
 Windows Server 2022
 x86-64
 64
 9.0
 Exchange
 2019 CU12
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU12
 64
 Windows Server 2022
 x86-64
 64
 9.0
 Exchange
 2019 CU11
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU10
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU9
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU8
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU7
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU6
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU5
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU4
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU3
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU2
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2019 CU1
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Exchange
 2016 CU23
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU23
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU22
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU22
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU21
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU21
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU20
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU20
 64
 Windows Server 2016
 x86-64
 64
 9.0
 

  page 16, Database Agents, Microsoft Exchange, Microsoft Exchange - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 Microsoft Exchange - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Exchange
 2016 CU19
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU19
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU18
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU18
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU17
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU17
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU16
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU16
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU15
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU15
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU14
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU14
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU13
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU13
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU12
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU12
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU12
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU11
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU11
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU11
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU10
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU10
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU10
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU9
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU9
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU9
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU8
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU8
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU8
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU7
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU7
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU7
 64
 Windows Server 2016
 x86-64
 64
 9.0
 

  page 17, Database Agents, Microsoft Exchange, Microsoft Exchange - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 Microsoft Exchange - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Exchange
 2016 CU6
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU6
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU6
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU5
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU5
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU5
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU4
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU4
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU4
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Exchange
 2016 CU3
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU3
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU2
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU2
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016 CU1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016 CU1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2016
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2016
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 SP1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 SP1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU23
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU22
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU22
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU21
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU21
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU20
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU20
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU19
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU19
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU18
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU18
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU17
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU17
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 

  page 18, Database Agents, Microsoft Exchange, Microsoft Exchange - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 Microsoft Exchange - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Exchange
 2013 CU16
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU16
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU15
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU15
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU14
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU14
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU13
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU13
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU12
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU12
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU11
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU11
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU10
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU10
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU9
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU9
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU8
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU8
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU7
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU7
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU6
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU6
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU5
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013 CU5
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Exchange
 2013 CU3
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Exchange
 2013
 64
 Windows Server 2012
 x86-64
 64
 9.0
 

  page 19, Database Agents, Microsoft SharePoint
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Microsoft SharePoint


 It is the best practice of NetBackup Quality Engineering to test the latest Service Pack for SharePoint. Supported Service Packs are listed in the table below.

SharePoint Granular Recovery
All of the configurations are supported for SharePoint Server Granular Recovery unless otherwise noted.

All Microsoft SQL Server Service Packs (SP) are not explicitly qualified and are supported by default, unless noted.
AlwaysOn Availability Groups (AGs) are not supported at this time.
Refer to the Database/Application vendor for specific Operating System requirements.
 
  Microsoft SharePoint - Supported Configurations

  Database Software

  Version

  DB Bit

  SQL Server Version

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SharePoint
 2019
 64
 2016, 2017, 2019
 Windows Server 2019
 x86-64
 64
 9.0
 SharePoint
 2019
 64
 2019
 Windows Server 2022
 x86-64
 64
 9.1
 SharePoint
 2016
 64
 2014, 2016
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SharePoint
 2016
 64
 2014, 2016
 Windows Server 2016
 x86-64
 64
 9.0
 SharePoint
 2016
 64
 2016, 2017
 Windows Server 2019
 x86-64
 64
 9.0
 SharePoint
 2013 SP1
 64
 2014
 Windows Server 2012
 x86-64
 64
 9.0
 SharePoint
 2013 SP1
 64
 2012, 2014
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SharePoint
 2013 SP1
 64
 2014
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SharePoint
 2013
 64
 2012, 2014
 Windows Server 2012
 x86-64
 64
 9.0
 

  page 20, Database Agents, Microsoft SQL Server
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Microsoft SQL Server


 NetBackup supports the following SQL Server versions and editions:

 SQL Server 2019: Enterprise, Standard, Express, Developer, Web
 SQL Server 2017: Enterprise, Standard, Express, Developer, Web
 SQL Server 2016: Enterprise, Standard, Express, Developer, Web
 SQL Server 2014: Enterprise, Standard, Express, Developer, Web, BI
 SQL Server 2012: Enterprise, Standard, Express, Business, Web
Microsoft SQL Server and Windows Server Service Packs (SPs) / Cumulative Updates (CU) are not explicitly qualified and are supported by default, unless noted.
Refer to the Database/Application vendor for specific Operating System requirements.

SQL Server Intelligent Policies
 SQL Server Intelligent Policies support SQL Server cluster environments. For information on the HA solutions that SQL Server supports, refer to your SQL Server documentation.
 Supports SQL Availability Groups using SQL Server Intelligent Policies
 NetBackup does not support protection of SQL Availability Groups on multi-domain clusters and in SQL Azure environments.

SQL Server on Linux is NOT supported at this time.
 
  Microsoft SQL Server - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SQL Server
 2019
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQL Server
 2019
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SQL Server
 2019
 64
 Windows Server 2022
 x86-64
 64
 9.0
 SQL Server
 2017
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2017
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2017
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQL Server
 2017
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SQL Server
 2017
 64
 Windows Server 2022
 x86-64
 64
 9.0
 SQL Server
 2016 SP3
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQL Server
 2016 SP3
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SQL Server
 2016 SP2
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2016 SP2
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2016 SP2
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQL Server
 2016 SP2
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SQL Server
 2016 SP1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2016 SP1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2016 SP1
 64
 Windows Server 2016
 x86-64
 64
 9.0
 

  page 21, Database Agents, Microsoft SQL Server, Microsoft SQL Server - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 Microsoft SQL Server - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SQL Server
 2016
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2016
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2016
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQL Server
 2014 SP3
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2014 SP3
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2014 SP3
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQL Server
 2014 SP3
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SQL Server
 2014 SP2
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2014 SP2
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2014 SP1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2014 SP1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2014
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2014
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2012 SP4
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2012 SP4
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2012 SP4
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQL Server
 2012 SP3
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2012 SP3
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2012 SP2
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2012 SP2
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2012 SP1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2012 SP1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQL Server
 2012
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SQL Server
 2012
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 

  page 22, Database Agents, MongoDB
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 MongoDB


 MongoDB Community and Enterprise Editions are supported.
English-only MongoDB environments are supported.
NetBackup supported platforms of Red Hat Enterprise Linux and SUSE Linux Enterprise Server can be used as Backup Hosts.
MongoDB Plugin can be installed on the NetBackup Client, NetBackup Media server or NetBackup Master server to make it a Backup Host. Veritas recommends that you have media
server as a backup host.
Protection of MongoDB enviornments deployed / managed using MongoDB Ops Manager is not supported.
 
  MongoDB - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 MongoDB
 4.4
 64
 CentOS 7.0
 x86-64
 64
 9.0
 MongoDB
 4.4
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 MongoDB
 4.4
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 MongoDB
 4.4
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 MongoDB
 4.4
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 MongoDB
 4.2
 64
 CentOS 7.0
 x86-64
 64
 9.0
 MongoDB
 4.2
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 MongoDB
 4.2
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 MongoDB
 4.2
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 MongoDB
 4.2
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 MongoDB
 4.0
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 MongoDB
 4.0
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 MongoDB
 3.6
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 MongoDB
 3.6
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0

 Note the following: 
MongoDB database on LVM is supported. MongoDB database path should be mounted on LVM 
Supported MongoDB Environments- 
1. Sharded cluster running with query router (mongos) 
2. Replica sets 
3. Standalone mongod instance 
4. Multiple mongod instances on single node 
Supported Authentication types: 
- Simple 
- Certificate based 
- Cluster without any authentication enabled under mongodb is also supported(no authentication) 
Incremental backup of standalone cluster is not supported, only full backup schedule works for Standalone instance. 
MongoDB data protection with Native encryption enabled is not supported. 
NetBackup supports the XFS and ext4 file systems for backup and restore:
 

  page 23, Database Agents, MongoDB, MongoDB - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	
 NetBackup supports the MongoDBâs WiredTiger storage engine. 
English-only MongoDB environments are supported. 
NetBackup supports the Differential Incremental backup for MongoDB along with a Full Backup. Cumulative Incremental backups are not supported currently. 
Install OpenSSH packages on all the MongoDB nodes to support SSH keys. Refer NetBackupâs Admin Guide for MongoDB to know more about prerequisites and configurations.
 

  page 24, Database Agents, MySQL 
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 MySQL


 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
Customers that have a license for the NetBackup for MySQL agent can download the install package at:
<https://my.veritas.com>
 
  MySQL - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 MySQL
 8
 64
 CentOS 8
 x86-64
 64
 9.0
 MySQL
 8
 64
 CentOS 7.7
 x86-64
 64
 9.0
 MySQL
 8
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 MySQL
 8
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 MySQL
 8
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 MySQL
 8
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 MySQL
 8
 64
 Windows 10
 x86-64
 64
 9.0
 MySQL
 8
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 MySQL
 8
 64
 Windows Server 2016
 x86-64
 64
 9.0
 MySQL
 8
 64
 Windows Server 2019
 x86-64
 64
 9.0
 MySQL
 5
 64
 CentOS 7.9
 x86-64
 64
 9.0
 MySQL
 5 [1]
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 MySQL
 5 [1]
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 MySQL
 5 [1]
 64
 Windows 8.1
 x86-64
 64
 9.0
 MySQL
 5 [1]
 64
 Windows 10
 x86-64
 64
 9.0
 MySQL
 5 [1]
 64
 Windows Server 2012
 x86-64
 64
 9.0
 MySQL
 5 [1]
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 MySQL
 5 [1]
 64
 Windows Server 2016
 x86-64
 64
 9.0
 1. NetBackup 9.0 requires MySQL 5.5.5 and later versions
 

  page 25, Database Agents, Oracle Database
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Oracle Database


 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
NetBackup Copilot for Oracle is supported on all the configurations listed below unless otherwise noted.

 Oracle RAC support is implied for Oracle versions listed.
   Oracle Database - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Oracle
 21c
 64
 Oracle Linux 8
 x86-64
 64
 9.0
 Oracle
 21c
 64
 Oracle Linux 7
 x86-64
 64
 9.0
 Oracle
 21c
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 Oracle
 21c
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 Oracle
 21c
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Oracle
 21c
 64
 Windows Server 2022
 x86-64
 64
 9.0
 Oracle
 19c
 64
 AIX 7.3
 POWER
 64
 9.0
 Oracle
 19c
 64
 AIX 7.2
 POWER
 64
 9.0
 Oracle
 19c
 64
 AIX 7.1
 POWER
 64
 9.0
 Oracle
 19c
 64
 HP-UX 11.31
 IA64
 64
 9.0
 Oracle
 19c
 64
 Oracle Linux 8
 x86-64
 64
 9.0
 Oracle
 19c
 64
 Oracle Linux 7
 x86-64
 64
 9.0
 Oracle
 19c [1]
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 Oracle
 19c
 64
 Red Hat Enterprise Linux 8
 z/Architecture
 64
 9.0
 Oracle
 19c
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Oracle
 19c
 64
 Solaris 11
 SPARC
 64
 9.0
 Oracle
 19c
 64
 Solaris 11
 x86-64
 64
 9.0
 Oracle
 19c
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 Oracle
 19c
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Oracle
 19c
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Oracle
 19c
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Oracle
 19c
 64
 Windows Server 2019
 x86-64
 64
 9.0
 Oracle
 18c
 64
 AIX 7.2
 POWER
 64
 9.0
 Oracle
 18c
 64
 HP-UX 11.31
 IA64
 64
 9.0
 Oracle
 18c
 64
 Oracle Linux 7
 x86-64
 64
 9.0
 

  page 26, Database Agents, Oracle Database, Oracle Database - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 Oracle Database - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Oracle
 18c
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Oracle
 18c
 64
 Solaris 11
 SPARC
 64
 9.0
 Oracle
 18c
 64
 Solaris 11
 x86-64
 64
 9.0
 Oracle
 18c
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Oracle
 18c
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Oracle
 18c
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Oracle
 12c R2
 64
 AIX 7.2
 POWER
 64
 9.0
 Oracle
 12c R2
 64
 AIX 7.1
 POWER
 64
 9.0
 Oracle
 12c R2
 64
 HP-UX 11.31
 IA64
 64
 9.0
 Oracle
 12c R2
 64
 Oracle Linux 7
 x86-64
 64
 9.0
 Oracle
 12c R2
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Oracle
 12c R2 [2]
 64
 Red Hat Enterprise Linux 7
 z/Architecture
 64
 9.0
 Oracle
 12c R2
 64
 Solaris 11
 SPARC
 64
 9.0
 Oracle
 12c R2 [2]
 64
 Solaris 11
 x86-64
 64
 9.0
 Oracle
 12c R2
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 Oracle
 12c R2
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Oracle
 12c R2
 64
 SUSE Linux Enterprise Server 12
 z/Architecture
 64
 9.0
 Oracle
 12c R2
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Oracle
 12c R2
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Oracle
 12c R2
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Oracle
 12c R1
 64
 AIX 7.2
 POWER
 64
 9.0
 Oracle
 12c R1
 64
 AIX 7.1
 POWER
 64
 9.0
 Oracle
 12c R1
 64
 HP-UX 11.31
 IA64
 64
 9.0
 Oracle
 12c R1
 64
 Oracle Linux 7
 x86-64
 64
 9.0
 Oracle
 12c R1
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Oracle
 12c R1
 64
 Solaris 11
 SPARC
 64
 9.0
 Oracle
 12c R1
 64
 Solaris 11
 x86-64
 64
 9.0
 Oracle
 12c R1
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 Oracle
 12c R1 [2]
 64
 SUSE Linux Enterprise Server 12
 z/Architecture
 64
 9.0
 Oracle
 12c R1
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 Oracle
 12c R1
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Oracle
 11g R2
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 

  page 27, Database Agents, Oracle Database, Oracle Database - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 Oracle Database - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Oracle
 11g R2
 64
 AIX 7.2
 POWER
 64
 9.0
 Oracle
 11g R2
 64
 AIX 7.1
 POWER
 64
 9.0
 Oracle
 11g R2
 64
 HP-UX 11.31
 IA64
 64
 9.0
 Oracle
 11g R2
 64
 Oracle Linux 7
 x86-64
 64
 9.0
 Oracle
 11g R2
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 Oracle
 11g R2
 64
 Solaris 11
 SPARC
 64
 9.0
 Oracle
 11g R2 [2]
 64
 Solaris 11
 x86-64
 64
 9.0
 Oracle
 11g R2
 64
 Windows Server 2012
 x86-64
 64
 9.0
 Oracle
 11g R1
 64
 HP-UX 11.31
 IA64
 64
 9.0
 1. Oracle database version 19.8 and later versions is recommended.
 2. This architecture does not support the ""Whole Database - Datafile Copy Share"" option.
 

  page 28, Database Agents, PostgreSQL
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 PostgreSQL


 NetBackup supports PostgreSQL and EDB Postgresâ¢ Advanced Server. Refer to Database/Application vendor for specific Operating System requirements.
Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise.
Customers that have a license for the NetBackup for PostgreSQL agent can download the install package at:
<https://my.veritas.com>
 
  PostgreSQL - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 PostgreSQL
 14
 64
 GNU/Linux 11
 x86-64
 64
 9.0
 PostgreSQL
 14
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 PostgreSQL
 14
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 PostgreSQL
 14
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 PostgreSQL
 14
 64
 Windows Server 2016
 x86-64
 64
 9.0
 PostgreSQL
 14
 64
 Windows Server 2019
 x86-64
 64
 9.0
 PostgreSQL
 14
 64
 Windows Server 2022
 x86-64
 64
 9.0
 PostgreSQL
 13
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 PostgreSQL
 13
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 PostgreSQL
 13
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 PostgreSQL
 13
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 PostgreSQL
 13
 64
 Windows Server 2016
 x86-64
 64
 9.0
 PostgreSQL
 13
 64
 Windows Server 2019
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 CentOS 7.6
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 Windows Server 2016
 x86-64
 64
 9.0
 PostgreSQL
 12
 64
 Windows Server 2019
 x86-64
 64
 9.0
 PostgreSQL
 11
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 PostgreSQL
 11
 64
 Windows 10
 x86-64
 64
 9.0
 PostgreSQL
 11
 64
 Windows Server 2016
 x86-64
 64
 9.0
 PostgreSQL
 11
 64
 Windows Server 2019
 x86-64
 64
 9.0
 

  page 29, Database Agents, PostgreSQL, PostgreSQL - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 PostgreSQL - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 PostgreSQL
 10
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 PostgreSQL
 10
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 PostgreSQL
 10
 64
 Windows 8.1
 x86-64
 64
 9.0
 PostgreSQL
 10
 64
 Windows 10
 x86-64
 64
 9.0
 PostgreSQL
 10
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 PostgreSQL
 10
 64
 Windows Server 2016
 x86-64
 64
 9.0
 PostgreSQL
 9
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 PostgreSQL
 9
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 PostgreSQL
 9
 64
 Windows 8.1
 x86-64
 64
 9.0
 PostgreSQL
 9
 64
 Windows 10
 x86-64
 64
 9.0
 PostgreSQL
 9
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 PostgreSQL
 9
 64
 Windows Server 2016
 x86-64
 64
 9.0
 Note the following:
 NetBackup does not support cluster aware backups / restore of PostgreSQL and EDB Postgresâ¢ Advanced Server.
 

  page 30, Database Agents, SAP ASE
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 SAP ASE

   SAP ASE - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SAP ASE
 16 SP4
 64
 AIX 7.3
 POWER
 64
 9.0
 SAP ASE
 16 SP4
 64
 AIX 7.2
 POWER
 64
 9.0
 SAP ASE
 16 SP4
 64
 HP-UX 11.31
 IA64
 64
 9.0
 SAP ASE
 16 SP4
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SAP ASE
 16 SP4
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP ASE
 16 SP4
 64
 Solaris 11
 SPARC
 64
 9.0
 SAP ASE
 16 SP4
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP ASE
 16 SP4
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP ASE
 16 SP4
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SAP ASE
 16 SP4
 64
 Windows Server 2022
 x86-64
 64
 9.0
 SAP ASE
 16 SP3
 64
 AIX 7.2
 POWER
 64
 9.0
 SAP ASE
 16 SP3
 64
 HP-UX 11.31
 IA64
 64
 9.0
 SAP ASE
 16 SP3
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SAP ASE
 16 SP3
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP ASE
 16 SP3
 64
 Solaris 11
 SPARC
 64
 9.0
 SAP ASE
 16 SP3
 64
 Solaris 11
 x86-64
 64
 9.0
 SAP ASE
 16 SP3
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP ASE
 16 SP3
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP ASE
 16 SP3
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SAP ASE
 16 SP3
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SAP ASE
 16 SP2
 64
 AIX 7.2
 POWER
 64
 9.0
 SAP ASE
 16 SP2
 64
 AIX 7.1
 POWER
 64
 9.0
 SAP ASE
 16 SP2
 64
 Solaris 11
 SPARC
 64
 9.0
 SAP ASE
 16
 64
 HP-UX 11.31
 IA64
 64
 9.0
 SAP ASE
 16
 64
 Solaris 11
 x86-64
 64
 9.0
 SAP ASE
 16
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP ASE
 16
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SAP ASE
 16
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 

  page 31, Database Agents, SAP ASE, SAP ASE - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 SAP ASE - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SAP ASE
 16
 64
 Windows Server 2016
 x86-64
 64
 9.0
 

  page 32, Database Agents, SAP HANA
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 SAP HANA


 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise.
SAP S/4 HANA and SAP Business One HANA products are supported with existing SAP HANA Backint interface used by NetBackup. Please refer below matrix for platform
compatibility with SAP HANA database.
Refer to Database/Application vendor for specific Operating System requirements.
 
  SAP HANA - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SAP HANA
 SPS 12
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP HANA
 SPS 12
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA
 SPS 11
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 07
 64
 Red Hat Enterprise Linux 8
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 07
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 07
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 07
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 06
 64
 Red Hat Enterprise Linux 8
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 06
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 06
 64
 SUSE Linux Enterprise Server 15
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 06
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 06
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 05 [1]
 64
 Red Hat Enterprise Linux 8
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 05
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 05
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 05
 64
 SUSE Linux Enterprise Server 15
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 05
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 05
 64
 SUSE Linux Enterprise Server 12
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 05
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 04 [2]
 64
 Red Hat Enterprise Linux 8
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 04
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 04
 64
 Red Hat Enterprise Linux 7
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 04
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 04
 64
 SUSE Linux Enterprise Server 15
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 04
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 

  page 33, Database Agents, SAP HANA, SAP HANA - Supported Configurations
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
 	 SAP HANA - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SAP HANA 2.0
 SPS 04
 64
 SUSE Linux Enterprise Server 12
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 04
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 03
 64
 Red Hat Enterprise Linux 7
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 03
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 03
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 03
 64
 SUSE Linux Enterprise Server 12
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 03
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 02
 64
 Red Hat Enterprise Linux 7
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 02
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 02
 64
 SUSE Linux Enterprise Server 12
 POWER
 64
 9.0
 SAP HANA 2.0
 SPS 02
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 01
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 01
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP HANA 2.0
 SPS 00
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 1. EEB for NetBackup 9.0, 9.0.0.1 and 9.1 can be obtained from Veritas Technical Support.
 2. Support for SAP HANA 2.0 SPS 04 on Redhat Enterprise Linux 8.0 is limited to IBM Power 9
 

  page 34, Database Agents, SAP MaxDB
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 SAP MaxDB

 The ""Version"" on SAP Max-DB represents the version of the SAP Kernel.

 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
   SAP MaxDB - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SAP MAXDB
 7
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 AIX 7.2
 POWER
 64
 9.0
 SAP MaxDB
 7
 64
 AIX 7.1
 POWER
 64
 9.0
 SAP MaxDB
 7
 64
 HP-UX 11.31
 IA64
 64
 9.0
 SAP MaxDB
 7
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 Solaris 11
 SPARC
 64
 9.0
 SAP MaxDB
 7
 64
 Solaris 11
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SAP MaxDB
 7
 64
 Windows Server 2016
 x86-64
 64
 9.0
 

  page 35, Database Agents, SAP Oracle
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 SAP Oracle

 The ""Version"" on SAP Oracle represents the version of the SAP Kernel BR*Tools. BR*Tools version 6.40, patch level 36 or greater, is required for SAP Snapshot backups through
RMAN proxy.

 SAP Oracle agent includes Oracle RAC support with Oracle Cluster File System and RAW partition.

 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
   SAP Oracle - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SAP Oracle
 7
 64
 AIX 7.2
 POWER
 64
 9.0
 SAP Oracle
 7
 64
 AIX 7.1
 POWER
 64
 9.0
 SAP Oracle
 7
 64
 HP-UX 11.31
 IA64
 64
 9.0
 SAP Oracle
 7
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SAP Oracle
 7
 64
 Solaris 11
 SPARC
 64
 9.0
 SAP Oracle
 7
 64
 Solaris 11
 x86-64
 64
 9.0
 SAP Oracle
 7
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SAP Oracle
 7
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SAP Oracle
 7
 64
 Windows Server 2012
 x86-64
 64
 9.0
 SAP Oracle
 7
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SAP Oracle
 7
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SAP Oracle
 7
 64
 Windows Server 2019
 x86-64
 64
 9.0
 

  page 36, Database Agents, SQLite
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 SQLite


 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
Customers that have a license for the NetBackup for SQLite agent can download the install package at:
<https://my.veritas.com>
 
  SQLite - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 SQLite
 3
 64
 Red Hat Enterprise Linux 8
 x86-64
 64
 9.0
 SQLite
 3
 64
 Red Hat Enterprise Linux 7
 x86-64
 64
 9.0
 SQLite
 3
 64
 SUSE Linux Enterprise Server 15
 x86-64
 64
 9.0
 SQLite
 3
 64
 SUSE Linux Enterprise Server 12
 x86-64
 64
 9.0
 SQLite
 3
 64
 Windows 8.1
 x86-64
 64
 9.0
 SQLite
 3
 64
 Windows 10
 x86-64
 64
 9.0
 SQLite
 3
 64
 Windows Server 2012 R2
 x86-64
 64
 9.0
 SQLite
 3
 64
 Windows Server 2016
 x86-64
 64
 9.0
 SQLite
 3
 64
 Windows Server 2019
 x86-64
 64
 9.0
 SQLite
 3
 64
 Windows Server 2022
 x86-64
 64
 9.0
 

  page 37, Database Agents, Sybase ASE
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 Sybase ASE

 Minor versions, Service Packs (SP), and patches released subsequent are supported by default, unless noted otherwise. Refer to Database/Application vendor for specific Operating
System requirements.
   Sybase ASE - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 Sybase ASE
 12.5
 64
 HP-UX 11.31
 IA64
 64
 9.0
 

  page 38, Database Agents, XBSA Extensible Client
	Â© 2023 Veritas Technologies LLC / 2023-09-14
	
Veritas NetBackup â¢ 9.0 - 9.x.x Database and Application Agent Compatibility List
	 XBSA Extensible Client

   XBSA Extensible Client - Supported Configurations

  Database Software

  Version

  DB Bit

  OS

  CPU Architecture

  OS Bit

  Start of Support

 XBSA Extensible Client
 NetBackup 7.5.0.6
 64
 Windows Server 2012
 x86-64
 64
 9.0
 XBSA Extensible Client
 NetBackup 7.0
 64
 HP-UX 11.31
 IA64
 64
 9.0
 

"
Non,"Load Balancing Kofax Equitrac 
Version 1.3.0   

Table of Contents
1. About this Guide	. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 3
2. Loadbalancer.org Appliances Supported	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . 	Â 3
3. Software Versions Supported	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . 	Â 3
3.1. Loadbalancer.org Appliance	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . 	Â 3
3.2. Kofax Equitrac	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 3
4. Kofax Equitrac	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 3
5. Load Balancing Kofax Equitrac	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . 	Â 3
5.1. Introduction and Overview of Different Modes	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . 	Â 3
5.2. Prerequisites	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 4
5.3. Overview of steps required	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . 	Â 4
6. Loadbalancer.org Appliance â the Basics	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . 	Â 4
6.1. Virtual Appliance	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 4
6.2. Initial Network Configuration	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . 	Â 5
6.3. Accessing the Appliance WebUI	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . 	Â 5
Main Menu Options	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 6
6.4. Appliance Software Update	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . 	Â 7
Determining the Current Software Version	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . 	Â 7
Checking for Updates using Online Update	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . 	Â 7
Using Offline Update	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 7
6.5. Ports Used by the Appliance	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . 	Â 8
6.6. HA Clustered Pair Configuration	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . 	Â 9
7. Appliance Configuration for Kofax Equitrac â Using DR Mode	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 9
7.1. Configuring the virtual service (VIP)	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . 	Â 9
7.2. Defining the Real Servers (RIPs)	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . 	Â 10
8. Appliance Configuration for Kofax Equitrac â Using SNAT Mode	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 10
8.1. Configuring the virtual service (VIP)	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . 	Â 10
8.2. Defining the Real Servers (RIPs)	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . 	Â 11
9. Configuring Print Servers for Load Balancing	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . 	Â 11
9.1. Registry Modifications	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . 	Â 12
Microsoft Windows Server 2008 Specific Registry Change	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 12
9.2. Configuring Name Resolution	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . 	Â 13
DNS Name Resolution (Windows 2000 & later)	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . 	Â 13
NetBIOS Name Resolution (legacy Environments)	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 13
9.3. Finalising the Server Configuration	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . 	Â 14
10. Testing & Verification	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 14
10.1. Installing and Configuring Couchbase and Equitrac DCE	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 15
11. Technical Support	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 15
12. Further Documentation	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . 	Â 15
13. Appendix	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 16
13.1. Solving the ARP Problem	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . 	Â 16
Windows Server 2012 & Later	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . 	Â 16
13.2. Configuring HA - Adding a Secondary Appliance	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 21
Non-Replicated Settings	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . 	Â 21
Adding a Secondary Appliance - Create an HA Clustered Pair	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 	Â 22
14. Document Revision History	
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . 	Â 24 

1. About this Guide
This guide details the steps required to configure a load balanced Kofax Equitrac environment utilizing 
Loadbalancer.org appliances. It covers the configuration of the load balancers and also any Kofax Equitrac
configuration changes that are required to enable load balancing. 
For more information about initial appliance deployment, network configuration and using the Web User Interface 
(WebUI), please also refer to the 	
Administration Manual	.	
2. Loadbalancer.org Appliances Supported
All our products can be used for load balancing Kofax Equitrac. For full specifications of available models please 
refer to 	
https://www.loadbalancer.org/products	. Some features may not be supported in all cloud platforms due
to platform specific limitations, please check with Loadbalancer.org support for further details.	
3. Software Versions Supported 
3.1. Loadbalancer.org Appliance
ï	
V8.4.1 and later	
The screenshots used throughout this document aim to track the latest Loadbalancer.org 
software version. If using an older software version, note that the screenshots presented here
may not match the WebUI exactly.	
3.2. Kofax Equitracï	
All versions	
4. Kofax Equitrac
Kofax Equitrac is a print management solution designed to simplify printer management. 
Printing costs can be monitored, and can be reduced by forcing users to follow budget saving printing habits. 
Secure and regulations-compliant printing is made possible by allowing users to 'pick up' and print their secure
documents in person at any printer. Flexible printing is achieved as users can print from anywhere, at anytime,
and print from wherever they like.
5. Load Balancing Kofax Equitrac 
5.1. Introduction and Overview of Different Modes
This guide details the configuration of a high availability DCE cluster for Equitrac Office and Express, using a 
Loadbalancer.org appliance. 
For a Kofax Equitrac deployment, the preferred and default load balancer configuration uses Layer 4 DR Mode 
(Direct Routing, aka DSR / Direct Server Return). This is a very high performance solution that requires little	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 3   

change to your existing infrastructure. It is necessary to solve ""the ARP problem"" on the real print servers. This is 
a straightforward process, and is detailed in the section 	
Configuring Print Servers for Load Balancing	.
It is also possible to load balance a Kofax Equitrac deployment using Layer 7 SNAT Mode. This mode might be 
preferable if making changes to the real print servers is not possible, although some Windows Registry keys need
to be added. Due to the increased amount of information at layer 7, performance is not as fast as at layer 4. Also
note that load balanced connections at layer 7 are not source IP transparent, which is not usually an issue when
load balancing print servers but should still be considered.	
5.2. Prerequisites
A load balanced Kofax Equitrac environment requires the following:	
ïMicrosoft Windows Server environment
ïInstallation of DCE server and Couchbase in High Availability setup
For installation instructions, refer to the Kofax support.	
5.3. Overview of steps required
Setting up a load balanced Kofax Equitrac environment can be summarised as follows:	
ïCreate a virtual service (VIP) on the load balancer that listens on the required ports
ïAssociate the print servers to the virtual service, i.e. define them as 'real servers' (RIPs) for the VIP
ïInstall and configure the Kofax Equitrac DCE Windows print servers
ïConfigure registry settings on the print servers to enable them to be accessed via a shared name
ïConfigure name resolution related settings on the print servers
ïPoint users at the VIP to access the print server and the printer shares	
6. Loadbalancer.org Appliance â the Basics 
6.1. Virtual Appliance
A fully featured, fully supported 30 day trial is available if you are conducting a PoC (Proof of Concept) 
deployment. The VA is currently available for VMware, Virtual Box, Hyper-V, KVM, XEN and Nutanix AHV and has
been optimized for each Hypervisor. By default, the VA is allocated 2 vCPUs, 4GB of RAM and has a 20GB virtual
disk. The Virtual Appliance can be downloaded 	
here	.	
The same download is used for the licensed product, the only difference is that a license key file 
(supplied by our sales team when the product is purchased) must be applied using the
applianceâs WebUI.
Please refer to 	Virtual Appliance Installation	 and the ReadMe.txt text file included in the VA
download for additional information on deploying the VA using the various Hypervisors.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 4    

The VA has 4 network adapters. For VMware only the first adapter (eth0) is connected by
default. For HyperV, KVM, XEN and Nutanix AHV all adapters are disconnected by default. Use 
the network configuration screen within the Hypervisor to connect the required adapters.	
6.2. Initial Network Configuration
After boot up, follow the instructions on the appliance console to configure the management IP address, subnet 
mask, default gateway, DNS Server and other network settings.	
Be sure to set a secure password for the load balancer, when prompted during the setup routine.	
6.3. Accessing the Appliance WebUI
The WebUI is accessed using a web browser. By default, users are authenticated using Apache authentication. 
Users can also be authenticated against LDAP, LDAPS, Active Directory or Radius - for more information, please
refer to 	
External Authentication	.	
There are certain differences when accessing the WebUI for the cloud appliances. For details, 
please refer to the relevant 	
Quick Start / Configuration Guide	.	
A number of compatibility issues have been found with various versions of Microsoft Internet 
Explorer and Edge. The WebUI has been tested and verified using both Chrome & Firefox.	
1.	Using a browser, navigate to the following URL: 
https://<IP-address-configured-during-the-network-setup-wizard>:9443/lbadmin/	
Youâll receive a warning about the WebUIâs certificate. This is due to the default self signed 
certificate that is used. If preferred, you can upload your own certificate - for more
information, please refer to 	
Appliance Security Features	.	
2.	Log in to the WebUI using the following credentials: 
Username : loadbalancer
Password : <configured-during-network-setup-wizard>	
To change the password, use the WebUI menu option:  Maintenance > Passwords.
Once logged in, the WebUI will be displayed as shown below:	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 5        

3.	Youâll be asked if you want to run the Setup Wizard. Click Dismiss if youâre following a guide or want to
configure the appliance manually. Click  Accept to start the Setup Wizard.	
The Setup Wizard can only be used to configure Layer 7 services.	
Main Menu Options
System Overview  - Displays a graphical summary of all VIPs, RIPs and key appliance statistics
Local Configuration  - Configure local host settings such as IP address, DNS, system time etc.
Cluster Configuration  - Configure load balanced services such as VIPs & RIPs
Maintenance  - Perform maintenance tasks such as service restarts and taking backups
View Configuration  - Display the saved appliance configuration settings
Reports  - View various appliance reports & graphs
Logs  - View various appliance logs
Support  - Create a support download, contact the support team & access useful links	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 6    

Live Chat - Start a live chat session with one of our Support Engineers
6.4. Appliance Software Update
To ensure that the appliance(s) are running the latest software version, we recommend a software update check 
is performed.
Determining the Current Software Version
The software version is displayed at the bottom of the WebUI as shown in the example below:
Checking for Updates using Online Update	
By default, the appliance periodically contacts the Loadbalancer.org update server and checks 
for updates. An update check can also be manually triggered as detailed below.	
1.	Using the WebUI, navigate to:  Maintenance > Software Update .	
2.	Select Online Update .	
3.	If the latest version is already installed, a message similar to the following will be displayed:	
4.	If an update is available, youâll be presented with a list of new features, improvements, bug fixes and security 
related updates.	
5.	Click Online Update  to start the update process.	
Do not navigate away whilst the update is ongoing, this may cause the update to fail.	
6.	Once complete (the update can take several minutes depending on download speed and upgrade version) 
the following message will be displayed:	
7.	If services need to be reloaded/restarted or the appliance needs a full restart, youâll be prompted accordingly.	
Using Offline Update
If the load balancer does not have access to the Internet, offline update can be used.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 7       

Please contact 	support@loadbalancer.org	 to check if an update is available and obtain the latest
offline update files.
To perform an offline update:	
1.	Using the WebUI, navigate to:  Maintenance > Software Update .	
2.	Select Offline Update .	
3.	The following screen will be displayed:	
4.	Select the Archive and Checksum  files.	
5.	Click Upload and Install .	
6.	If services need to be reloaded/restarted or the appliance needs a full restart, youâll be prompted accordingly.	
6.5. Ports Used by the Appliance
By default, the appliance uses the following TCP & UDP ports:
Protocol	Port	Purpose	
TCP	22	SSH	
TCP & UDP	53	DNS	
TCP & UDP	123	NTP	
TCP & UDP	161	SNMP	
UDP	6694	Heartbeat between Primary & Secondary appliances in HA mode	
TCP	7778	HAProxy persistence table replication	
TCP	9080	WebUI - HTTP (disabled by default)	
TCP	9081	Nginx fallback page	
TCP	9443	WebUI - HTTPS	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 8    

6.6. HA Clustered Pair Configuration
Loadbalancer.org recommend that load balancer appliances are deployed in pairs for high availability. In this 
guide a single unit is deployed first, adding a secondary unit is covered in 	
Configuring HA - Adding a Secondary
Appliance	
.	
7. Appliance Configuration for Kofax Equitrac â Using DR 
Mode 
7.1. Configuring the virtual service (VIP)
1.	
Using the web user interface, navigate to  Cluster Configuration > Layer 4 â Virtual Services  and click on Add
a new Virtual Service .	
2.	Define the Label for the virtual service as required, e.g.  EQDCEHA.	
3.	Set the Virtual Service IP Address  field to the required IP address, e.g.  10.10.10.190.	
4.	Set the Ports as needed, depending on your MFP vendor:	
ïFor Lexmark and Ricoh, use port  2939.
ïFor HP OXPd, use ports  2939 and 7627 .	
5.	Click  Update  to create the virtual service.	
6.	Click Modify  next to the newly created VIP.	
7.	Make sure that the  Persistent checkbox is not selected.	
8.	Set the Check Port  for server/service online to  2939.	
9.	Click  Update .	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 9   

7.2. Defining the Real Servers (RIPs)1.	
Using the web user interface, navigate to  Cluster Configuration > Layer 4 â Real Servers  and click on Add a
new Real Server  next to the newly created VIP.	
2.	Define the Label for the real server as required, e.g.  DCE1.	
3.	Set the  Real Server IP Address  field to the required IP address, e.g.  192.168.100.20.	
4.	Click Update .	
5.	Repeat these steps to add additional print servers as required.	
8. Appliance Configuration for Kofax Equitrac â Using 
SNAT Mode 
8.1. Configuring the virtual service (VIP)
1.	
Using the web user interface, navigate to  Cluster Configuration > Layer 7 â Virtual Services  and click on Add
a new Virtual Service .	
2.	Define the Label for the virtual service as required, e.g.  PrintService.	
3.	Set the Virtual Service IP Address  field to the required IP address, e.g.  192.168.10.10.	
4.	Set the Ports to 445 .	
5.	Set the  Layer 7 Protocol  to TCP Mode .	
6.	Click Update .	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 10   

8.2. Defining the Real Servers (RIPs)1.	
Using the web user interface, navigate to  Cluster Configuration > Layer 7 â Real Servers  and click on Add a
new Real Server  next to the newly created VIP.	
2.	Define the Label for the real server as required, e.g.  DCE1.	
3.	Set the  Real Server IP Address  field to the required IP address, e.g.  192.168.10.20.	
4.	Leave the Real Server Port  field blank.	
5.	Click Update .	
6.	Repeat these steps to add additional print servers as required.	
7.	Click on  Reload HAProxy  when prompted to do so in the ""Commit changes"" box that appears. This will apply
the new changes and put the new virtual service and its associated virtual servers into use.	
9. Configuring Print Servers for Load Balancing
The following steps should be carried out on each print server defined in the virtual service:	
1.	Join the server to the same domain as the client PCs.	
2.	Install the  Print and Document Service  role / Print Server  service.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 11    

3.	Install and share the printers (use exactly the same share names and permissions across all servers).	
4.	If DR mode is used, solve the ""ARP problem"" on each print server, to that DR mode will work. For detailed 
steps on solving the ARP problem for the various versions of Windows, please refer to 	
Solving the ARP
Problem	
 for more information.	
When configuring the Loopback Adapter to solve the ARP Problem, the following options  must
also be checked (ticked):
Client for Microsoft Networks & File & Printer Sharing for Microsoft Networks	
9.1. Registry Modifications
To enable the print servers to be accessed via a shared name ( EQDCEHA in the example virtual service in this
guide), add the following registry entries to each print server:
Key: HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Lsa Value: DisableLoopbackCheckType: REG_DWORDData: 1
Key: HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\lanmanserver\parameters Value: DisableStrictNameCheckingType: REG_DWORDData: 1
Key: HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\lanmanserver\parameters Value: OptionalNamesType: REG_MULTI_SZData: EQDCEHA	
In the example presented here, EQDCEHA is the name that will be used to access the load 
balanced print servers via the virtual service (VIP) created on the load balancer. This can be set
to any appropriate name. Whatever name is used, it must resolve to the IP address of the VIP as
explained in the section below.	
Microsoft Windows Server 2008 Specific Registry Change
If Microsoft Windows Server 2008 is used as the operating system for the printer servers, an additional registry 
entry change is required. The following registry entry should be changed from a DWORD to a QWORD:
Key: HKLM\SYSTEM\CurrentControlSet\Control\Print\DNSOneWire Value: DnsOnWireType: REG_QWORDData: 1	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 12    

9.2. Configuring Name Resolution
For printer load balancing to work, either DNS or NetBIOS name resolution should be configured as detailed
below.
DNS Name Resolution (Windows 2000 & later)
To configure DNS name resolution, the following steps should be completed:	
1.	NetBIOS over TCP/IP should be disabled on  all interfaces of  each print server, as shown here:	
2.	A host name and corresponding ""Host (A)"" record for the virtual DCE that matches the virtual IP (VIP) 
address for the load balancer should be created.
When configuring printers to connect back to the highly available DCE, the DCE hostname / IP address should be
the VIP address and not the individual DCE host name or IP address.	
NetBIOS Name Resolution (legacy Environments)
To configure NetBIOS name resolution, the following steps should be completed:	
1.	NetBIOS over TCP/IP should be  disabled on the main NIC and left enabled on the Loopback adapter  on
each  print server.	
2.	Either a WINS server should be set up and all clients configured to use this,  or pre-loaded entries in the
LMHosts file of each client should be set up.	
As shown in the flow chart in 	this Technet article	, for a default H-node client, NetBIOS name	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 13    

resolution occurs in the following order:	
1.	Local NetBIOS cache.	
2.	WINS server.	
3.	NetBIOS broadcast.	
4.	Local LMHosts file.
Therefore, to avoid broadcast, LMHost entries must be declared as pre-loaded to ensure they 
are available in the local NetBIOS cache.	
Configuring the LMHosts file
This is done by creating an entry like so:
EQDCEHA 192.168.100.10 #PRE
Entries with the #PRE directive are loaded into the cache on reboot, or can be forced using the command:
nbtstat -R
The following command can be used to view the cache and verify that the entry has been added:
nbtstat -c
9.3. Finalising the Server Configuration
To finalise the print server configuration changes,  each print server must be rebooted.
10. Testing & Verification	
For additional guidance on diagnosing and resolving any issues you may have, please also refer to 	
Diagnostics & Troubleshooting	.
The load balanced print service can be tested, either by browsing to the virtual service IP address or the share 
name. In the example presented in this document, this would be done by going to	
\10.10.10.190
or
\EQDCEHA
Any shared printers and shared folders that have been configured on the real print servers should be visible.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 14   

10.1. Installing and Configuring Couchbase and Equitrac DCE
The Couchbase and Equitrac DCE software should be set up by following the steps outlined in the installation 
document. To obtain a copy, reach out to Kofax support.
11. Technical Support
For more details about configuring the appliance and assistance with designing your deployment please donât 
hesitate to contact the support team using the following email address: 	
support@loadbalancer.org	.	
12. Further Documentation
For additional information, please refer to the 	Administration Manual	.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac15  

13. Appendix 
13.1. Solving the ARP Problem
Windows Server 2012 & Later
Windows Server 2012 and later support Direct Routing (DR) mode through the use of the Microsoft Loopback 
Adapter that must be installed and configured on each load balanced (Real) Server. The IP address configured on
the Loopback Adapter must be the same as the Virtual Service (VIP) address. This enables the server to receive
packets that have their destination set as the VIP address. If a Real Server is included in multiple DR mode VIPs,
an IP address for each VIP must be added to the Loopback Adapter. 
In addition, steps must be taken to set the strong/weak host behavior on each Real Server. This is used to either 
prevent or allow interfaces to receive packets destined for a different interface on the same server.	
The following 3 steps must be completed on all Real Servers associated with the VIP.	
Step 1 of 3: Install the Microsoft Loopback Adapter	
1.	Click  Start, then run  hdwwiz to start the Hardware Installation Wizard.	
2.	Once the Wizard has started, click  Next.	
3.	Select  Install the hardware that I manually select from a list (Advanced) , click Next.	
4.	Select  Network adapters , click Next.	
5.	Select  Microsoft  & Microsoft KM-Test Loopback Adapter , click Next.	
6.	Click  Next to start the installation, when complete click  Finish.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 16    

Step 2 of 3: Configure the Loopback Adapter	
1.	Open Control Panel and click Network and Sharing Center.	
2.	Click Change adapter settings .	
3.	Right-click the new Loopback Adapter and select  Properties.	
You can configure IPv4 or IPv6 addresses or both depending on your requirements.
when configuring the loopback adapter properties, make sure that  Client for Microsoft
Networks  and File & Printer Sharing for Microsoft Networks  is also checked as shown below.
IPv4 Addresses	
1.	Uncheck all items except  Client for Microsoft Networks , File & Printer Sharing for Microsoft Networks  and
Internet Protocol Version 4 (TCP/IPv4)  as shown below:	
2.	Ensure that Internet Protocol Version (TCP/IPv4)  is selected, click Properties and configure the IP address
to be the same as the Virtual Service address (VIP) with a subnet mask of  255.255.255.255, e.g.
192.168.2.20/255.255.255.255  as shown below:	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 17     

192.168.2.20 is an example, make sure you specify the correct VIP address.
If a Real Server is included in multiple DR mode VIPs, an IP address for each VIP must be 
added to the Loopback Adapter.	
3.	Click OK then click  Close to save and apply the new settings.
IPv6 Addresses	
1.	Uncheck all items except  Client for Microsoft Networks , File & Printer Sharing for Microsoft Networks  and
Internet Protocol Version 6 (TCP/IPv6)  as shown below:	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 18     

2.	Ensure that Internet Protocol Version (TCP/IPv6)  is selected, click Properties and configure the IP address
to be the same as the Virtual Service (VIP) and set the  Subnet Prefix Length to be the same as your network
setting, e.g.  2001:470:1f09:e72::15/64  as shown below:	
2001:470:1f09:e72::15/64 is an example, make sure you specify the correct VIP address.
If a Real Server is included in multiple DR mode VIPs, an IP address for each VIP must be	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 19      

added to the Loopback Adapter.	
3.	Click OK then click  Close to save and apply the new settings.	
Step 3 of 3: Configure the strong/weak host behavior
The strong/weak host behavior can be configured using either of the following 2 methods:	
ïOption 1 - Using Network Shell (netsh) commands
ïOption 2 - Using PowerShell cmdlets
The commands in this section assume that the LAN Adapter is named "" net"" and the Loopback Adapter is named
"" loopback "" as shown in the example below:	
Either adjust the commands to use the names allocated to your LAN and loopback adapters, or 
rename the adapters before running the commands. Names are case sensitive so make sure
that the interface names used in the commands match the adapter names exactly.
Option 1 - Using Network Shell (netsh) Commands 
To configure the correct strong/weak host behavior run the following commands:
For IPv4 addresses:	
netsh interface ipv4 set interface ""net"" weakhostreceive=enabled netsh interface ipv4 set interface ""loopback"" weakhostreceive=enablednetsh interface ipv4 set interface ""loopback"" weakhostsend=enabled
For IPv6 addresses:
netsh interface ipv6 set interface ""net"" weakhostreceive=enabled netsh interface ipv6 set interface ""loopback"" weakhostreceive=enablednetsh interface ipv6 set interface ""loopback"" weakhostsend=enablednetsh interface ipv6 set interface ""loopback"" dadtransmits=0
Option 2 - Using PowerShell Cmdlets 
For IPv4 addresses:	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 20    

Set-NetIpInterface -InterfaceAlias loopback -WeakHostReceive enabled -WeakHostSend enabled -DadTransmits 0 -AddressFamily IPv4
Set-NetIpInterface -InterfaceAlias net -WeakHostReceive enabled -AddressFamily IPv4
For IPv6 Addresses:
Set-NetIpInterface -InterfaceAlias loopback -WeakHostReceive enabled -WeakHostSend enabled -DadTransmits 0 -AddressFamily IPv6
Set-NetIpInterface -InterfaceAlias net -WeakHostReceive enabled -AddressFamily IPv6
13.2. Configuring HA - Adding a Secondary Appliance
Our recommended configuration is to use a clustered HA pair of load balancers to provide a highly available and 
resilient load balancing solution. 
We recommend that the Primary appliance is configured first and then the Secondary should be added. Once the 
Primary and Secondary are paired, all load balanced services configured on the Primary are automatically
replicated to the Secondary over the network using SSH/SCP.	
For Enterprise Azure, the HA pair should be configured first. In Azure, when creating a VIP using 
an HA pair, 2 private IPs must be specified â one for the VIP when itâs active on the Primary and
one for the VIP when itâs active on the Secondary. Configuring the HA pair first, enables both IPs
to be specified when the VIP is created.
The clustered HA pair uses Heartbeat to determine the state of the other appliance. Should the active device 
(normally the Primary) suffer a failure, the passive device (normally the Secondary) will take over.	
Non-Replicated Settings
A number of settings are not replicated as part of the Primary/Secondary pairing process and therefore must be 
manually configured on the Secondary appliance. These are listed by WebUI menu option in the table below:
WebUI Main Menu 
Option	Sub Menu Option	Description	
Local Configuration	Hostname & DNS	Hostname and DNS settings	
Local Configuration	Network Interface 
Configuration	All network settings including IP address(es), bonding 
configuration and VLANs	
Local Configuration	Routing	Routing configuration including default gateways and static routes	
Local Configuration	System Date & time	All time and date related settings	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 21   

WebUI Main Menu 
Option	Sub Menu Option	Description	
Local Configuration	Physical â Advanced 
Configuration	Various settings including Internet Proxy, Management Gateway, 
Firewall connection tracking table size, NIC offloading, SMTP relay,
logging and Syslog Server	
Local Configuration	Security	Appliance security settings	
Local Configuration	SNMP Configuration	Appliance SNMP settings	
Local Configuration	Graphing	Appliance graphing settings	
Local Configuration	License Key	Appliance licensing	
Maintenance	Software Updates	Appliance software update management	
Maintenance	Firewall Script	Appliance firewall (iptables) configuration	
Maintenance	Firewall Lockdown 
Wizard	Appliance management lockdown settings	
Make sure that if these settings/updates have been configured on the Primary appliance, theyâre 
also configured on the Secondary appliance.	
Adding a Secondary Appliance - Create an HA Clustered Pair	
If you have already run the firewall lockdown wizard on either appliance, youâll need to ensure 
that it is temporarily disabled on both appliances whilst performing the pairing process.	
1.	Deploy a second appliance that will be the Secondary and configure initial network settings.	
2.	Using the WebUI on the Primary appliance, navigate to: Cluster Configuration > High-Availability
Configuration .	
3.	Specify the IP address and the  loadbalancer userâs password for the Secondary (peer) appliance as shown in
the example above.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 22     

4.	Click Add new node .	
5.	The pairing process now commences as shown below:	
6.	Once complete, the following will be displayed on the Primary appliance:	
7.	To finalize the configuration, restart heartbeat and any other services as prompted in the ""Commit changes"" 
message box at the top of the screen.	
Clicking the Restart Heartbeat  button on the Primary appliance will also automatically restart
heartbeat on the Secondary appliance.
For more details on configuring HA with 2 appliances, please refer to 	Appliance Clustering for
HA	
.	
For details on testing and verifying HA, please refer to 	Clustered Pair Diagnostics	.	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac 23       

14. Document Revision History
Version	Date	Change	Reason for Change	Changed By	
1.0.0	8 March 2018	Initial version	AH	
1.0.1	6 December 2018	Added the new ""Company Contact 
Information"" page	Required updates	AH	
1.1.0	9 December 2019	Styling and layout	General styling 
updates	AH	
1.1.1	8 June 2020	New title page 
Updated Canadian contact details
Added standard section 
'Loadbalancer.org Appliance â the Basics' 
Added standard appendix section 
'Clustered Pair Configuration â Adding a
Secondary Unit'	Branding update 
Change to Canadian 
contact details 
Consistency with 
other deployment
guides	AH	
1.1.2	15 October 2020	Name change from Nuance to Kofax	Kofax Acquisition of 
Nuance Document
Imaging	OW	
1.2.0	1 November 2021	Converted the document to AsciiDoc	Move to new 
documentation
system	AH, RJC, ZAC	
1.2.1	28 September 2022	Updated layer 7 VIP and RIP creation 
screenshots	Reflect changes in 
the web user
interface	AH	
1.2.2	5 January 2023	Combined software version information 
into one section 
Added one level of section numbering
Added software update instructions
Added table of ports used by the 
appliance 
Reworded 'Further Documentation' 
section 
Removed references to the colour of 
certain UI elements	Housekeeping 
across all
documentation	AH	
1.2.3	2 February 2023	Updated screenshots	Branding update	AH	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac24  

Version	Date	Change	Reason for Change	Changed By	
1.2.4	7 March 2023	Removed conclusion section	Updates across all 
documentation	AH	
1.3.0	24 March 2023	New document theme 
Modified diagram colours	Branding update	AH	
Â© Copyright Loadbalancer.org â¢ Documentation â¢ Load Balancing Kofax Equitrac25  

About Loadbalancer.org 
Loadbalancer.orgâs mission is to ensure that its clientsâ businesses are never interrupted. The load balancer  
experts ask the right questions to get to the heart of 
what matters, bringing a depth of understanding to 
each deployment. Experience enables Loadbalancer.org 
engineers to design less complex, unbreakable solutions - 
and to provide exceptional personalized support.	
Visit us:  www.loadbalancer.org
Phone us:  +44 (0)330 380 1064
Phone us:  +1 833 274 2566
Email us:  info@loadbalancer.org
Follow us:  @loadbalancer.org   

"
Non,"
With Prometheus and Grafana Integrations	
	

Supplemented with Predictive Analytics	
	
Automating Couchbase Monitoring	
	

Ashish Rana 	
	


Data Engineering Specialist	
      


Who will be beneï¬ted from this session ?	
	

`
I
ndustry professionals looking for scalable monitoring for multiple 

Couchbase VMs requiring minimal human interventions.	
	
Key Takeaways from this Session	
	

A	
 Complete automation approach for monitoring solution	
	

â	
Running integration tools as services	
	

â	
In house developed tool orchestration	
	

â	
Required customizations to these tools	
	

â	
Total failure recovery and high availability	
	

â	
Predictive maintenance and anomaly analysis 	
   


Couchbase Monitoring Integration with Prometheus and Grafana	
	

by	
 Karim Meliani	
	

The Couchbase Blog	
	

Stepping Stones for Integrations	
	

Couchbase	
	

AWS EC2 

Instances	
	

Couchbase	
	

Exporter	
	

Prometheus 	

Database	
	
ReST Endpoints	
	
Query	
	

Collected metrics 	

from targets	
	

Datasource 

Connection	
	
Metric Flow Sequence: Couchbase VM 	
â	
 CB-Exporter 	
â	
 Prometheus DB 	
â	
 Grafana Dashboards	
	

pools/default	
	

[/bucket, node, tasks]	
	

Where it all begins	
    


Steps Required for adding an Instance	
	
â	
Spinning up a couchbase exporter process for given instance to be monitored.	
	
Execute Command: $ ./couchbase-exporter --couchbase.username admin_user 

--couchbase.password --web.listen-address="":9420"" --couchbase.url=""http://235.19.xx.xx:8091""	
	

â	
Adding scraping target instances in conï¬g ï¬le and running the prometheus instance.	
	
â	
Starting the grafana dashboard service & setting url for your Prometheus datasource.	
	
Execute Command: $ sudo service grafana-server start	
	

Grafana UI Setup: Login 	
â	
 Settings 	
â	
 Data Sources 	
â	
 Set Prometheus datasource.	
	

Inside conï¬guration prometheus.yml ï¬le: - targets: [âlocalhost:9420â, âlocalhost:9421â]	
	

Execute Command: $ ./prometheus --conï¬g.ï¬le=prometheus.yml 	
   


Grafana Data-Source Setup	
	

Step 1	
	
Step 2	
	
Step 3	
	
General (localhost:9420)	
	
Cluster Level Stats !!	
          


Limitations	
	

Manual Efforts required 

for new Couchbase VMs 	

that limits scalability.	
	
01	
	
03	
	
02	
	
04	
	
05	
	
06	
	

Combinations of tools not 	

being in sync when new 	

VM added.	
	

No customizations for 

Grafana & CB-Exporter 	

metrics captured.	
	

Total system failure of 

monitoring solution & 

high availability.	
	

Alerting, notifying and 

mitigation reactions for 

faulty instances.	
	

Predictive repairs  & 

outlier analysis from 

metric data streams.	
   


Automated Monitoring	
	

01	
	

ReST service architecture for adding Couchbase 

instances for monitoring	
  

   


Customizations	
	

02	
	

Add new features to Grafana dashboards, Prometheus 

database and Couchbase Exporter tool	
  


3X CUSTOMIZATIONS	
	
â	
Adding new variables and graphs to observe metrics at bucket or node level.	
	
Query 1 : ""label_values(couchbase_bucket_basicstats_dataused_bytes{cluster=""$cluster""}, bucket)â	
	

Query 2 : ""label_values(couchbase_node_interestingstats_couch_spatial_data_size{cluster=""$cluster""},node)""	
	

	

â	
Running Node Exporter for dynamic target analysis & AlertManager for managing alerts.	
	
â	
Creating your metrics to measure with different Couchbase ReST endpoints like /indexStatus	
	
Creating Go struct object for index metric 	
â	
 Then, coding Collector object and collector function for this 	

metric as well. 	
â	
 Finally, adding index object to main.go ï¬le. 	
â	
 Build the exporter with 	
go	
  build.	
	

- alert: CouchbaseNotBalanced	
	


expr: couchbase_cluster_balanced == 0 and couchbase_task_rebalance_progress == 0	
	

  	

severity: critical	
   


High Availability & 

Total Recovery	
	

03	
	

Prometheus runs as standalone instance on a VM.	
	

Red Flag:	
 What if that VM goes down or needs rebooting? 	
	

  


High Availability for Monitoring Solution	
	

HIGH AVAILABILITY 	
	

CB-Exporter service 	

duplicates request to both 	

servers.	
	

	

Both servers when live 	

maintains same 

targets.json ï¬le.	
	

â	
CB-Exporter Recovery: Initiating CB-Exporter after Couchbase VM patching or Reboot.	
	

â	
Prometheus, Grafana, Node Exporter and AlertManager Recovery: Initiating service after the reboot 

and starting the two ReST HTTP servers as well.	
	

Total Recovery Initiation	
    


Predictive Maintenance 	
	

04	
	

Analyzing stream of system health data Couchbase Instance	
	

& taking predictive actions with outlier analytics	
  


Predictive Maintenance from data streams	
	
â	
Unsupervised outlier detection analysis with 	
PyOD	
 on forecasted data with 	
fbprophet.	
	
Step 1:	
 Forecasting for CPU%, Request 	

Counts, Network and DB consumption 

metrics.	
	

Couchbase VM	
	
Step 3:	
 Sanity Checks on model 	

predictions and deployment 

performance..	
	

75-25 Split	
	
h	
pred	
 = 1 day	
	
Step 2:	
 MAE evaluation 	

for forecast model 

performance analysis.	
	

Exporting Prometheus Data	
	

HTTP server scraping data with ReST 

API: /query resource endpoint. Beware, 	

not to overload !! 	
 
	

Modelling Techniques	
	

Forecasting: Additive models for each 

variable.  Outlier analysis with LCSP: 	
 	

combine(LOF, LODA, CBLOF) models.	
     


Important Resources	
	
â	
K	
arim  Melianiâs	
  article 	
âCouchbase  Monitoring  Integration  with  Prometheus  and 	

Grafanaâ	
	

â	
A	
shish Ranaâs	
 case study âDissecting the 	
âCouchbase Monitoring Integration with 	

Prometheus & Grafanaâ â 	
and all the discussed 	
artifacts	
.
	

â	
D	
ocumentation  for 	
Prometheus  Database	
  & 	
AlertManager,	
 
  mesh  setup	
;
 	
Grafana	
 	

dashboards PromQL queries	
 and 	
grafonnet	
; 	
Couchbase Exporter package.	
	

â	
D	
ocumentation  for 	
/query  API  endpoints	
, 	
PyOD  Package	
  & 	
fbprophet	
 	
to  proceed 	

with predictive maintenanace implementations.	
  


Contact	
	

prod.ashish.rana@gmail.com	
   

"
Non,"Microsoft Azure Solution Architect	 	
 	
ï· 	Work	 Experience:	 6-8 years	 	
ï· 	Location:	 Pune/Remote	 	
ï· 	Must	 Have	 Skills:	 Microsoft Azure Architecture	 	
ï· 	Educational	 Qualification:	 Bachelors	 or higher degree in Computer Science or a related discipline	 	
ï· 	Desired	 Certification	: MCSA  Cloud  Platform  Azure  Training  Certification  MCSE  Cloud  Platform 	
Infrastructure Certification	 	
 
Key	 Responsibilities:	  	
Solution/technical  architecture  in  the  cloud  for  infrastructure  migration	, 	Big 	
Data/analytics/information	. 	analysis/database  managemen	t  in  the  cloud	, 	IoT/event	-	
driven/microservices in the cloud	/ 	
 
Job	 description	: 	
ï· 	Architect	 and	 design	 solutions	 to meet	 functional	 and	 non	-functional	 requirements	 	
ï· 	Create	 and	 review	 architecture	 and	 solution	 design	 artifacts	 	
ï· 	Implement	 innovative	 cloud	 solutions	 on	 Microsoft	 Azure	 that	 drive	 tangible	 benefits	 	
ï· 	Responsible	 for	 deployment,	 maintenance	 and	 monitoring	 of production/test/dev	 systems	 	
running	 on	 MS	 Azure.	 	
ï· 	Support	 all	 phases	 of the	 deployment	 and	 maintenance	 of the	 system	 environments	 on	 Azure.	 	
ï· 	Identify,	 communicate	 and	 mitigate	 Risks,	 Assumptions,	 Issues	 and	 Decisions	 throughout	 full	 	
lifecycle	 	
 
Desired	 Candidate	 Profile	: 	
ï· 	Experience with private public cloud architectures, pros/cons, and migration considerations 	- 	
Extensive hands	-on experience 	implementing infra migration, data migration and data 	
processing using Azure services: Azure Functions, Serverless Architecture, ARM Templates, 
Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure Analysis Service 	- Must 	
have experience in Azure Ia	aS	 	
ï· 	DevOps on an Azure platform	- Experience developing and deploying ETL solutions on Azure	- 	
Strong in Power BI, Java, C, Spark, PySpark, Unix shell/Perl scripting	 	
ï· 	Broad understanding of all the leading cloud providers with hands	-on experience of 	
configurin	g Microsoft Azure PaaS and IaaS.	 	
ï· 	Experience with automating server builds on Azure or cloud solutions using infrastructure as 
code	 	
ï· 	Experience implementing software defined datacenter architectures (including storage, 
network, and compute) and infrastructur	e as code.	 	
ï· 	Experience with public cloud portal administration and resource automation	 	
ï· 	Experience of configuring Service automation tools: (Chef, Puppet, etc.), and Hypervisors 
(Hyper	-V, VMware etc.)	 	
ï· 	Hands on with Visual Studio, Microsoft SCCM, Power Shell 	and at least one scripting 	
language such as Python, Perl, PHP or Bash.	 	
ï· 	Experience with any of the following: O365, Azure, Azure Stack, Azure AD	 	
ï· 	Understanding of challenges and considerations to migrate workloads from on premise to 
Azure.	 	
ï· 	Install and config	ure user applications Azure VMs.	 	
ï· 	Understanding and optimizing existing user workloads.	 	
ï· 	Experience in implementing and deploying W indows Virtual Desktop (WVD) architecture.	 	
ï· 	Excellent understanding of active directory.	 	
ï· 	Experience on 	Azure SQL, Azure Data Lak	e, HD Insights, Hadoop, Cloudera, MongoDB, 	
MySQL, Neo4j, Cassandra, Couchbase	 	
ï· 	Knowledge of 	Software development full lifecycle methodologies, patterns, frameworks, 	
libraries and tools	 	
ï· 	Knowledge of programming and scripting languages such as JavaScript, Pow	erShell, Bash, 	
SQL, .NET, Java, Python, PHP, Ruby, PERL, C++, etc.	 	
ï· 	Experience with data ingestion technologies such as Aspera, MoveIT, Azure Data Factory, 
SSIS, Pentaho, Alteryx	 	
ï· 	Experience with visualization tools such as Tableau, Spotfire, Power BI, D3, S	crapinghub	 	
ï· 	Experience with analytics tools including SPSS	 	
ï· 	Experience with machine learning tools such as Azure ML, IBM, AWS, etc.	    	ï¼ï¼	ï¼  

"
Non,"	
	
erwin	 Data	 Modeler	
Feature	 Tour	
Release	 2021	 R1	
   

	
2	
Legal	 Notices	
This	 Documentation,	 which	 includes	 embedded	 help	 systems	 and	 electronically	 distributed	 	
materials	 (hereinafter	 referred	 to	 as	 the	 âDocumentationâ),	 is for	 your	 informational	 pur	-	
poses	 only	 and	 is subject	 to	 change	 or	 withdrawal	 by	 Quest	 Software,	 Inc	 and/or	 its	 affiliates	 	
at	 any	 time.	 This	 Documentation	 is proprietary	 information	 of	 Quest	 Software,	 Inc	 and/or	 its	 	
affiliates	 and	 may	 not	 be	 copied,	 transferred,	 reproduced,	 disclosed,	 modified	 or	 duplicated,	 	
in	 whole	 or	 in	 part,	 without	 the	 prior	 written	 consent	 of	 Quest	 Software,	 Inc	 and/or	 its	 affil	-	
iates	  	
If you	 are	 a licensed	 user	 of	 the	 software	 product	(s)	 addressed	 in	 the	 Documentation,	 you	 	
may	 print	 or	 otherwise	 make	 available	 a reasonable	 number	 of	 copies	 of	 the	 Documentation	 	
for	 internal	 use	 by	 you	 and	 your	 employees	 in	 connection	 with	 that	 software,	 provided	 that	 	
all	 Quest	 Software,	 Inc	 and/or	 its	 affiliates	 copyright	 notices	 and	 legends	 are	 affixed	 to	 each	 	
reproduced	 copy.	 	
The	 right	 to	 print	 or	 otherwise	 make	 available	 copies	 of	 the	 Documentation	 is limited	 to	 the	 	
period	 during	 which	 the	 applicable	 license	 for	 such	 software	 remains	 in	 full	 force	 and	 effect.	 	
Should	 the	 license	 terminate	 for	 any	 reason,	 it is your	 responsibility	 to	 certify	 in	 writing	 to	 	
Quest	 Software,	 Inc	 and/or	 its	 affiliates	 that	 all	 copies	 and	 partial	 copies	 of	 the	 Docu	-	
mentation	 have	 been	 returned	 to	 Quest	 Software,	 Inc	 and/or	 its	 affiliates	 or	 destroyed.	 	
TO	 THE	 EXTENT	 PERMITTED	 BY	 APPLICABLE	 LAW,	 QUEST	 SOFTWARE,	 INC.	 PROVIDES	 THIS	 	
DOCUMENTATION	 âAS	 ISâ	 WITHOUT	 WARRANTY	 OF	 ANY	 KIND,	 INCLUDING	 WITHOUT	 	
LIMITATION,	 ANY	 IMPLIED	 WARRANTIES	 OF	 MERCHANTABILITY,	 FITNESS	 FOR	 A PARTICULAR	 	
PURPOSE,	 OR	 NONINFRINGEMENT.	 IN	 NO	 EVENT	 WILL	 QUEST	 SOFTWARE,	 INC.	 BE	 LIABLE	 TO	 	
YOU	 OR	 ANY	 THIRD	 PARTY	 FOR	 ANY	 LOSS	 OR	 DAMAGE,	 DIRECT	 OR	 INDIRECT,	 FROM	 THE	 	
USE	 OF	 THIS	 DOCUMENTATION,	 INCLUDING	 WITHOUT	 LIMITATION,	 LOST	 PROFITS,	 LOST	 	
INVESTMENT,	 BUSINESS	 INTERRUPTION,	 GOODWILL,	 OR	 LOST	 DATA,	 EVEN	 IF	 QUEST	 	
SOFTWARE,	 INC.	 IS	 EXPRESSLY	 ADVISED	 IN	 ADVANCE	 OF	 THE	 POSSIBILITY	 OF	 SUCH	 LOSS	 OR	 	
DAMAGE.	 	
The	 use	 of	 any	 software	 product	 referenced	 in	 the	 Documentation	 is governed	 by	 the	 applic	-	
able	 license	 agreement	 and	 such	 license	 agreement	 is not	 modified	 in	 any	 way	 by	 the	 terms	 	
of	 this	 notice.	 	
The	 manufacturer	 of	 this	 Documentation	 is Quest	 Software,	 Inc	 and/or	 its	 affiliates. 

	
3	
Provided	 with	 âRestricted	 Rights.â	 Use,	 duplication	 or	 disclosure	 by	 the	 United	 States	 	
Government	 is subject	 to	 the	 restrictions	 set	 forth	 in	 FAR	 Sections	 12.212,	 52.227-	14,	 and	 	
52.227-	19	(c)	(1)	 - (2)	 and	 DFARS	 Section	 252.227-	7014	(b)	(3),	 as	 applicable,	 or	 their	 suc	-	
cessors.	 	
Copyright	 Â©	 2021	 Quest	 Software,	 Inc	 and/or	 its	 affiliates	 All	 rights	 reserved.	 All	 trademarks,	 	
trade	 names,	 service	 marks,	 and	 logos	 referenced	 herein	 belong	 to	 their	 respective	 com	-	
panies.	  

	
4	
Contact	 erwin	
Understanding	 your	 Support	
Review	 support	 maintenance	 programs	 and	 offerings	.	
Registering	 for	 Support	
Access	 the	 erwin	 support	 site	 and	 click	 Sign	 in	 or	 Sign	 up	 to	 register	 for	 product	 support.	
Accessing	 Technical	 Support	
For	 your	 convenience,	 erwin	 provides	 easy	 access	 to	 ""One	 Stop""	 support	 for	 all	 editions	 of	 	
erwin	 Data	 Modeler	, and	 includes	 the	 following:	
Online	 and	 telephone	 contact	 information	 for	 technical	 assistance	 and	 customer	 ser	-	
vices	 	
Information	 about	 user	 communities	 and	 forums	 	
Product	 and	 documentation	 downloads	  	
erwin	 Support	 policies	 and	 guidelines	  	
Other	 helpful	 resources	 appropriate	 for	 your	 product	  	
For	 information	 about	 other	 erwin	 products,	 visit	 http://erwin.com/products	.	
Provide	 Feedback	
If you	 have	 comments	 or	 questions,	 or	 feedback	 about	 erwin	 product	 documentation,	 you	 	
can	 send	 a message	 to	 techpubs@erwin.com	.	
erwin	 Data	 Modeler	 News	 and	 Events	
Visit	 www.erwin.com	 to	 get	 up-	to-	date	 news,	 announcements,	 and	 events.	 View	 video	 	
demos	 and	 read	 up	 on	 customer	 success	 stories	 and	 articles	 by	 industry	 experts. 

	
5	
Contents	
Legal	 Notices	2	
Contents	5	
Introduction	7	
erwin	 Mart	 Administrator	 UI	 Facelift	8	
NoSQL	 Modeling	9	
MongoDB	 Support	10	
Cassandra	 Support	11	
Couchbase	 Support	13	
Migrating	 Relational	 Models	 to	 NoSQL	 Models	14	
Migration	 by	 Changing	 the	 Target	 Database	14	
Migration	 by	 Deriving	 a Model	16	
Reverse	 Engineering	 Models	23	
Forward	 Engineering	 Models	33	
Comparing	 Changes	 using	 Complete	 Compare	38	
JSON	 and	 AVRO	 Support	54	
Reverse	 Engineering	 Models	 - JSON	 and	 AVRO	55	
Forward	 Engineering	 Models	 - JSON	 and	 AVRO	59	
Oracle	 Support	 Summary	65	
Microsoft	 SQL	 Server	 Support	68	
Microsoft	 Azure	 SQL	 Server	 Support	69	
MySQL	 Support	72	
Data	 Vault	 2.0	 Support	74 

	
6	
Productivity	 and	 UI	 Enhancements	76	
Welcome	 Page	76	
Objects	 Count	 Pane	78	
Properties	 Pane	78	
Object	 Browser	79	
Normalization	 and	 Denormalization	80	
Reverse	 Engineering	 and	 Forward	 Engineering	 Wizard	 Redesign	81	
Improved	 Speed	 Mode	83	
JDBC	 Support	85 

	
7	
Introduction
The	 Feature	 Tour	 guide	 walks	 Data	 Architects,	 Data	 Administrators,	 Application	 Admin	-	
istrators,	 Database	 Administrators,	 and	 Partners	 through	 the	 features	 introduced	 in	 erwin	 	
Data	 Modeler	 (DM)	 2021	 R1	 release.	
The	 features	 and	 enhancements	 introduced	 in	 this	 release	 are:	
erwin	 Mart	 Administrator	 UI	 Facelift	
NoSQL	 Modeling	
JSON	 and	 AVRO	 Support	
Oracle	 12c	 R2,	 18c,19c,	 and	 21c	 	
Microsoft	 SQL	 Server	 2019	
Microsoft	 Azure	 SQL	
MySQL	 	
Data	 Vault	 2.0	 Support	
Productivity	 and	 UI	 Enhancements	
JDBC	 Support	
For	 additional	 information	 about	 a feature,	 in	 erwin	 Data	 Modeler,	 click	 Help	 > Help	 Topics	 	
on	 the	 toolbar	 or	 press	 F1	. 

	
8	
erwin	 Mart	 Administrator	 UI	 Facelift	
erwin	 Mart	 Administrator	 now	 comes	 with	 a brand	 new	 UI	 that	 follows	 Googleâs	 Material	 	
Design	 principles.	 The	 redesigned	 UI	 offers	 an	 improved	 user	 experience	 with	 its	 modern	 	
look	 and	 feel,	 dark	 and	 light	 modes,	 and	 graphical	 buttons	 and	 icons.	
Apart	 from	 the	 overall	 facelift,	 the	 wiki-	like	 editable	 Home	 page	 lets	 you	 add	 information,	 	
such	 as	 key	 text,	 process	 diagrams,	 important	 hyperlinks,	 resources,	 and	 much	 more.	 Also,	 	
the	 configurable	 Dashboard	 letâs	 you	 add	 and	 view	 a pictorial	 presentation	 of	 your	 data	 and	 	
actions	 on	 the	 Mart.	 You	 can	 add	 charts	 for	 your	 data	 footprint,	 model	 overview	 and	 his	-	
tory,	 profile	 data,	 and	 session	 overview.	
For	 more	 information,	 refer	 to	 the	 erwin	 Mart	 Online	 Help	. 

	
9	
NoSQL	 Modeling	
Along	 with	 relational	 databases,	 erwin	 Data	 Modeler	 (DM)	 now	 supports	 the	 following	 	
NoSQL,	 non-	relational	 databases	 as	 target	 databases:	
MongoDB	 4.x	
Cassandra	 3.x	
Couchbase	 6.x	
These	 NoSQL	 databases	 support	 all	 the	 erwin	 DM	 features	 and	 functions.	 The	 following	 sec	-	
tions	 will	 take	 you	 through	 these	 features	 with	 MongoDB	 database	 as	 an	 example:	
Migrating	 a relational	 model	 to	 NoSQL	 model	
Reverse	 engineering	 models	 from	 database	 and	 script	
Forward	 engineering	 models	 to	 database	
Comparing	 changes	 using	 Complete	 Compare	
 
  

	
10	
MongoDB	 Support	
erwin	 Data	 Modeler	 (DM)	 now	 supports	 MongoDB	 4.x	 as	 a target	 database.	 This	 imple	-	
mentation	 supports	 the	 following	 objects:	
Databases
Collection	
Collation	
Index
Relationships
User	 IDs
Roles	
View	
The	 following	 table	 lists	 the	 supported	 data	 types:	
Numeric	String	 Lit	-	
erals	
Date	 and	 Time	Other	
 l	double	
 l	binary	
 l	int	
 l	integer	
 l	boolean	
 l	minKey	
 l	maxKey	
 l	long	
 l	decimal	
 l	string	 l	date	
 l	timestamp	
 l	object	
 l	array	
 l	null	
 l	objectId	
 l	regex	
 l	code 

	
11	
Cassandra	 Support	
erwin	 Data	 Modeler	 (DM)	 now	 supports	 Cassandra	 3.x/4.x	 as	 a target	 database.	 This	 imple	-	
mentation	 supports	 the	 following	 objects:	
Aggregate
Function
Keyspace
Materialized	 View	
Materialized	 View	 Column	
Role
Table	
Table	 Column	
Index	
User	 Type	
The	 following	 table	 lists	 the	 supported	 data	 types:	
Category	Data	 Type	Supported	 Constants	
Native	 l	ascii	
 l	bigint	
 l	blob	
 l	boolean	
 l	counter	
 l	date	
 l	decimal	
 l	double	
 l	float	
 l	string	
 l	integer	
 l	blob	
 l	boolean	
 l	integer	
 l	integer,	 string	
 l	integer,	 float	
 l	integer,	 float	
 l	integer,	 float 

	
12	
 l	inet	
 l	int	
 l	smallint	
 l	text	
 l	time	
 l	timestamp	
 l	timeuuid	
 l	tinyint	
 l	uuid	
 l	varint	
 l	string	
 l	integer	
 l	integer	
 l	string	
 l	integer,	 string	
 l	integer,	 string	
 l	uuid	
 l	integer	
 l	uuid	
 l	integer	
Collection	 l	list	
 l	map	
 l	set	
 	
Tuple	 l	tuple	 	
  

	
13	
Couchbase	 Support	 	
erwin	 Data	 Modeler	 (DM)	 now	 supports	 Couchbase	 6.x	 as	 a target	 database.	 This	 imple	-	
mentation	 supports	 the	 following	 objects:	
Bucket
Document	
Field	
Full	 Text	 Index	
Global	 Index	
User	 ID	
View	
Following	 are	 the	 supported	 data	 types:	
MISSING
NULL
BOOLEAN
NUMBER
STRING
ARRAY
OBJECT
BINARY 

	
14	
Migrating	 Relational	 Models	 to	 NoSQL	 Models	
You	 can	 convert	 and	 migrate	 your	 relational	 models	 to	 NoSQL	 models	 in	 two	 ways:	
Changing	 the	 target	 database	
Deriving	 a model	
This	 topic	 walks	 you	 through	 the	 steps	 to	 migrate	 a SQL	 Server	 model	 to	 a MongoDB	 model.	 	
Similarly,	 you	 can	 migrate	 your	 relational	 models	 to	 Cassandra	 and	 Couchbase	 models.	
Note	: Ensure	 that	 you	 keep	 a backup	 of	 your	 original	 models.	
Migration	 by	 Changing	 the	 Target	 Database	
To	 migrate	 by	 changing	 the	 target	 database,	 follow	 these	 steps:	
 1.	 Open	 your	 relational	 model	 in	 erwin	 Data	 Modeler	 (DM).	
Note	: Ensure	 that	 you	 are	 in	 the	 Physical	 mode.	
For	 example,	 the	 following	 image	 uses	 the	 sample	 eMovies.erwin	 model.	 In	 the	 	
Objects	 Count	 pane,	 note	 the	 number	 of	 tables,	 columns,	 and	 relationships. 

	
15	
 2.	 On	 the	 ribbon,	 click	 Actions	 > Target	 Database	 or	 on	 the	 status	 bar,	 click	 the	 database	 	
name.
The	 erwin	 Data	 Modeler	 -- Target	 Server	 screen	 appears.	
 3.	 In	 the	 Database	 drop-	down	 list,	 select	 MongoDB.	
By	 default,	 the	 Auto	 Denormalization	 check	 box	 is selected.	 Keep	 it selected.	
 4.	 Click	 OK	.	
The	 conversion	 process	 starts. 

	
16	
Once	 the	 conversion	 is complete,	 the	 existing	 model	 in	 migrated	 to	 a NoSQL database.	
In	 the	 Objects	 Count	 pane,	 note	 that	 instead	 of	 tables	 and	 columns,	 we	 now	 have	 col	-	
lections	 and	 fields.	 Also,	 the	 Relationships	 count	 has	 changed	 to	 0.	 The	 migration	 pro	-	
cess	 converts	 and	 merges	 multiple	 tables,	 columns,	 and	 relationships	 to	 the	 NoSQL	 	
format	 according	 to	 the	 database	 that	 you	 select.	                  	
Note	: This	 migration	 method	 overwrites	 the	 existing	 model	 once	 you	 save	 it.	 Hence,	 	
we	 recommend	 that	 you	 keep	 a backup	 of	 your	 original	 model.	
Migration	 by	 Deriving	 a	 Model	
To	 migrate	 by	 deriving	 a model,	 follow	 these	 steps: 

	
17	
 1.	 Open	 your	 relational	 model	 in	 erwin	 Data	 Modeler	 (DM).	
Note	: Ensure	 that	 you	 are	 in	 the	 Physical	 mode.	
For	 example,	 the	 following	 image	 uses	 the	 sample	 eMovies.erwin	 model.	 In	 the	 	
Objects	 Count	 pane,	 note	 the	 number	 of	 tables,	 columns,	 and	 relationships.	
 2.	 On	 the	  ribbon,	  click	 Actions	 > Design	 Layers	 > Derive	 New	 Model	.	
The	 Derive	 Model	 screen	 appears.	 By	 default,	 the	 Source	 Model	 is set	 to	 your	 current	 	
model. 

	
18	
 3.	 In	 the	 Database	 drop-	down	 list,	 select	 MongoDB	.	
By	 default,	 the	 Auto	 Denormalization	 check	 box	 is selected.	 Keep	 it selected. 

	
19	
 4.	 Click	 Next	.	
Note	: If the	 Type	 Resolution	 screen	 appears,	 click	 Finish	.	
The	 Type	 Selection	 section	 appears. 

	
20	
 5.	 Select	 the	 types	 of	 objects	 that	 you	 want	 to	 derive	 into	 the	 target	 MongoDB	 model.	
 6.	 Click	 Next	.	
The	 Object	 Selection	 section	 appears.	 Based	 on	 the	 object	 types	 you	 selected	 in	 step	 	
5,	 it displays	 a list	 of	 objects. 

	
21	
 7.	 Select	 the	 objects	 that	 you	 want	 to	 derive	 into	 the	 target	 MongoDB	 model.	
 8.	 Click	 Derive	.	
The	 model	 derivation	 process	 starts. 

	
22	
Once	 the	 conversion	 is complete,	 the	 existing	 model	 in	 migrated	 to	 a NoSQL database.	
In	 the	 Objects	 Count	 pane,	 note	 that	 instead	 of	 tables	 and	 columns,	 we	 now	 have	 col	-	
lections	 and	 fields.	 Also,	 the	 Relationships	 count	 has	 changed	 to	 0.	 The	 migration	 pro	-	
cess	 converts	 and	 merges	 multiple	 tables,	 columns,	 and	 relationships	 to	 the	 NoSQL	 	
format	 according	 to	 the	 database	 that	 you	 select.	  

	
23	
Reverse	 Engineering	 Models	
You	 can	 create	 a data	 model	 from	 a database	 or	 a script	 using	 the	 Reverse	 Engineering	 pro	-	
cess.
This	 topic	 walks	 you	 through	 the	 steps	 to	 reverse	 engineer	 a MongoDB	 model.	 Similarly,	 you	 	
can	 reverse	 engineer	 a model	 from	 your	 Cassandra	 Keyspace	 and	 Couchbase	 Bucket.	
To	 reverse	 engineer	 a model:	
 1.	 In	 erwin	 Data	 Modeler	 (DM),	 click	 Actions	 > Reverse	 Engineer	.	
The	 New	 Model	 screen	 appears.	
 2.	 Click	 Logical/Physical	 and	 set	 Database	 to	 MongoDB.	
 3.	 Click	 Next	.	
The	 Reverse	 Engineer	 Process	 Wizard	 appears. 

	
24	
 4.	 Click	 one	 of	 the	 following	 options:	
 l	Database	: Use	 this	 option	 to	 reverse	 engineer	 a model	 from	 your	 database.	
 l	Script	 File	: Use	 this	 option	 to	 reverse	 engineer	 a model	 from	 a script.	 Selecting	 	
this	 option	 enables	 the	 File	 field.	 Click	 Browse	 and	 select	 the	 necessary	 script	 	
file.
Note	: If you	 click	 Script	 File	, jump	 to	 step	 8 below	 and	 ensure	 that	 Document	 	
Count	 or	 Document	 %	 is not	 set	 to	 zero	 (0).	
 5.	 Click	 Next	. 

	
25	
The	 Connection	 section	 appears.	 Use	 this	 section	 to	 connect	 to	 the	 database	 from	 	
which	 you	 want	 to	 reverse	 engineer	 the	 model.	 You	 can	 connect	 to	 the	 database	 dir	-	
ectly	 or	 using	 a connection	 string.	 The	 following	 table	 explains	 the	 connection	 para	-	
meters:
Connection	 	
Method	
Parameters/Values	
Connection	 	
String	
Specify	 the	 MongoDB	 Connection	 String.	
For	 example:	 mongodb+srv://	<abcd>	:****@	<xyz>	.mon	-	
godb.net/test?retryWrites=true&w=majority
Replace	 <abcd>	 with	 your	 username	 and	 <xyz>	 with	 host	 name.	 The	 host	 	
name	 parameter	 would	 change	 based	 on	 your	 MongoDB	 deployment;	 	
standalone,	 replica	 set,	 or	 a sharded	 cluster.	
Direct	
Specify	 the	 host	 name	 and	 port	 number	 of	 your	 	
MongoDB deployment. Also,	 specify	 the	 database	 that	 you	 want	 to	 con	-	
nect	 to. 

	
26	
In	 the	 following	 image,	 for	 example,	 the	 connection	 is being	 established	 using	 a con	-	
nection	 string.	
 6.	 Click	 Connect	.	
On	 successful	 connection,	 your	 connection	 information	 is displayed	 under	 Recent	 Con	-	
nections.	
 7.	 Click	 Next	. 

	
27	
The	 Database	 section	 appears.	 It displays	 a list	 of	 available	 databases.	
 8.	 Under	 Available	 Databases	, select	 the	 databases	 that	 you	 want	 to	 reverse	 engineer.	 	
Then,	 click	 	.  

	
28	
This	 moves	 the	 selected	 databases	 under	 Selected	 Databases.	
 9.	 Click	 Next	.	
The	 Collection	 section	 appears.	 It displays	 a list	 of	 available	 collections	 in	 the	 data	-	
bases	 that	 you	 selected	 in	 step	 8.	
 10.	 Use	 the	 following	 options: 

	
29	
 l	Document	 Count	/Document	 (%)	: Use	 this	 option	 to	 specify	 the	 number	 of	 doc	-	
uments	 or	 percentage	 of	 total	 records	 that	 the	 newly	 generated	 model	 schema	 	
would	 contain.	
 l	Deep	 Search	: Use	 this	 option	 to	 specify	 whether	 the	 deep	 search	 algorithm	 is 	
used	 to	 retrieve	 the	 right	 samples	 for	 schema	 generation.	
 l	Sampling	: Use	 the	 Sequence	 or	 Random	 sampling	 methods	 to	 sample	 records	 in	 	
the	 selected	 collections.	 Sampling	 enables	 you	 to	 retrieve	 right	 estimates	 for	 	
accurate	 collection	 schema	 generation.	
 11.	 Under	 Available	 Collections	, select	 the	 collections	 that	 you	 want	 to	 reverse	 engineer.	 	
Then,	 click	 	.	
This	 moves	 the	 selected	 collections	 under	 Selected	 Collections.	
 12.	 Click	 Next	.	
The	 Option	 Set	 section	 appears.	 It displays	 the	 default	 option	 set.	 You	 can	 either	 use	 	
the	 default	  or	 a custom	 option	 set. 

	
30	
 13.	 Click	 Next	.	
The	 Detail	 Options	 section	 appears.	 Set	 up	 appropriate	 options	 based	 on	 your	 require	-	
ment. 

	
31	
 14.	 Click	 OK	. 

	
32	
The	 reverse	 engineering	 process	 starts.	
Once	 the	 process	 is complete,	 based	 on	 your	 selections,	 a schema	 is generated	 and	 a 	
model	 is created. 

	
33	
Forward	 Engineering	 Models	
You	 can	 generate	 a physical	 database	 schema	 from	 a physical	 model	 using	 the	 Forward	 	
Engineering	 process.	 	
This	 topic	 walks	 you	 through	 the	 steps	 to	 forward	 engineer	 a MongoDB	 model.	 Similarly,	 	
you	 can	 forward	 engineer	 a model	 to	 your	 Cassandra	 Keyspace	 and	 Couchbase	 Bucket.	
To	 forward	 engineer	 a model:	
 1.	 Open	 your	 MongoDB	 model	 in	 erwin	 Data	 Modeler	 (DM).	
Note	: Ensure	 that	 you	 are	 in	 the	 Physical	 mode.	
For	 example,	 the	 following	 image	 uses	 a MongoDB	 model	 with	 two	 collections.	
 2.	 Click	 Actions	 > Schema	.	
The	 Forward	 Engineer	 Schema	 Generation	 Wizard	 appears. 

	
34	
 3.	 Click	 Option	 Selection	.	
The	 Option	 Selection	 section	 displays	 the	 default	 option	 set.	 Clear	 the	 Drop	 check	 	
boxes	 and	 select	 other	 syntax	 check	 boxes	 as	 required. 

	
35	
 4.	 Click	 Next	.	
The	 Collection	 Filter	 section	 appears.	 It displays	 a list	 of	 collections	 available	 in	 your	 	
model. 

	
36	
 5.	 Select	 the	 collections	 that	 you	 want	 to	 forward	 engineer. 

	
37	
 6.	 Click	 Preview	 to	 view	 the	 schema	 script.	
Use	 the	 following	 options:	
 l	Auto	 Error	 Check	: Select	 this	 option	 to	 enable	 auto	 error	 check	 by	 the	 forward	 	
engineering	 wizard.	
 l	Error	 Check	 (	): Use	 this	 option	 to	 run	 an	 error	 check.	 Based	 on	 the	 results,	 	
you	 can	 correct	 the	 generated	 script.	
 l	Text	 Options	 (	): Use	 this	 option	 to	 configure	 the	 preview	 text	 editor's	 look	 	
and	 feel,	 such	 as	 window,	 font,	 syntax	 color	 settings.	 For	 more	 information,	  

	
38	
refer	 to	 the	 Forward	 Engineering	 Wizard	 - Preview	 Editor	 topic.	
 l	Save	 (	): Use	 this	 option	 to	 save	 the	 generated	 script	 in	 the	 JSON	 or	 BSON	 	
format.	
 7.	 Click	 Generate	.	
The	 forward	 engineering	 process	 starts.	 The	 script	 generates	 your	 physical	 database	 	
schema.	 You	 can	 access	 your	 database	 and	 verify	 the	 newly	 generated	 schema.	
 	
Comparing	 Changes	 using	 Complete	 Compare	
You	 can	 compare	 your	 model	 with	 database,	 script,	 or	 another	 local	 model	 to	 check	 for	 dif	-	
ferences	 using	 the	 Complete	 Compare	 wizard.	 Based	 on	 the	 results,	 you	 can	 then	 resolve	 or	 	
merge	 differences.	 Thus,	 maintaining	 a consistent	 model	 and	 database.	 	
This	 topic	 walks	 you	 through	 the	 steps	 to	 compare	 a MongoDB	 model	 with	 database.	 Sim	-	
ilarly,	 you	 can	 compare	 your	 Cassandra	 and	 Couchbase	 models.	
To	 compare	 models	 with	 database:	
 1.	 Open	 your	 MongoDB	 model	 in	 erwin	 Data	 Modeler	 (DM).	
Note	: Ensure	 that	 you	 are	 in	 the	 Physical	 mode. 

	
39	
For	 example,	 the	 following	 image	 uses	 a MongoDB	 model	 with	 two	 collections.	
 2.	 Click	 Actions	 > Complete	 Compare	.	
By	 default,	 the	 Complete	 Compare	 wizard	 assigns	 the	 open	 model	 as	 the	 Left	 Model.	 	
Hence,	 the	 Right	 Model	 section	 appears. 

	
40	
 3.	 Click	 Database/Script	.	
By	 default,	 the	 Allow	 Demand	 Loading	 option	 is selected.	 	
 4.	 Click	 Load	.	
The	 New	 Model	 dialog	 box	 appears.	 This	 starts	 the	 reverse	 engineering	 process	 to	 pull	 	
a model	 from	 the	 database	 to	 compare. 

	
41	
 5.	 Ensure	 that	 the	 Database	 is set	 to	 the	 correct	 one.	 In	 this	 case,	 MongoDB.	 Then,	 click	 	
Next	.	
The	 Reverse	 Engineer	 Process	 Wizard	 appears. 

	
42	
 6.	 Click	 Database	. Then,	 click	 Next	.	
The	 Connection	 section	 appears.	 Use	 this	 section	 to	 connect	 to	 the	 database	 from	 	
which	 you	 want	 to	 reverse	 engineer	 the	 model	.	
 7.	 After	 connection	 is established,	 click	 Next	. 	
The	 Database	 section	 appears.	 It displays	 a list	 of	 available	 databases. 

	
43	
 8.	 Under	 Available	 Databases	, select	 the	 databases	 that	 you	 want	 to	 reverse	 engineer.	 	
Then,	 click	 	.	
This	 moves	 the	 selected	 databases	 under	 Selected	 Databases. 

	
44	
 9.	 Click	 Next	 and	 in	 the	 Collection	 section,	 click	 	. 	
This	 selects	 all	 the	 available	 collections.	 Also,	 ensure	 that	 the	 Document	 Coun	-	
t/Document	 %	 is not	 set	 to	 zero	 (0). 

	
45	
 10.	 Click	 Next	 and	 in	 the	 Option	 Set	 section,	 keep	 the	 default	 configuration.	
 11.	 Click	 Next	 and	 in	 the	 Detail	 Options	 section,	 keep	 the	 default	 configuration.	
 12.	 Click	 OK	.	
The	 reverse	 engineering	 process	 starts.	 Once	 the	 process	 is complete,	 the	 Right	 Model	 	
is set	 to	 the	 one	 that	 you	 reverse	 engineered. 

	
46	
 13.	 Click	 Next	 and	 in	 the	 Type	 Selection	 section,	 select	 the	 appropriate	 options. 

	
47	
For	 example,	 the	 following	 image	 shows	 the	 default	 options.	
 14.	 Click	 Next	 and	 in	 the	 Left	 Object	 Selection	 section,	 select	 the	 appropriate	 options. 

	
48	
For	 example,	 the	 following	 image	 shows	 the	 default	 options.	
 15.	 Click	 Next	 and	 in	 the	 Right	 Object	 Selection	 section,	 select	 the	 appropriate	 options. 

	
49	
For	 example,	 the	 following	 image	 shows	 the	 default	 options.	
 16.	 Click	 Compare	.	
The	 comparison	 process	 runs,	 and	 the	 Resolve	 Differences	 dialog	 box	 appears.	 It dis	-	
plays	 the	 differences	 between	 your	 model	 and	 database.	
For	 example,	 the	 following	 image	 shows	 that	 the	 Rating	 collection	 is available	 in	 your	 	
model	 but	 not	 in	 the	 database. 

	
50	
Select	 the	 Rating	 collection	 and	 click	 	. This	 will	 move	 the	 Rating	 collection	 to	 the	 	
right	 model	 (from	 the	 database).	 Similarly,	 resolve	 other	 differences.	
 17.	 As	 differences	 were	 moved	 to	 the	 right	 model,	 click	 	.	
This	 launches	 the	 Forward	 Engineering	 Alter	 Script	 Generation	 Wizard. 

	
51	
 18.	 Click	 Option	 Selection	 and	 clear	 all	 the	 Drop	 check	 boxes.	
 19.	 Click	 Collection	 Filter	 and	 select	 or	 verify	 the	 collections	 to	 be	 included	 on	 the	 for	-	
ward	 engineering	 script. 

	
52	
 20.	 Click	 Preview	 to	 view	 and	 verify	 the	 alter	 script.	
 21.	 Click	 Generate	 and	 connect	 to	 your	 MongoDB	 database.	
The	 forward	 engineering	 process	 starts.	 The	 script	 generates	 your	 physical	 database	 	
schema.	 You	 can	 access	 your	 database	 and	 verify	 the	 newly	 generated	 schema.	
 22.	 Click	 OK	. Then	 click	 Finish	.	
This	 closes	 the	 Resolve	 Differences	 dialog	 box	 and	 displays	 the	 Complete	 Compare	 wiz	-	
ard.	
 23.	 Click	 Close	.	
  

	
53	
  

	
54	
JSON	 and	 AVRO	 Support	
erwin	 Data	 Modeler	 (DM)	 now	 includes	 modeling	 support	 for	 JSON	 and	 AVRO	 file	 formats.	 	
The	 following	 table	 lists	 the	 supported	 objects	 and	 data	 types	 for	 each	 format:	
File	 	
Format	
Objects	Data	 Types	
JSON	JSON	 Objects	
Fields
Relationships	
Object
Array
Integer
Null
String
Number
Boolean	
AVRO	Records
Fields
Relationships	
Array
Boolean
Union
Map
int
Double
Object
String
Byte
enum
Fixed
Long 

	
55	
Similar	 to	 relational	 or	 NoSQL	 databases,	 JSON	 and	 AVRO	 as	 target	 databases	 support:	
Reverse	 engineering	 models	 from	 scripts	
Forward	 engineering	 models	
 
Reverse	 Engineering	 Models	 - JSON	 and	 AVRO	
You	 can	 create	 a data	 model	 from	 JSON	 and	 AVRO	 scripts	 using	 the	 Reverse	 Engineering	 pro	-	
cess.
Note	: For	 reverse	 engineering	 German	 language	 JSON	 scripts,	 ensure	 the	 script	 Encoding	 is 	
set	 to	 Convert	 to	 ANSI.	
This	 topic	 walks	 you	 through	 the	 steps	 to	 reverse	 engineer	 a JSON	 model	 from	 a script	 file.	 	
Similarly,	 you	 can	 reverse	 engineer	 a model	 from	 your	 AVRO script	 file.	
To	 reverse	 engineer	 a model:	
 1.	 In	 erwin	 Data	 Modeler	 (DM),	 click	 Actions	 > Reverse	 Engineer	.	
The	 New	 Model	 screen	 appears.	
 2.	 Click	 Logical/Physical	 and	 set	 Database	 to	 JSON.	
 3.	 Click	 Next	.	
The	 Reverse	 Engineer	 Process	 Wizard	 appears. 

	
56	
 4.	 Select	 Json	 Data	 or	 Schema	 File	 format	 option.	 Then,	 click	 Browse	 and	 select	 one	 or	 	
multiple	 script	 files.	
 5.	 Click	 Next	.	
The	 Detail	 Options	 section	 appears.	 Set	 up	 appropriate	 options	 based	 on	 your	 require	-	
ment. 

	
57	
 6.	 Click	 OK	. 

	
58	
The	 reverse	 engineering	 process	 starts.	
Once	 the	 process	 is complete,	 based	 on	 your	 selections,	 a schema	 is generated,	 and	 a 	
model	 is created. 

	
59	
Forward	 Engineering	 Models	 - JSON	 and	 AVRO	 	
You	 can	 generate	 a physical	  schema	  from	 a physical	 model	 using	 the	 Forward	 Engineering	 	
process	 and	 then,	 save	 it in	 the	 JSON	 and	 AVRO	 file	 formats.	 	
This	 topic	 walks	 you	 through	 the	 steps	 to	 forward	 engineer	 a JSON	 model.	 Similarly,	 you	 can	 	
forward	 engineer	 an	 AVRO	 model.	
To	 forward	 engineer	 a model:	
 1.	 Open	 your	 JSON	 model	 in	 erwin	 Data	 Modeler	 (DM).	
Note	: Ensure	 that	 you	 are	 in	 the	 Physical	 mode.	
For	 example,	 the	 following	 image	 uses	 a JSON	 model	 with	 16	 tables.	
 2.	 Click	 Actions	 > Schema	.	
The	 Forward	 Engineer	 Schema	 Generation	 Wizard	 appears. 

	
60	
 3.	 Click	 Option	 Selection	.	
The	 Option	 Selection	 section	 displays	 the	 default	 option	 set.	 Select	 appropriate	 syntax	 	
options. 

	
61	
 4.	 Click	 Next	.	
The	 Table	 Filter	 section	 appears.	 It displays	 a list	 of	 tables	 (JSON	 objects)	 available	 in	 	
your	 model. 

	
62	
 5.	 Select	 the	 tables	 (JSON objects)	 that	 you	 want	 to	 forward	 engineer. 

	
63	
 6.	 Click	 Preview	 to	 view	 the	 schema	 script.	
Use	 the	 following	 options:	
 l	Auto	 Error	 Check	: Select	 this	 option	 to	 enable	 auto	 error	 check	 by	 the	 forward	 	
engineering	 wizard.	
 l	Error	 Check	 (	): Use	 this	 option	 to	 run	 an	 error	 check.	 Based	 on	 the	 results,	 	
you	 can	 correct	 the	 generated	 script.	
 l	Text	 Options	 (	): Use	 this	 option	 to	 configure	 the	 preview	 text	 editor's	 look	 	
and	 feel,	 such	 as	 window,	 font,	 syntax	 color	 settings.	 For	 more	 information,	  

	
64	
refer	 to	 the	 Forward	 Engineering	 Wizard	 - Preview	 Editor	 topic.	
 l	Save	 (	): Use	 this	 option	 to	 save	 the	 generated	 script.	
 7.	 Click	 Generate	.	
The	 following	 screen	 appears.	
 8.	 Use	 the	 following	 options:	
Set	 Path	: Use	 this	 option	 to	 set	 the	 location	 to	 save	 the	 script	 file.	
Generate	 Multiple	 Files	: By	 default,	 a single	 script	 file	 is created.	 Select	 this	 	
option	 to	 save	 the	 script	 into	 multiple	 files	 by	 objects.	
File	 Name	 Prefix	: Select	 this	 option	 to	 add	 a script	 file	 name.	 Enter	 a file	 name.	 	
If this	 option	 is not	 selected,	 the	 script	 file	 is saved	 with	 a default	 name	 (Erwin_	
FE_	Script.json).	
 9.	 Click	 Save	.	
Your	 script	 file	 is saved	 at	 the	 configured	 location.	 You	 can	 open	 it in	 any	 text	 editor	 	
and	 verify. 

	
65	
Oracle	 Support	 Summary	
erwin	 Data	 Modeler	 (DM)	 now	 supports	 Oracle	 12c	 R2,	 18c,	 19c,	 and	 21c	 as	 target	 data	-	
bases.	 This	 implementation	 supports	 the	 following	 objects:	
Cluster
Column
Comment
Context
Database
Database	 Link	
Directory
Disk	 Group	
Function
Index	 Editor	 for	 Clusters	
Index	 Editor	 for	 Materialized	 Views	
Index	 Editor	 for	 Tables	
Library
Materialized	 Views	
Materialized	 View	 Log	
Package
Package	 Body	
Pre	 and	 Post	 Scripts	
Rollback	 Segment	
Sequence
Stored	 Procedure	
Synonym 

	
66	
Table
Tablespace
Tablespace	 Group	
Trigger
Views	
The	 following	 table	 lists	 the	 supported	 data	 types:	
Numeric	String	 Literals	Date	 and	 Time	Other	
 l	BINARY_
DOUBLE	
 l	BINARY_
FLOAT	
 l	DEC	
 l	DEC	()	
 l	DEC	(,)	
 l	DECIMAL	
 l	DECIMAL	()	
 l	DECIMAL	(,)	
 l	DOUBLE	 	
PRECISION	
 l	REAL	
 l	FLOAT	
 l	FLOAT	()	
 l	INT	
 l	INTEGER	
 l	NUMBER	
 l	CHAR	
 l	CHAR	()	
 l	CHARVARYING	()	
 l	CHARACTER	
 l	CHARACTER	()	
 l	CHARACTERVARYING
()	
 l	CLOB	
 l	DBURITYPE	
 l	URITYPE	
 l	HTTPURITYPE	
 l	JSON	
 l	JSON	()	
 l	LONG	
 l	NATIONAL	 CHAR	
 l	NATIONAL	 CHAR	 	
VARYING	()	
 l	NATIONAL	 CHAR	()	
 l	DATE	
 l	INTERVAL	 	
DAY	 TO	 	
SECOND	
 l	INTERVAL	 	
YEAR	 TO	 	
MONTH	
 l	TIMESTAMP	
 l	TIMESTAMP	 	
WITH	 LOCAL	 	
TIMEZONE	
 l	TIMESTAMP	 	
WITH	 	
TIMEZONE	
 l	TIMESTAMP
()	
 l	TIMESTAMP
() WITH	 	
LOCAL	 	
TIMEZONE	
 l	TIMESTAMP
() WITH	 	
 l	JSON*	
 l	ANYDATA	
 l	ANYDATASET	
 l	ANYTYPE	
 l	BFILE	
 l	BLOB	
 l	LONGRAW	
 l	ORDAUDIO	
 l	ORDDICOM	
 l	ORDDOC	
 l	ORDIMAGE	
 l	ORDVIDEO	
 l	RAW	()	
 l	SDO_	GEOMETRY	
 l	SDO_	GEORASTER	
 l	SI_	AVERAGECOLOR	
 l	SI_	COLOR 

	
67	
 l	NUMBER	()	
 l	NUMBER	(,)	
 l	NUMERIC	
 l	NUMERIC	()	
 l	NUMERIC
(,)	
 l	ROWID	
 l	SMALLINT	
 l	NATIONAL	 	
CHARACTER	
 l	NATIONAL	 	
CHARACTER	 	
VARYING	()	
 l	NATIONAL	 	
CHARACTER	()	
 l	NCHAR	
 l	NCHAR	 VARYING	()	
 l	NCHAR	()	
 l	NCLOB	
 l	NVARCHAR2	()	
 l	UROWID	
 l	UROWID	()	
 l	VARCHAR	()	
 l	VARCHAR2	()	
 l	XDBURITYPE	
 l	XMLTYPE	
TIMEZONE	 l	SI_
COLORHISTOGRAM	
 l	SI_	FEATURELIST	
 l	SI_
POSITIONALCOLOR	
 l	SI_	STILLIMAGE	
 l	SI_	TEXTURE	                 	
*This	 datatype	 is supported	 only	 for	 Oracle	 21c. 

	
68	
Microsoft	 SQL	 Server	 Support	
Support	 for	 Microsoft	 SQL	 Server	 2019	 as	 a target	 database	 has	 been	 enhanced	 to	 imple	-	
ment	 the	 following	 objects:	
External	 Library	
External	 Language	
External	 Data	 Source	
External	 File	 Format	
External	 Table	
Statistics	
For	 detailed	 information	 on	 supported	 objects	 and	 data	 types,	 refer	 SQL	 Server	 support	 sum	-	
mary	. 

	
69	
Microsoft	 Azure	 SQL	 Server	 Support	
Microsoft	 Azure	 SQL	 support	 in	 erwin	 DM	 has	 been	 revamped.	 It is now	 supported	 on	 top	 of	 	
Microsoft	 SQL	 Server	 to	 leverage	 common	 functionality.	 On	 the	 New	 Model	 and	 Target	 Data	-	
base	 dialog	 boxes,	 you	 can	 find	 Azure	 under	 SQL	 Server	 Version	 drop-	down	 list.	
The	 following	 table	 lists	 the	 supported	 objects:	
Supported	 Objects	
 l	Always	 Encrypted	 	
Keys	
 l	Application	 Roles	
 l	Assemblies*	
 l	Asymmetric	 Keys	
 l	Certificates	
 l	Credentials*	
 l	Database	 Roles	
 l	Databases	
 l	Database	 Triggers	
 l	External	 Data	 	
 l	Resource	 Pools*	
 l	Schemas	
 l	Sequences	
 l	Server	 Audits*	
 l	Server	 Audit	 Spe	-	
cification*	
 l	Statistics	
 l	Stored	 Procedures	
 l	Symmetric	 Keys	
 l	Spatial	 Indexes	 (Table)	
 l	Synonyms 

	
70	
Source	
 l	External	 File	 	
Format*	
 l	External	 Library*	
 l	External	 Table	
 l	Full-	Text	 Catalogs	
 l	Full-	Text	 Indexes	 	
(Table)	
 l	Full-	Text	 Stoplists	
 l	Functions	
 l	Logins	
 l	Partition	 Func	-	
tions	
 l	Partition	 Schemes	
 l	Tables	
 l	ColumnStore	 Indexes	 	
(Table)	
 l	XML	 Indexes	 (Table)	
 l	Indexes	 (Table)	
 l	Table	 Triggers	
 l	Triggers	
 l	User	 Ids	
 l	Views	
 l	View	 Indexes	
 l	View	 Triggers	
 l	XML	 Schema	 Col	-	
lections	
* These	 objects	 are	 supported	 only	 for	 Azure	 SQL	 Managed	 Instance.	
The	 following	 table	 lists	 the	 supported	 data	 types:	
Exact	 	
Numerics	
Approx	-	
imate	 	
Numer	-	
ics	
Date	 and	 	
Time	
Char	-	
acter	 	
Strings	
Unicode	 	
Char	-	
acter	 	
Strings	
Binary	 	
Strings	
Geo	 Types	Others	
 l	bigin	-	
t	
 l	num	-	
eric	
 l	bit	
 l	smal	-	
 l	fl-
o-
at	
 l	re	-	
al	
 l	date	
 l	dat	-	
etime	-	
offset	
 l	dat	-	
etime2	
 l	ch	-	
ar	
 l	va	-	
rc	-	
h-
ar	
 l	te	-	
 l	nc	-	
har	
 l	nv	-	
arc	-	
har	
 l	nt	-	
ext	
 l	bin	-	
ary	
 l	var	-	
bin	-	
ary	
 l	im	-	
age	
 l	rowver	-	
sion	
 l	hier	-	
archyid	
 l	uniquei	-	
den	-	
tifier	
 l	
CHA	-	
R 
VAR	-	
YING	
 l	CHA	-	
RAC	- 

	
71	
lint	
 l	deci	-	
mal	
 l	small	-	
mon	-	
ey	
 l	int	
 l	tiny	-	
int	
 l	mon	-	
ey	
 l	smalld	-	
ate	-	
time	
 l	dat	-	
etime	
 l	time	
xt	 l	sql_	vari	-	
ant	
 l	xml	
 l	geo	-	
metry	
 l	geo	-	
graphy	
TER	
 l	CHA	-	
RAC	-	
TER	 	
VAR	-	
YING	
 l	NATI	-	
ONA	-	
L 
CHA	-	
R 
VAR	-	
YING	
 l	NATI	-	
ONA	-	
L 
CHA	-	
R	
 l	NATI	-	
ONA	-	
L 
CHA	-	
RAC	-	
TER	
 l	NATI	-	
ONA	-	
L 
TEXT	                

	
72	
MySQL	 Support	
erwin	 Data	 Modeler	 (DM)	 now	 supports	 MySQL	 8.x	 as	 a target	 database.	 This	 imple	-	
mentation	 supports	 the	 following	 objects:	
Database
Event
Function
Function_	UDF	
Logfile	 Group	
Server
Spatial	 Ref	 System	
Stored	 Procedure	
Table	
Index
Table	 Column	
Tablespace
Trigger
User	 ID	
Validation	 Rule	
View	
View	 Column	
The	 following	 table	 lists	 the	 supported	 data	 types:	
Numeric	String	 Literals	Date	 and	 Time	Other	
 l	TINYINT	
 l	SMALLINT	
 l	MEDIUMINT	
 l	INT,	 	
INTEGER	
 l	CHAR	
 l	VARCHAR	
 l	BINARY	
 l	CHAR	 BYTE	
 l	DATE	
 l	TIME	
 l	DATETIME	
 l	TIMESTAMP	
 l	YEAR	 	
 l	Geometry	 Type	    	
 l	POINT	 	
 l	LINESTRING	 	
 l	POLYGON	 	
 l	MULTIPOINT	  

	
73	
 l	BIGINT	
 l	DECIMAL,	 	
DEC,	 	
NUMERIC,	 	
FIXED	
 l	FLOAT	
 l	DOUBLE,	 	
DOUBLE	 	
PRECISION,	 	
REAL	
 l	BIT	
 l	VARBINARY	
 l	TINYBLOB	
 l	BLOB	
 l	BLOB	 and	 	
TEXT	 Data	 	
Types	
 l	MEDIUMBLOB	
 l	LONGBLOB	
 l	TINYTEXT	
 l	TEXT	
 l	MEDIUMTEXT	
 l	LONGTEXT	
 l	JSON	 Data	 	
Type	
 l	ENUM	
 l	Set	 Data	 Type	
 l	MULTILINESTRING	
 l	MULTIPOLYGON	 	
 l	GEOMETRYCOLLECTION	
 l	GEOMETRY	                  	
Note:	
Refer	 to	 MySQL	 database	 documentation	 for	 detailed	 information	 on	 specific	 MySQL	 	
objects	 and	 properties.	 erwinÂ®	 Data	 Modeler	 documentation	 for	 the	 property	 editors	 	
provides	 brief	 descriptions	 of	 the	 controls	 on	 each	 dialog	 box	 and	 tab,	 which	 you	 can	 	
use	 as	 a point	 of	 reference	 while	 working	 with	 database	 design	 features.	
As	 a best	 practice,	 use	 the	 MySQL	 ANSI	 ODBC	 driver	 for	 Reverse	 Engineering	 from	 	
Database	 (REDB)	 while	 using	 erwinÂ®	 Data	 Modeler 

	
74	
Data	 Vault	 2.0	 Support	
erwin	 Data	 Modeler	 (DM)	 now	 Data	 Vault	 2.0	 as	 a modeling	 technique	 across	 all	 target	 data	-	
bases.	 This	 implementation	 supports	 the	 following	 Data	 Vault	 2.0	 components	 by	 default	 	
through	 API:	
Hub
Link
Satellite
Reference
PIT
Bridge	
These	 components	 are	 available	 on	 Home	 tab	 of	 the	 ribbon.	
To	 enable	 Data	 Vault	 2.0	 on	 your	 model,	 follow	 these	 steps:	
 1.	 Right-	click	 the	 model	 and	 click	 Properties	.	
 2.	 On	 the	 Model	 Editor	 > General	 tab,	 select	 the	 Data	 Vault	 2.0	 check	 box. 

	
75	
Once	 enabled,	 Data	 Vault	 2.0	 components	 are	 available	 via	 the	 Model	 Explorer.	 You	 	
can	 now	 convert	 your	 model	 to	 a Data	 Vault	 model	.	
You	 can	 also	 create	 custom	 components	 and	 apply	 them	 to	 tables.	 However,	 these	 	
custom	 components	 do	 not	 appear	 on	 the	 ribbon. 

	
76	
Productivity	 and	 UI	 Enhancements	
Several	 additions	 and	 enhancements	 have	 been	 implemented	 to	 improve	 erwin	 Data	 	
Modeler's	 (DM)	 productivity	 and	 usage	 experience.	 These	 enhancements	 are:	
Welcome	 Page	
Objects	 Count	 Pane	
Properties	 Pane	
Object	 Browser	
Normalization	 and	 Denormalization	
Reverse	 Engineering	 and	 Forward	 Engineering	 Wizard	 Redesign	
Improved	 Speed	 Mode	
Welcome	 Page	
The	 Welcome	 page	 is a starter	 page	 that	 helps	 new	 users	 to	 get	 started	 with	 erwin	 DM.	 It 	
appears	 when	 you	 launch	 erwin	 DM	 and	 is also	 accessible	 via	 Help	 > Welcome	. It contains	 	
shortcuts	 to	 key	 actions	 that	 are	 performed	 frequently,	 such	 as	 opening	 models	 or	 creating	 	
new	 ones,	 running	 reverse	 engineering	 or	 complete	 compare	 wizards,	 and	 connecting	 to	 the	 	
Mart.	 Apart	 from	 these,	 the	 Welcome	 page	 provides	 access	 to	 recently	 used	 files,	  erwin	 DM	 	
Tools,	 Technical	 Support,	 and	 Help	 links. 

	
77 

	
78	
Objects	 Count	 Pane	
As	 data	 models	 become	 complex	 and	 large,	 it becomes	 necessary	 to	 get	 a snapshot	 of	 the	 	
objects	 within	 models.	 To	 facilitate	 this,	 erwin	 DM	 now	 includes	 an	 Objects	 Count	 pane.	 This	 	
pane	 displays	 information	 about	 a selected	 model	 and	 a count	 of	 all	 the	 objects	 present	 in	 	
it.	 Also,	 it displays	 a snapshot	 of	 this	 information	 in	 the	 pictorial	 format,	 which	 you	 can	 cus	-	
tomize	 using	 the	 Style	 and	 Pallete	 options.	 By	 default,	 this	 pane	 opens	 on	 the	 right-	side	 of	 	
the	 application.	 You	 can	 also	 access	 it via	 View	 > Panes	 > Objects	 Count	 Pane	.	
Properties	 Pane	
While	 working	 on	 data	 models,	 accessing	 the	 property	 editors	 to	 view	 or	 edit	 the	 model	 and	 	
its	 object's	 properties	 can	 get	 tedious	 and	 slow	 you	 down.	 To	 address	 this,	 erwin	 DM	 now	 	
includes	 a Properties	 pane.	 This	 pane	 enables	 you	 to	 view	 and	 edit	 the	 selected	 object's	 	
properties	 along	 with	 the	 model	 diagram,	 side-	by-	side.	 By	 default,	 this	 pane	 opens	 on	 the	  

	
79	
right-	side	 of	 the	 application.	 You	 can	 also	 access	 it via	 View	 > Panes	 > Properties	 Pane	 on	 	
the	 ribbon.	
Object	 Browser	
The	 Object	 Browser	 is a one-	stop	 location	 where	 you	 can	 view	 tables,	 views,	 materialized	 	
views,	 indexes,	 relationships,	 and	 the	 complete	 model's	 or	 specific	 table's	 DDL.	 You	 can	 	
export	 this	 information	 as	 a report	 in	 CSV,	 HTML,	 or	 PDF formats.	 To	 access	 the	 Object	 	
Browser,	 on	 the	 Properties	 pane,	 click	 	 or	 on	 the	 ribbon,	 click	 Tools	 > Object	 Browser	. 	
For	 more	 information,	 refer	 to	 the	 Object	 Browser	 topic. 

	
80	
Normalization	 and	 Denormalization	
The	 Normalization	 and	 Denormalization	 features	 enable	 you	 to	 define	 relationships	 in	 a 	
NoSQL	 model.	 Normalization	 splits	 the	 fields	 in	 a collection	 into	 multiple	 collections	 based	 	
on	 the	 selected	 relationship	 type.	 Whereas	 Denormalization	 embeds	 multiple	 collections	 	
into	 a single	 collection	 based	 on	 the	 selected	 embedding	 type.	 To	 access	 these	 features,	 on	 	
the	 ribbon,	 click	 Actions	. Then,	 click	 Normalization	 or	 Denormalization	. For	 more	 inform	-	
ation,	 refer	 to	 the	 Defining	 Relationships	 Using	 Embedding	 Method	 topic. 

	
81	
Reverse	 Engineering	 and	 Forward	 Engineering	 Wizard	 Redesign	
The	 Reverse	 Engineering	 and	 Forward	 Engineering	 wizards	 have	 been	 redesigned	 for	 better	 	
arrangement	 of	 properties	 and	 ease	 of	 use.	 For	 more	 information,	 refer	 to	 the	 Reverse	 	
Engineering	 and	 Forward	 Engineering	 topics. 

	
82 

	
83	
Improved	 Speed	 Mode	
The	 Load	 Diagram	 with	 speed	 mode	 option	 now	 provides	 another	 option,	 Load	 Diagram	 	
with	 entity/table	 view.	 This	 option	 improves	 the	 model	 load	 performance	 in	 case	 of	 large	 	
models	 significantly.	 It does	 so	 by	  rendering	 the	 model	 in	 the	 simplest	 way	 possible,	 with	 	
only	 the	 entities	 or	 tables,	 and	 their	 relationships.	 Also,	 by	 default,	 it disables	 the	 PK-	FK	 high	-	
light	 feature	 (Display	 Diagram	 Highlight	 option	 on	 Model	 Editor).	
This	 option	 is available	 on	 Tools	 > Options	 dialog	 box. 

	
84	
  

	
85	
JDBC	 Support	
erwin	 DM	 now	 includes	 JDBC	 support	 for	 the	 following	 databases:	
Oracle
SQL	 Server	
Azure	 SQL	
Cassandra
Couchbase
MongoDB
MySQL
MariaDB	
Along	 with	 this,	 the	 JDBC	 support	 for	 Snowflake	 has	 been	 updated	 to	 support	 new	 features.	
The	 JDBC	 database	 connection	 parameters	 for	 the	 above	 databases	 are	 as	 follows:	
Oracle	
Instance	
Specifies	 the	 JDBC	 instance	 to	 which	 you	 want	 to	 connect.	
For	 a cloud-	based	 connected,	 the	 instance	 name	 is as	 follows:	
TNS_	ADMIN=<Path	 of	 unzipped	 cloud	 wallet	 file>	
For	 example,	 TNS_	ADMIN=C:\\Users\\MyUser\\Wallet_	DBTEST	
Note:	 Ensure	 that	 you	 have	 downloaded,	 saved,	 and	 unzipped	 the	 cloud	 wallet	 	
file.	
Connection	 String	
Specifies	 the	 connection	 string	 based	 on	 your	 JDBC	 instance	 in	 the	 following	 	
format: 

	
86	
jdbc:oracle:thin:@//<servername>:1521/
For	 example,	 JDBC:ORACLE:thin:@//localhost:1521/	
For	 a cloud	 instance,	 the	 connection	 string	 is as	 follows:	
jdbc:oracle:thin:@<dbname_	priority>?	
For	 example,	 jdbc:oracle:thin:@dbtest_	medium?	
SQL	 Server	
Connection	 Type	
Specifies	 the	 type	 of	 connection	 you	 want	 to	 use.	 Select	 Use	 Native	 Connection	 	
to	 connect	 using	 the	 API	 provided	 by	 the	 SQL	 Server	 Native	 client	 software.	 	
Select	 Use	 ODBC	 Data	 Source	 to	 connect	 using	 the	 ODBC	 data	 source	 that	 you	 	
have	 defined.	 Select	 Use	 JDBC	 Connection	 to	 connect	 using	 JDBC.	
Instance	
Specifies	 the	 JDBC	 instance	 to	 which	 you	 want	 to	 connect.	
Database	
Specifies	 the	 name	 of	 the	 database	 that	 you	 want	 to	 connect	 to.	
Connection	 String	
Specifies	 the	 connection	 string	 based	 on	 your	 JDBC	 instance	 and	 SQL	 Server	 	
database	 name	 in	 the	 following	 format:	
jdbc:sqlserver://<servername>:1433=<SqlDBname>
For	 example,	 jdbc:sqlserver://localhost:1433	
SQL	 Azure	
Connection	 Type	
Specifies	 the	 type	 of	 connection	 you	 want	 to	 use.	 Select	 Use	 Native	 Connection	 	
to	 connect	 using	 the	 API	 provided	 by	 the	 SQL	 Server	 Native	 client	 software.	 	
Select	 Use	 ODBC	 Data	 to	 connect	 using	 the	 ODBC	 data	 source	 that	 you	 have	 	
defined.	 Select	 Use	 JDBC	 Connection	 to	 connect	 using	 JDBC. 

	
87	
Instance	
Specifies	 the	 JDBC	 instance	 to	 which	 you	 want	 to	 connect.	
Database	
Specifies	 the	 name	 of	 the	 database	 that	 you	 want	 to	 connect	 to.	
Connection	 String	
Specifies	 the	 connection	 string	 based	 on	 your	 JDBC	 instance	 and	 SQL	 Server	 	
database	 name	 in	 the	 following	 format:	
jdbc:sqlserver://<servername>:<port>
For	 example,	 jdbc:sqlserver://localhost:1433	
Connect	 to	 Managed	 Instance	
Specifies	 whether	 the	 connection	 should	 be	 to	 an	 Azure	 SQL	 Managed	 Instance.	 	
Cassandra	
Connection	 Method	
Specifies	 the	 type	 of	 connection	 you	 want	 to	 use.	 Select	 Direct	 to	 connect	 to	 	
connect	 to	 your	 cluster	 directly.	 Select	 Connection	 String	 to	 connect	 to	 your	 	
cluster	 using	 a connection	 string.	
Hostname/IP	
Specifies	 the	 hostname	 or	 IP address	 of	 the	 server	 where	 your	 cluster	 is hosted.	
Port	
Specifies	 the	 port	 configured	 for	 your	 cluster.	
Connection	 String	
Specifies	 the	 path	 to	 the	 secure	 connect	 ZIP	 file	 in	 the	 following	 format:	
C:\<file	 name>.zip	
For	 example,	 C:\TempCass\secure-	connect-	testdb.zip	
Couchbase 

	
88	
Connection	 Method	
Specifies	 the	 type	 of	 connection	 you	 want	 to	 use.	 Select	 Direct	 to	 connect	 to	 	
connect	 to	 your	 bucket	 directly.	 Select	 Connection	 String	 to	 connect	 to	 your	 	
bucket	 using	 a connection	 string.	
Hostname/IP	
Specifies	 the	 hostname	 or	 IP address	 of	 the	 server	 where	 your	 bucket	 is hosted.	
Port	
Specifies	 the	 port	 configured	 for	 your	 bucket.	
Bucket	
Specifies	 the	 name	 of	 the	 bucket	 to	 which	 you	 want	 to	 connect.	
SSL	 Certificate	 Path	
Specifies	 the	 path	 to	 the	 SSL	 certificate,	 if you	 have	 one.	 You	 can	 leave	 this	 field	 	
blank.	
Connection	 String	
Specifies	 the	 connection	 string	 in	 the	 following	 format:	
couchbases://<database	 server>/<bucket>?ssl=no_	verify	
For	 example,	 couchbases://server1.dp.cloud.couchbase.com/testbucket?ssl=no_	
verify	
MongoDB	
Connection	 Method	
Specifies	 the	 type	 of	 connection	 you	 want	 to	 use.	 Select	 Direct	 to	 connect	 to	 	
connect	 to	 your	 database	 directly.	 Select	 Connection	 String	 to	 connect	 to	 your	 	
database	 using	 a connection	 string.	
Hostname/IP	
Specifies	 the	 hostname	 or	 IP address	 of	 the	 server	 where	 your	 database	 is hos	-	
ted. 

	
89	
Port	
Specifies	 the	 port	 configured	 for	 your	 database.	
Database	
Specifies	 the	 name	 of	 the	 database	 to	 which	 you	 want	 to	 connect.	
Connection	 String	
Specifies	 the	 connection	 string	 in	 the	 following	 format:	
mongodb://	[username:password@]host1	[:port1]	[,...hostN	[:portN]]	[/	[defaultau	-	
thdb]	[?options]]	
For	 example,	 mongodb+srv://myusername:****	
@cluster0.v7gra.mongodb.net/test?retryWrites=true&w=majority	
MySQL	
Hostname/IP	
Specifies	 the	 hostname	 or	 IP address	 of	 the	 server	 where	 your	 database	 is hos	-	
ted.	
Port	
Specifies	 the	 port	 configured	 for	 your	 database.	
Database	
Specifies	 the	 name	 of	 the	 database	 to	 which	 you	 want	 to	 connect.	
Note	: For	 the	 JDBC	 connection	 to	 work	 seamlessly,	 ensure	 that	 you	 download	 the	 	
required	 JDBC	 driver	 and	 rename	 it to	 mysql-	connector-	java-	8.0.22.jar.	
MariaDB	
Connection	 String	
Specifies	 the	 connection	 string	 in	 the	 following	 format:	
jdbc:mariadb 

	
90	
Hostname/IP	
Specifies	 the	 hostname	 or	 IP address	 of	 the	 server	 where	 your	 database	 is hos	-	
ted.	
Port	
Specifies	 the	 port	 configured	 for	 your	 database.	
Database	
Specifies	 the	 name	 of	 the	 database	 to	 which	 you	 want	 to	 connect.	
Note	: For	 the	 JDBC	 connection	 to	 work	 seamlessly,	 ensure	 that	 you	 download	 the	 	
required	 JDBC	 driver	 and	 rename	 it to	 mariadb-	java-	client-	2.6.1.jar. 

"
Non,"Big Ideas for CS 251	â¨	Theory of Programming Languages	â¨	Principles of Programming Languages	""	â¯""#$%&'()*(+,,-.*&/+.*0+*12&""3(-.*&#4%56&/7.&80(9+:&â¯""#$%&'(#)'â¯*+â¯,*($-'#&â¯./0#)/#â¯1#22#32#4â¯,*22#5#â¯	
P	rogramming 	L	anguages	""	
â¢â¯	What is a PL?	""	
â¢â¯	Why are new PLs created?  	""	â	â¯	What are they used for?	""	â	â¯	Why are there so many?	""	â¢â¯	Why are certain PLs popular?	""	â¢â¯	What goes into the design of a PL? 	""	â	â¯	What features must/should it contain?	""	â	â¯	What are the design dimensions?	""	â	â¯	What are design decisions that must be made?	""	â¢â¯	Why should you take this course? What will you learn?	""	
Big ideas 2	""	
67â¯03â¯(4â¯$%330*)8â¯
â¢â¯	First PL project in 1982 as intern	â¨	at Xerox PARC	""	
â¢â¯	Created visual PL for 1986 MIT 	â¨	
masters thesis	""	
â¢â¯	1994 MIT PhD on PL feature 	â¨	
(synchronized lazy aggregates)	""	
â¢â¯	1996 â 2006: worked on types 	â¨	
as member of Church project	""	
â¢â¯	1988 â 2008: 	Design Concepts in Programming Languages	â¯	â¢â¯	2011 â current: lead 	TinkerBlocks	 research team at Wellesley	""	â¢â¯	2012 â current: member of App Inventor development team	""	
 ""	Big ideas 3	""
9#)#&%2â¯6-&$*3#â¯673â¯	
Python 
Fortran 
C/C	++ 	Java 
;+<:1=&ML Haskell CommonLisp	 	
Perl Ruby 
JavaScript 	
Big ideas 4	""	
ScalaC#  

""*(%0)!.$#/0:/!673!;"".73<!	IDL 
CSS	 	
PostScript	!	>8?/&OpenGL LaTeX	 	Excel Matlab	 	
R 
Swift 	
Big ideas 5	""	
Digital AmatiJINJA	 	6&*5&%((0)5!7%)5-%5#3=!>#/?%)0/%2!@0#A!!	B!/*($-'#&!03!%!(%/?0)#C!D-&!%0(!03!'*!(%E#!'?#!(%/?0)#!$#&+*&(!3*(#!3$#/0:#F!%/G*)3C!!10'?!3*(#!(%/?0)#3!A#!(05?'!#H$&#33!*-&!0)'#)G*)3!I4!F#$&#330)5!E#43J!$-3?0)5!I-K*)3J!&*'%G)5!E)*I3J!#'/C!!L*&!%!/*($-'#&J!A#!/*)3'&-/'!%!3#M-#)/#!*+!0)3'&-/G*)3!;'?03!03!%!NN$&*5&%(OO<!%)F!$&#3#)'!'?03!3#M-#)/#!'*!'?#!(%/?0)#C!!!!!!!!!""!#$%&'()'!*+,-(./(0!1$.)$2!1&/3&$44-(3!  	
Big ideas 6	""	
6&*5&%((0)5!7%)5-%5#3=!70)5-03G/!@0#A!!	B!/*($-'#&!2%)5-%5#!P!03!%!)*Q#2!+*&(%2!(#F0-(!+*&!#H$&#330)5!0F#%3!%I*-'!(#'?*F*2*54J!)*'!R-3'!%!A%4!'*!5#'!%!/*($-'#&!'*!$#&+*&(!*$#&%G*)3C!!6&*5&%(3!%&#!A&0K#)!+*&!$#*$2#!'*!&#%FJ!%)F!*)24!0)/0F#)'%224!+*&!(%/?0)#3!'*!#H#/-'#C!!!!!!!!!!!!!!!!!!!!!	â	!5$&/26!*7'2./(!$(6!8'&$26!9:!	Sussman	!	  	
Big ideas 7	""
 	Religious	 	!@0#A3!	
T?#!-3#!*+!,DUD7!/&0$$2#3!'?#!(0)FV!0'3!'#%/?0)5!3?*-2FJ!'?#&#+*&#J!I#!&#5%&F#F!%3!%!/&0(0)%2!*W#)3#C!	X!	Edsger	!Dijkstra	!	
Y'!03!$&%/G/%224!0($*330I2#!'*!'#%/?!5**F!$&*5&%((0)5!'*!3'-F#)'3!'?%'!
?%Q#!?%F!%!$&0*&!#H$*3-&#!'*!UB.Y,=!%3!$*'#)G%2!$&*5&%((#&3!'?#4!%&#!(#)'%224!(-G2%'#F!I#4*)F!?*$#!*+!&#5#)#&%G*)C!!	X!	Edsger	!Dijstra	!	
Z*-O&#!0)'&*F-/0)5!4*-&!3'-F#)'3!'*!$&*5&%((0)5!0)!,[!!!Z*-!(05?'!%3!A#22!
50Q#!'?#(!%!+&*)'%2!2*I*'*(48!!	X!	*!)/22'$3%'!/?!4-('	!	
B!7Y.6!$&*5&%((#&!E)*A3!'?#!Q%2-#!*+!#Q#&4'?0)5J!I-'!'?#!/*3'!*+!)*'?0)5C!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
\!!	*2$(!1'&2-.!!	
Y!?%Q#!)#Q#&!(#'!%!3'-F#)'!A?*!/-'!'?#0&!'##'?!0)!%)4!*+!'?#3#!2%)5-%5#3!
%)F!F0F!)*'!/*(#!%A%4!$&*+*-)F24!F%(%5#F!%)F!-)%I2#!'*!/*$#C!	P!	Y!(#%)!	'?03!&#%F3!'*!(#!Q#&4!30(02%&24!'*!'#%/?0)5!3*(#*)#!'*!I#!%!/%&$#)'#&!I4!3'%&G)5!'?#(!*W!A0'?!$2%3G/!'*4!'**23!%)F!'#220)5!'?#(!'*!5*!3/-2$'!3%)F!*)!'?#!I#%/?C!	\!!	9/@(!	Haugeland	0!/(!72/),.!2$(3%$3'.	!	
B!2%)5-%5#!'?%'!F*#3)O'!%W#/'!'?#!A%4!4*-!'?0)E!%I*-'!$&*5&%((0)5J!03!)*'!
A*&'?!E)*A0)5C!!!	\!!	*2$(!1'&2-.!	!	  	
Big ideas 8	"" 

CS111 Big idea  #1: Abstraction 	
Function & 	â¯	Data Abstraction	â¯	Implementer	Function & 	â¯	Data Abstraction	â¯	User / Client	Contract / API	Big ideas 9	""	
1?0/?â¯6&*5&%((0)5]67â¯^%'â¯F*â¯Z*-â¯1#%&[â¯
Programming Language 	Designer	
6&*5&%((0)5â¯6%&%F05(3â¯	
â¢â¯	Impera've	â¯A':3:â¯B0â¯1C+@/(D	:â¯,*($-'%G*)â¯03â¯3'#$\I4\3'#$â¯#H#/-G*)â¯*)â¯%â¯	stateful	â¯%I3'&%/'â¯(%/?0)#â¯0)Q*2Q0)5â¯(#(*&4â¯32*'3â¯%)Fâ¯(-'%I2#â¯F%'%â¯	3'&-/'-&#3Câ¯â¯
â¢â¯	Func'onal	0â¯/*+,'-+0-%1$+2$34	(e.g	â¯E$),'+0â¯F#0â¯5$.,'22D	=â¯,*($-'%G*)â¯03â¯	
#H$&#33#Fâ¯I4â¯/*($*30)5â¯+-)/G*)3â¯'?%'â¯(%)0$-2%'#â¯0((-'%I2#â¯F%'%Câ¯
â¢â¯	Object-oriented	â¯A':3:â¯	Simula	0â¯;4$22+$2,0â¯9$G$D	=â¯,*($-'%G*)â¯03â¯#H$&#33#Fâ¯0)â¯	
'#&(3â¯*+â¯	stateful	â¯*IR#/'3â¯'?%'â¯/*((-)0/%'#â¯I4â¯$%330)5â¯(#33%5#3â¯'*â¯*)#â¯	%)*'?#&Câ¯â¯
â¢â¯	Logic-oriented	â¯A':3:â¯1&/2/3D	=â¯,*($-'%G*)â¯03â¯#H$&#33#Fâ¯0)â¯'#&(3â¯*+â¯F#/2%&%GQ#â¯	
&#2%G*)3?0$3Câ¯â¯
@)=1A&	Y)â¯$&%/G/#Jâ¯(*3'â¯673â¯0)Q*2Q#â¯(-2G$2#â¯$%&%F05(3Câ¯_C5Câ¯â¯	â¢â¯	64'?*)â¯3-$$*&'3â¯+-)/G*)%2â¯+#%'-&#3â¯;(%$Jâ¯:2'#&Jâ¯203'â¯/*($&#?#)30*)3<â¯%)Fâ¯	
*IR#/'3â¯â¯
â¢â¯	S%/E#'â¯%)Fâ¯>7â¯?%Q#â¯0($#&%GQ#â¯+#%'-&#3Câ¯â¯	
Big ideas 10	""	quicksort 	::	 Ord	 a 	=>	 [a] 	->	 [a]	quicksort	 []     	= []	quicksort	 (	p:xs	) 	= 	        (	quicksort	 lesser	) 	â¨	        	++	 [p] 	        ++	 (	quicksort	 greater	)	    	where	        lesser  	= filter	 (	< p) 	xs	        greater 	= filter	 (	>=	 p) 	xs	
6%&%F05(â¯_H%($2#=â¯`-0/E3*&'â¯	
void 	qsort	(int	 a[], 	int	 lo, 	int	 hi) {	  	int	 h, l, p, 	t;	  if (lo < hi) {    l = lo;    h = hi;    p = a[hi];    do {      while ((l < h) && (a[l] <= p))           l = l+1;      while ((h > l) && (a[h] >= p))          h = h-1;      if (l < h) {          t = a[l];          a[l] = a[h];          a[h] = t;      }    } while (l < h);    a[hi] = a[l];    a[l] = p;    	qsort	( a, lo, l-1 );	    	qsort	( a, l+1, hi );	  }}Y($#&%GQ#â¯.'42#â¯â¯;0)â¯,Vâ¯a%Q%â¯A*-2Fâ¯I#â¯30(02%&<â¯L-)/G*)%2â¯.'42#â¯;0)â¯^%3E#22<â¯	Big ideas 11	""	673â¯â¯F0W#&â¯I%3#Fâ¯*)â¯F#/030*)3â¯2%)5-%5#â¯F#305)#&3â¯(%E#â¯0)â¯(%)4â¯F0(#)30*)3Câ¯_C5C=â¯
â¢â¯	)1%:20,.&::4(&.*$:	Hâ¯A?%'â¯Q%2-#3â¯/%)â¯I#â¯)%(#FJâ¯$%33#Fâ¯%3â¯%&5-(#)'3â¯'*â¯	
+-)/G*)3Jâ¯&#'-&)#Fâ¯%3â¯Q%2-#3â¯+&*(â¯+-)/G*)3Jâ¯3'*&#Fâ¯0)â¯F%'%â¯3'&-/'-&#3Câ¯â¯1?0/?â¯*+â¯'?#3#â¯%&#â¯:&3'\/2%33â¯0)â¯4*-&â¯+%Q*&0'#â¯67=â¯%&&%43Jâ¯+-)/G*)3Jâ¯Q%&0%I2#3[â¯â¯
â¢â¯	Naming	=â¯""*â¯Q%&0%I2#3]$%&%(#'#&3â¯)%(#â¯#H$&#330*)3Jâ¯'?#â¯Q%2-#3â¯&#3-2G)5â¯	
+&*(â¯#Q%2-%G)5â¯#H$&#330*)3Jâ¯*&â¯(-'%I2#â¯32*'3â¯?*2F0)5â¯'?#â¯Q%2-#3â¯+&*(â¯#Q%2-%G)5â¯#H$&#330*)3[â¯â¯^*Aâ¯%&#â¯)%(#3â¯F#/2%&#Fâ¯%)Fâ¯&#+#&#)/#F[â¯1?%'â¯F#'#&(0)#3â¯'?#0&â¯3/*$#[â¯â¯
â¢â¯	State	=â¯1?%'â¯03â¯(-'%I2#â¯%)Fâ¯0((-'%I2#Vâ¯0C#CJâ¯A?%'â¯#)GG#3â¯0)â¯'?#â¯2%)5-%5#â¯	
;Q%&0%I2#3Jâ¯F%'%â¯3'&-/'-&#3Jâ¯*IR#/'3<â¯/%)â¯/?%)5#â¯*Q#&â¯G(#Câ¯â¯
â¢â¯	Control	=â¯1?%'â¯/*)3'&-/'3â¯%&#â¯'?#&#â¯+*&â¯/*)'&*2â¯b*Aâ¯0)â¯'?#â¯2%)5-%5#Jâ¯#C5Câ¯	
/*)F0G*)%23Jâ¯2**$3Jâ¯)*)\2*/%2â¯#H0'3Jâ¯#H/#$G*)â¯?%)F20)5Jâ¯/*)G)-%G*)3[â¯â¯
â¢â¯	Data	=â¯1?%'â¯E0)F3â¯*+â¯F%'%â¯3'&-/'-&#3â¯%&#â¯3-$$*&'#Fâ¯0)â¯'?#â¯2%)5-%5#Jâ¯0)/2-F0)5â¯	
$&*F-/'3â¯;%&&%43Jâ¯'-$2#3Jâ¯&#/*&F3Jâ¯F0/G*)%&0#3<Jâ¯3-(3â¯;*$G*)3Jâ¯	oneofs	Jâ¯	Q%&0%)'3<Jâ¯3-(\*+\$&*F-/'3Jâ¯%)Fâ¯*IR#/'3Câ¯â¯
â¢â¯	Types	=â¯â¯B&#â¯$&*5&%(3â¯3'%G/%224â¯*&â¯F4)%(0/%224â¯'4$#F[â¯1?%'â¯'4$#3â¯%&#â¯	
#H$&#330I2#[â¯	
PL Dimensions	""	
Big ideas 12	"" 

1?4!	study	!67[!	
â¢â¯	,&*33&*%F3!*+!,.!	â¢â¯	B$$&*%/?!$&*I2#(3!%3!%!	2$(3%$3'!6'.-3('&:	!	â	â¯	I*!3//6!J&/3&$44-(3!2$(3%$3'!-.!$!)/()'J+%$2!%(-G'&.'!?/&!+@-(,-(3!$7/%+!	J&/3&$44-(3K!!	\\!B2%)!6#&203!	â	â¯	_Q%2-%'#J!/*($%&#J!%)F!/?**3#!2%)5-%5#3!	â	â¯	U#/*(#!I#K#&!%'!2#%&)0)5!)#A!2%)5-%5#3!	â	â¯	U#/*(#!%!I#K#&!$&*5&%((#&!I4!2#Q#&%50)5!$*A#&+-2!+#%'-&#3!	;:&3'\/2%33!+-)/G*)3J!'&##!&#/-&30*)J!3-(\*+\$&*F-/'!	datatypes	J!$%K#&)!	(%'/?0)5<!â	â¯	Z*-!$&*I%I24!A*)c'!F#305)!%!5#)#&%2\$-&$*3#!67J!I-'!(05?'!F#305)!%!"".7!	â	â¯	Q0#A!B6Y!F#305)!%3!2%)5-%5#!F#305)!	â¢â¯	B3E=!	â	â¯	1?4!%&#!673!%&#!'?#!A%4!'?#4!%&#[!	â	â¯	^*A!/*-2F!'?#4!;*&!/*-2F)O'!'?#4<!I#!I#K#&[!	â	â¯	1?%'!03!'?#!/*3'\/*)Q#)0#)/#!'&%F#\*W!+*&!+#%'-&#!d[!	
Big ideas 13	""	
Administrivia	!	
â¢â¯	Schedule, 	psets	, quizzes, lateness policy, etc.:	â¨	see 	http://cs.wellesley.edu/~cs251/	. 	""	
â¢â¯	Do (most of) PS0 tonight	""	â	â¯	Fill out âget to know youâ 	Introze	 introduction. 	""	â	â¯	Review course syllabus and policies 	â¨	
(weâll go over these tomorrow)	""	
â	â¯	Read Wed slides on âbig-step semanticsâ of Racket	""	â	â¯	Install Dr. Racket 	""	â¢â¯	PS1 is available; due next Friday. Start it this week!	""	â¢â¯	Credit/non is a 	bad idea 	for 251. Talk to me ï¬rst!	""	â¢â¯	Visit me in o	ï¬	ce hours before next Friday!	""	
Big ideas 14	""	
PL Parts	""	
Syntax	: 	form	 of a PL 	""	â¢â¯	What a P in a given L look like as symbols?	""	â¢â¯	Concrete syntax 	vs	 abstract syntax trees (ASTs)	""	Semantics	: 	meaning 	of a PL 	""	â¢â¯	Dynamic Semantics	: What is the behavior of P? What actions does it 	perform? What values does it produce? 	""	â	â¯	Evaluation rules: what is the result or e	ï¬	ect of evaluating each language 	
fragment and how are these composed? 	""	
â¢â¯	Static Semantics: 	What can we tell about P before running it?	""	â	â¯	Scope rules: to which declaration does a variable reference refer?	""	â	â¯	Type rules: which programs are well-typed (and therefore legal)?	""	Pragmatics	: 	implementation 	of a PL (and PL environment) 	""	â¢â¯	How can we evaluate programs in the language on a computer?	""	â¢â¯	How can we optimize the performance of program execution?	â¨	""	Big ideas 15	""	
Syntax (Form) vs. Semantics (Meaning)	â¨	in Natural Language	""	Furiously sleep ideas green colorless.	""	
Colorless green ideas sleep furiously.	""	Little white rabbits sleep soundly. 	""	
""
""	
Big ideas 16	"" 

,*)/&#'#!.4)'%H=!BI3*2-'#!@%2-#!L-)/G*)!	
Logo	: to abs :n 	ifelse	 :n < 0 [output (0 - :n)] [output :n] end	""	
Javascript	: 	function abs (n) {if (n < 0) return -n; else return n;}	""	Java: 	public static 	int	 abs (	int	 n) {if (n < 0) return -n; else return n;}	""	Python:                                   	App Inventor: 	!	
def abs(n):	""	  if n < 0:	""	    return -n	""	  else: 	""	    return n	""	
Scheme/Racket: 	(deï¬ne abs (lambda (n) (if (< n 0) (- n) n)))	""	PostScript: 	/abs {dup 0 	lt	 {0 swap sub} if} def	""	
Big ideas 17	""	
 BI3'&%/'!.4)'%H!T&##!;B.T<=	!!	BI3*2-'#!@%2-#!L-)/G*)	!	
varref	!	&#'-&)!	)!	&#'-&)!intlit	!	0relaGonalOperaGon	!	varref	!	)!	condiGonalStatement	!	funcGonDeclaraGon	!	%I3!n+'.+!+@'(!7/6C!params	!	funcLonName	!	&$(6N!($4'!($4'!arithmeGcOperaGon	!	G$2%'!3-I'&%/'!varref	!	)!	($4'!G$2%'!intlit	!	0lessThan	!	G$2%'!&$(6N!T?03!B.T!%I3'&%/'3!*Q#&!'?#!/*)/&#'#!34)'%H!+*&!'?#!7*5*J!a%Q%./&0$'J!%)F!64'?*)!F#:)0G*)3C!!T?#!*'?#&!F#:)0G*)3!A*-2F!?%Q#!F0W#&#)'!B.T3C!	Big ideas 18	""	
""4)%(0/!.#(%)G/3!_H%($2#!f!	
Big ideas 19	""	
1?%'!03!'?#!(#%)0)5!*+!'?#!+*22*A0)5!#H$&#330*)[	!	
(1 + 11) * 10 	
!	""4)%(0/!.#(%)G/3!_H%($2#!g!	1?%'!03!$&0)'#F!I4!'?#!+*22*A0)5!$&*5&%([!	!	!
a = 1; b = a + 20; print(b); a = 300 print(b); count = 0; fun 	inc	() { count = count + 1; return count; } 	fun 	dbl	(ignore, x) { return x + x; } 	print(	dbl	(	inc	(), 	inc	()) 	 	
Big ideas 20	"" 

""4)%(0/!.#(%)G/3!_H%($2#!h!	
.-$$*3#!	a	!03!%)!%&&%4!;*&!203'<!/*)'%0)0)5!'?#!'?&##!0)'#5#&!Q%2-#3!feJ!geJ!%)F!he!	0)!'?#!+*22*A0)5!2%)5-%5#3C!1?%'!03!'?#!(#%)0)5!*+!'?#!+*22*A0)5!#H$&#330*)3]3'%'#(#)'3!0)!Q%&0*-3!2%)5-%5#3!;'?#!34)'%H!(05?'!F0W#&!+&*(!A?%'c3!3?*A)<C!
!!!
a[1] 	a[3] 	a[2] = 	""	foo	""  	a	[3] = 17 	a%Q%!	,!	64'?*)!	a%Q%./&0$'!	6%3/%2!	B$$!Y)Q#)'*&!	^*A!F*!4*-!F#'#&(0)#!'?#!%)3A#&3[[[!
!!!	
Big ideas 21	""	
.#(%)G/3!_H%($2#!h	&	
.-$$*3#!	a	!03!%)!%&&%4!;*&!203'<!/*)'%0)0)5!'?#!'?&##!0)'#5#&!Q%2-#3!feJ!geJ!%)F!he!	0)!'?#!+*22*A0)5!2%)5-%5#3C!1?%'!03!'?#!(#%)0)5!*+!'?#!+*22*A0)5!#H$&#330*)3]3'%'#(#)'3!0)!Q%&0*-3!2%)5-%5#3!;'?#!34)'%H!(05?'!F0W#&!+&*(!A?%'c3!3?*A)<C!
!!!
a[1] 	a[3] 	a[2] = 	""	foo	""  	a	[3] = 17 	a%Q%!	!!	,!	!!	64'?*)!	!!	a%Q%./&0$'!	!!	6%3/%2!	!!	B$$!Y)Q#)'*&!	!!^*A!F*!4*-!F#'#&(0)#!'?#!%)3A#&3[!	8(7&-.&-,3B1,1.=+C).D&<).20B=&E)<0,1.=+C).&	Big ideas 22	""	
.'%G/!.#(%)G/3!_H%($2#!f=!T4$#!,?#/E0)5!	
1?0/?!*+!'?#!+*22*A0)5!a%Q%!#H%($2#3!/%)!I#!A#22\'4$#F!;0C#CJ!$%33!'?#!'4$#!/?#/E#&<[!!^*A!F*!4*-!E)*A[!1?%'!%33-($G*)3!%&#!4*-!(%E0)5[!
!!
2 * (3 + 4) 2 < (3 + 4) 2 < True if (a < b) {    c = a + b;  } else {    c = a * b; } if (a) {    c = a + b;  } else {    c = a * b; } if (a < b) {    c = a + b;  } else {    c = a > b; } public 	boolean	 f(	int	 i, 	boolean	 b) { 	   return b && (	i > 0); 	} public 	int	 g(	int	 i, 	boolean	 b) { 	   return 	i * (b ? 1 : -1); 	} public 	int	 p(	int	 w) { 	   if (w > 0) { return 2*w; } } public 	int	 q(	int	 x) { return x > 0; } 	public 	int	 r(	int	 y) { return g(y, y>0); } 	public 	boolean	 s(	int	 z) { return f(z); } 	ABCDEFGHQ!9!KL	Big ideas 23	""	
Static Semantics Example 2: 	â¨	Detecting Loops	""	1?0/?!*+!'?#3#!64'?*)!$&*5&%(3!?%3!0)$-'3!+*&!A?0/?!0'!2**$3!+*&#Q#&[	""	def	 f(x): 	  	return	 x+1 	def	 g(x): 	  	while	 True: 	    	pass 	  	return	 x 	def	 h2(x): 	  	if	 x <= 0: 	     	return	 x 	  	else	:  	     	return	 h2(x+1) 	def	 h(x): 	  	while	 x > 0: 	    x = x+1   	return	 x 	def	 g2(x): 	  	return	 g2(x) 	def	 	collatz	(x): 	  	while	 x != 1: 	    	if (	x % 2) == 0: 	      x = x/2     	else	:  	      x = 3*x + 1   	return	 1 Big ideas 24	"" 

Static Semantics and 	Uncomputability	""	
Y'!03!5#)#&%224!	impossible	!'*!%)3A#&!%)4!0)'#&#3G)5!M-#3G*)!%I*-'!	3'%G/!$&*5&%(!%)%243038!!T?03!03!%!/*)3#M-#)/#!*+!	;-<1F2&8G1)(1,&	;3##!,.ghi<C!!	!For example, will this program ever:	""	â¢â¯	halt on certain inputs	""	â¢â¯	encounter an array index out of bounds error?	""	â¢â¯	throw a 	NullPointerException	?	""	â¢â¯	access a given object again?	""	â¢â¯	send sensitive information over the network?	""	â¢â¯	divide by 0?	""	â¢â¯	run out of memory, starting with a given amount available?	""	â¢â¯	try to treat an integer as an array?	""	
!!""	Big ideas 25	""	â¢â¯	!G0(<GH80(-.*&8G12-2	=!,*($-'%I020'4!03!'?#!/*((*)!3$0&0'!#(I*F0#F!I4!	'?03!/*22#/G*)!*+!+*&(%203(3C!
â¢â¯	T?03!'?#303!03!%!/2%0(!'?%'!03!A0F#24!I#20#Q#F!%I*-'!'?#!0)'-0GQ#!)*G*)3!*+!	
algorithm	!%)F!	#W#/GQ#!/*($-'%G*)	C!!Y'!03!)*'!%!'?#*&#(!'?%'!/%)!I#!	$&*Q#FC!!
â¢â¯	U#/%-3#!*+!'?#0&!30(02%&0'4!'*!2%'#&!/*($-'#&!?%&FA%&#J!T-&0)5!(%/?0)#3!	
;,.ghi<!?%Q#!I#/*(#!'?#!5*2F!3'%)F%&F!+*&!#W#/GQ#24!/*($-'%I2#C!!
â¢â¯	We	 ll!3##!0)!,.gif!'?%'!,?-&/?c3!2%(IF%\/%2/-2-3!+*&(%203(!03!'?#!	
+*-)F%G*)!*+!(*F#&)!$&*5&%((0)5!2%)5-%5#3C!!
â¢â¯	B!/*)3#M-#)/#=!$&*5&%((0)5!2%)5-%5#3!%22!?%Q#!'?#!	 same	 !	
/*($-'%G*)%2!	 power	 !0)!'#&(!*+!A?%'!'?#4!/%)!#H$&#33C!B22!3-/?!	2%)5-%5#3!%&#!3%0F!'*!I#!	Turing-complete	C!!	
T?#!,?-&/?\T-&0)5!T?#303!%)F!T-&0)5\,*($2#'#)#33!	
Big ideas 26	""	
_H$&#330Q#)#33!%)F!6*A#&!	
â¢	â¯	BI*-'=!	â	â¯#%3#!	â	â¯#2#5%)/#!	â	â¯/2%&0'4!	â	â¯(*F-2%&0'4!	â	â¯%I3'&%/G*)!	â	â¯CCC!	â¢	â¯	j*'!%I*-'=!/*($-'%I020'4!	â¢	â¯	""0W#&#)'!$&*I2#(3J!F0W#&#)'!2%)5-%5#3!	â	â¯L%/#I**E!*&!A#I!I&*A3#&!0)!%33#(I24!2%)5-%5#[!	
Big ideas 27	""	
Pragmatics: Ra	ï¬	e App In App Inventor	""	I12-*.1(&J-.E)K&LB)<:2&ME-=)(&T*!#)'#&!'?#!&%k#J!'#H'!(#!)*A!A0'?!!!%)!#($'4!(#33%5#=!	NN5H##$H4#OP&	?K$=]]%0gC%$$0)Q#)'*&C(0'C#F-!^*A!?%&F!03!'?03!'*!F*!0)!(*&#!'&%F0G*)%2!F#Q#2*$(#)'!#)Q0&*)(#)'3!+*&!B)F&*0F]iOS	?&	Big ideas 28	"" 

Pragmatics: 	Metaprogramming	""	
673!%&#!0($2#(#)'#F!0)!'#&(3!*+!	metaprogams	!l!$&*5&%(3!'?%'!	(%)0$-2%'#!*'?#&!$&*5&%(3C!!
T?03!(%4!3*-)F!A#0&FJ!I-'!$&*5&%(3!%&#!R-3'!'&##3!;B.T3<J!3*!%!
metaprogram	!03!R-3'!%!$&*5&%(!'?%'!(%)0$-2%'#3!'&##3!;'?0)E!%!	(*&#!/*($2#H!Q#&30*)!*+!,.ghe!I0)%&4!'&##!$&*5&%(3<C!!
Y($2#(#)'%G*)!3'&%'#50#3=!!â¢â¯	Interpreta'on	=!0)'#&$&#'!%!$&*5&%(!6!0)!%!3*-&/#!2%)5-%5#!.!0)!'#&(3!*+!%)!	
0($2#(#)'%G*)!2%)5-%5#!YC!!
â¢â¯	?%&+:.&'-+4A,-""#1.&'-+B	=!'&%)32%'#!%!$&*5&%(!6!0)!%!3*-&/#!2%)5-%5#!.!'*!%!	
$&*5&%(!6c!0)!%!'%&5#'!2%)5-%5#!T!-30)5!%!'&%)32%'*&!A&0K#)!0)!0($2#(#)'%G*)!2%)5-%5#!YC!!
â¢â¯	Embedding	=!#H$&#33!$&*5&%(!6!0)!3*-&/#!2%)5-%5#!.!0)!'#&(3!*+!F%'%!	
3'&-/'-&#3!%)F!+-)/G*)3!0)!0($2#(#)'%G*)!2%)5-%5#!YC!!
!	
Big ideas 29	""	
Metaprogramming	=!Y)'#&$&#'%G*)!	
Y)'#&$&#'#&!!	+*&!2%)5-%5#!7!!*)!(%/?0)#!>!>%/?0)#!>!6&*5&%(!0)!2%)5-%5#!7!!	
Big ideas 30	""	
Metaprogramming	=!T&%)32%G*)!	
Y)'#&$&#'#&!!	+*&!2%)5-%5#!U!!*)!(%/?0)#!>!>%/?0)#!>!6&*5&%(!0)!2%)5-%5#!B!!B!'*!U!'&%)32%'*&!!	!	6&*5&%(!0)!2%)5-%5#!U!	
Big ideas 31	""	
Metaprogramming	=!U**'3'&%$$0)5!6-mm2#3!	^*A!/%)!%!a%Q%!/*($02#&!I#!A&0K#)!0)!a%Q%[!!^*A!/%)!%!S%/E#'!0)'#&$&#'#&!I#!A&0K#)!0)!S%/E#'[!!^*A!/%)!	gcc	!;%!,\'*\Hno!/*($02#&<!I#!A&0K#)!0)!,[!!	
Big ideas 32	"" 

Metaprogramming	=!6&*5&%((0)5!7%)5-%5#!7%4#&3!	
E#&)#2!34)'%/G/!3-5%&!$&0(0GQ#!!values/	datatypes	!	343'#(!20I&%&0#3!-3#&!20I&%&0#3!	Big ideas 33	""	
Metaprogramming	=!_(I#FF0)5!	
Y)'#&$&#'#&!!	+*&!2%)5-%5#!U!!*)!(%/?0)#!>!>%/?0)#!>!6&*5&%(!0)!2%)5-%5#!B!	#(I#FF#F!0)!	2%)5-%5#!U!!	
Big ideas 34	""	
6&*5&%((0)5!7%)5-%5#!_33#)G%23!	6&0(0GQ#3!>#%)3!*+!,*(I0)%G*)!>#%)3!*+!BI3'&%/G*)!
T?0)E!*+!'?#!2%)5-%5#3!4*-!E)*AC!1?%'!(#%)3!*+!%I3'&%/G*)!F*!'?#4!?%Q#[!!	Big ideas 35	""	
1?4[!1?*[!1?#)[!1?#&#[!	""#305)!%)F!B$$20/%G*)!	
â¢â¯	Historical context	""	â¢â¯	Motivating applications	""	â	â¯	Lisp: symbolic computation, logic, AI, experimental programming	""	â	â¯	ML: theorem-proving, case analysis, type system	""	â	â¯	C: Unix operating system	""	â	â¯	Simula	: simulation of physical phenomena, operations, objects	""	â	â¯	Smalltalk: communicating objects, user-programmer, 	pervasiveness	""	â¢â¯	Design goals, implementation constraints	""	â	â¯	performance, productivity, reliability, modularity, abstraction, 	extensibility, strong guarantees, â¦	""	â¢â¯	Well-suited to what sorts of problems?	""	
Big ideas 36	"" 

"
Non,"Digital Technology Permanent Salary 	
Guide for Bristol & Bath 

Within the Tech sector, salaries are never stagnant, 
trends are consistently on the up. Year on year, 
thereâs always an increasing demand for talent, 
with the levels of experience remaining the same.
With more of a push for businesses to become 
digitally focused in order to keep up with market 
competition, the number of available candidates 	
doesnât quite correlate. 

At ADLIB, weâre always keeping close to market trends and how things can 	
fluctuate between start-ups, scale-ups, agencies, software houses, 	
larger organisations and corporates. 	
Here, we aim to provide an overall range of salaries based on a variety of roles. The following rates are 
based on our work with digital agencies, software houses and SMEs, rather than large enterprises or 	
corporations.	
Note: There are always factors to consider when assigning a salary to a role, such as the skill demand, the 
complexity of the technology, how popular it is in the market, level of responsibility as well as what other 	
benefits are on offer, such as remote working. 	
Itâs also worth noting that the ânumber of yearsâ of experience is not an ideal measure of technical 	
proficiency. However, as a general guide, it should provide some ballparks and pointers... 

Salary Benchmarks
Permanent Roles	Junior 
(0-2 years)	
Mid-Level  
(2-4 years)	
Senior 
(4-6 years)	
Lead/Head  
(6+ years)	
Front-End DeveloperHTML5, CSS3, JavaScript/JQuery, CMS, no JS MVC	Â£20K - Â£28K	Â£28K - Â£35K	Â£35K - Â£50K	Â£50K+	
React/Angular/Vue DeveloperHTML5, CSS3, React/Angular/Vue	Â£25K - Â£35K	Â£35K - Â£50K	Â£50K - Â£65K	Â£65K+	
Full-Stack JS Developer React/Angular/Vue & Node	Â£30K - Â£40K	Â£40K - Â£55K	Â£55K - Â£70K	Â£70K+	
Node DeveloperNode.js	Â£30K - Â£40K	Â£40K - Â£55K	Â£55K - Â£70K	Â£60K+	
PHP DeveloperPHP, MySQL, Laravel/Symfony, CMS: Drupal, Magento, WordPress	
Â£25K - Â£35K	Â£35K - Â£50K	Â£50K - Â£60K	Â£65K+ 	
Full-Stack PHP DeveloperPHP, JavaScript: React/Angular.Vue	Â£30K - Â£36K	Â£36K - Â£50K	Â£50K - Â£65K	Â£65K+ 

Salary Benchmarks
Permanent Roles	Junior 
(0-3 years)	
Mid-Level  
(3-5 years)	
Senior 
(5-8 years)	
Lead/Architect  
(8+ years)	
.Net DeveloperC#, .Net, MVC/.Net Core, Azure, Dynamics, CMS: Umbraco, Sitecore etc.	
Â£25K - Â£35K	Â£35K - Â£50K	Â£50K - Â£65K	Â£65K+	
Ruby DeveloperRuby, Rails, Heroku	Â£28K - Â£40K	Â£40K - Â£55K	Â£55K - Â£70K	Â£70K+	
Python DeveloperPython, Django, Flask	Â£28K - Â£40K	Â£40K - Â£55K	Â£55K - Â£70K	Â£70K+	
Mobile DeveloperIOS, Android, Kotlin, Swift, React Native, Xamarin	Â£28K - Â£40K	Â£40K - Â£55K	Â£55K - Â£70K	Â£70K+	
Functional Programming Scala, Haskell, Clojure, Elixir, Erlang etc.	Â£28K - Â£40K	Â£40K - Â£55K	Â£55K - Â£70K	Â£70K+	
Java Developer Java, Spring, Hibernate	Â£25K - Â£35K	Â£35K - Â£50K	Â£50K - Â£65K	Â£65K+ 

Salary Benchmarks
Permanent Roles	Junior 
(0-3 years)	
Mid-Level  
(3-5 years)	
Senior 
(5-8 years)	
Lead/Architect  
(8+ years)	
DevOps EngineerAWS, GCP, Azure	Â£32K-Â£40K	Â£40K-Â£55K	Â£55K-Â£70K	Â£70K+	
Tester/QAISTQB, Selenium, Automation, Cucumber	Â£24K - Â£30K	Â£30K - Â£40K	Â£40K - Â£55K	Â£55K+	
Infrastructure SupportLinux, Windows, Helpdesk, IT Manager	Â£20K - Â£30K	Â£30K - Â£40K	Â£40K - Â£55K	Â£55K+	
If you find yourself between the scales, then you are likely to be at a point between junior and 
mid-level or if you find yourself at the top of one scale and have been there for a while, then maybe 
you are ready to take the next step up! 

Contact us	
If youâd like market specific information please feel free to get in touch with the ADLIB team.	
Find us at St Bartholomews House, Bristol, BS1 2NH and contact us on 	0117 926 9530	
Connect with us at 	LinkedIn	 and follow us on 	Twitter	
www.adlib-recruitment.co.uk
Creating teams. Shaping futures.	
We are a Recruitment Agency with purpose. Proudly B Corp certified. Our mission and impact go 	
far beyond recruitment. A trusted partner, supporting growth, change and success at pace since 2001.	
Technology | Data | Engineering | Science | Sustainability | eCommerce | Marketing | Design	
Technology	Data	Design	Engineering	Science	Marketing	Sustainability	eCommerce 

"
Non,"
TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884-1894,Â ISSNÂ 2217-8309,Â DOI:Â 10.18421/TEM104-52,Â NovemberÂ 2021.Â 
1884Â 
Â TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ 4Â /Â 2021.Â 	
Analysis of MOOC on Programming  
for IT Specialist Training	
 	
Sergii Sharov 	1, Vira Kolmakova 	2, Tetiana Sharova 	3, Anatolii Pavlenko 	4	
1Â Department of Computer Sciences, Dmytro Motornyi  Tavria State Agrotechnological University,  
72312, Melitopol, Ukraine 	
2Â  Department of Computer Science, Information a nd Communication Technologies, Pavlo Tychyna Uman 
State Pedagogical University, 20301, Uman, Ukraine 	
3Â  Department of Social Sciences and Humanities, Dmyt ro Motornyi Tavria State Agrotechnological 
University, 72312, Melitopol, Ukraine 	
4Â  Department of Social Work, Khortytsia Nati onal Educational and Rehabilitation Academy, 
 69017, Zaporizhzhia, Ukraine	
 	
Abstract â  The  article  provides  a  quantitative 
analysis  of  online  programming  courses  on  such 
platforms  as  Alison,  Udemy,  Edx,  Coursera,  Udacity, 
Codecademy as of July 2021. The authors analyzed the 
number of courses that can be found either in thematic 
sections  or  by  automatic  keyword  search.  It  was  found 
that  students  can  find  6,979  online  programming 
courses  on  the  analyzed  platforms  with  the  help  of 
thematic  sections.  The  Udemy  platform  provides  the 
largest  number  of  programming  courses.  Popular 
programming  languages  which  have  the  largest 
number  of  online  courses  turned  out  to  be  Python, 
JavaScript and Java.	
  	
Keywords â MOOC,  IT  specialist,  programming, 
high school, improvement of professional skills. 
1. Introduction
The  modern  information  society  is  characterized 
by an annual increase in information, rapid 	
 
 DOI:Â 10.18421/TEM104-52Â 
https://doi.org/10.18421/TEM104-52Â 
	
CorrespondingÂ author:Â 
SergiiÂ Sharov,Â Â 
DepartmentÂ  ofÂ  ComputerÂ  Sciences,Â  DmytroÂ  MotornyiÂ 
TavriaÂ  StateÂ  Agrotechnologi calÂ  University,Â  Melitopol,Â 
Ukraine.Â 
Email:Â  segsharov@gmail.com Â 
Received:Â Â Â 03Â AugustÂ 2021.Â 
Revised:Â Â Â Â Â 05Â NovemberÂ 2021.Â 
Accepted:Â Â Â 11Â NovemberÂ 2021.Â 
Published:Â Â 26Â NovemberÂ 2021.Â 
Â©Â  2021Â  SergiiÂ  SharovÂ  etÂ  al;Â  publishedÂ  byÂ 
UIKTEN.Â 	

ThisÂ  workÂ  isÂ  licensedÂ  underÂ  theÂ  CreativeÂ 
CommonsÂ  AttributionâNonCommercialâNoDerivsÂ  4.0Â 
License.
TheÂ  articleÂ  isÂ  publishedÂ  withÂ  OpenÂ  AccessÂ  atÂ 
www.temjournal.com	
 Â 	
technological  development,  and  introduction  of 
information  and  communication  technologies  (ICT) 
in most areas of human activity. Â This trend has led to 
an  increase  in  demand  for  IT  specialists  [1],  in 
particular  software  engineers. Â This  profession  has 
recently  become  one  of  the  most  prestigious  and 
highly paid ones [2].Â  Moreover, knowledge of IT and 
programming  skills  are  part  of  digital  competence 
which  has  now  become  a  key  competence  for  all 
people [3].  The  demand  for  IT  specialists  in  the  labor  market, 
variety  of  programming  languages  and  software 
actualize  the  need  for  specialist  training.  Depending 
on  the  specialization,  IT  specialists  must  be  able  to 
develop  different  types  of  software,  work  with 
databases,  administer  operating  systems  and 
networks  [4],  provide  information  security,  process 
graphic  files,  create  video  and  3D  models,  etc.  Since 
the  technological  development  of  society  is  ongoing, 
IT  specialists  must  constantly  improve  their  skills 
and meet the requirements of the time.  IT specialist training is a complex and multifaceted 
task.  It  is  solved  through  theoretical  and  practical 
training  in  various  areas  [5],  focus  on  the  most 
popular  programming  languages  [6],  practical 
orientation  of  the  knowle dge  acquired.  Student-
centered  learning  and  specifics  of  the  subject  area 
have  influenced  the  methods  and  teaching  aids  used 
to  train  future  programmers.  The  effective  means  are 
the  immediate  assessment  of  the  program  code  in 
automatic mode [7], the use o f analytical projects [2], 
combination  of  related  disciplines  for  the  integrated 
development of professional competencies [4].  The introduction of ICT in the educational process 
allows  us  to  diversify  the  process  of  future  specialist 
training  and  to  increase  its  efficiency.  In  order  to 
train  future  software  engineers,  as  well  as  anyone 
interested,  various  software  solutions  can  be  used: 
distance  learning  systems  [3],  educational  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ Â 4Â /Â 2021.Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 1885Â 	
programming resources [8], and massive open online 
courses (MOOC). 
Recently,  MOOC  have  become  quite  popular  due 
to  their  availability  and  massive-scale  nature.  The 
effectiveness  of  MOOC  to  study  programming  has 
been  proved  by  courses  on  various  programming 
languages,  such  as  Java  [9],  [10],  Visual  Basic  [11], 
Haskell  [12]  and  more.  At  the  same  time,  there  is  a 
need  for  a  more  detailed  quantitative  analysis  of 
available  programming  courses  on  various  online 
platforms.Â Â 
 
2. Background Research 
 
2.1. IT Specialist Training in the Field of 
Programming  
 
In  the  conditions  when  information  society  is 
rapidly  developing,  one  can  notice  constant 
development  of  new  software  and  hardware,  as  well 
as  updating  and  modification  of  the  existing  ones.  It 
is  especially  true  for  training  future  computer 
specialists  who  work  directly  with  software  and 
hardware.  As  a  result,  the  requirements  for  the  IT 
specialist  training  are  constantly  modified  and 
supplemented. 
Usually, software development is the responsibility 
of  software  engineers.  They  must  be  familiar  with 
modern  technologies  and  methods  of  processing, 
storage,  protection,  transmission  of  information  [13], 
they must know modern programming languages and 
integrated  development  environments  (IDE);  they 
must  be  able  to  apply  the  acquired  knowledge  and 
skills  to  develop  computer  programs.  One  of  the 
main  requirements  on  the  part  of  employers  is 
knowledge  of  a  specific  language  /  environment  / 
programming  technology  or  a  certain  list  of  these 
components. The acquired experience and knowledge 
of  specific  software  will  increase  programmersâ 
productivity  in  the  process  of  professional  activity 
[1];  it  will  reduce  the  time  and  financial  costs  for 
their retraining. 
Training  of  computer  specialists,  including  future 
software engineers, has its own characteristics due to 
the  availability  of  courses  on  software  development 
and  ICT  use.  Each  programming  language  uses  a 
certain programming technology (procedural, logical, 
functional,  OOP);  it  has  its  own  syntax  which  helps 
to implement the task that is set before the 
programmer  [10].  This  is  especially  true  of  the 
syntax  of  different  programming  languages,  such  as 
C++,  Pascal,  Basic.  At  the  same  time,  programmers 
must  have  the  ability  to  create  algorithm,  divide  the 
problem into several logical and consistent subtasks.  
In  addition  to  professional  competencies,  it  is 
necessary  for  modern  IT  specialists  to  have  well-
developed  soft  skills.  In  particular,  it  is  the  ability  to work  in  a  team  and  perform  project  activities,  the 
ability  to  analyze  and  synthesize  the  information 
obtained,  independence,  the  ability  to  find  necessary 
information  in  various  sources.  Now  we  can  observe 
a  trend  when  software  development  projects  are  not 
performed  independently  by  only  one  programmer, 
but they appear as a result  of teamwork. At the same 
time,  project  participants  may  be  from  different 
countries  working  outsourced.  Therefore,  well-
developed communication skills [2], as well as social 
competence  [14]  are  of  particular  importance  for 
future  IT  specialists.  They  help  to  communicate 
effectively  through  social  networks,  instant 
messaging programs and more. 
English  is  also  highly  important  for  software 
engineers.  It  is  explained  by  the  English  syntax  of 
programming  languages  which  allow  you  to 
practically  implement  the  algorithm  using  words  and 
constructions  which  are  intuitively  understood.  The 
work  [15]  identified  a  system  of  English-language 
training for future software engineers which involves 
the  use  of  active  teaching  methods.  In  the  process  of 
communicative  training,  future  IT  specialists  can 
form  various  communication  strategies,  as  well  as  a 
glossary of IT and programming terminology. 
Similar  to  any  specialty,  IT  specialist  training 
depends  on  the  demand  in  the  labor  market, 
employersâ  requirements  for  future  professionals  [6]. 
Comprehensive  theoretical  and  practical  professional 
training,  as  well  as  formation  of  soft  skills,  can  be 
obtained  at  a  higher  education  institution.  Moreover, 
training of future software engineers should be based 
on a competency-based approach, modern techniques 
and  technological  achievements.  Only  in  this  case, 
the  IT  specialist  will  be  competitive.  In  this  context 
the  work  [5]  indicates  the  main  points  aimed  at 
quality  training  of  future  software  engineers: 
systematic  review  and  updating  of  educational 
programs,  taking  into  account  the  requirements  of 
employers  and  other  participants  of  the  educational 
process;  obtaining  current  and  final  information  on 
student  performance,  making  appropriate 
adjustments;  improving  the  quality  of  teaching; 
providing  studentsâ  practical  training  in  real 
conditions  at  enterprises;  advanced  training  of 
teachers;  providing  educational  process  with  the 
necessary  methodological,  technological  and  other 
support. 
Usually,  training  of  a  future  specialist  in  computer 
science  involves  mastering  professional  disciplines, 
performing  term  papers,  academic  and  on-the-job 
practices.  At  the  end of  the  training,  students  have  to 
defend their qualification work (project) and / or take 
the  state  final  exam.  These  forms  of  state 
certification  allow  you  to  check  the  formation  of 
professional  and  general  competencies  in  accordance 
with the chosen specialty.  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
1886Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ 4Â /Â 2021.Â 
Today,  higher  education  institutions  pay  great 
attention  to  digitalization  of  the  educational  process. 
For  this  purpose,  they  use  information  environments 
to  manage  the  educational  process  [16],  as  well  as 
virtual  social  networks  [17],  augmented  reality 
technologies  [18],  various  electronic  educational 
resources. The Learning Management System (LMS) 
and  MOOC  have  proven  themselves  well  for  the 
professional  training  of  future  software  engineers. 
Moreover, the correct selection of online courses will 
provide  effective  training  for  students  with  different 
levels of training [7], [12].Â Â 
Â 
2.2. Specific Features of Online Programming 
Courses 
 
MOOC is a modern electronic educational resource 
which  supports  the  principles  of  massive-scale  and 
open  education  which  is  based  on  network 
technologies  and  which  allows  you  to  learn 
independently  and  improve  your  skills.  Compared  to 
the  LMS,  which  are  designed  to  organize  distance 
learning,  they  have  more educational  opportunities 
and  are  not  tied  to  a  specific  educational  institution. 
The advantages of the MOOC include: 
 
ï§ availability  of  a  large  number  of  online  courses 
on various topics [19]; 
ï§ support for multilingual interface; 
ï§ flexibility  and  individual  working  mode  with 
educational material; 
ï§ the  opportunity  to  join  the  courses  created  by 
teachers of leading educational institutions [9]; 
ï§ availability  of  multimedia  and  interactive  tools 
that allow students and teachers to work together 
[10]; 
ï§ the  opportunity  to  obtain  a  certificate  after 
successful completion of the course. 	
 
Most  MOOC  are  characterized  by  elements  of  the 
traditional  education  system,  including  lecture 
material, standardized control in the form of answers 
to  testing  tasks.  They  use  objectivist-individual 
approach  in  learning  which  involves  independent 
processing  of  information  and  high  motivation  to 
learn  [20].  In  addition,  capabilities  of  Web  2.0  allow 
you  to  integrate  video  lectures,  quizzes,  surveys, 
graphic  representation  of  the  course  dynamics, 
feedback, etc. into the MOOC. 
Online  programming  courses  can  be  studied  by 
different categories of students: 
 
ï§ students  who  additionally  receive  information 
within a similar discipline; 
ï§ IT  specialists  who  want  to  learn  a  new 
technology or programming language; 
ï§ teachers who want to get acquainted with leading 
teaching practices and technologies; 
ï§ any users who are interested in programming. The  specific  features  of  online  programming 
courses  are  explained  by  the  specifics  of  software 
engineersâ  work,  the  need  to  form  studentsâ  abstract 
and  logical  thinking,  glossary  of  computing  terms 
[11].  Knowledge  of  the  syntax  of  specific 
programming  languages,  the  principles  of  work  with 
tool  and  application  software  is  important  in  the 
study  of  programming.  It  is  very  good  when  online 
programming  courses  contain  educational  videos, 
examples  of  program  code  with  an  explanation  of  its 
work logic. In this case, students will be able to use it 
for producing software products in the future. 
Similar  to  the  content  of  any  programming 
discipline,  online  courses  should  focus  on  the 
practical application of the knowledge acquired. As a 
result,  they  should  include  exercises  for  creating 
algorithms,  programs  or  individual  program  blocks. 
The effectiveness of such online courses is proved by 
[10],  [12].  In  addition,  there  are  corresponding  voice 
agents  [9]  to  provide  interactivity  and  effective 
feedback for some online platforms. 
One  of  the  problematic  issues  to  consider  when 
studying  programming  with  MOOC  is  the  lack  of 
direct  contact  with  the  teacher  [12].  Due  to  the  low 
level  of  understanding  of  the  learning  material,  the 
high  level  of  algorithmization  and  abstraction  of 
certain  topics,  some  novice  programmers  cannot 
complete the course on their own [9]. In this case, the 
teacher  acts  as  an  instructor  who  can  explain  the 
logic  of  the  program  code,  who  can  advise  on 
additional  sources  of  information  and  show  another 
solution to the problem, etc. On the other hand, some 
teachers  have  a  low  level  of  motivation  and  digital 
preparation  for  the  use  of  MOOC  in  the  educational 
process. 
In  todayâs  pandemic  environment  when  it  is 
impossible  to  fully  conduct  face-to-face  training, 
MOOC  can  be  used  effectively  in  blended  learning 
[21].  In  this  case,  learning  materials,  which  are 
offered  to  students  within  the  discipline,  are 
supplemented  with  information  from  online  courses. 
The  basic  learning  material  can  be  located  in  the 
LMS to ensure that all participants of the educational 
process  registered  in  the  system  can  get  access  to  it. 
This  form  of  the  educational  process  organization 
will help to avoid certain shortcomings of the MOOC 
and improve the quality of learning. 
 
3. Methodology 
 
For  the  quantitative  analysis  of  online 
programming  courses,  we  analyzed  six  MOOC 
(Alison,  Udemy,  Edx,  Coursera,  Udacity, 
Codecademy)  as  of  July  2021.  Two  methods  were 
chosen  to  form  the  sample:  a)  search  of  online 
courses  in  thematic  rubricators  (catalogues  that 
contain  online  courses  on  a  particular  topic);  b)  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ Â 4Â /Â 2021.Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 1887Â 	
automatic  search  of  online  courses  (names  of 
programming languages were used as keywords). 
The Alison online platform contains 160 courses in 
the  thematic  block  âITâââProgrammingâ.  Of  these, 
50  courses  are  designed  to  study  programming 
languages and they have different levels of difficulty. 
Automatic  search  allowed  us  to  find  79  online 
programming  courses.  With  this  method  it  was  not 
possible  to  count  the  number  of  online  courses  to 
study  C,  C++,  C#.  This  is  due  to  the  inclusion  of 
these  symbols  in  the  names  of  non-programming 
courses.  All  courses  are  free  and  certified  by  the 
independent  organization Continuing  Professional 
Development.  It  confirms  the  high  level  of 
educational  content  which  is  presented  in  the  online 
courses on this platform. 
On  the  online  platform  Udemy,  there  are  6,759 
courses  to  learn  programming  languages.  They  can 
be  found  in  the  thematic  block  âDevelopmentââ
âProgramming  languagesâ.  Through  this  search  you 
can  find  courses  on  9  programming  languages. 
Automatic  search  allowed  us  to  find  93,428  online 
courses on 16 programming languages. 
The  Edx  online  platform  contains  53  courses  on 
programming  languages.  They  can  be  found  in  such 
sections  as  âComputer  Scienceâ  and  âDataAnalisys 
&  Statisticsâ.  The  sample  of  online  courses  which 
was  found  by  automatic  keyword  search  included 
1,317  courses.  This  method  did  not  let  us  count  the 
number  of  online  courses  in  the  C,  C++,  and  R 
programming languages due to the inclusion of these 
characters in the names of non-programming courses. 
The  analysis  of  programming  languages  on  the 
Coursera  platform  was  carried  out  in  the  section 
âComputer  scienceâ  by  means  of  automatic  keyword 
search.  Searching  with  subject  headings 
demonstrated  ambiguous  results,  so  they  were  not 
taken  into  account.  The  names  of  the  programming 
languages  were  used  as  keywords  in  the  automatic 
search.  This  resource  allows  you  to  view  3,623 
online  programming  courses.  Due  to  the 
coincidences in the course names, it was not possible 
to  perform  an  automatic  search  of  the  C  and  Go 
programming languages. 
On the Codecademy platform, the sample of online 
courses for the quantitative analysis was made on the 
basis of the âLanguagesâ section. There you can find 
12  programming  languages  with  two  levels  of 
difficulty  (âBeginner  friendly  coursesâ  and 
âIntermediate  coursesâ).  The  total  number  of  online 
programming  courses  was  117.  Using  automatic 
search  by  programming  language  names,  we 
managed  to  find  only  59  online  courses  on  the 
Codecademy platform. 
On the Udacity platform, the formation of a sample 
of  online  programming  courses  was  made  on  the 
basis of the âProgramming & Developmentâ section. The total number of online courses on the platform is 
8 courses, so their number was not taken into account 
in  the  quantitative  analysis.  For  our  study,  the 
Udacity  platform  was  useful  for  its  specific  features 
(availability  of  syllabuses  and  reviews  of  online 
courses). 
 
4. Result 	
 
On  the  Alison  platform,  you  can  improve  your 
qualification  in  different  directions:  IT,  healthy 
lifestyle,  business,  marketing,  personal  development, 
education  and  science.  Learning  on  the  platform  is 
free.  However,  the  certificate  is  payable.  There  is  a 
system  of  discounts  (from  10%)  for  different 
categories of users. The thematic block âITâ contains 
courses  that  allow  users  to  learn  about  data  safety, 
computer  networks,  information  systems, 
engineering, programming, etc. In the thematic block 
âITâââProgrammingâ  users  can  learn  nine 
programming languages: Java, JavaScript, Python, Ð¡, 
C++,  Perl,  C#,  Swift,  Ruby.  Apart  from  thematic 
sections,  the  user  can  try  an  automatic  keyword 
search of online courses. In addition, you can specify 
the  selection  criteria  and  choose  duration  (in  hours), 
the  type  (certificate  or  diploma),  level  (beginner, 
intermediate, advanced), etc. The summarized data as 
for the number of online programming courses on the 
Alison platform are shown in Table 1. 
 
Table 1.  Number of online programming courses on the 
Alison platform  	
Programming 
language Method of search 	
Rubricator Automatic 
search 	
Java  3  11 
JavaScript  4  13 
Python  12  40 
Ð¡  4  â 
C++  12  â 
C#  9  â 
Perl  2  2 
Swift  2  9 
Ruby  2  4 	
Total 50  79 	 
According  to  the  presented  data  (see  Table  1.), 
with the help of the rubricator, the user can access up 
to  50  online  programming  courses.  The  smallest 
number  of  courses  is  devoted  to  such  programming 
languages  as  Perl,  Swift,  Ruby.  They  are  represented 
by  2  (4%)  courses  each.  The  most  popular 
programming  languages  are  Python  and  C++  which 
have 12 (24%) online courses each. 
The  sample,  which  was  formed  by  an  automatic 
search, includes 79 online programming courses. The 
least  popular  turned  out  to  be  the  Perl  programming 
language  (2  courses,  2.5%).  The  largest  number  of  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
1888Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ 4Â /Â 2021.Â 	
online  courses  is  intended  for  learning  the  Python 
language  (40  courses,  50.6%  of  the  total  number  of 
online courses found with the automatic search). 
On the Udemy platform, the thematic block 
âDevelopmentâââProgramming  languagesâ  contains 
online  courses  on  Python,  Java,  C#,  React,  C++, 
JavaScript,  C,  Spring,  Go.  Using  the  automatic 
search  function  for  a  specific  programming  language 
gives  users  a  possibility  to  get  more  information 
about  available  online  courses.  With  the  help  of  the 
rubricator,  seven  more  programming  languages  are 
added  to  the  previously  found  programming 
languages, namely: Go, Kotlin, PHP, Swift, R, Ruby, 
Scala,  Scratch.  Quantitative  indicators  of  online 
courses  found  on  the  Udemy  platform  are  shown  in 
Table 2. 
 
Table 2.  Number of online programming courses on the 
Udemy platform  
Programming 
language Method of search 	
Rubricator  Automatic search 	
Python  2372  10000+ 
Java  1093  10000+ 
C#  693  5850 
React  510  4824 
C++  393  2915 
JavaScript  1125  10000+ 
Ð¡  309  10000+ 
Spring  138  8337 
Go  126  10000+ 
Kotlin  0  1017 
PHP  0  6925 
Swift  0  1961 
R  0  5178 
Ruby  0  603 
Scala  0  541 
Scratch  0  5277 	
Total 6759  93428 	 
Comparison  of  the  obtained  data  (see  Table  2.) 
allows  us  to  draw  a  conclusion  about  the  significant 
difference  in  the  number  of  online  courses  found 
with  the  use  of  different  methods.  The  section 
âProgramming  languagesâ  contains  6,759  online 
courses on 9 programming languages. Most of online 
courses  deal  with  such  programming  languages  as 
Python  (2,372  courses,  35.1%  of  the  total  number  of 
online  courses  in  the  âProgramming  languagesâ 
section)  and  JavaScript  (1,125  courses,  16.6%).  The 
smallest  number  of  courses  in  the  section  deals  with 
the Go language (126 courses, 1.9%). 
In  its  turn,  the  automatic  search  allows  you  to  see 
93,428 online courses on 16 programming languages. 
Searching for courses in automatic mode let us find 5 
programming languages (Python, Java, JavaScript, C, 
Go)  which  have  the  maximum  number  of  courses 
(more  than  10,000).  The  smallest  sample  of  online 	
courses  deals  with  the  Scala  programming  language 
(541 courses). 
To  search  for  online  programming  courses  on  the 
Edx  platform,  you  should  go  to  the  categories 
âComputer  Scienceâ  and  âDataAnalisys  & 
Statisticsâ.  According  to  these  categories,  6 
programming  languages  were  identified  (Python, 
Java,  C,  C++,  Swift,  R).  You  can  review  all  the 
presented  courses  and  clarify  the  following  aspects: 
course  duration,  weekly  workload  for  learning  the 
programming  language,  the  cost  of  the  course  and 
certificate,  educational  institution  presenting  the 
course,  subject  to  which  the  course  belongs,  level  of 
difficulty, language and type of the course. 
Using  automatic  search  you  can  see  more  online 
programming  courses  compared  to  the  rubricator 
search.  Additionally,  courses  on  React,  JavaScript, 
PHP,  Ruby,  Scratch,  PyTorch,  Arduino  become 
available.  The  obtained  quantitative  indicators  are 
shown in Table 3. 
Â 
Table 3. Number of online programming courses on the 
Edx platformÂ 
 
Programming 
language Method of search 	
Rubricator  Automatic search 	
Python  27  112 
Java  4  583 
Ð¡  13  â 
C++  2  â 
React  â  179 
JavaScript  â  30 
PHP  â  6 
Swift  1  107 
R  6  â 
Scala  â  248 
Ruby  â  21 
Scratch  â  17 
PyTorch  â  4 
Arduino  â  10 	
Total 53  1317 	 
Having analyzed the results of the tabular data (see 	
Table 3.) we can state that according to the rubricator 
the  smallest  number  of  courses  is  devoted  to  the 
Swift  programming  language  (1  course,  1.9%  of  the 
total  number  of  courses  in  rubricators)  and  C++  (2 
courses,  3.7%).  The  most  popular  language  appeared 
to  be  Python,  27  online  courses  were  developed  to 
learn  it  (50.9%  of  the  total  number  of  courses  found 
with  the  use  of  the  rubricator).  The  second  place  in 
the  number  of  online  courses  is  taken  by  the  C 
language (13 courses, 24.5%). 	
The  analysis  of  the  courses  received  by  means  of 	
the automatic search allowed us to show much higher 
indicators.  Using  this  method  we  managed  to  find 
1,317 programming courses. We found that when we 
automatically  search  for  a  specific  programming 
language,  the  system  reproduces  the  sample  not  only  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ Â 4Â /Â 2021.Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 1889Â 	
by  the  name,  but  it  also  takes  into  account  the 
internal  structure  and  content  of  the  course.  The 
course  title  does  not  always  contain  programming 
languages,  so  it  is  difficult  to  understand  what  the 
course  includes.  The  most  popular  language  on  the 
Edx  platform  appeared  to  be  Java  with  583  online 
courses  (44.2%  of  the  total  number  of  programming 
courses  found  with  automatic  search).  The  least 
number  of  online  courses  deals  with  PyTorch  (4 
courses) and PHP (6 courses). 	
If  users  want  to  learn  programming  with  the 	
Coursera  platform,  they  should  focus  on  the 
âComputer  Scienceâ  section.  It  contains  5 
subcategories  âSoftware-developmentâ,  âMobile  and 
web  developmentâ,  etc.  Each  subcategory  presents 
the  most  popular  topics,  including  programming 
languages.  However,  the  list  of  programming 
languages  is  missing.  Therefore,  it  is  not  possible  to 
correlate  programming  languages  with  particular 
thematic sections.  	
The  Coursera  platform  has  a  developed  function 	
for  a  keyword  search  of  online  courses.  To  simplify 
the  search,  you  can  filter  courses  by  the  following 
criteria:  language,  level,  duration,  topic,  skills, 
collaboration, and learning product. The summarized 
data on the number of online courses on the Coursera 
platform is shown in Table 4. 
 
 
 
 
 
 
 
 
 
 	
Table 4.  Number of online programming courses on the 
Coursera platform  	
Programming 
language Method of search 	Automatic 
search 	Percentage of the total 
number of courses 	
Java 	1427  39,4 	
Python 	938  25,9 	
PHP  42  1,2 
Scala  414  11,4 	
JavaScript 	271  7,5 	
C++ 	112  3,1 	
C# 	100  2,7 	
Swift 	203  5,6 	
Ruby 	71  2 	
PyTorch 	27  0,7 	
Arduino 	18  0,5 	
Total 	3623   	
 
As  we  can  see  from  Table  4.,  the  Coursera 
platform  mostly  deals  with  Java  (1,427  courses, 
39.4%  of  the  total  number  of  online  programming 
courses)  and  Python  (938  courses,  25.9%).  The  least 
number  of  the  courses  on  the  platform  deals  with 
Arduino  (18  courses,  0.5%)  and  PyTorch  (27 
courses,  7.4%).  Since  Arduino  is  promising  for 
robotics,  we  hope  that  the  number  of  courses  on  this 
programming  language  will  increase  significantly  in 
the  future.  Most  courses  are  free.  However,  the 
certificate must be paid for. 
The  online  platform  Codecademy  contains 
programming  courses  in  the  section  âCatalogâ  â 
âLanguagesâ.  There  are 12  programming  languages 
(HTML,  CSS,  Python,  JavaScript,  Java,  etc.). 
Courses  are  designed  for  two  levels  of  training 
(Beginner  and  Intermediate).  Also,  keyword  search 
of  online  courses  is  available  for  users.  The  obtained 
quantitative indicators are shown in Table 5. 	 	
 
Table 5.  Number of online programming courses on the Codecademy platformÂ Â  	
Programming 
language Method of search 	Rubricator 
Automatic search 	
Beginner  Intermediate  Total 	
HTML & CSS  5  8  13  10 
Python  12  26  38  10 
JavaScript  10  24  34  10 
Java  10  3  13  10 
Ruby  1  2  3  3 
C++  1  0  1  1 
R  2  4  6  6 
C#  1  2  3  3 
PHP  0  1  1  1 
Go  1  0  1  1 
Swift  2  1  3  3 
Kotlin  1  0  1  1 	
Total 46  71  117  59 	  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
1890Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ 4Â /Â 2021.Â 
According  to  the  data  obtained,  117  online 
programming  courses  are  available  for  the  future 
programmer.  Most  of  the  online  courses  in  the 
âLanguagesâ  deal  with  the  Python  language  (38 
courses, 32.56% of the total number of programming 
courses).  JavaScript  (34  courses,  29%)  takes  the 
second  place  in  the  number  of  online  courses.  There 
is  only  one  course  for  the  C++,  PHP,  Go,  Kotlin 
programming languages. 
With  the  help  of  automatic  online  search,  the 
platform displays only 59 online courses. Analysis of 
the  number  of  courses  obtained  through  different 
methods  revealed  a  discrepancy  of  58  courses.  The 
most popular programming languages appeared to be 
HTML & CSS, Python, JavaScript, Java; there are 10 
courses for each one of them. 
Some of the considered online courses are partially 
paid.  There  are  courses  where  only  a  few  classes  are 
paid and the course itself is free. For students there is 
a  system  of  discounts  on  the  cost  of  courses  (up  to 
35%).  However,  such  a  discount  is  active  after 
confirmation  of  the  student  status  and  is  valid  for  no 
more than 4 years. To receive bonuses in the form of 
a  reduced  cost  for  an  online  course,  you  must 
annually confirm your student status. 
On the Udacity platform, you can search for online 
courses  with  the  âProgramming  &  Developmentâ 
section.  There  are  courses  on  Python,  JavaScript, 
Java, C++, Swift, Xcode, Kotlin, Android Basics and 
more.  You  can  also  use  the  search  bar  and  filter 
function  (program  type,  skill  level,  course  duration, 
skills). A small number of programming courses (one 
course  for  each  programming  language)  was  found 
on  this  platform,  so  we  did  not  consider  them  in  the 
overall analysis. 	
A  specific  feature  of  the  Udacity  platform  is  the 
display  of  reviews  of  the  online  course.  The  number 
of  reviews  ranges  from  105  to  3,114  depending  on 
the  programming  language.  If  the  course  has  not 
been  completed  by  users,  it  does  not  contain  any 
reviews (Table 6.). 
Â 
Table 6.  Number of online programming courses on the 
Udacity platformÂ   	
Programming 
language Number of 
courses Number of 
course reviews 
Python  1  951 
Java  1  New course 
Front End Web 
Developer 1  986 
Swift Xcode  1  835 
C++  1  New course 
JavaScript  1  105 
Kotlin  1  196 
Android Basics  1  3114 	
Total 	8 	6187 	
 
Another  advantage  of  this  resource  is  the 
availability  of  a  syllabus  which  describes  the  course 
program  in  detail.  Such  information  is  greatly 
important  for  users  who  will  be  able  to  optimally 
plan their own learning process. Detailed information 
about  the  course  is  contained  in  a  file  together  with 
the  specification  of  terms,  number  of  classes,  course 
teachers, etc. 
The  summarized  data  on  the  number  of  online 
programming  courses  on  different  platforms  which 
can be found with the help of rubricators is shown in 
Table 7.  	
 
Table 7.  Number of online programming courses displayed in rubricators   	
Programming 
language 	MOOC 	Number 	Alison  Udemy  Edx  Codecademy 	
Java  3  1093  4  13  1113 	
JavaScript  4  1125  â  34  1163 
Python  12  2372  27  38  2449 	
Ð¡  4  309  13  â  326 
C++  12  393  2  1  408 
C#  9  693  â  3  705 
Perl  2  â  â  â  2 
Swift  2  â  1  3  6 
Ruby  2  â  â  3  5 
React  â  510  â  â  510 	
Spring Framework  â  138  â  â  138 	
Go  â  126  â  1  127 
Kotlin    â  â  1  1 
PHP  â  â  â  1  1 
R  â  â  6  6  12 	
HTML & CSS  â  â  â  13  13 	
Total 50 6759 53 117 6979 	  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ Â 4Â /Â 2021.Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 1891Â 
According  to  Table  7.,  you  can  find  6,979  online 
courses  with  the  help  of  rubricators  on  four 
platforms.  Most  courses  are  presented  on  the  Udemy 
online platform (6,759 online courses, 96.8%). 
On  all  the  considered  platforms  there  are  online 
courses  on  programming  languages  such  as  Java, 
Python,  C++.  Their  total  number  is  3,970  online 
courses which is 56.9% of the total number of online 
programming  courses.  The  leaders  in  popularity 
appeared  to  be  such  programming  languages  as 
Python  (2,449  courses,  35%  of  the  total  number  of 
online  programming  courses),  JavaScript  (1,163 
courses,  16,6%)  and  Java  (1,113  courses,  15.9%). 
The  least  popular  programming  languages  appeared 
to  be  Perl  (2  courses),  Kotlin  and  PHP  (1  course 
each).  Regarding  the  PHP  web-programming 
language,  the  obtained  data  require  additional 
analysis.  Besides,  some  programming  languages 
(Scala,  Scratch,  PyTorch,  Arduino)  could  not  be 
found with the help of the rubricator. 
 
 	
Based on the analysis of the total number of online 
courses  for  each  programming  language  which  are 
displayed  in  the  thematic  sections  we  can  talk  about 
the  popularity  of  the  programming  language  on 
different MOOC (Figure 1.).  	
 	 	
Figure 1.  Ranking of the programming language 	
popularity by the number of online courses 	 
The  summarized  data  on  the  number  of  online 
programming  courses  on  different  platforms  which 
can  be  found  by  automatic  keyword  search  is  shown 
in Table 8.  	
Table 8.  Number of online programming courses using auto search  	
Programming 
language 	MOOC 	Number 	Alison  Udemy  Edx 	Coursera 	Codecademy 	
Java  11  10000+  583  1427  10  12031 	
JavaScript  13  10000+  30  271  10  10324 
Python  40  10000+  112  938  10  11100 	
Ð¡  â  10000+  â  â  â  10000 
C++  â  2915  â  112  1  3028 
C#  â  5850  â  100  3  5953 
Perl  2  â  â  â  â  2 
Swift  9  1961  107  203  3  2283 
Ruby  4  603  21  71  3  702 
React  â  4824  179  â  â  5003 
Spring  â  8337  â  â  â  8337 
Go 	â  10000+  â  â  1  10001 	
Kotlin  â  1017  â  â  1  1018 
PHP  â  6925  6  42  1  6974 
Scala 
 541  248  414  â  1203 
R  â  5178  â  â  6  5184 	
Scratch  â  5277  17  â  â  5294 
PyTorch  â  â  4  27  â  31 
Arduino  â  â  10  18  â  28 	
HTML & CSS  â  â  â â  10  10 	
Total 79 93428 1317 3623 59 98506 	
 
The  analysis  of  Table  8.  revealed  a  much  larger 
number  (98,506)  of  online  programming  courses 
compared to the number of online courses found with 
the help of the rubricator (see Table 7.). 
On  all  the  considered  platforms  there  are  online 
courses  on  the  following  programming  languages: 
Java,  JavaScript,  Python,  Swift,  Ruby.  Their  total 
number is 36,440 online courses which is 37% of the 
total  number  of  online  programming  courses.  The 
most  popular  languages  found  with  keyword  search 
appeared  to  be  Java  (12,031  courses,  12.2%  of  the 	
total  number  of  courses),  Python  (11,100  courses, 
11.2%) and JavaScript (10,324 courses, 10.4%). 
 
5. Discussion 
 
A  large  number  of  online  programming  courses 
presented  on  various  MOOC  platforms  will  improve 
the  skills  of  not  only  a  beginner,  but  also  an 
experienced  programmer.  Criteria  for  selecting  an 
online  programming  course  can  be:  popularity  of  the 
programming  language,  previous  work  experience, 	
2449 
1163 
1113 
705 
510 
408 
326 
305 
0 1000 2000 3000 

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
1892Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ 4Â /Â 2021.Â 	
requirements  of  the  software  customer,  teacherâs 
instructions.  When  choosing  the  first  programming 
language  to  study,  you  should  consider  the 
compatibility  of  syntax  with  popular  programming 
languages, the clarity of program code, free compiler 
or  IDE  [6],  availability  of  free  sources  providing 
reference information (guides, forums, etc.). 
Each online programming course requires a certain 
level  of  training  (understanding  the  principles  of 
algorithms,  ICT  skills,  at  least  a  minimum  level  of 
English).  Out  of  doubt,  the  initial  skills  of  working 
with  IT  and  programming  languages  are  formed  at 
school.  The  basic  course  of  programming  will  allow 
students  to  form  mathematical  and  digital 
competence  [3],  they  will  show  them  the 
attractiveness  of  being  an  IT  specialist,  and  they  will 
prepare  them  for  using  ICT  in  a  more  active  way.  At 
the  same  time,  students  who  have  demonstrated  high 
abilities  for  programming  could  consider  the  future 
career well in advance [8]. 
In  our  study,  we  used  two  ways  to  find  online 
programming  courses.  The  first  method  involved  the 
analysis  of  online  courses  by  thematic  sections.  In 
this  case,  the  exact  number  of  courses  related  to  a 
specific  programming  language  was  found.  The 
second  method  meant  using  automatic  keyword 
search  of  online  courses.  Programming  languages 
were  used  as  keywords.  This  method  allowed  us  to 
get  a  much  larger  number  of  courses.  However,  the 
disadvantage  of  this  method  is  that  programming 
courses  are  not  always  included  in  the  sample.  It 
could  include  courses  on  office  programs,  public 
speaking,  English,  etc.  We  witnessed  this  situation 
with  such  programming  languages  whose  names 
consist of 1-2 letters (C, R, Go, etc.). In this case, the 
user will have to browse the names of online courses 
and choose the right one. 
Regarding  the  organization  of  the  educational 
process  at  university,  it  is  recommended  to  combine 
the  capabilities  of  the  MOOC  and  traditional 
learning.  The  effectiveness  of  such  a  combination  is 
mentioned  in  [11],  [12],  [22].  In  the  conditions  of 
blended  learning,  students  have  the  opportunity  to 
communicate  with  the  teacher,  use  printed  and 
electronic  sources  of  information,  increase  cognitive 
interest,  as  well  as  to  optimize  the  amount  of 
educational  material  and  time  for  its  study.  All  this, 
in turn, will provide simultaneous training of students 
with  different  levels  of  training  [7]  and  will 
positively  affect  their  academic  success  [21]. 
Additionally,  to  provide  feedback  between  the 
teacher  and  students,  you  can  use  software  solutions 
that  allow  you  to  check  and  compile  the  program 
code  [10],  provide  recommendations  in  natural 
language  in  real  time  using conversational  agents  [9] 
or bots [23]. It  should  be  noted  that  the  content  of  training 
provided  for  future  software  engineers  in  the 
educational  institution  should  be  periodically 
improved.  First  of  all,  it  concerns  the  revision  of 
educational programs and curricula [5] in accordance 
with  the  technological  development  of  society, 
popularity  of  programming  languages  [6],  industry 
demands and requirements for future professionals in 
the  labor  market  [13].  Similarly,  working 
programmers  also  need  to  improve  their 
programming  and  teamwork  skills  [1].  Hence,  it  will 
have  a  positive  effect  on  the  demand  for  the 
specialists and their salary. 	
 
6. Conclusion 
 
Thus,  mastering  programming  skills  and 
improving  knowledge  of  IT  is  one  of  the  key 
competencies  of  modern  man.  Training  of  future  IT 
specialists,  in  particular  software  engineers,  involves 
the  acquisition  of  theoretical  knowledge,  formation 
of professional competencies, practical  orientation of 
educational  activities.  Such  training  has  specific 
features  related  to  the  need  to  develop  algorithmic 
thinking,  to  master  various  information  technologies 
and  tool  environments,  to  know  the  syntax  of 
programming  languages  and  the  rules  of  writing  a 
program code. 
Online  education  remains  an  effective  means  of 
acquiring  knowledge  and  developing  competencies 
in  a  distant  way.  It  can  be  applied  to  all  users  who 
have access to the Internet and desire to learn. One of 
the effective means of online education is the MOOC 
which provides access to a large array of information 
on various topics. 
Quantitative  analysis  of  online  programming 
courses presented on such online platforms as Alison, 
Udemy,  Edx,  Coursera,  Codecademy  allowed  us  to 
identify  the  most  useful  MOOC  for  programmers.  A 
comparison of the quantitative indicators which were 
obtained  with  the  two  methods  used  to  search  for 
online  courses  revealed  significant  differences  in  the 
number  of  online  programming  courses  that  were 
found.  We  found  6,979  online  programming  courses 
in  the  thematic  sections  (rubricators)  of  the 
considered  online  platforms.  At  the  same  time,  we 
managed  to  find  98,506  online  courses  using 
automatic keyword search of online courses. 
Online  courses  on  21  programming  languages 
were  analyzed  on  different  platforms.  The  maximum 
number  of  programming  courses  is  presented  on  the 
Udemy online platform (6,759 online courses, 96.8% 
of the total number of online courses displayed in the 
rubricators).  The  Udacity  online  platform  has  the 
fewest  number  of  programming  courses  (8  courses). 
It  was  analyzed  to  demonstrate  its  key  features.  In  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ Â 4Â /Â 2021.Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 1893Â 	
particular,  it  concerns  the  availability  of  syllabuses 
and reviews of each online programming course. 
The  analysis  of  the  total  number  of  online  courses 
on  all  platforms  revealed  that  the  largest  number  of 
courses  in  the  rubricators  are  presented  by  such 
programming  languages  as  Python  (2,449  courses, 
35%  of  the  total  number  of  online  programming 
courses),  JavaScript  (1,163  courses,  16.6%)  and  Java 
(1,113  courses,  15.9%).  If  you  use  automatic  search 
of  online  courses,  you  see  that  the  largest  number  of 
online  courses  are  devoted  to  such  programming 
languages as Java (12,031 courses, 12.2% of the total 
number  of  courses),  Python  (11,100  courses,  11.2%) 
and JavaScript (10,324 courses, 10, 4%).Â 
In  the  future,  it  is  planned  to  analyze  the 
capabilities  of  the  MOOC  for  software  engineersâ 
training  in  various  fields  (development  of  desktop 
and mobile applications, web-programming, etc.). 
 
References 
 
[1]. Saputra,  R.,  &  Napitupulu,  T.  A.  (2014).  Factors 	
Affecting  Programmer's  Performance  on  Web-Based 
Programming. Journal of Theoretical & Applied 
Information Technology, 63(2). 	
[2]. Donina,  I.  A.  (2020).  Analytical  projects  as  an 	
innovative  means  in  training  of  programmers.  In 
EpSBS  (pp.  506-513),  European  Publisher. 
https://doi.org/10.15405/epsbs.2020.08.02.66 	
[3]. Shokaliuk, S. V., Bohunenko, Y. Y., Lovianova, I. V., 
&  Shyshkina,  M.  P.  (2020). Technologies of distance 
learning for programming basics lessons on the 
principles of integrated development of key 	
competences.	 Proceedings  of  the  7th  Workshop  on 
Cloud Technologies in Education (CTE 2019), Kryvyi 
Rih, Ukraine. 
[4]. VavreckovÃ¡,  S.  (2020).  The  Combination  of  Skills 
Training  for  IT  Administrators  and  Programmers. 
In ISSEP (CEURWS Volume) (pp. 152-159). 	
[5]. Kruhlyk,  V.  (2018).  Quality  assurance  system  for 	
professional  training  of  future  programmers  in  the 
Ukrainian higher educational institutions. Engineering 
and Educational Technologies, 6(4), 58-65. 
https://doi.org/10.30929/2307-9770.2018.06.04.06 	
[6]. Prokop, Y., Trofimenko, E., Loginova, N., Zadereyko, 	
A.,  &  Gerganov,  M.  (2019,  July).  Multivariate 
analysis  when  choosing  the  first  programming 
language  studied  in  universities.  In 2019 IEEE 2nd 
Ukraine Conference on Electrical and Computer 
Engineering (UKRCON) (pp. 1224-1228). IEEE. 
https://doi.org/10.1109/UKRCON.2019.8879810 	
[7]. Skalka, J., & et al. (2021). Conceptual framework for 	
programming  skills  development  based  on 
microlearning  and  automated  source  code  evaluation 
in  virtual  learning  environment. Sustainability, 13(6), 
1-30.    https://doi.org/10.3390/su13063293 	
[8]. Ãetinkaya,  A., &  Baykan,  Ã. K. (2020). Prediction of 	
middle  school  students'  programming  talent  using 
artificial  neural  networks. Engineering Science and 
Technology, an International Journal, 23(6),  1301-
1307.    https://doi.org/10.1016/j.jestch.2020.07.005 	
[9]. CatalÃ¡n,  A.  C.,  GonzÃ¡lez-Castro,  N.,  Delgado,  K.  C., 
Alario-Hoyos,  C.,  &  MuÃ±oz-Merino,  P.  J.  (2021). 
Conversational  agent  for  supporting  learners  on  a 
MOOC on programming with Java. Computer Science 
and Information Systems, (00), 20-20. 	
https://doi.org/10.2298/csis200731020c 	
[10]. KirÃ¡ly,  S.,  NehÃ©z,  K.,  &  HornyÃ¡k,  O.  (2017).  Some 
aspects  of  grading  Java  code  submissions  in 
MOOCs. Research in Learning Technology, 25. 	
https://doi.org/10.25304/rlt.v25.1945 	
[11]. X.  Zhang,  X.  Huang,  F.  Wang  and  X.  Cao.,(2018). 
""Research  on  MOOC-based  blended  learning  of 
programming  language  course"", DEStech Trans. 
Social Sci. Edu. Human Sci., pp. 586-591.  
[12]. Dale, V. H., & Singer, J. (2019). Learner experiences 
of  a  blended  course  incorporating  a  MOOC  on 
Haskell  functional  programming. Research in 
Learning Technology, 27. 	
https://doi.org/10.25304/rlt.v27.2248 	
[13]. Farmonovich,  T.  M.  (2021).  Formation  Of 	
Professional  Communicative  Competence  Of 
Programmers  Them  In  A  Competitive 
Environment. Psychology and Education 
Journal, 58(2), 1327-1330. 
https://doi.org/10.17762/pae.v58i2.2278 	
[14]. Sharov, S., Vorovka, M., Sharova, T., & Zemlianska, 	
A. (2021). The Impact of Social Networks on the 
Development  of  Studentsâ  Social 
Competence. International Journal of Engineering 
Pedagogy, 11(3). 
https://doi.org/10.3991/IJEP.V11I3.20491 	
[15]. Mykytenko,  N.,  Rozhak,  N.,  &  Semeriak,  I.  (2019). 	
Teaching  Communication  Strategies  to  the  Computer 
Programming  Students. Advanced Education, 6(12), 
49-54.   https://doi.org/10.20535/2410-8286.167148 	
[16]. Morozov,  A.  V.,  &  Vakaliuk,  T.  A.  (2021,  March). 	
An  electronic  environment  of  higher  education 
institution  (on  the  example  of  Zhytomyr  Polytechnic 
State  University).  In Journal of Physics: Conference 
Series (Vol. 1840, No. 1, p. 012061). IOP Publishing. 
https://doi.org/10.1088/1742-6596/1840/1/012061 	
[17]. Bijari,  B.,  Javadinia,  S.  A.,  Erfanian,  M.,  Abedini, 	
M., & Abassi, A. (2013). The impact of virtual social 
networks  on  studentsâ  academic  achievement  in 
Birjand  University  of  Medical  Sciences  in  East 
Iran. Procedia-Social and Behavioral Sciences, 83, 
103-106. https://doi.org/10.1016/j.sbspro.2013.06.020 	
[18]. Abidin, Z. Z., & Zawawi, M. A. A. (2020). OOP-	
AR:  Learn  Object  Oriented  Programming  Using 
Augmented  Reality. International Journal of 
Multimedia and Recent Innovation, 2(1), 60-75. 
https://doi.org/10.36079/lamintang.ijmari-0201.83 	
[19]. Sharov  S.,  Zemlianskyi  A.,  Sharova  T.,  &  Viktor  H. 	
(2021). Ukrainian  MOOC:  Quantitative  and  Thematic 
Analysis  of  Online  Courses. International Journal on 
Advanced Science Engineering Information 
Technology, 11(3), 1143-1149. 
https://doi.org/10.18517/ijaseit.11.3.13705 	
[20]. Toven-Lindsey,  B.,  Rhoads,  R.  A.,  &  Lozano,  J.  B. 
(2015).  Virtually  unlimited  classrooms:  Pedagogical 
practices in massive open online courses. The internet 
and higher education, 24, 1-12. 	
https://doi.org/10.1016/j.iheduc.2014.07.001  

TEMÂ Journal.Â VolumeÂ 10,Â IssueÂ 4,Â PagesÂ 1884â1894,Â ISSNÂ 2217â8309,Â DOI:Â 10.18421/TEM104â52,Â NovemberÂ 2021.Â 
1894Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â TEMÂ JournalÂ âÂ VolumeÂ 10Â /Â NumberÂ 4Â /Â 2021.Â 	
[21]. Larionova,  V.,  Brown,  K.,  Bystrova,  T.,  &  Sinitsyn, 	
E.  (2018).  Russian  perspectives  of  online  learning 
technologies  in  higher  education:  An  empirical  study 
of  a  MOOC. Research in comparative and 
international education, 13(1), 70-91. 
https://doi.org/10.1177/1745499918763420 	
[22]. Vihavainen,  A.,  Luukkainen,  M.,  &  Kurhila,  J. 	
(2012,  October).  Multi-faceted  support  for  MOOC  in 
programming.  In Proceedings of the 13th annual 
conference on Information technology education (pp. 
171-176).  https://doi.org/10.1145/2380552.2380603 	
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 [23]. Dominic,  J.,  Ritter,  C.,  &  Rodeghero,  P.  (2020, 
June).  Onboarding  Bot  for  Newcomers  to  Software 
Engineering.  In Proceedings of the International 
Conference on Software and System Processes (pp. 	
91-94). https://doi.org/10.1145/3379177.3388901 
 
  

"
Non,"Traceability and 
Transparency	
Diagnosing Distributed Systems in 2022
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Tobi
Principal Cloud Architect
Social Coding Ambassador	
Alin
Senior Key Expert 
Platform Architect  

Why do I need this?	
And what is even going on?
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

The elusive service	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Unique functionality
Can be deployed individually
Complex inner structure   

The elusive service forest?	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Identity	
Storage	
Purchase	Billing	Shopping 	
Cart
Shop	
Inventory	
Gateway          

Chatter in the elusive service forest	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Identity	
Partner	
Purchase	Billing	Shopping 	
Cart
Shop	
Inventory	
Gateway	
User           

One error to rule them all	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Identity	
Partner	
Purchase	Billing	Shopping 	
Cart
Shop	
Inventory	
Gateway	
User	
500 Internal Server Error            

Combing the forest	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Identity	
Partner	
Purchase	Billing	Shopping 	
Cart
Shop	
Inventory	
Gateway	
User	
500 Internal Server Error                     

Donât panic!	
Some simple solutions to improve your life or what we currently do in MindSphere AWS
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

Operational basics	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
â¢	Redundancy is complicated	
â¢	Lifecycle operations are difficult	
â¢	Basically, the 	devil	
â¢	Can be containerized	
â¢	Deployment and updating is 
rather simple	
â¢	Most probably stateless	
â¢	We like this, this is nice 	
Frontend
Backend
Storage	
Cloud native 
preferred
Easy to self 
operate     

Log storage	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
S3 Standard	S3 Glacier	
cold storage	
Identity	Billing	Shop                        

Log queries	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
S3	
AWS Glue	
S3
S3	
Amazon Athena     

Easy to manage, eats data like no other	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

Azure solution (demonstrate multi cloud)	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Custom appender	
Azure 	
Event Hub	
Azure 	
Data Explorer	
Custom appender
Custom appender       

Correlation ids	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Identity	Shop	Inventory	Gateway	User	
uuid	uuid	uuid	
Message
+uuid	
Message
+uuid	
Message
+uuid	
Message
+uuid	
logs	logs	logs	logs           

Monitoring	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
Prometheus
Prometheus	
InfluxDB
CloudWatch
OpenSearch	
Grafana     

Building to scale	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
1075 
users
1051 	
dashboards	
22k 	
metrics
1800 	
alert 	rules	
99.99%	
availability
1.860M/min	
InfluxDB
datapoints	
100+ 	
services	
50+ 	
dev teams  

What we have and what we are still looking for	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
=	
?   

OSS 	solutions	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

Diagnosis and	o	bservability 	g	o 	h	and	-in	-hand
Observability	
Monitoring	
Logging	Tracing	Profiling	
Code	
Logging	Tracing	
Now	
Future
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

(Distributed) Tracing, 	more 	than 	correlation	
User	
2	
3	
1	
4	
5	
6	
1 Trace
6 Spans	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17              

One 	tool 	to 	rule 	them 	a	ll: 	n	o 	m	ore 	c	ontext 	s	witch 	
Observability	
Monitoring	
Logging	Tracing	Profiling	
Code	
Logging	Tracing	
Application Performance Management (APM)	
Diversity	Automation	Contextualization	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

OSS 	building 	b	locks 	for 	observability	
Data Collection + 	A	pplication 	P	erformance 	M	anagement (APM)	
OpenTelemetry	Elastic APM, Grafana Tempo, â¦ 	OSS :	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
elastic 	apm   

Vision	
Identity	
Partner	
Purchase	Billing	Shopping 	
Cart
Shop	
Inventory	
Gateway	
User	
APM	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17                   

OTEL 	
Collector
Gateway	
OpenTelemetry 	reference 	a	rchitecture	
APM	
OTEL 	
Collector
Gateway	
Host	
App	
OTEL 
SDK
OTEL 	
Collector
(Agent)	
Host	
App	
OTEL 
SDK
OTEL 	
Collector
(Agent)	
Host	
ÂµService	
OTEL 
SDK
OTEL 	
Collector	
Agent	
â¢	Auto instrumentation	
â¢	Manual instrumentation	
â¢	Translation	
â¢	Buffering	
â¢	Batching	
â¢	Retry	
â¢	Tail	-based sampling	
â¢	API security consolidation	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
â¢	Metrics	
â¢	Traces	
â¢	Logs   

OTEL 	
Collector
Gateway	
Lots of 	extension 	p	oints 	e	xist	
APM	
OTEL 	
Collector
Gateway	
Host	
App	
OTEL 
SDK
OTEL 	
Collector
(Agent)	
Host	
App	
OTEL 
SDK
OTEL 	
Collector
(Agent)	
Host	
ÂµService	
OTEL 
SDK
OTEL 	
Collector	
Agent	
â¢	Auto instrumentation	
â¢	Manual instrumentation	
â¢	Translation	
â¢	Buffering	
â¢	Batching	
â¢	Retry	
â¢	Tail	-based sampling	
â¢	API security consolidation	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
â¢	Metrics	
â¢	Traces	
â¢	Logs   

OpenTelemetry 	scope	
Spec
API
SDKs
Integrations	
Foreach {language}	
Foreach {framework | library}	
Framework / Library 	
Developer	
Application
Developer	
Effort	
Binary Instrumentation, 
Hooks, Events, Interceptors	
Automation,
Fine	-tuning	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

OpenTelemetry 	scope	
SDKs
Integrations	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
C++, .NET, Erlang/Elixir, 
Go, Java, JavaScript, PHP, 
Python, Ruby, Rust, Swift	
Akka Actors, Akka HTTP, Apache Axis2, Apache Camel, Apache CXF JAX	-RS, Apache CXF 	
JAX	-RS Client, Apache CXF JAX	-WS, Apache Dubbo, Apache 	HttpAsyncClient	, Apache 	
HttpClient	, Apache Kafka Producer/Consumer, Apache Kafka Streams API, Apache 	
MyFaces	, Apache 	RocketMQ	, Apache Struts, Apache Tapestry, Apache Wicket, 	Armeria	, 	
AsyncHttpClient	, AWS Lambda, AWS SDK, Azure Core, Cassandra Driver, Couchbase 	
Client, 	Dropwizard	Views, Eclipse Grizzly, Eclipse Jersey, Eclipse Jetty HTTP Client, Eclipse 	
Metro, Eclipse Mojarra, Elasticsearch API, Elasticsearch REST Client, 	Finatra	, Geode Client, 	
Google HTTP Client, Grails, 	GraphQL	, gRPC	, Guava 	ListenableFuture	, GWT, Hibernate, 	
HttpURLConnection	, Hystrix	, Java Executors, Java Http Client, 	java.util.logging	, JAX	-RS, 	
JAX	-RS Client, JAX	-WS, JDBC, Jedis, JMS, JSP, Kotlin Coroutines, Kubernetes Client, 	
Lettuce, Log4j 1, Log4j 2, 	Logback	, Micrometer, MongoDB Driver, Netty, 	OkHttp	, Play, Play 	
WS, Quartz, RabbitMQ Client, Ratpack, Reactor, Reactor Netty, 	Rediscala	, Redisson	, 	
RESTEasy	, Restlet	, RMI, 	RxJava	, Scala 	ForkJoinPool	, Servlet, Spark Web Framework, 	
Spring Batch, Spring Data, Spring Integration, Spring Kafka, Spring RabbitMQ, Spring 
Scheduling, Spring Web MVC, Spring Web Services, Spring 	WebFlux	, Spymemcached	, 	
Twilio, Undertow, 	Vaadin	, Vert.x	Web, 	Vert.x	HttpClient	, Vert.x	Kafka Client, 	Vert.x	RxJava2  

Zooming	-in on the 	collector	
Receivers	Exporters	Processors	
OTEL Collector	
Batch	
Memory 
Limiter	Sampling	Attributes	
Memory 
Limiter	Batch	
OTLP
Jaeger	
Prometheus	
OTLP
Jaeger	
Prometheus	
Transformation Pipelines	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
elastic 	apm	
Batch	
Queued
Retry	â¦	
â¦	â¦     

Application Performance Management (APM) 	key 	features	
Latency	
Time	
Latency: â¦
Throughput: â¦
Error Rate: â¦	
Component Map	Infrastructure Monitoring	Application Monitoring	
F1
F2
F3
F4
F5	
End	-to	-end Tracing & Profiling	
{""Time"": ""2022	-05	-17 15:00"", ""Level"": 	
""Info"", ""Message"": âAPM talk starts""}
{""Time"": ""2022	-05	-17 15:34"", ""Level"": 	
""Info"", ""Message"": âAPM talk ends""}
{""Time"": ""2022	-05	-17 15:35"", ""Level"": 	
""Info"", ""Message"": âAPM Q&A starts""}
{""Time"": ""2022	-05	-17 15:44"", ""Level"": 	
""Info"", ""Message"": âAPM Q&A ends""}	
Log Query	
from flask import Flask
app = Flask(__name__)
@app.route('/')
def index():	
return 'Open Source@Siemens â22'	
app.run	(host='0.0.0.0', port=8081)
Access to Code	
Alerting	
%	
Latency	
Basic Analytics	
Normal
Abnormal	
Advanced Analytics	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

OSS APM 	coverage	
Latency	
Time	
Latency: â¦
Throughput: â¦
Error Rate: â¦	
Component	Map	Infrastructure Monitoring	Application Monitoring	
{""Time"": ""2022	-05	-17 15:00"", ""Level"": 	
""Info"", ""Message"": âAPM talk starts""}
{""Time"": ""2022	-05	-17 15:34"", ""Level"": 	
""Info"", ""Message"": âAPM talk ends""}
{""Time"": ""2022	-05	-17 15:35"", ""Level"": 	
""Info"", ""Message"": âAPM Q&A starts""}
{""Time"": ""2022	-05	-17 15:44"", ""Level"": 	
""Info"", ""Message"": âAPM Q&A ends""}	
Log Query	
from flask import Flask
app = Flask(__name__)
@app.route('/')
def index():	
return 'Open 	Source@Siemens	â22'	
app.run	(host='0.0.0.0', port=8081)
Access to Code	
Alerting	
%	
Latency	
Basic Analytics	
Normal
Abnormal	
Advanced Analytics	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17	
F1
F2
F3
F4
F5	
End	-to	-end 	Tracing	& Profiling  

Questions and answers	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

Reminder: Key Points	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17   

Contact
Published by Siemens 22
Alin 	Murarasu	
Platform Architect
DI SW CAS R&D AN  |  T CED SES	-DE	
E-mail 	alin.murarasu@siemens.com	
Tobi Appl
Principal Cloud Architect
DI SW CAS MP OPS
E-mail 	tobias.appl@siemens.com	
Unrestricted | Â© Siemens 2022 | Alin Murarasu & Tobi Appl | 	2022	-05	-17  

"
resume,"Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfCurriculum Vitae
Regan Russell BScNovember 6, 2023
Software Consultant ABN 76 121 343 744
Phone: +61 41 428 7577
Email: regan @ pymblesoftware.com regan.russell @ gmail.com Latest CV
Synopsis:
Professional software developer with over 30 years of industry experience. For the past decade,
my focus has been on Native Mobile App development. I started in Palm Pilots, Blackberry
& Windows phone which led to a successful contracting career in both iPhone and Android de-
velopment - I have also gone on to create 30+ apps of my own & written a book 1
. I am
comfortable in Android & iOS Dev roles and have been using Kotlin & Swift for the past 5
years.
The team leader of 3 (twice) and team leader of 5, management and mentoring skills.
Worked in various environments including Agile, Scrum, GIT, JIRA, and KANBAN boards,
Experienced with Beacons, NFC, Bluetooth communications, RESTful APIs, IoT integration, Accessibility compliance, Pen-
testing assessment and remediation.
Published on several app stores/marketplaces, including current iOS apps: https://itunes.apple.com/au/artist/pymble-software-pty-ltd/id553990081 Android apps on Google Play:
https://play.google.com/store/apps/developer?id=PymbleSoftware+Pty+Ltd&hl=en C# / XAML Windows Phone 8 Apps:
http://www.windowsphone.com/en-US/store/publishers?publisherId=PYMBLE%2BSOFTWARE%2BPTY%2BLTD. GitHub:
https://github.com/pymblesoftware Current personal pro ject:
https://ob jective-c2swift.com 1
iBooks, Kindle and professionally edited by a publisher
1   

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfPrevious Experience
July 2023 - Octrober 2023 Title: Cross-platform mobile developer with Flutter / PHP Laravel (Lumens) developer (Contract)
Charitable Organisation Sadaqa Welfare Fund.
Flutter (3.10.6), Flutter Realm (1.3.0 ), Dart, Stripe (utter_stripe:9.3.0), credit_card_scanner (1.0.5),
PHP, Laravel. Lumens.
Attempted to be GDPR and PCI compliant as possible, only static content kept in an in-memory database, not even
encrypted in storage.
Over 12,000 lines of utter code. Over 800 lines of PHP web service code.
I read somewhere that the average US programmer averages 200 lines of code per day. In the rst month, I averaged over
500 lines of code per day. Greeneldpro ject from start to nish. Donation app for water and food aid pro jects. Cross
platform withutter . Did all the web services with Laravel/Lumens and MySQL database.
Payment gateway integration withStripe .
January 2023 - April 2023 Title: iOS and Android Developer (Native)
Company Humm Group
https://humm-group.com https://apps.apple.com/au/app/humm/id1455391873
https://apps.apple.com/au/developer/humm-pro-pty-ltd/id1532343413
https://apps.apple.com/ca/app/humm-ca/id1586818651
Apps: Bundll, QANTAS Pay, Humm Pro, Humm Canada, Humm Cards and legacy apps which included the following
tools and libraries:
AFNetworking 2
, FLAnimatedImage 3
, Auth0 4
, NVActivityIndicatorView 5
, Floating Panel, JWTDecode 6
, Kingsher 7
, Fire-
base, Dynatrace, AppFlyerFramework, CardIO 8
, EMSMobileSDK, Alamore 9
, Lottie 10
, KeychainSwift 11
, Branch, Swipe,
Validator, SwiftEntryKit 12
, SVProgressHUD 13
, SwiftLint, Quick, Nimble, IOSSecuritySuite, Snapkit, SwipeCellKit 14
, Ob-
jectMapper, SwiftyJSON 15
, DateToolsSwift 16
, RangeSeekSlider, NVActivityView, IQKeyboardManager, SalesForce market-
ing cloud 17
, kotlinkit, cardview, androidx, Gson 18
, threetenabp 19
, brentrielly navigation, picasso 20
, retrot2 21
, OkHttp3 22
,
biometric, rootbeer 23
, llrembedded, viewpagerdots 24
, letree, play services, swipe reveal layout, EML payments 25
, RxAn-
droid/RxJava 26
, epoxy, TMX, securebank, swiperefreshlayout 27
, liveness 2
https://cocoapods.org/pods/AFNetworking
3 https://cocoapods.org/pods/FLAnimatedimage
4 https://auth0.com/
5 https://cocoapods.org/pods/NVActivityIndicatorView
6 https://cocoapods.org/pods/JWTDecode
7 https://cocoapods.org/pods/Kingsher
8 https://cocoapods.org/pods/CardIO
9 https://cocoapods.org/pods/Alamore
10 https://cocoapods.org/pods/lottie-ios
11 https://cocoapods.org/pods/KeychainSwift
12 https://cocoapods.org/pods/SwiftEntryKit
13 https://cocoapods.org/pods/SVProgressHUD-0.8.1
14 https://cocoapods.org/pods/SwipeCellKit
15 https://cocoapods.org/pods/SwiftyJSON
16 https://cocoapods.org/pods/DateToolsSwift
17 https://developer.salesforce.com/docs/marketing/marketing-cloud/guide/mobile-push-sdk.html
18 https://github.com/google/gson
19 https://github.com/JakeWharton/ThreeTenABP
20 https://github.com/square/picasso
21 https://github.com/square/retrot
22 https://square.github.io/okhttp/
23 https://github.com/scottyab/rootbeer
24 https://github.com/afollestad/viewpagerdots
25 https://www.emlpayments.com/
26 https://github.com/ReactiveX/RxAndroid
27 https://developer.android.com/jetpack/androidx/releases/swiperefreshlayout
2 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfHumm Group white labels credit cards and provides nancial services such as small to medium-enterprise equipment leas-
ing. Specically, Humm provides consumer credit cards / Store cards such as the Farmers 28
chain in New Zealand and the
QANTAS Pay card.
Since I have had experience with nancial services and credit card processing online in one form or another since work-
ing with Plink libraries at BDE in 1997; I was able to quickly adapt to the workows, codebases/business practices of the
credit card services provided by Humm Group to provide bug xes and minor maintenance on 192342 lines of Swift, 879 lines
of Ob jective C, 334037 lines of Kotlin, 47,982 lines of java, excluding libraries. To develop the changes to these codebases I
reviewed Figma diagrams and incorporated changes to UI elements in Android and iOS code as per the requirements of the
JIRA tickets.
To modernise the build environment, I added Azure DevOps / Fastlane CI/CD build pipelines for a few of the apps. 28
https://www.farmers.co.nz/
3 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfJuly 2022 - December 2022
Title: (Native) iOS and Android Developer
Company Milieu Labs . (Contract)
https://apps.apple.com/au/app/milieu-climate/id1566271872 Consumer thermostat systems.
Pre-existing tech stack: Amazon IoT29
, AWS, SwiftFromat, RxSwift 30
, RxCocoa 31
, ReSwift 32
, AWS SDK iOS, Alamore,
Crashlytics, Freddy, Butterknife 33
, MQTT 34
, RxJava 26
, Redux 35
, Mockito, Jetpack. AWS Cognito, AWS IoT, Dagger2,
OkHttp 22
, retrot2 21
Milieu Labs provides controls for domestic air conditioning systems. The control unit of the air conditioner is replaced with
a module that receives MQTT messages from a server which is indirectly managed through a user's account on the app. The
pro ject added a Zone information screen. Zone state change handling which entailed unpacking MQTT Zone state messages,
and translating that through a middleware layer onto the screen through the UI controls.
July 2022 - October 2022 Title: Android Developer
Company Oracle(Contract)
Worked on JSON parsing code for an Oracle internal pro ject for external clients.
The pro ject was under strict NDA. There was an intense level of security around the pro ject. The pro ject had international
security implications.
Technologies employed included OkHttp 22
, Retrot 21
, bumptech/glide, Appsyer, Facebook SDK, Firebase, Kakao, Line
SDK, SnapChat SDK, Twitter SDK, VK 36
SDK.
January 2022 - June 2022 Title: Android Developer
Company Australian Retirement Trust/Sunsuper (Contract)
https://play.google.com/store/apps/details?id=com.sunsuper.prod Java, Kotlin (Android)
Minor maintenance work.
Took Figma diagrams and updated UI elements in Android code.
October 2021 - December 2021 Title: Developer
Company BP (British Petroleum) (Contract)
No link: The company's internal app is only available on the corporate portal.
Flutter , Kotlin (Android), AWS CloudWatch, S3, node.js Lambdas, DynamoDB,
Azure DevOps
This was a suite of 3 apps that are four internal use by point of sale BP employees.
I introduced the featureof local notications in Kotlin/Android with an Android AlarmManager boot time background
service in less than 500 lines of MVVM architected code. 29
https://aws.amazon.com/iot/
30 https://github.com/ReactiveX/RxSwift
31 https://cocoapods.org/pods/RxCocoa
32 https://cocoapods.org/pods/ReSwift
33 https://github.com/JakeWharton/butterknife
34 https://mqtt.org
35 https://cocoapods.org/pods/Redux
36 VK is like the Russian version of Facebook
4 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfThis was so that station employees could be reminded to rotate stock at certain points in the day.
Bug xing in Kotlin, Flutter and node.js Lambdas.
Meetings with the UK are scheduled at 8 pm. Late-night meetings are scheduled for every night of the week. Dealing with
Teams messages at 12:46 am CoverMoore
Password
Screen
July 2021 - September 2021 Title: Android developer
Company CoverMore (Contract)
Kotlin, LiveData, Navigation graph, MVVM.
I rewrote the password reset screen. The design was such that the business logic was in a view
model and as the user entered values the view model validated the input against the password rules.
Publishing/observing, as the input matched the rules the label of each rule turned from red to
green.
I introduced the featureof data binding into the app. Worked on the common button and some of the
reusable widgets.
Android Material design, reusable software components and UI widgets.
April 2021 - July 2021 Company
Nightlife music (Contract)
https://apps.apple.com/us/app/crowddj/id911666442?ls=1 Java, Ob jective-C maintenance work and small pro jects including work on 8-year-old code base in Ob jective-C on iOS and
Java on Android. Heritage
Bank
March 2020 - April 2021 Company
Heritage Bank (Contract)
https://play.google.com/store/apps/details?id=au.com.heritage.app .
https://apps.apple.com/au/app/heritage-mobile-banking/id386772598?ls=1 .
Originally 100 per cent Ob jective-C, I rewrotescheduled payments screen in Swift adding new sections and
refactored the design into separate legacy and MVVM modules.
Merged all the complex build steps and frameworks into folders. Reduced the build time from 8 minutes to
under one minute.
Accessibility audit for vision impaired. Added featuresfor Apple's VoiceOver support.
Provisioning credit cards into Apple Wallet via Apple APIs, encrypting on bank servers with the encryption
keys from Apple's API. Leading and mentoring junior developers.
Cocoapod
Fixing the build system.
PassKit
September 2019 - December 2019 Company
Xinja Bank (Contract)
https://xinja.com.au/ Title: Android Developer.
Technologies: Kotlin
Maintenance and minorbug xing of Kotlin NeoBank banking app. Koin and View Model, Live Data, Android material
design.
There were over 20 onboarding screens in creating an account. I was tasked with adding more.
Technologies employed included androidx, biometric, rebase, Lottie, GSon 18
, jodaTime, newRelic, ok2Curl, retrot 21
, zen-
desk
August 2019 - September 2019 Client
NSW Government DFSI (Contract)
Title: iOS and Android developer
Technologies: Swift / Kotlin
5 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfhttps://play.google.com/store/apps/details?id=au.gov.nsw.onegov.parknpay.release
https://apps.apple.com/au/app/parknpay/id1453474761?mt=8
Watch App
Added f
	eature of Braintree payments integration in Swift and Kotlin in an app for NSW Government including
Apple Pay and Google Pay.
The previous payment integrationhad reportedly been problematic and the payment gateway vendor for
Braintree was almost eortless.
It took less eort and resources than expected so I added the featuresof Apple Watch and Android Gear
Watch integration into the app for the NSW government.
Did a custom notication where the countdown timer and buttons appear in the phone notications even on
the lock screen. Payments
The app communicates with an internal server, which acts as a proxy for another server that manages parking
meters.
Part of the credit card processing integration occurred on the parking meter servers and part was within the
app.
The idea of the app is to pay for parking meters from the phone and be notied of the need to
top up the parking meter if it is about to expire by tapping on the watch and the phone notica-
tions.
Complex integration due to the number of servers involved and the number of moving parts and the level of
security.
All the watch app extensions, all the app to payment gateway integration.
June 2019 - July 2019 Client
Rydo Taxi app . (Contract)
https://apps.apple.com/au/app/rydo/id1150318596 https://play.google.com/store/apps/details?id=com.Rydotechnologies.Rydo&hl=en
Title: IOS and Android developer.
Technologies: Swift / Kotlin
Maintenance work and small pro jects involving C#, Swift, Java, and Kotlin (Mostly Java on the Android side of things).
Fixed about a dozen bugs each week on Android then about a dozen on iOS the next week. The code was breathtakingly
bad, if the server found no results in the database it would return 404 instead of 200 and an indication of an empty dataset
in the JSON payload.
The server would 500 or 400 frequently on requests that worked a minute ago.
Somehow previous developers had considered the foreground and background of the app to be handled in view controllers
instead of the app delegate, there was a lot of surreal code.
Whoever wrote it had no idea of the application life cycle or standard industry practices like not abusing HTTP error codes.
I was doing work on iOS, Android and ASP.NET and completing featuresin two weeks they had waited 2 years to be
completed.
February 2019 - May 2019 Client
ServCorp (Contracts)
https://www.servcorp.com.au/en/oneap/ Technologies: Swift / Kotlin / Ob jective-C / Java / PHP / Larvel / CodeIgniter / L
A
T
EX
Comments: Replaced back-end login process with Auth0 4
(an OAuth implementation). Maintenancework in PHP and
iOS and Android code to replace naive login and session verication.
Did a localisation for the Japanese ( o%; ) version. I have some understanding of Japanese having once worked in
(Citrix 2005/2006) and visited the country several times.
Wrote extensive documentation, and xed bugs. Refactoredlocal login to an OAuth login with Auth0 4
.
June 2018 - January 2019 Client:
eDale / ServCorp (Contracts)
Technologies: Swift / Kotlin / Ob jective-C / Java / Node.js / Heroku / PHP / Larvel / CodeIgniter / L A
T
EX/ Android
Comments: Several overlapping short-term contracts. Some clean-up of previous work for Edale, bug xing, and random ad
6 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfhoc changes. Hand over and manage a developer I hired through my own company to take over work.
Short-term work on Bluetooth tile beacons for nding car keys - type tag beacons. Another USB-based Bluetooth beacon
(Sensoro) and QR code pro ject for allocating desktop handset phones and Wi access. Worked 3 days a week at Servcorp
doing maintenance work on an iOS and Android app that allows for provisioning of desktop handsets and (guest) wi access Servcorp
OneAp
Sensoro beacons, translating Chinese comments into English.
Implemented feature ofAndroid circular
reveal animation like the iOS animation. completed button handlers, dierentiated between my wi and guest
wi buttons and called dierent web service endpoints accordingly. Scale and slide up login animation to
match the iOS version. Made UI consistent with the iOS version. Added disconnect handset screen in Kotlin.
Added feature of pull to refresh the menu on the Android version to be consistent with the iOS version.
Added site features refresh endpoint call and UI update. Added background handset polling. Used to be
location dependent. Fixed refresh, device count wrong and other bugs in. both iOS and Android versions.
A branch was created in the source code control system called allow_handset_choice to allow the choice of
a beacon/handset other than the nearest found beacon. An extra button was added to the found handset
screen. Touching this button launches another screen with a list of handsets detected. On both the iOS and
Android versions all the beacons that were available in the area are stored in collections in the bowels of the
Bluetooth code. In both versions, the collections were propagated up through the software layers until they
are presented in a UITableView or a recycler view and the on-item selection handlers then put the selected
MAC address in place of the nearest found beacon MAC address and then call the occupy handset code which
calls the web service endpoint (Function 16).
Half of the endpoints of the ServCorp server were in CodeIgniter and half were in Larvel in another repository. I set up a
local copy of the code bases on my development machine for debugging server-side and client-side issues.
May 2018  June 2018 Client:
Edale Holdings Pty Ltd (Master contract)
The app has been removed from the app store.
Technologies: Android/iOS/Heroku/Amazon S3, Kotlin, retrot2 21
, Swift4, node.js, mongoDB.
Title: Front-end and back-end developer. Team leader
Comments: Greeneld development Created a node.js web service with about a dozen endpoints. Created an iOS app
that allows users to register, log in and like other users of a dating app. User images on S3 displayed on a dating app. Created
an Android version of the same app to talk to the same web service. Full control over everything changing web services to
t the needs of mobile apps and changing mobile apps to deal with web service restrictions. Next to no formal specication.
Lots of client hand-holding, random inconsistent changes, About 2700 lines of Kotlin, about 1000 lines of node.js and about
2200 lines of Swift4in the rst week.
I eventually hired another developer through my own company. Complete app start to nish Bluetooth
cattle scan-
ner
February 2018 - May 2018 Client:
Ob ject consulting (Contract)
Technologies: Kotlin, Android, Bluetooth, Retrot2 21
Title: Android developer
Comments: Greeneld development . Created an app that read from an RFID tag wand. The
RFID tags would be assigned as a collar or an animal tag. The list of tags was uploaded as
JSON to the RESTful web service. Used retrot2 for accessing the web service. Used Realm to
store the tags. Used some example Bluetooth code to read the wand and send it simple com-
mands.
Complete app start to nish
January 2018 - Skiing in Japan.
September 2017  December 2017 .
Client: Frollo(Contract)
7 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfFrollo
https://apps.apple.com/app/id1179563005 Technologies: Kotlin,
Title: Android developer
Comments: Greeneld development . Created an app that integrates nancial data from all bank accounts
from all banks and hits about 60 endpoints on a single web service. Created multi-dimensional build variants
so there was product A, production, staging, and develop and the same for product B. Wrote about 21,000 lines
of Kotlin and about 800 lines of Java in 2 months. Initially not very idiomatic Kotlin but progressively more
stylistic Kotlin. Wrote JUnit tests. Used com.github.PhilJay:MPAndroidChart to present data pulled from
com. square-up.retrot2:retrot:2.3.0 into recycler views. Used
com.github.vicpinm:krealmextensions:1.1.5 to store data from the endpoints. Used Picasso to fetch and display
bank icons. Used bouncy castle to encrypt data put or posted to endpoints. Used Android Account manager
to store account information. Created one-time passwords to send credentials and nancial data to the server.
Used SSL-pinning. Created code that read data from an end-point and dynamically created controls from it almost like a
web browser. Used an Airbnb library on GitHub to do deep linking to handle frollo:// and https://m.frollo.us
Complete app start to nish The entire app had already between done on the other platform and all decisions had been
made so it was a matter of making like for like.
March 2017  May 2017 Client:
Seven Studio at the Easter Show (2 weeks), AMP Banking App (Multiple Contracts)
Technologies: Android, iOS (iPad), AlamoFire, mpeg, Swift 3, AWS, S3., Carthage, mongoose.js, node.js, MongoDB, Kotlin,
Title: iOS/Node.js, MongoDB, Android developer.
Comments:
AMP Android banking app - Kotlin, RxJava, Dagger dependency injection. A mess of deeply nested Kotlin templates. Clean
design  model, view, presenter, use case, contracts.
7Studio, an App that records videos, displays a teleprompter. The recorded video gets an overlay graphic applied and
then merged with a news break intro and news break closing theme. RecordedUI XC unit test. Memory leak debugging with
instruments. Run at the Easter show for kids to be a reporter for a day.
December 2016  March 2017 Client:
Accenture/Foxtel (Contract)
https://play.google.com/store/apps/details?id=com.foxtel.epg&hl=en_AU&gl=US Technologies: Android, Web Services
Title: Android developer
Comments: Maintenance work on 100,000 plus line of code program guide / billing-account management app with hun-
dreds of thousands of users. Read crash reports, and patched code with defensive programming techniques like checking if
things are null before attempting to use them. Fairly basic maintenance work. The code at times was textbook on what not
to do.
June 2016  November 2016 Client:
Invocare (Contract)
No link: Internal Corporate portal app
Technologies: iOS. Ob jective-C (pre-dating ARC), iOS, Web Services
Title: iOS developer, Architect. Team leader .
Comments: Maintenance work on an iPad app used to sell funeral services. Salesforce, Magic Record, SDWebImage, Ma-
sonary, Cocopods. There were about 100 cases in JIRA. I go through, pick a case, read a chain of emails going for 6 months,
put in a x for 5 lines of code in 20 minutes, stick it on a branch in bitbucket. I end up creating a dozen or so branches, all
dierent fairly trivial xes, never code reviewed, never tested. I'm told they're doing a hybrid app now (Cordova, JavaScript
) and my role is now Application Architect. I have to look at issues like the ow of data app to SalesForce to Epicor is
truncating data and organise workshops with end users, and I digging around in T-SQL on SQL Server and APEX classes
in SalesForce triggers and I end up churning out about 1000 pages of documentation and writing 50 to 100 lines of code in
total. I hired a handful of developers. Lots of meetings, gathering requirements with end users.
8 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfResolution of issues that had been outstanding for over a year
RaceNet
January 2016  June 2016 Client:
RaceNet (Contract)
No link: The app was replaced after several years.
Technologies: Android and iOS. Java and Swift.
Title: iOS, Android, Web Services developer.
Comments: Greeneld development. For the Android version I wrote about 20,000 lines of code in
the rst month. About 5,000 lines a week. GSon ??
, Android studio. Got the basic version of the An-
droid app completed. Worked with Web services developer on dening data models and endpoints. Used
about 30 web service endpoints. I used Cocoapods and SwiftyJSON. Wrote the iOS version in Swift.
I ended up churning out about 30,000 lines of Android code in Java and about 20,000 lines of code in
Swift.
Complete app start to nish
March 2015  December 2015 Client:
AiiMS (Contract)
Technologies: iOS 8, CoreData, Xcode 6, XCTest unit testing framework.
Title: Team leader iOS, Android, Web Services.
Comments: Mentoring Taught PHP developer Ob jective-C, the use of libraries like Cocopods and Cococontrols.com.
Taught use of AFNetworking 2
and JSONModel. Taught C# .NET developer Android, Gradle, Maven, use of libraries and
search.maven.org. Taught both the use of git. A bit of mucking around with web services such as node.js, and Ruby on Rails
before settling back on C# .NET. Technical leadershipon various things such as selecting CMS - bespoke vs Joomla vs
others.
August 2014 - March 2015 Client:
Macquarrie University nance department, TriBeeCam (Multiple Contracts)
Technologies: Perl on Windows, NAB data source, Excel Spreadsheets and VS Script. iOS 8, CoreData, Xcode 6, XCTest
unit testing framework.
Title: Freelance pro ject contractor.
Comments: Did maintenance work for a university that brings me back every couple of years.
Did AFNetworking 2
to JSONModel wrapper for a camera app. UIKit, everything is done in code, no storyboards, nib les,
or anything. Git, JSONModel, JSON. DoppelTime
May 2014  August 2014 Client:
DoppelTime . (Contract)
Link:https://apps.apple.com/gb/app/dopel/id1434777360 https://play.google.com/store/apps/details?id=com.dopel.dopel
Technologies: iOS 7, CoreData, Xcode 5, Unit testing framework.
Title: iOS developer.
Comments: Completed an iOS app, added social elements, camera roll picker, camera control, voice recorder,
bug xing and nished o half-completed code. The company is a start-up in a start-up incubator. High
pressure for quick results. Used Cocopods, AFNetworking 2
, UIKit, GCD, Flurry Analytics, GoogleMaps,
Core Graphics, added a bunch of UIViews to a core animation layer, did some cute explode-out pseudo-button
animations and added gesture recognizers to the UIViews. Core Location, AVFoundation for Camera & Audio
record/playback. Some node.js debugging of the services the iOS app used. Set up node.js for local testing of
mobile code.
October 2013  May 2014 Client:
SmartBill (Contract)
No Link: Removed - App Store policies
Technologies: Android, Sqlite3, iOS 7, CoreData, Xcode 5s Unit testing framework.
Title: iOS/Android developer.
Comments: Greeneld development . Developed an Android and iOS app to gather data usage and call log data and send
it to a server for `smart bill' analysis against phone plans. Used silent push notications to wake the app up for processing.
9 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfAFNetworking
2
, working on ASP.NET code for Apple push notication servers. SQL Lite.
Complete app start to nish
July 2013  October 2013 Client:
Industrea/GE Mining (Contract)
Technologies: ARM Embedded Linux, Sqlite3, rtrees, sockets.
Title: C++ developer.
Comments: Added features. Developed a GPS Fence Daemon for a Collision Avoidance System (CAS). Point in Polygon,
pulling fence data from web services. About 9000 lines of code in 3 months (about 100  200 lines of code a day). SQL Lite.
August 2012 - April 2013 Client:
Kordia (Contract)
No link: The app was available through the corporate portal only.
Technologies: JSONKit, YAJL JSON parser, Telerik C# controls, Visual Studio 2010, SQL Server 2010, Team Foundation
Server. ASP.NET web services.
Title: iOS Developer.
Comments: Maintenance work on KST, Kordia's iPod app that communicates with a web service pulling down JSON data
for Telco site planning, such as Telstra, Downer and NBN. Converted JSON requests to background SAX style streaming
requests updating a UITableView as large pro jects with lots of assets were taking a long time to update with no indication
to the user of any activity. Employed code blocks, ARC, Multi-threading and other more recent or advanced iOS techniques.
A little C#/XAML but mostly Telerik controls.
December 2011  August 2012 Client:
ShueMaster (Star Games) (Poker machines)
Technologies: OpenGL, Qt, C++, Linux and embedded Linux. SDL, Fedora Core 4, OpenSUSE 11.4
Title: Linux C/C++ developer.
Comments: Worked on Roulette, Baccarat, and Sicbo, player terminals and dealer terminals. Bug xing assisted with the
development of the concurrent pro jects, implemented a couple of cute animations and some tab drawings, tab switching
functions.
May 2011  November 2011 Client:
Mercurien (Contract)
Technologies: OpenCV, Hadoop 37
, ZooKeeper 38
, Cloud Computing technologies, C++, JSON, REST, AJAX, Eclipse, Java,
Ant. Maven, Cisco 3400E, Netgear FVS 336. Bamboo, Maven, Ant.
Title: IT Specialist. (network administration, programming, cabling, etc).
Comments: Evaluated cloud computing technologies. Creating VPNs, production and internal Development/Test subnets.
Building Java and C++ ANPR 39
software. Shell scripting in bash, creating Java build plans in Bamboo. Conguration
and release management. OS-X Server administration. SVN management. Installing cameras in car parks, porting OpenCV
(C++ computer vision) to OpenIndiana 40
(Open Solaris). Some OS-X Ob jective-C coding.
June 2010  May 2011 Client:
Samsung Electronics Australia (SEAU) (Contract)  Mobile development and wrote a mobile development book
for a publisher
Technologies: C++, bada, JSON, REST, AJAX, OpenGL ES, Widgets, iPhone, Android. StarUML, PowerVR, Internet@TV
Widget SDK. Eclipse. Flex/Flash Lite for mobile.
Title: Bada (Mobile) Specialist.
Comments: Developer support specialist, helping people port applications from Android and iPhone. Digging through iOS,
Android and Widget code. Site visits to companies like Blue Pebble (Essendon Football Club) and Fairfax Digital. Assisted 37
https://hadoop.apache.org/
38 https://zookeeper.apache.org/
39 (Automatic Number Plate Recognition)
40 https://www.openindiana.org/
10 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfwith development of the MyCareer app, sole responsibility for the
Greeneld developmentof Domain app working on-site
at Fairfax oces working directly with Fairfax as Samsung's clients. Helping developers port from iOS to Bada, digging
through iOS and bada code.
Greeneld development Wrote the entire Essendon Football Club App within one week, Shows match xtures, with scores
if played, from JSON data as logo v- logo, Downloads thumbnail images from URL in JSON data, and shows images in
news items. Player proles, injury lists, scores, football club shop, live chat, statistics, etc. All are downloaded live from the
ocial site. YouTube of app running on phone:http://www.youtube.com/watch?v=LvmnGqPC6Gw January 2010 - April 2010 (Series of very short contracts)
Client: Open Systems Consulting
Technologies: RedHat Linux, Oracle, SCO UNIX Apache.
Title: C/PERL Analyst/Programmer for iPaq mobile devices and infrastructure.
Comments: General UNIX administrationactivities, scripting, adapting cron jobs, setting up IMAP servers, Apache
conguration, buying SSL certicates, SCO UNIX and Linux, maintaining very very old legacy C code called carry and
directbook. Creating WSDL specications for SOAP::Lite interface and code for a system that accepts SOAP requests
and transfers the XML data to whichever state it is destined for. Some PERL. 3 days a week, consulting work. Gathering
requirements directly from managing directors of transport companies and implementing changes or making bug xes and
direct deployments to live systems.
August 2009  December 2009 (Contract covering someone on leave)
Company: Open Systems Consulting
Technologies: RedHat Linux, Oracle,
Title: Analyst/Programmer for iPaq mobile devices and infrastructure.
Comments: Maintenance work covering for a sta member on leave. Data migration wrapped a SOAP layer around an
XML-RPC-like application for interfacing with the SAP PI SOAP interface. Miscellaneous Apache/Linux xing.
April 2009  July 2009 Contract
Company: Telstra Bigpond
Technologies: Solaris, C/C++, OpenLDAP
Title: Analyst/Programmer.
Comments: Bug xing and documentation of some LDAP 41
and RADIUS 42
code on the system that handles the leases on
the Telstra Bigpond cable modems and interfaces into the billing system.
February 2009 - April 2009 (Contract for term of pro ject)
Company: Department of Innovation and Industry Research,
National Measurement Institute.
Technologies: Linux, C++
Title: Analyst/Programmer
. Comments: Greeneld development Linux daemon to certify the synchronisation of the Network Time Protocol with
the atomic clocks for all of Australia. Provision of ocial time for all of Australia: Industry, Government, etc. Got to know
the inner workings of the NTP protocol well. Stued data into the extensions elds of NTP packets. Wrote a network snier
daemon program that snied the network for NTP packets and extracted and logged the extension elds. On completion of
the pro ject did a quick port of the ntpd to Windows, taking less than a week.
June 2008 - November 2008 Contract
Company: Infoplex.
Technologies: Linux, mod_perl, CVS, Blackberry mobile developmentJDE, kSoap
Title: PERL Analyst/Programmer.
Comments: Maintenance on Linux mod_perl code some C# ASP.NET, Visual Studio 2005, Visual Studio 2008, Java
Server Pages, SOAP calls from Blackberry, ocr-xtr, AutoVue jVue, Apache conguration, AJAX, wrote an 877-page doc-
ument explaining the current system. Recongure hylafax, postx, created virtual machines, some Crystal Reports, SQL
server 2008, RFC 2445. Lots of Postgres and CPAN and Perl DBI. Some IIS. 41
https://en 
wikipedia.org/wiki/Lightweight_Directory_Access_Protocol
42 https://en.wikipedia.org/wiki/RADIUS
11 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfNovember 2007  June 2008 Contract
Company: Saint George Bank Treasury Core Systems Development.
Technologies: Solaris, Sybase, C++, PERL, Eclipse, Java, KSH scripting, CVS.
Title: C++ and PERL Analyst/Programmer.
Comments: Maintenance on (www.misys.com) Risk Vision add-ons and (www.demica.com) Citadel extensions in C++,
PERL, Java and KSH. Developed a patch in C++ for (Sybase, New Era Of Networks) NEON 2038 (32 bit) date problem
43 . Moved some systems from crontabs to AppWorks, and a lot of very small patches in PERL and Korn shell scripts.
June 2007 September 2007 (Contract  Master contractor over subcontractors).
Company: Keycorp
Technologies: Apache, CodeCharge, PHP, C++, RedHat
Enterprise Linux 4, XML, pThreads, sockets, Postgres 7.4.5. (Database) Title: Analyst/Programmer. (Manager of other
developers through my own company) Comments: Digging for missing source code. Reconstructing missing and broken
software, untangling mess, and editing image les with a hex editor because the image specication was a poorly worded
paragraph in a long email chain that had not been understood. Hiring and ring programmers. Dealing with specications
that were still changing months after delivery had taken place. Moving columns back and forth between tables to match specs
that changed daily. Fixing other programmers' code. Small amounts of Python and PERL in the XML2Db loader. Shell
scripting and a lot of system administration of a few RedHat Enterprise Linux 4, servers running as virtual machines under
VMWare ESX server. I hired two developers through my own company and managed their work including specication,
verication and delivery of their work. I managed another notoriously dicult developer. Working on multiple pro jects with
multiple pro ject managers and allocating time.
February 2007  June 2007 Contract.
Company Macquarie Bank. Quantitative Applications Division.
Technologies: Solaris, XP Pro64, Sybase, Orbix (CORBA 44
) Java and C++ sides of client server. Reuters SFC/SSL.
Title: Quantitative Analyst/Programmer.
Comments: Imputation credits to Indextool Java/C++ CORBA-based client/server application and analysis of Sybase
database. Some Reuters SFC/SSL code.
August 2006  February 2007 Company
VeCommerce.
Technologies: Access, Visual C++
Title: C++ Callow Developer.
Comments: Built telephony application that handles entry of credit-card numbers and activation of SIM cards. This gave
me the skills to build my DTMF-based phone system.
February 2006  August 2006 Company
SpamMATTERS.
Technologies: SQL Server, Visual Studio, SysInternals tools, Ethereal. PostgreSQL, FreeTDS.
Title: Team Leader of 5 C++ Developers
Comments: CGI-BIN/PERL scripting. Maintenanceof web site.Greeneld development . Implemented a system that
accepts mail into 20 accounts pulls the recipient eld out registers them into PostgreSQL table posts values into a web page
and parses the result page. Done in 3 days without prior knowledge of PostgreSQL.
August 2005  January 2006 Company
Citrix.
Technologies: Citrix Presentation Server 4.0, Web interface. Metaframe for Solaris, SQL server, Xinerama, Oracle, Gnome,
WinDbg, ASP, Visual Studio, SysInternals tools, Ethereal. 43
https://en.wikipedia.org/wiki/Year_2038_problem
44 https://corba.org/
12 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfTitle: Lead Escalation Engineer
Comments: Read kernel crash dumps and Dr Watson dumps with WinDbg. Debugging device drivers. Code investigation
concerning trace logs. Dealt with customer issues. Business trips to Japan and Hong Kong.
December 2003  August 2005 (Two week contract that got extended)
Company Optus / NCS  Part of SingTel.
Technologies: C++ (aCC/cxx), Tru64 4.0D, AIX,
ORACLE 9.2, Mac OS9, AppleScript, VisualAge C++. Tuxedo8.1, Visual C#, HP-UX 11.0, Solaris, Mac OS9. Weblogic
8.1, XML, ant, Enterprise Java Beans (EJBs)
Title: C++/C#/Tuxedo Team lead/Programmer/ J2EE Programmer
Comments: Porting/remediation pro ject replacing SII middleware with Tuxedo.
Boris remediation pro ject. It is a 3-tier client/server application. Apple Mac client in OOPL communicates via OpenUI to
the COGS middleware. The COGS talks (via SII replaced by Tuxedo) to the Boris server which contains Pro*C code to talk
to the Oracle (7.3.4 replaced by 9.2) RDBMS. The SII section I was responsible for was the SIDL which is like the CORBA
45 IDL and is kept in a repository which is like the Windows registry. I wrote code to load SIDL to emulate the repository.
The original target platform was HP and later moved to AIX. I did a partial port to Linux to do work on my laptop.
Testing of the interface to another system that communicates via ORACLE database pipes. Took a reworked C# client that
communicated via OpenUI to the SII Boris and got it to call the Tuxedo Boris from the last two contracts. Led a team of
three developers. Training developers, administration, pro ject planning, architecting solution, etc, etc.
SNMP Support for the previous application. Wrote a server that polls a shared memory segment and dumps content to a
log le to be retrieved by CA Unicenter Log Agent 3.0 and sent to the SNMP port. Also wrote a debug test harness that
forces exceptions to be thrown for the catch and send SNMP trap code. Prototype for EJB interface for the previous
application. Conguration of a WebLogic8.1 and Tuxedo8.1 server on Windows XP. WebLogic to Tuxedo (WTC) code. Java
server pages to call the EJB. Ant scripts in XML to compile and deploy the EJB in the WLS. A C++ test program to call
the test target service in BORIS as a prototype for the JSP/EJB prototype. WebLogic and Tuxedo domain conguration,
on Linux, Windows XP and AIX. Documentation and support for previous pro jects. Maintenance work on the C++ and
OpenUI OPL source code on the Macintosh client. Wrote C++ Tuxedo test harness for SIBEL interface.
October 2001- March 2003 Company
National Bank of New Zealand, Wellington, New Zealand.
Technologies: C++, Solaris, Windows. Sybase, Sunsoft C++, Cytrix, Borland C++ Builder, Java, PERL/Tk, Tools++,
DBTools++, GreenLeaf Comm++, SNA 46
, LU-62, Systematics OWL, Paradox engine 3.0, Comms++, Protoview Datat-
able, Seagate Crystal reports, InstallShield, Borland C++ 4.52, 5.0, C++ Builder 5.0. Microsoft Word, Excel.
Title: C++ Programmer.
Comments: I was one of two programmers responsible for Direct Link maintenance. Transaction processing in excess of
$5 billion daily. Code for the $ 2.5 billion problem, numerous reports. The system contained several components including:
Client-side: monolithic 16bit application capable of running on Windows 3.1, an X.25 network interface, a 16-bit to 32-
bit thunking layer 47
for connection to the server via Secure Socket Layer (SSL), CREEP protocol a modied form of
DES encryption for a secure connection to the server. Maintenance on several attempts that had been made to port the
OWL/Paradox-based 16bit application to 32 bits using various products including Borland C++ and C++ Builder and
libraries/tools like DBTools++, etc.
Server-side: a modied form of DES (CREEP), an interface to other internal applications in the bank including updates to
FOREX (FOReign EXchange) rate boards in the branches, connections to a Screen Scraper for communication to the IBM
3090 MVS mainframes via LU6.2 bridges. Access to Sybase DBMS and at les. Access to DEC VMS systems, TCP/IP to
SNA bridges, etc.
Disaster recovery systems. Communications via multiple networks to SSL authenticators, etc.
Sentinel a suite of packages in XView (XWindows UNIX dialogues) for the Direct Link Call centre to monitor transaction
processing and accept/reject transaction batches and administrate client accounts. Call centre reports in Crystal Reports. 45
https://corba.org/
46 https://bitsavers.org/pdf/ibm/sna/GA27-3102-0_SNA_General_Information_Jan75.pdf
47 https://learn.microsoft.com/en-us/windows-hardware/drivers/kernel/why-thunking-is-necessary
13 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfApril 2001-June 2001 Contract as a subcontractor.
Company Compaq(On site at ADC broadband), Brisbane, Australia
Technologies: C++, Tru64. GNU CC. Tuxedo.
Title: C++ Programmer.
Comments: Maintenance on-site development at ADC broadband. This was a conversion pro ject for ADC on behalf of
Compaq as a result of a request from one of ADCs' Pacic clients. I ported code from Sun/SGI/AIX/HP-UX/Win32 to
include conditional compilation for the Compaq (Now HP) version of UNIX (Tru-64). The code base is several millions
of lines of code for a telecommunication billing system. This included spotting known issues and xing them. It required
compiling on the new platform, rerunning unit tests and xing compile errors and unit test failure bugs.
January 2001-April 2001 Contract
Company Printrak, Brisbane, Australia
Technologies: Visual C++ 6.0, MFC.
Title: C++ Programmer.
Comments: Emergency service response dispatch software, Fire, Ambulance, Police. 911 Call centre operations. Added
additional dialogues for accessing Microsoft SQL Server 7.0 DBMS in C++/MFC/ODBC. Mostly this was to fetch some
rows from a table and update the controls in the dialogue type code. Some critical systems accessed systems in TADEM/NON-
STOP KERNEL subsystem.
Technologies: MFC, Microsoft SQL server 7.0. Tandem COBOL.
14 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfOctober 2000 January 2001
Company
Active Sky , Gold Coast, Queensland, Australia
Technologies: Palm Pilot, Windows CE mobile development , Solaris, Linux
Title: C++ Programmer.
Comments: Video compression streaming to hand-helds. I did some architectural work regarding common le I/O libraries
for both client and server-side communication and mentoredsome of the developers. I trained the Windows NT system
administrators to congure and manage Solaris and Linux servers and deal with the programmer requests properly which
they were not doing. X1 X2
YA YB
ZMarch 2000  September 2000 Contract for some small
pro jects.
Company Open Telecommunications , Sydney, Aus-
tralia
Technologies: Solaris and Tru64, SunSoft C++, GNATTS,
PERL, expect, CORBA 48
. TAO, Orbix, ACE, pThreads,
Rational Rose, UML / BOOCH Rmakeit, Nedit, Emacs.
Title: C++ Programmer.
Comments: Maintenance work . This was a telecommu-
nication company that mostly built digital switches (Sig-
nal Control Processors - SCPs). I scanned the bug list in
the GNATTS database, resolved the bugs and submitted
progress updates. One of the bugs in the systems was a
multithreaded construction/destruction bug which was re-
lated to multiple inheritance and the C++ diamond problem as can be inferred from the following code snippet and opposing
UML diagrams:-
classY:publicX{};insteadofclassY:virtualpublicX{};andclassZ:publicYA,YB{}; X
YA YB
Z Once the bug list was reduced, I migrated the code from the Or-
bix, ORB to the ACEs TAO ORB because the company wanted to
use more open-source software and not pay for commercial licenses. I
reran the unit tests for all relevant parts of the system and worked
with other team members to resolve any issues. I rewrote the log-
ging code which provided streams ( << and  >> operators were over-
loaded) and URL style logging methods (such as le:, socket:, etc
). We had internal seminars on SS7, Voice Over IP, etc. The
main customer was One.Tel which was a spectacular dot.bomb fail-
ure. Open Telecommunications no longer exists. The documentation
method was UML using Rational Rose, and Source code control was
in CVS. Unit tests were in scripting languages such as expect, awk or
PERL. 48
https://corba.org/
15 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfSeptember 1999  March 2000
Company:
Thompson-CFS , Dee Why, Sydney, Australia
Technologies: IRIX, DomainOS, Tru64 UNIX, Solaris, WinCenter, VME, VIMIC, MIL-STD-1553 49
, ARINC-429 50
, MIL-
STD498, Ada, C, DOORS, Interleaf.
Title: C Programmer.
Comments: Defense training organization. I developed software to simulate and stimulate the MIL-STD-1553 and AR-Inc
429 buses. The environment was MIL-STD 498 documentation process. The buses interacted with the rest of the environment
via VME boards.
June 1999 - July 1999 (One of two concurrent contracts, after hours, part-time, concurrent with SMA below)
Company: Transport Management Group , CBD, Sydney, Australia.
Technologies: Windows/MFC, ORACLE, Windows NT, Visual C++ 6.0, QVCS
Title: C++ Programmer.
Comments: Train scheduling. I produced graphical reports in MFC/C++, (eg zig-zag graphs that show when trains are
scheduled to arrive/depart at points up and down the line). The environment was ORACLE Pro*C which was wrapped within
smart pointers which loaded an internal cache, pre-fetching and lazy-evaluating as required. The development environment
was initially extremely chaotic which I resolved to structure. I introduced, set up and maintained, QVCS as no source code
control system was used and QVCS (a free/cheap product) was used at SMA where I was working concurrently (together
from 6:30 am to 9 pm every day and sleeping through the weekends).
November 1998  September 1999 (one of two concurrent contracts)
Company: Scientic Management Associates , Lane Cove, Sydney, Australia
Technologies: Windows 98, Windows NT, Windows 98, Visual C++ 6.0, DirectX 6.0, QVCS, VME 51
, MIL-STD-1397 52
,
3D Studio Max.
Title: C++/3D Game Engine Programmer for Defence Pro ject.
Comments: This was originally a 6-month contract which was extended to 11 months to coincide with the completion of the
pro ject. This was an extremely challenging pro ject which required examining a real piece of equipment (EOSS) a system
much like a periscope on a submarine and developing a design to simulate it. A director head with DLTV and Thermal
imager sits about three-quarters the way up the mast of the Huon class mine hunters. On the bridge of the ship is a console
that was simulated. I had to put 3 video cards into one computer and get all the device drivers to work together. Then I had
to get hardware-accelerated Direct-3D to function on two video cards, and load textures and vertexes into each card, while
the other video card displayed a menu system, I wrote that mimicked the controls on the real bridge. Some video cards would
detect that they were not the primary display device and switch to software rendering. Other video cards would steal vertex
lists or not load textures. The thermal imager had White hot and Black Hot modes and therefore two sets of textures had
to be loaded for each ob ject and ipped between them in the scene as the controls were accessed. The daylight TV camera
had an intensity control on the touch-sensitive control panel and therefore I had to walk the vertex list on the video card
and adjust the lighting intensity of each vertex in the scene. The glow and dim eect was quite spectacular and was quite
cool to play with. Additional functionality included socket code to interact with an Instructors station (which someone else
wrote). There was also a Digi I/O board added to the machine which enabled digital/analogue conversion of signals. The
pro ject was completed 2 two weeks ahead of schedule and I spent some time proling and optimising it as much as possible.
The Navy was happy to sign o on the pro ject.
I tendered a pro ject to replace the MADS (disk packs) on the submarine pro ject. The small tender (roughly $100,000) was
successful but the pro ject ($100 million plus) was suspended indenitely (pending a Royal Commission). If the tender was
not revoked I would have had additional continuing contracts with SMA.
This was about the same time that I had the BeOS C++ Ray Tracer article published in Doctor Dobbs Journal. 53
February 1998  October 1998 .
Company: TowerTechnology , Lane Cove, Sydney, Australia
Technologies: Solaris, HP-UX, AIX, Digital Unix, Windows NT 4.0/5.0 beta, Windows 98. Title: UNIX/Windows NT, SCSI
Device driver developer. 49
https://en.wikipedia.org/wiki/MIL-STD-1553
50 https://web.archive.org/web/20111029161330/http://www.holtic.com/category/352-arinc-429.aspx
51 https://en.wikipedia.org/wiki/VMEbus
52 http://www.interfacebus.com/Design_Connector_NTDS_Bus.html
53 Russell R., (Nov 1999) ""BeRays: A ray tracer for BeOS"", Doctor Dobbs Journal.
https://drdobbs.com/tools/the-berays-ray-tracer/184411102
16 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfComments: TowerTechnology develops document and image processing systems and workow solutions. I was responsi-
ble for maintaining the device driver code for the medium changers (large mag-optical disk libraries which contain crypts
for disks and several drives). I became an expert on the SCSI bus protocol. I wrote a class factory pattern-based diagnostic
tool. The class factory would generate ob jects of all medium changers that the company supported and dump all kinds of
diagnostic information.
I completed a device driver for Sun Solaris to access the medium changers via a pass-through SCSI device driver, debugged
multithreaded kernel panics on the Tru64 platform and debugged faults on the HP-UX, AIX, Solaris, Tru64, and Windows
NT drivers.
One of the faults included reworking some of the drivers when the disk capacity increased from 2.6Gb to 5Gb, 32-bit limits were
exceeded and block orientated seeks had to be replaced with ioctl()s on the devices. Some faults required some functionality
to be moved from the upper layer of the kernel to the lower-level drivers or vice versa
Technologies: Purify, Clear Case, SunSoft Visual Workshop, GNU C++. SCSI-View (SCSI Analyser hardware). RCS. Ra-
tional Rose, Paradigm Plus, UML. Lotus Notes. Windows NT Kernel debugger (i386kd.exe) and crash dumps. PA-RISC,
PowerPC, Intel and SPARC machine code and assembler. Figure 1:
CyberSwine
game
May 1997 - February 1998 Company:
Brilliant Digital Entertainment , Double Bay, Sydney, Australia
Technologies: Windows NT 4.0 / Windows95 / Linux, ISAPI, Windows Registry, MFC, Visual
C++ 5, COM, Automation, InstallShield 3/5, IntraBuilder, Borland C++ Builder, Delphi 3, Mi-
crosoft SQL server 6.5, Java, JavaScript, Perl, inline 80x86 assembler, WinSock32, Plink., SAMBA,
PKware.
Title: C++ Programmer.
Comments: BDE is a small dynamic games software house. I was involved in various aspects of real-time
interactive movies. I took over the installer. I did all of the Unix work and wrote code to interact with other
parts of the system including the ticket server. I also did the credit card validation code via Plink.
January 1997 - May 1997 Company:
Scientia Systems , North Sydney, Australia.
Technologies: AIX / SunOS / Solaris / SCO / Windows NT 4.0 / Windows95. Visual C++ 5.0, Borland C++ Builder,
C-ISAM, SAGA-C, ISDN, Win Gate, SMIT, Humming Bird Exceed XDK, Motif, SAMBA, Microsoft TCP/IP, POP3.
Title: C/C++ Programmer.
Comments: Scientia is a software house that produces a scheduler used by manufacturing called Synchro. The main output
is a Gantt chart with the capacity for drag and drop and running under the Motif system.
I had previously worked for the company in 1988 when it was known as Scientia-Whitehorse. The system was Accounting
(invoicing, accounts receivable, payroll) and manufacturing (Just-In-Time (JIT) and MRP-II).
May 1996 - December 1996 Company:
EyeOn Software , Crows Nest, Sydney, Australia
Technologies: Windows NT 3.51/4.0 Intel/Alpha. Visual C++ 4.x RISC Visual C++ 5.0 Intel, MFC, MCI, Install Shield,
OLE, Windows registry, ISDN, Notes, TCP/IP.
Title: C++ Computer Graphics Programmer
.
Comments: The product ""Digital Fusion"" was an Ob ject Oriented, multi-processor optimised, multi-threaded spline-based,
resolution-independent video compositing system. My role was to design and implement features le format loaders and
savers for the ma jority of graphic le formats (two dozen variants like JPEG, Sun Raster, PNG, TIFF, Gif, etc). I wrote
Windows Registry code and various graphic processing code including Sobel and La Placian, edge detection, and blur lters.
I wrote MFC/GUI code for custom controls like a rubbery range control (like 2 slider controls in the same control, which
stretched and contracted at limits), screw control with innite wrap-around looping behaviour. The entire GUI was based
on ray-traced images and was extremely slick. Amiga style intuition layer framework so that existing components would
benet from underlying extensions. DLLs could be dropped in so that at load time the system would recognise and register
new components. Aspect-orientated/delegate style development. System administration of Lotus Notes, Novell Netware,
network, and CISCO router. Build a system with Install Shield.
For some months all of the sta went to meetings with angel investors, and business partners overseas and attend SIGGRAPH
conferences. I was left alone in charge of the company, answering phones, making sales, sourcing suppliers, deciding markups,
17 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfarranging conferences, transferring money, paying bills, and myself.
November 1995 - May 1996 Organisation:
Department of Computer Science . James Cook University, Townsville, Australia
Technologies: HTML, CGI, PERL, PASCAL, Novell 3.11, OSF/1, Digital UNIX. ULTRIX, SOLARIS, IRIX, C, PVM, MPI
54 . Various supercomputers including SGI, Cray. Processor farms.
Title: Tutor. Semester-long Contract. Research Assistant/Programmer. Contract. Various supercomputers.
Comments: Taught students PASCAL, data structures. Did some WWW development. Wrote a 25-page Literature review
on distributed data structures. Wrote a Scalable 3D torus distributed termination simulation on multiple networked work-
stations representing processing elements or nodes of the torus in PVM. The simulation is an accurate model of a distributed
termination algorithm for the Cray T3D massively parallel processor 55
. Wrote a pseudo device driver for DEC Alpha under
Linux. Modied Linux system to run OSF/1 binaries.
Dr B. Mans developed a distributed priority queue out of work from his PhD. thesis and work in Scotland. The Message
Passing Interface (MPI) library was unavailable for any of the departments' equipment, so I was asked to rewrite the best
part of a large pro ject to use the Parallel Virtual Machine (PVM) library on a mixture of workstation virtual machine groups
and supercomputers. The pro ject was delivered ahead of schedule and very few modications were requested.
January 1995 - November 1995 Company:
Agire. Townsville, Australia
Technologies: SCO UNIX/XENIX, SPARC Solaris.
Title: Salesman/Technical support.
Comments: Having previously worked on small pro jects in Informix, XENIX and C for AGIRE. I was asked to join as front
oce sales and handle local technical support while most of the team travelled.
January 1994 - December 1994 Organisation:
Department of Psychology , James Cook University. Townsville, Australia.
Technologies: DOS, Windows, Turbo PASCAL, Borland C++.
Title: Research Assistant/Programmer. Contract
Comments: Stereopsis is the post-processing of images by the front of the brain giving the 3D eect found by squinting at
the images in MAGIC EYE books. The Psychology department was interested in dierent eects, shapes and reaction times.
The initial main focus was to develop dierent tests for sub jects using in-house developed libraries. METAcode for Windows,
a real-time multi-pass event logger, that produced graphs of statistics of events, was designed by Dr Ryan and myself. 5657
January 1990 - December 1993 .
Helicopter Pilot, Outback Australia. Consulting as a University student.
Part-time Real Estate agent. Various student jobs.
(AGIRE, BTC, FNQEB, MCD Consulting, etc) Short-term contracts. Technologies: Informix, Pick, C-ISAM, Zinc, C, DOS,
UNIX, BASIC, 8051, 8052. Windows, ORACLE Pro*C, METAwindows, AS/400, DECSystem10, VMS, CP/M. MP/M,
PC-MOS, XENIX
Comments: While studying and ying I worked on small pro jects for various companies and organisations.
September 1988 - November 1989 .
Company: Scientia WhiteHorse . Crows Nest, Sydney Australia
Technologies: NCR UNIX (Tower 32, Tower XP), XENIX, DOS, C, SAGA. VMS, DIBOL
Title: C Programmer. 54
https://www.mpi-forum.org/
55 https://en.wikipedia.org/wiki/Cray_T3D
56 Russell, R., & Ryan C (1994) METAcoder for windows: real-time and multi-pass event logging and analysis in the social and behavioural
sciences. Psychology Teaching Review. 57 Russell, R., & Ryan C. (1994) METAcoder for windows: real-time and multi-pass event logging and analysis in the social and behavioural
sciences. Psychology Software News.
18 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfComments: Scientia White Horse was a software development company producing accounting and manufacturing systems.
I assisted in the development of the Dental front oce system. The dental front oce system had additional features such
as a history sensitive teeth-charting system. From existing designs I implemented accounts payable account reconciliation,
accounts receivable, invoicing, and general ledger postings. The shipping container system was a very specialised stock control
system. Shipping containers each have there own ID with a check-digit and may reside in yards for years or get written o
such as getting lost at sea.
Summary
 30 iOS apps in the app store, several apps on other app stores, wrote a book on mobile development (iBooks), consulted on
app development (e.g. AFL, Newspapers). Broad spectrum of expertise: UNIX, Windows, Mainframe, mobile and Embedded,
Middleware (WebLogic, Tuxedo, CORBA, SII, sockets client/server, SOAP, RESTful WS) Various languages Ob jective-C,
C/C++,C#, Java, PERL, PHP, Scripting, Cocoa, Swift, UIKit, XML, SQLite, Facebook/Twitter SDK integrations, REST
with AFnetworking, MapKit, Quartz2D, CoreAnimation, CoreData, Magic Record and Mogentator, Multi-threading and
GCD, XCTest, native C/C++ code, Interface Builder, HTML5, JS, JQuery Mobile, AJAX, PHP, Magento, Node.js, Neo4j.
Experience in professional software development since 1986. Team leader of 3 (twice) and team leader of 5, management and
mentoring skills. Agile, Scrum, MIL-STD-498 and MIL-STD-1267A. Published on several app stores/marketplaces, including
current iOS apps: https://itunes.apple.com/au/artist/pymble-software-pty-ltd/id553990081 Android apps on Google Play:
https://play.google.com/store/apps/developer?id=PymbleSoftware+Pty+Ltd&hl=en C# / XAML Windows Phone 8 Apps:
http://www.windowsphone.com/en-US/store/publishers?publisherId=PYMBLE%2BSOFTWARE%2BPTY%2BLTD. 19 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfEducation
Pilots license, PPL(Helicopter) License #425348, Flight School, BGT 58
Diploma, Programming (Accounting, BASIC, COBOL, JCL, IBM System/34, RPG-II, CP/M 2.2) Control Data Institute
59
Certicate, UNIX Administration NCR
60
Card, PADI certied open water diver PADI
Certicate, Spanish ,
Institute of modern languages, JCU
Degree, Bachelor of Science, Computer Science James Cook University
CP1000 Introduction to Computer Science
GE1010 The Geographical Environment
EV1001 Introduction to Environmental Science
LI1101 Introduction to Linguistics
LI1102 Introduction to Descriptive Linguistics
CP1500 Information Systems
CO1501 Introduction to Commercial Law
CP2000 Computer Science II
CP2050 Computer Science IIA
TG2100 Intro Geographic Inform. Systems
CP2600 Database Systems
CP2700 Theory of Computer Science
CO2801 Business Information Systems II
CP3050 Algorithms and Complexity
CP3060 (Computer) Graphics
CP3070 Computer Architecture and Communications
CP3080 Advanced Programming Languages
CP3100 Formal Languages and Compilers
CP3110 Fundamentals of Software Engineering
CP3120 Advanced Software Engineering
CP3210 Fundamentals of Articial Intelligence
CP3220 Advanced Articial Intelligence
Certicate Clear Case fundamentals for UNIX Rational University
Certicate DOORS Internal, Thompson-CFS
Certicate RSA Reach
.
Certicate RCG TCP
Certicate Citrix Certied Administrator Internal, Citrix
Certicate Developing for Windows Phone 8 ITM/Charles Sturt University
Certicate Super Computers R(language)/MPI/OpenMP/Xeon Phi ITM/Charles Sturt University
61
Certicate Digital Marketing ITM/Charles Sturt University
Certicate Unity 3D ITM/Charles Sturt University
Certicate Prepare your BAS Macquarie Community College
Certicate Import/Export Business Macquarie Community College
Certicate Surf rescue / CPR Surng NSW
Certicate AWS Solution Architect IT Masters/Charles Sturt University
Certicate IT Leadership ITL4,Agile,Scrum, SAFe, PRINCE2, BABOK, ITM/CSU
Certicate Agile Data & Information Management ITM/CSU
Certicate Cryptography IT Masters / CSU
Certicate Comparative cloud technologies GCP, Azure, AWS ITM/CSU
Badge AWS Cloud foundations AWS Academy
Course Jetpack Compose crash course Udemy
Course Reactive Programming in iOS with RxSwift Udemy
Certicate DevOps IT Masters/Charles Sturt University
Certicate Golang Sololearn
Certicate Introduction to Javascript Sololearn
Certicate Introduction to Java Sololearn
Certicate React+ Redux Sololearn
Certicate Angular + NestJS Sololearn
20 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfCerticate Introduction to C Sololearn
Certicate Intermediate C Sololearn
Certicate Introduction to C++ Sololearn
Certicate Intermediate C++ Sololearn
Certicate Introduction to C# Sololearn
Certicate Intermediate C# Sololearn
Certicate SQL Sololearn
Certicate PHP Sololearn
Certicate Kotlin Sololearn
Certicate Swift4 Sololearn
Certicate Ruby Sololearn
Certicate Introduction to Python Sololearn
Certicate Intermediate Python Sololearn
Certicate Python Data Structures Sololearn
Certicate Python Core Sololearn
Certicate Intermediate Javascript Sololearn
Certicate Intermediate Java Sololearn
Certicate Game Development with JS Sololearn
Certicate jQuery Sololearn
Certicate CSS Sololearn
Certicate Responsive Web Design Sololearn
Certicate Data Science Sololearn
Certicate Python for Data Science Sololearn
Certicate R Language Sololearn
Certicate Machine Learning Sololearn
Certicate Python for Finance Sololearn
Badge Introduction Black Opal Quantum Computing
Badge Superposition Black Opal Quantum Computing
Badge Qubits Black Opal Quantum Computing
Badge Measurement Black Opal Quantum Computing
Badge Introduction to IBM z/OS 62
IBM
Certicate Academic Integrity Module Macquarie University
Certicate Cyber Defence Strategies ITM/CSU
Course Create a tiny app with ReactNative Udemy
Certicate Practical AI for Non-Coders 63
ITM/CSU
Certicate Excel Skills for Business: Essentials Macquarie University/Coursera
Certicate Excel Skills for Business: Intermediate I Macquarie University/Coursera
Certicate Excel Skills for Business: Intermediate II Macquarie University/Coursera
Certicate Fundamentals of Intellectual Property for Industry University of Technology Sydney
Certicate Introduction to Quantum Computing LinkedIn Learning
Certicate Introduction to Quantum Cryptography LinkedIn Learning
Certicate Internet of Things 64
ITM/CSU
Certicate Conducting a SWOT Analysis LinkedIn Learning
Certicate Introduction to Commercial Real Estate Analysis LinkedIn Learning
(In Progress - Graduate Certicate of Global Business Practice / Global MBA Macquarie University ) GMBA8001 Know your people
GMBA8102 Build Personal Resilience
GMBA8012 Be Global (supply chain and logistics) 58
Jet engine maths and physics, pass at 97%
59 Control Data Corporation - Supercomputer vendor in the 1980s which chief architect Seymour Cray left to found Cray
60 NCR used to run training courses on UNIX mainly around the lesser-known SVR3. eg Motorola 68k based Tower32 and TowerXP machines
61 Won university prize and had access to number 18 in the world at the time, as ranked bytop500.org 62
I have messed about with IBM mainframes (MVS 3.8J) under emulation for years but no one cares if you have a 3.5-dimensional hypercube
or play games patched to run on 3D shutter glasses with rockets coming out the screen or do stereographic programming at home. Any kind of
qualication seems to hold more weight than writing apps in the latest technologies during downtime. 6395 % score.
64 93 % score.
21 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfPublications
ÂRussell R., (2012) Programming bada, Kindle, iBooks and PDF le fromwww.pymblesoftware.com/book .
www.pymblesoftware.com/book/bada-short.pdf .
http://itunes.apple.com/au/book/programming-bada/id543013439?mt=11&ls=1 https://www.amazon.com/Programming-bada-Regan-Russell-ebook/dp/B007LFX608
Â
Russell R., (Nov 1999) ""BeRays: A ray tracer for BeOS"", Doctor Dobbs Journal.
Â Russell, R., & Ryan C (1994) METAcoder for windows: real-time and multi-pass event logging and analysis in the
social and behavioural sciences. Psychology Teaching Review.
Â Russell, R., & Ryan C. (1994) METAcoder for windows: real-time and multi-pass event logging and analysis in the
social and behavioural sciences. Psychology Software News.
Â Review of Windows NT Device Driver Development Doctor Dobbs electronic review of computer books (ERCB).
Â Review of The Windows NT Device Driver Book: A Guide for Programmers ERCB.
Â Review of Developing Windows NT Device Drivers ERCB.
Â Review of Writing a UNIX Device Driver, Second Edition. ERCB.
Â Review of Panic: Unix crash dump analysis, ERCB.
Â Review of Advance animation and rendering techniques, ERCB
Â Review of Windows TCP/IP. ERCB
Â Review of Open source development with CVS. ERCB
Â Review of System performance tuning. ERCB
Â Review of Learning the vi editor. ERCB
Â Review of Ada for experienced programmers. ERCB.
Interests Reading Harvard Business
Review, Non-ction like 'Stolen
Focus', Podcasts like 'The Knowl-
edge Pro ject', 'The Finn (Review)',
'Sengoku daimyo', and 'NTI's Japan Real
Estate', General Psychology and Economics
podcasts, Travel, Outdoors, Playing Ice Hockey,
Playing Squash, Snow Skiing, Hiking, Aviation,
Sailing, Surng, Computer Graphics and Parallel and distributed processing and Computer Architecture (especially SIMD, MIMD), hypercubes, CUDA,
MPI, PVM, FPGAs, Antique super comput-ers and Mainframes, UNIX kernel internals. Japanese, and French. Playing with Rasp- berry Pis, Arduinos, Robotics, andmanaging 6 server racks, routers
and various subnets around the house. PADI qualied diver.
22 

Latest CV:https://www.pymblesoftware.com/cv.pdf Latest CV:https://www.pymblesoftware.com/cv.pdfOther information
Previous Defence Clearance, Heavy vechile license (National), Civilian private helicopter pilots license (CAA ref #425348). Doc Dobbs
Journal BeRays
Article AWS
Academy
Badge Latest CV
23   

"
resume,"A S H I S H K A B R A
Contact Information Boulevard de Constance, INSEAD, Fontainebleau, France - 77300
E-mail : ashish.kabra@insead.edu
Phone: +33-770415105
Webpage : www.akabra.com
Education 2017 INSEAD
(E X P E C T E D )Technology and Operations Management
Committee:
Karan Girotra (Chair) (email: karan.girotra@insead.edu)
Elena Belavina (email: elena.belavina@chicagobooth.edu)
Serguei Netessine (email: serguei.netessine@insead.edu)
2005-2009 B I R L AIN S T I T U T E O F TE C H N O L O G Y A N D SC I E N C E (BITS), P I L A N I, IN D I A
Computer Science, Bachelor of Engineering
Research Interests Peer-to-Peer Marketplaces, Sustainable Transportation, Online Grocery Retail, New Product
Introductions.
Articles under Revision/Review Kabra A., E. Belavina and K. Girotra, ÂBike-Share Systems: Accessibility and AvailabilityÂ,
Revision invited at Management Science.
First Place, MSOM Best Student Paper Competition 2015
Runner Up, POMS Best Student Paper Award in Sustainability 2015
Belavina E., K. Girotra and A. Kabra, ÂOnline Grocery Retail: Revenue Models and Environ-
mental ImpactÂ
Conditionally accepted at Management Science.
Work in Progress Kabra A., E. Belavina and K. Girotra, ÂThe Efcacy of Incentives in Scaling MarketplacesÂ
Kabra A., ÂRole of Demand Asymmetry in Bike-Share SystemsÂ
Jain A., A. Kabra and N. Rudi, ÂNew Product Introductions: Improving Demand Information
and Supply ResponsivenessÂ
Industry Experience/Internships 1 

2014-2015 Data Science Consultant, Easy Taxi, Singapore
2009Â2011 Product Developer, Bravo Lucy Technologies Pvt. Ltd., India ( A supply chain analytics
startup)
2009 (Jun-Nov) Software Developer, Adobe Systems, India
2009 (Jan-June) Research Intern, Computational Research Laboratories, India. (now part of Tata Consul-
tancy Services, India)
2008 (May-Jul) Research Intern, Chennai Mathematical Institute, India (Advisor: Prof. Samir Datta)
2007 (May-Jun) Project Intern, Ballarpur Industries Ltd., India
2006 (May-Jun) Nurture Camp, Indian Statistical Institute-Kolkata, India
Honors and Awards 2004 In top 30 (11th) at Indian National Mathematics Olympiad (INMO).
2005 In top 50 at Indian National Physics Olympiad (IPhO).
2003 Awarded NCERT scholarship after being selected among 0.05% students in India in National Talent Search Examination.
2008 Second place, ACM Coding Competition, APOGEE 2008, an all India level annual technical festival conducted at BITS-Pilani, India.
2015 Part of Winning Team, Hack4Insead, a data mining competition at INSEAD.
Professional Service Reviewer for
Management Science, Manufacturing & Service Operations Management, IIE Trans-
actions.
Programming R, Stata, Mata, C++, Perl, Java, MySQL.
Teaching 2012 Co-taught PhD Mathematics for incoming class of PhD students.
2012 Assisted Microeconomics PhD course.
2015 Assisted MBA elective Identifying New Business Opportunities .
Selected Presentations 2015 Georgia Institute of Technology, Atlanta, USA
MSOM Sustainable OM Special Interest Group, Toronto, Canada
MSOM, Toronto, Canada
INFORMS, Annual Meeting, Philadelphia, USA
2 

2014 Wharton Empirical OM Workshop, Philadelphia, USA
INFORMS, Annual Meeting, San Fransisco, USA
31st Sustainability Executive Roundtable, INSEAD, France
POMS-ISB, Hyderabad, India
2013 INFORMS, Annual Meeting, Minneapolis, USA MSOM, Fontainebleau, France
3 

"
resume,"Freddie Nicholson! ! !+44-7412-985538 â 	fgn21@ic.ac.uk	#	fr3ddie.me	 | linkedin.com/in/freddienic/	 EDUCATION	 	Imperial College London	 	South Kensington, London â 	MEng	 Design Engineering	                                                      OCT 2022 - JUL 2026          	 	1st Year Grade: First Class Honours          Modules Include: Research Computing, Applied Mathematics, Robotics	 	Paid Outreach and Recruitment Student Ambassador, Residential Assistant and Undergrad. Teaching Assistant.  #Abingdon School                                                                                                                  	SEPT 2016 - JUN 2021	 	A Levels: A*AA (Computer Science, Mathematics, Physics)       	 	WORK EXPERIENCE	 	Oxford Sensors Ltd.	  	 	 	 	 	                                                 AUG 2023 - SEPT 2023	 	Embedded Software Intern, Bicester, United Kingdom	 	â¢Developed a C# weld tracking solution using computer vision processing to provide a custom solution for critical applications.	 	â¢Prototyping a feature-tracking machine learning model to score visuals, reducing false positives and leading to a more reliable detection rate.	 	Apple Inc.	 	 	 	 	 	 	                   	 	                                     NOV 2021 - SEPT 2022	 	Associate Apple Solutions Consultant (AASC), Oxford, United Kingdom	 	â¢Acted as a business partner to every location I worked with, significantly impacting the Apple business, the retailer and its employees.	 	â¢Responsible for many aspects of the business, from inventory to merchandising to training and collaborating with the Apple UK channel program organisation.	 	Abingdon School	 	 	 	 	 	 	                                                 JUN 2021 - SEPT 2022	 	Information Systems (IS) Development, Abingdon, United Kingdom	 	â¢Created a Python Django-based internal communication system, reducing org-wide emails by ~80% and delivering more relevant messages.	 	â¢Leading the technical development of a career personal development tracking system, bringing paper systems online, allowing for more flexible workflows.	 	ATOM Science and Technology Festival 	  	 	                                                   MAY 2021 - JUN 2021	 	Sci Cache Challenge App Developer, Remote, United Kingdom	 	â¢Developed Apple and Android apps for the festival, coordinating with 22 sponsors to create a virtual exploratory experience, allowing people to experience the festival safely during COVID-19.	 	PROJECTS	 	OrionHack 23 - Winner                                                                                                                               	JUL 2023	 	Utilised PyTorch to create a unique mechanical neural network web-based simulation representing a 3D-printed physical model we designed to learn an XOR gate.	 	IC HACK 23 - Runner-Up                                                                                                                           	FEB 2023    	                             	 	Collaborated to build a unique fridge-based AI web app. The entry was described as âoutstandingâ by title sponsor Cisco. Chosen as runner-up for the âCreate a Sustainable Futureâ challenge.	 	Bouncer Swift AR - on App Store                                                                                                             	APR 2023	 	Visualised a unique way to check mechanics calculations by modelling them in Augmented Reality (AR) using Swift Playgrounds. I gave a talk at NSLondon with a physical prototype I designed.	 	OpenAI GPT Reasoning Eval Generator                                                                                                 	MAR 2023	 	Created an open-source model of complex diagrammatic problems that OpenAI uses to benchmark their models.	 	AWARDS & KEY SKILLS	 	Gold Duke of Edinburgh Award                                                                                                               	AUG 2021	 	Google Code-In (Open-Source) Grand Prize Winner                                                       	                        JUL 2019	 	Chosen as a grand prize winner for an expenses-paid trip to Google HQ in San Francisco. Mentored in 2019.	 	Programming: Python, Django, JavaScript, Node.js, MATLAB, C#, Swift, SQL, PHP, Java, React, HTML5, CSS, Angular, PyTorch, TensorFlow, OpenCV, Numpy, REST, API, JavaScript	 	Skills: Adobe, Solidworks / CAD, Collaboration, Embedded Applications, Electronics, AI / Machine Learning, Computer Vision, Data Analysis, Linux / Unix, Project Management, Back-End Engineering, Leadership	 	Hobbies: Imperial Cross Country & Athletics (committee member). Chair of AR / VR society.	 	Videos and more info on my website: 	fr3ddie.me	PORTFOLIO 

"
resume," 
 	
Yuhan (Jimmy) Lin	 	
jimmylin@umd.edu	  	 	=	   	Linkedin: https://www.linkedin.com/in/jimmy0017	 	
 	
EDUCATION:	 	
The University of 	Maryland	, 	College Park	, 	MD	                                                                                        	 	A	nticipated 202	4	 	
Doctor of Philosophy	 	in	 	Education	 	
 	Department: 	Teaching and Learning, Policy and Leadership	 	
 	Specialization	:	 	Technology, Learning, and 	Leadership	 	
 	Advisor: 	Dr. David Weintrop	 	
 
University of Pennsylvania	, Philadelphia, PA	 	 	 	 	 	 	       	 	          	 	          	Dec 2019	 	
Master of Science in Education	, Learning Sciences and Technologies	 	
 	Thesis Title:	 	Motivation and Learning in Computer 	Science Programming Education	 	
 
The University of Georgia	, Athens, GA                                                                                              	 	         	May 2018	 	
Bachelor of Science, 	Mathematics	 	
Bachelor of Science, Education	, Mathematics Education	 	
 	Concentration	:	 	Teaching Advanced Mathematics	 	
 
Keble College, University of Oxford	, Oxford, UK                                                                                      Mar 	â	 	June 2016	 	
 
University of Innsbruck, 	Innsbruck, Austria                                                                                      	 	 	Jul 	â	 	Aug 2016	 	
 
PUBLICATIONS:	 	
Li, T., McCalla, L. E., Zheng, H., &	 	Lin, Y.	 	(202	3	). 	Exploring the influence of magic performance on design 	
creativity	.	 	Thinking Skills and 	Creativity	,	 	47	,	 	101223	.	 	doi:	 	10.1016/j.tsc.2022.101223	 	
Lin, Y.	, & Weintrop, D. (2021). The landscape of Block	-	based programming: Characteristics of block	-	based 	
environments and how they support the transition to text	-	based programming.	 	Journal of Computer Languages	, 	
101075.	 	doi:	 	10.1016/j.cola.2021.101075	 	
Walker, B. B., 	Lin, Y.	, & Mccline, R. M. (2018). Q Methodology and Q	-	PerspectivesÂ® Online: Innovative Research 	
Methodology and Instructional Technology. 	TechTrends	. doi:	10.1007/s11528	-	018	-	0314	-	5	 	
 
CONFERENCE & WORKSHOP PRESENTATIONS: 	 	
Lin, Y.	, Weintrop, D., Selkowitz, A. & McKenna, J. (2023). Act Happy! Act Crazy! Using Emotion	-	based Commands to 	
Engage Young Learners in Robotics Programming. Poster 	presented at Constructionism/FabLearn 2023. New York, 	
New York, USA.	 	 	
L	in, Y.	, Weintrop, D., & McKenna, J. (2023). Coder and Coder Cards: A Novel Tangible Programming Approach to 	
Support Young Programmers. Paper presented at 	2023 IEEE Symposium on Visual Languages and Human	-	Centric 	
Computing (VL/HCC)	, Washington, DC, USA, 2023, pp. 25	-	30, doi: 	10.1109/VL	-	HCC57772.2023.00011	 	 	
(Awarded Best Short Paper)	 	
Lin, Y.	, Weintrop, D., Selkowitz, A. & McKenna, J. (2023). Itâs as Easy as 123: Multiple Programming Approaches on a 	
Single Device to Support Novices. Demo presented at 	2023 IEEE Symposium on Visual Languages and Human	-	
Centric Computing (VL/HCC), Washington, DC, USA, 2023, pp. 263	-	265, doi: 	10.1109/VL	-	
HCC57772.2023.00048	 	
Lin, Y.	 	(2023)	 	Switch Mode: Exploring Authoring Python inside a Block	-	Based Programming Environment	. Pape	r 	
presented at	 	2023 IEEE Symposium on Visual Languages and Human	-	Centric Computing (VL/HCC)	, Washington, 	
DC, USA, 2023, pp. 312	-	313, doi: 	10.1109/VL	-	HCC57772.2023.00064	 	
Lin, Y.	, Weintrop, D. & McKenna, J.	 	(202	3	). 	Switch Mode: Building a middle ground between Block	-	based and Text	-	
based programming.	 	Paper	 	presented at 	2023 Symposium on Learning, Design and Technology (LDT '23)	. Chicago, 	
IL, USA.	 	doi: 	10.1145/3594781.3594803	 	
Lin, Y.	, Weintrop, D.	,	 	McKenna, J.	 	& Luo, M	 	(202	3	). 	ä½¿ç¨	VEX123	æ©å¨äººå¹«å©ä½å¹´ç´å­¸ç¿èéæ¥å¯¦é«èèæ¬ç¨å¼è¨­	
è¨	 	(	Connecting Physical and Virtual Programming for K	-	3 Students with VEX 123	)	.	 	Paper	 	presented at 	2023  

Yuhan (Jimmy) 	Lin 	P	2	 	
 
 	
ICEET	æ¸ä½å­¸ç¿èæè²ç§æåéç è¨æ	 	(International Conference on E	-	learning and Educational Technology)	. 	
National Chengchi University	, 	Taipei, Taiwan	.	 	
Lin, Y.	, Weintrop, D.	,	 	McKenna, J.	 	& Luo, M	 	(202	3	). 	Switch Mode 	å»ºç«è¦è¦ºåç¨å¼èªè¨åæå­å¼ç¨å¼èªè¨çä¸­éé	
æ¸¡å°å¸¶	 	(	Switch Mode: Building a Middle Ground between Block	-	based and Text	-	based Programming	)	.	 	P	oster	 	
presented at 	2023 ICEET	æ¸ä½å­¸ç¿èæè²ç§æåéç è¨æ	 	(International Conference on E	-	learning and 	
Educational Technology)	. 	National Chengchi	 	University	, Taipei, Taiwan	.	 	
Lin, Y.	, Weintrop, D. & McKenna, J.	 	(202	3	). 	Lowering the Floor with VEX123: Bridging Physical and Virtual 	
Programming for Young Learners.	 	P	oster	 	presented at the Annual Meeting of the American Educational Research 	
Association (AERA) 202	3	. 	Chicago	, 	IL	.	 	
Lin, Y.	, Weintrop, D. & McKenna, J.	 	(202	3	). 	Switch Mode: Scaffolding Learners From Block	-	Based to Text	-	Based 	
Programming	. P	oster	 	presented at 	the Annual Meeting of the American Educational Research Association (AERA) 	
202	3	. 	Chicago	, 	IL	.	 	
Lin, Y.	, Weintrop, D. & McKenna, J.	 	(202	3	). 	Switch Mode: A Visual Programming Approach for Transitioning from 	
Block	-	based to Text	-	based Programming.	 	I	n	 	Proceedings of the 54th ACM Technical Symposium on Computer 	
Science Education V. 2	 	(pp. 1262	-	1262).	 	doi:	 	10.1145/3545947.3573235	 	
Lin, Y.	, Weintrop, D. & McKenna, J.	 	(202	3	). 	Switch	-	Mode: Authoring Text	-	based Programming in Block	-	based 	
Programming Environment	. Paper presented at 202	3	 	FETC	 	Annual Conference. 	New Orleans	, 	LA	.	 	
Lin, Y. 	(2022).	 	Switch Mode 	-	 	Scaffolding the Block	-	to	-	Text Transition in a Introductory Programming Environment	. 	
Workshop presented at the 2022 Learning Sciences Graduate Student Conference (LSGSC), 	I	ndiana University 	-	 	
Bloomington	, 	Bloomington	, I	N	.	 	
Lin, Y.	, Weintrop, D. & McKenna, J.(2022). Mixed Mode: A New Approach to Bridging Block	-	based and Text	-	based 	
Programming. Paper presented at 2022 CSTA Annual Conference. Chicago, IL.	 	
Sirinterlikci, A., Harter, L., McKenna, J., 	Lin, Y.	, & Oravec, R.(2022). Learning Robot Programming Anywhere: 	
VEXcode VR. Paper presented at 2022 ASEE Annual Conference and Exposition. Minneapolis, MN.	 	
Lin, Y.	 	(2022). 	Exploring the Child	-	Robot Interaction with the Programming in Mind: Bridging Physical and Virtual 	
Programming for Young Children	. Paper presented at the 	21	
st	
 	ACM Interaction Design and Children (IDC) 	
Conference	. 	Braga	, 	Portugal	.	 	doi:	 	10.1145/3501712.3538834	 	
Lin, Y.	, Weintrop, D. & McKenna, J.(2022). Designing a Physical Robotic for Youth Supporting Multiple Programming 	
Approaches. 	Poster	 	presented at the University of Maryland 39th Annual HCIL Symposium. College Park, MD.	 	
Lin, Y.	, Weintrop, D. & McKenna, J.(2022). Supporting Multiple Programming Approaches in Early Elementary School 	
Computer Science Education. Poster presented at the 2022 The Conference on Research in Equity and Sustained 
Participation in Engineering, Computing, a	nd Technology (RESPECT), Philadelphia, PA.	 	
McKenna, J. , Weintrop, D. & 	Lin, Y. 	(2022). 	Intro to VEXcode VR Enhanced & Advanced +	 	Panel	 	Discussion	. 	
P	resented at the	 	VEX Educator Conference @ VEX Robotics World Championship	. 	Dallas	,	 	TX	.	 	
Lin, Y	. (2021).	 	Understanding Middle Studentsâ Transition Between Block	-	Based Programming and Text	-	Based 	
Programming in a Summer Course. 	Poster	 	presented at the 2021 Learning Sciences Graduate Student Conference 	
(LSGSC), University of Illinois at Urbana	-	Champaign, Champaign, IL.	 	
Lin, Y	. & Weintrop, D. (2021).	 	Bridging the Gap from Blocks	-	to	-	Text: Designs for Supporting Learners moving from 	
Block	-	based to Text	-	based Programming. 	Poster	 	presented at the University of Maryland 38	
th	
 	Annual HCIL 	
Symposium. Virtual.	 	
Lin, Y	. & Weintrop, D. (	2021	).	 	The Current Landscape of Block	-	based Programming Environments. Paper presented at 	
the Annual Meeting of the American Educational Research Association (AERA) 2021. Virtual.	 	
Lin, Y	. & Weintrop, D. (2021).	 	Bridging the Gap from Blocks	-	to	-	Text: Designs for Supporting Learners moving from 	
Block	-	based to Text	-	based Programming	. Paper presented at the 	University of Maryland 	C	O	E Graduate Student 	
Organization Student Research Symposium. Virtual.	 	
Fields, D. A., 	Lin, Y.	, 	Jayathirtha, G. & Kafai, Y. B. (2020). A Redesigned Reconstruction Kit for Rapid Collaborative 	
Debugging and Designing of E	-	Textiles. The Lightning Debugging Symposium: A Conversation About Research 	
and Practice in K	-	12 CS Education, Convened by Creative T	echnology Research Lab (CTRL), College of 	
Education, Univ. of Florida. Virtual.	  

Yuhan (Jimmy) 	Lin 	P	3	 	
 
 
Lin, Y	. & Fields, D. (2020).	 	Understanding High School Studentsâ Debugging Strategies through Think	-	Aloud Protocols. 	
Paper presented at the 2020 Learning Sciences Graduate Student Conference (LSGSC), University of Wisconsin, 
Madison, WI.	 	
Fields, D. A., 	Lin, Y.	, Jayathirtha	, G., & Kafai, Y. B. (2020). A Redesigned Reconstruction Kit for Rapid Collaborative 	
Debugging and Designing of E	-	Textiles. In	 	Proceedings of the FabLearn 2020	-	9th Annual Conference on Maker 	
Education	 	(pp. 98	-	101). doi:	 10.1145/386201.386207	 	
Walker, B., & 	Lin, Y	. (2019).	 	Custom Your Q: Real Time Results for Classrooms and Participatory Q Conversations. 	
Paper presented at the 35	
rd	
 	Annual Conference for International Society for the Scientific Study of Subjectivity, 	
University of Naples Federico II & Associazione Scientifica Centro di Portici, Naples, Italy.	 	
Walker, B., & 	Lin, Y.	 	(2018).	 	Q	-	Methodology primer: A mixed methods approach to research.	 	Paper presented at the 	
Association for Educational Communications & Technology Conference, Kansas City, MO	 	
Walker, B., & 	Lin, Y.	 	(2018).	 	Deepening Reflection and Discussion in the Classroom: Hearing all Student Voices with Q	-	
PerspectivesÂ®. Paper presented at the Innovation in Teaching Conference, University of Georgia, Athens, GA	 	
Walker, B., & 	Lin, Y.	 	(2018).	 	Customized Online, Flipped, and F2F Classroom Use of Q	-	PerspectivesÂ® with Real	-	Time 	
Results. Paper presented at the 34	
rd	
 	Annual Conference for International Society for the Scientific Study of 	
Subjectivity, Charlotte, NC	 	
Lin, Y.	 	(2018).	 	Understanding Studentsâ Subjective Understanding with Q	-	PerspectivesÂ®.	 	Poster session presented at the 	
2018 University of Georgia Center for Undergraduate Research Opportunities Symposium, Athens, GA	 	
Walker, B., & 	Lin, Y.	 	(2017).	 	Q	-	Methodology primer: A mixed methods approach to research.	 	Paper presented at the 	
Association for Educational Communications & Technology Conference, Jacksonville, FL	 	
Walker, B., & 	Lin, Y. 	(2017).	 	Reflection, learning, and scholarship with Q	-	Perspectives. Paper presented at the 	
Innovation in Teaching Conference, University of Georgia, Athens, GA	 	
Walker, B., 	Lin, Y.,	 	& Li, T. (2017).	 	Q	-	PerspectivesÂ®: Inviting new audiences to Q with real	-	time classroom results. 	
Paper presented at the 3	
rd	
 	Annual Conference for International Society for the Scientific Study of Subjectivity, 	
Glasgow Caledonian University, Glasgow, Scotland, UK.	 	
Walker, B., & 	Lin, Y.	 	(2017).	 	Designing for real	-	time results. Paper presented at the Instructional Design and 	
Development at the University of Georgia Conference, Athens, GA, USA.	 	
 
INVITED TALKS:	 	
Walker, B., & 	Lin, Y.	 	(2020).	 	Q	-	Methodology workshop	 	Invited Talk at the Learning, Design and Technology 	
Department, P	ur	due University, Online	 	
 
RESEARCH EXPERIENCE:	 	
The 	University of 	Maryland	, 	College Park	, 	MD	 	
Research Assistant for 	David Weintrop	 	                                                                                                   	May	 	20	20	 	â	 	Present	 	
âª	 	Develop taxonomy for Block	-	based programming environment and examine the transition for Block	-	based 	
programming to text	-	based programming	 	
âª	 	Research on VEX Virtual Robotics (	https://vr.vex.com	) platform	 	and design Switch M	ode	  	 	
âª	 	Examine the design principle of VEX 123 Robot (	https://123.vex.com	) 	 	
âª	 	Research on the 	Including Neurodiversity in Foundational and Applied 	Computational Thinking	 	(INFACT) project	 	
âª	 	Develop and maintain Impact Libraries project website and assessment buffet tool	 	
 
Robomatter Inc, 	Pittsburgh,	 	PA	 	
Educational Research Intern	 	 	                                                        	 	May 	â	 	Aug	 	202	1, May	 	â	 	Aug	 	2022	, May 	â	 	Aug 2023	 	
âª	 	Research on the most effective ways to transition students from block to text	-	based programming	 	
âª	 	Research on teachersâ perception of a successful transition from block to text	-	based programming	 	
âª	 	Design prototype of 	Switch Mode which 	will	 	support studentsâ	 	transition from blocks to text	-	based programming in 	
VEXcode programming software	 	
  

Yuhan (Jimmy) 	Lin 	P	4	 	
 
 
University of Pennsylvania, 	Philadelphia, PA	 	
Research Assistant for Yasmin Kafai	 	 	                                                                                              	Sep 2018 	â	 	May 2020	 	
âª	 	Develop and test 	prototype using Micro:bit with Python for Explore Computer Science (ECS) Electronic	-	textile 	
curriculum	 	
âª	 	Rewrite ECS E	-	textile curriculum unit from 	Arduino 	C to Python	 	
âª	 	Create prototype with paper circuit and Chibitronic and co	-	facilitate after	-	school program with 3 other graduate 	
students with 16	 	high school	 	students in Franklin Institute	 	
âª	 	Perform qualitative research on studentsâ computer programming debugging skills with reconstruction kit	 	
âª	 	Transcribe and create coding rubric on studentsâ pre	-	interview of computer programming debugging skills.	 	
âª	 	Create BioMakerLab website (	https://sites.google.com/view/biomakerlab/	)	 	
Teaching Assistant for 	Catalyst @ Penn GSE	 	 	 	 	 	 	 	 	   	Sep 2019 	â	 	May 2020	 	
âª	 	Code previous 	Experiences in Applied Computational Thinking (EXACT)	 	professional development 	program 	
interview data about	 	teacher PD	 	design feature	s	 	
âª	 	Develop Raspberry Pi Workshop	 	curriculum	 	for 	Computational Thinking in Action (CTIA)	 	
Teaching Assistant for Iryna Kozlova	 	 	                                                                                               	Sep 2019 	â	 	Nov 2019	 	
âª	 	Create GRE Mathematics task in 3D virtual world â	Virbela	â for professional development to test preparation teachers 	
from China New Oriental Group	 	
âª	 	Facilitated two workshops with total of 70 teachers from New Oriental Group	 	
 
University of Georgia, 	Athens, GA	 	
Research Assistant 	for	 	Research for the Advancement of Innovative Learning	 	 	 	    	Sep 2016 	â	 	Dec 2016	 	
âª	 	Created Robotics Curriculum targeted for 4	
th	
 	grade students with RoboRobo for them to understand basic robotics 	
movement	 	
âª	 	Facilitated two Robotics Training session for Middle School Science Education Program with about 20 pre	-	serviced 	
teachers each session	 	
 
TECHNICAL EXPERIENCE:	 	
J.W. Fanning Institute for Leadership Development, University of Georgia, 	Athens, GA	 	
Summer Computer Programmer	 	(Full	-	stack)        	                                                                             	May 2018 	â	 	Aug 2018	 	
âª	 	Migrated 	Ruby on Rails	 	application to Amazon Web Service (AWS)	 	
âª	 	Maintained sustainability of the 	Ruby on Rails	 	application on AWS	 	
âª	 	Created new functions and features for Q	-	PerspectivesÂ®	 	https://app.qperspectives.com	 	
âª	 	Created user manual for the surveys and webinars created as a student worker.	 	
Student Worker	 	â	 	Full	-	stack Computer Programmer	                                                                      	Aug 2015 	â	 	May 2018	 	
âª	 	Created a database web application by 	Ruby on Rails 	for Q	-	PerspectivesÂ® by using Q	-	Methodology factor analysis 	
written with 	R	 	for real	-	time analysis in over 50 sessions 	https://app.qperspectives.com	 	 	
âª	 	Created database web apps by 	Ruby on Rails	 	for Athens Peer Court attendance system, Conflict Style Quiz, 	
Mentoring Style Quiz and Risk Propensity Quiz with easy to use user interfaces and real	-	time reports for faculty to 	
use in their leadership training	 	
âª	 	Created an online learning website by 	Ruby on Rails	 	for Youth Leadership in Action	 	
âª	 	Created computer games by 	HTML5 & JavaScript	 	and website by 	Ruby on Rails	 	for youth leadership development	 	
âª	 	Received: University of 	Georgia Center for Undergraduate Research Opportunities (CURO) Research Assistantship	 	
 
E Fund Management Co., Ltd	.,	 	Shanghai, China	 	 	
Intern	 	 	                                                                                                                             	           	June 2017 	â	 	July 2017	 	
âª	 	Tested user interface and gave suggestions about the format and design across all different platforms	 	
âª	 	Researched ways to promote mutual fund products to end	-	users	 	
 
TEACHING EXPERIENCE:	 	
University of Maryland, 	College Park, MD	 	
âª	 	INST 	208Z	 	-	 	Designing Tools for Tinkering and Learning	 	â	 	Co	-	Instructor of Record	 	 	 	      	202	3	 	Fall	 	
âª	 	INST 408Q 	-	 	Teaching and Learning in the Information 	-	 	Teaching	 	Assistant	 	 	 	 	      	202	2	 	Spring	 	
  

Yuhan (Jimmy) 	Lin 	P	5	 	
 
 
University of Pennsylvania, 	Philadelphia, PA	 	
âª	 	EDUC	 	508	 	-	 	Maker Studio	 	-	 	Studio Assistant	 	 	 	 	 	 	 	      	2020 Spring	 	
âª	 	EDCE 592	 	-	 	Using Machines for Problem Solving	 	-	 	Teaching	 	Assistant	 	 	 	 	      	2020 Spring	 	
âª	 	EDCE 595 	-	 	Using Data Practices for Problem 	Solving	 	-	 	Teaching	 	Assistant	 	 	 	 	      	2020 Spring	 	
âª	 	EDCE 596 	-	 	Computational Thinking with Scratch	 	-	 	Teaching	 	Assistant	 	 	 	 	      	2020 Spring	 	
âª	 	EDCE 590	 	-	 	Programming using Python	 	-	 	Teaching	 	Assistant	 	 	 	 	 	 	      	2019	 	Fall	 	
âª	 	EDCE 592	 	-	 	Using Machines for 	Problem Solving	 	-	 	Teaching	 	Assistant	 	 	 	 	      	2019	 	Fall	 	
âª	 	EDCE 595	 	-	 	Using Data Practices for Problem Solving	 	-	 	Teaching	 	Assistant	 	 	 	 	      	2019	 	Fall	 	
 
Athens Technical College	, Athens, GA	 	                    	                	       	 	
âª	 	MATH	 	1101	 	-	 	Mathematics 	Modeling	 	-	 	Student 	Teaching	 	 	 	 	 	 	      	2017	 	Fall	 	
 
Jefferson High School	, 	Jefferson, GA	 	
âª	 	Advanced Algebra 	â	 	Practicum	 	 	 	 	 	 	 	 	 	      	2017	 	Spring	 	
 
Clarke Middle School, 	Athens, GA	 	
âª	 	Practicum	 	 	 	 	 	 	 	 	 	 	 	 	      	2016	 	Fall	 	
 
Clarke 	Central High School 	, 	Athens, GA	 	
âª	 	Practicum	 	 	 	 	 	 	 	 	 	 	 	 	      	201	5	 	Fall	 	
 
REVIEWER:	 	
JOURNAL	S	:	 	
Educational Technology Research and Development	 	 	 	 	 	 	 	     	2020	 	â	 	20	2	3	 	
Journal of Computer Languages	 	 	 	 	 	 	 	 	 	 	 	    	2021	 	
 
CONFERENCES:	 	
Constructionism/FabLearn	 	 	 	 	 	 	 	 	 	 	 	     	2023	 	
ACM CHI Conference on Human Factors in Computing Systems	  	 	 	 	 	 	 	     	2022	 	
International Conference of the Learning Sciences	 	 	 	 	 	 	 	      	2021	 	â	 	2022	 	
ACM Interaction Design and Children (IDC)	 	 	 	 	 	 	 	 	 	     	202	1	 	
Learning Science and Graduate Student Conference 	 	 	 	 	 	 	 	      	2020	 	â	 	2022	 	
ACM SIGCSE Technical Symposium	 	 	 	 	 	 	 	 	 	      	2022	 	â	 	2023	 	
American Educational Research Association	 	 	 	 	 	 	 	 	 	     	2022	 	
 
COMPETITIONS	:	 	
Milken	-	Penn	 	GSE	 	Education	 	Business	 	Plan	 	Competition	  	 	 	 	 	 	      	2020	 	-	 	2021	 	
 
AWARD	S:	 	
IEEE Symposium on Visual Languages and Human	-	Centric Computing	 	Best Short Paper	 	 	 	 	     	2023	 	
University of Maryland 	Graduate School Special Deanâs Fellowship 	$25,000	 	 	 	 	      	2020	 	â	 	2023	 	
University of	 	Pennsylvania Graduate School of Education	 	Merit Scholarship $10,000	 	 	 	 	     	2018	 	
University of Georgia Center for Undergraduate Research Opportunities (CURO) Research Assistantship	 	$1000   	      	2017	 	
 
SERVICES	:	 	
UNIVERSITY	:	 	
Technology, Learning & Leadership Faculty Search Committee	. 	University of Maryland College of 	Education	 	  	   	2	023	 	
Computing Education Committee	.	 	University of Maryland College of Information Studies	  	 	  	2021 	â	 	Present	 	
 
CONFERENCE	:	 	
Social	 	Committee	 	Co	-	Chair	. 	Learning Science Graduate Student Conference	 	 	 	 	 	  	   	2	023	 	
Submission	 	Committee	. 	Learning Science Graduate Student Conference	 	 	 	 	 	 	  	   	2022	 	
 
PROFESSIONAL ORGANIZATIONS:	  

Yuhan (Jimmy) 	Lin 	P	6	 	
 
 
Association for Computing Machinery	 	 	 	 	 	 	 	 	  	2023 	â	 	Present	 	
American Educational Research Association	 	 	 	 	 	 	 	 	  	2020 	â	 	Present	 	
International Society of the Learning Sciences	 	 	 	 	 	 	 	  	20	20	 	â	 	Present	 	
International Society for the Scientific Study of Subjectivity	 	 	 	 	 	 	  	2017 	â	 	2019	 	
Association for Educational Communications and Technology	 	 	 	 	 	               	2017 	â	 	2019	 	
National Council of Teachers of Mathematics	 	  	 	 	 	 	                            	2015 	â	 	2018	 	
 
CERTIFICATIONS:	 	
Mental Health First Aid USA	 	 	 	 	 	 	 	 	 	    	Jan 2019 	â	 	Jan 2022	 	
Apple Teacher Swift Playground Certificate	 	 	 	 	 	 	  	 	 	           	Feb 2019	 	
Certificate in Educational Psychology and Instructional Technology 	From 	The University of Georgia 	 	         	May 2018	 	
Apple Teacher Certificate	 	 	 	 	 	 	 	 	  	 	 	           	Oct 2016	 	
 
TRADEMARK:	 	
âª	 	Owner of âQ	-	PerspectivesÂ® Onlineâ	 	 	
 
SKILLS:	 	
Programming 	Languages	: Proficient in Full stack development, Ruby on Rails, Python, Django, HTML5, JavaScript, 	
MySQL, PostgreSQL, CSS, Git, Heroku, R. Good working knowledge of Ubuntu, Docker, React, PHP, Node.JS, 
Socket.IO, JAVA, Ionic Framework, AngularJS.	 	
â¨	 	
Computer	: Proficient in Apple Configurator, MDM (Mobile Device Management), Office 365, Google Apps, Google 	
Classroom	, Canvas LMS	.	 	
 
Languages	: Fluent in Chinese and English, both written and verbal.	  

"
resume,"ANDREAS PAHLER
27D St. John's Lane London EC1M 4BU United Kingdom
andreas@thompsonfrench.com - 0044 798 482 8826
Freelance ful l-stack software developer with a background in physics and economics
and seven years of experience in consulting, nancial services and technology startups.
ENGINEERING PROFILE Languages JavaScript/Node.js
(5 years)TypeScript (2 years)
Java SE 7/8 (3 years)
C# 4.0/5.0 (4 years)
Ruby, Python, Haskell, Idris, Clo jure (side pro jects)
Front-end React (Redux etc.),Angular(1, 2) (advanced)
HTML5, CSS3, Sass, Less + various frameworks (intermediate)
npm, webpack, Browserify, Selenium, Nightwatch, Capybara, WebSockets etc.
Back-end Node.js { Express, Koa, RxJS, XStream, Mocha, Chai, supertest etc.
Java { Dropwizard, Guava, Guice, JUnit, Mockito etc.
.NET { WCF, ASP.NET, Ninject, NUnit etc.
Databases PostgreSQL, MySQL, MongoDB, Neo4j
Other RabbitMQ, Elasticsearch, Redis
DevOps Docker, Kubernetes, OpenShift, TeamCity, Jenkins, Travis CI, Heroku, AWS
Practices Agile, TDD, BDD, multi-threading, distributed systems, reactive programming
RESTful web services, microservices/SOA, continuous integration/delivery
WORK EXPERIENCE { FREELANCE McKinsey & Company Inc.
May 2017 { Present
Ful l-stack Developer Berlin, Germany
 Digital McKinsey (digital transformation practice)
 Technologies: Node.js, TypeScript, React, Angular 2, PostgreSQL, Neo4j, Docker, Kubernetes, OpenShift.
ICSA Software International Ltd. November 2016 { April 2017
Ful l-stack Developer London, UK
 Boardroom management SaaS vendor
 Technologies: React, Redux, TypeScript, C#, ASP.NET.
RangeRoom Ltd. May 2016 { October 2016
Lead Developer London, UK
 Early-stage fashion technology startup
 Technologies: Node.js (Express), React, Mongo, Travis CI, AWS (EC2, S3).
MatchesFashion Ltd. July 2015 { December 2015
Ful l-stack Developer London, UK
 Leading luxury fashion retailer
 Technologies: Node.js (Express, Koa), Angular, React, Mongo, MySQL, AWS (EC2, S3 etc.). 

WORK EXPERIENCE { FULL-TIME
RefME Ltd.
July 2014 { June 2015
Lead Developer London, UK
 RefME is an education technology startup that builds productivity tools for university students.
 I hired and led a team of four junior developers { our team designed and implemented two of the company's
main backend systems (reference search, web page metadata scraping).
 Technologies: Node.js (Express etc.), Ruby on Rails, Java (Dropwizard), Angular, React, PostgreSQL,
MongoDB, Redis, RabbitMQ, Elasticsearch, Travis CI, Heroku.
UBS AG January 2014 { June 2014
Software Engineer (Associate Director) London, UK
 My team in Counterparty & Treasury IT built a CVA (credit value adjustment) and credit risk
system that covers the bank's OTC and exchange-traded derivatives positions.
 I worked as an analyst/developer in the team that extended the system's core risk engine to support FVA
(funding value adjustment) calculations. We also refactored large parts of our system towards a service-
oriented architecture.
 Technologies: C# (ASP.NET Web API), Node.js (Express), Angular, Oracle.
swissQuant Group AG June 2013 { December 2013
Quant Engineer Zurich, Switzerland
 swissQuant is a boutique quant nance consultancy and software provider based in Zurich.
 As a quant analyst and developer I designed and implemented the risk calculation engine of swissQuant's
portfolio management solution for wealth managers.
 I also managed the day-to-day coordination with two supplier companies.
 Technologies: Java (Dropwizard etc.), jQuery, MySQL, PostgreSQL, Jenkins.
d-ne Ltd. September 2010 { May 2013
Senior Consultant London, UK
 d-ne is a risk management consultancy with main oces in Frankfurt.
 On my rst pro ject my team built a counterparty credit risk systemfor the front oce of a ma jor
investment bank in London.
 I worked on the team that designed and implemented the system's core calculation engine, covering require-
ments of the CVA trading desk, credit and market risk control, and Basel III regulatory compliance for both
overnight and intraday risk calculations.
 On my second pro ject I worked on the migration of a commodities trading and risk management
system at an investment bank in Zurich.
 Technologies: C#, Java, Mongo, Apache Solr, memcached, TeamCity, etc.
EDUCATION London School of Economics
September 2009 { August 2010
M.Sc. in Economics and Philosophy London, UK
 Graduated with distinction.
Jacobs University Bremen September 2005 { June 2008
B.Sc. in Physics Bremen, Germany
 GPA of 1.28 (on scale from 1.0 to 5.0). 

"
resume,"Pierre Zemb
INFRASTRUCTURE ENGINEER
24 rue Yves Collet - 29200 Brest
ï(+33) 7 86 95 61 65 |
ï contact@pierrezemb.fr
| ïhttps://pierrezemb.fr
| ïPierreZ
| ïpierre-zemb-8004125b
Edu cation
Deep Learning Specialization Coursera
MOOC 2017
â¢
Neural Networks and Deep Learning, Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization,
Structuring Machine Learning Projects and Convolutional Neural Networks
ISEN (Institut SupÃ©rieur du NumÃ©rique et de lâElectronique) Brest, France
MASTER DEGREE Sept. 2010 - Sept 2016
â¢
Software Engineering Specialization: Tomcat, RPC, JEE, Spring, Spring Boot, JUnit, Android), Perl
â¢
Computer and Network Specialization: C, C++, Java, PHP, MySQL, HTML5/CSS3, JavaScript, CCNA
Exp erience
OVH Brest, France
INFRASTRUCTURE ENGINEER ON METRICS DATA PLATFORM Since Oct. 2016
â¢
Taken part of most of Metrics development, from internal management to Ingress/Egress queries
â¢
Implementation and deployement of a distributed and scalable alerting system using Apache Flink
â¢
On-call duty on more than 500 servers, including: - 3 Warp10 fully distributed clusters, including one that handling 1.5 millions datapoints per second
- 3 Hadoop clusters, including a 150 Hadoop cluster running a 40k regions Hbase table
- Various Apache technologies, such as Kafka, Zookeeper and Flink
â¢
Training and support for both external and internal client of Metrics, including WarpScript
â¢
Used Flink, HBase, Hadoop, Kafka, Ansible, Go, Rust, Java, Linux, WarpScript on a daily basis
OVH Wroclaw, Poland then Brest, France
PART-TIME INTERNSHIP: SOFTWARE ENGINEER Juin 2015 - Sept. 2016
â¢
Implementation and distribution of a new monitoring solution.
â¢
Implementing of automatic detection of Common Vulnerabilities and Exposures (CVE) of a server.
â¢
Used Flink, Hbase, Phoenix, gRPC, Kafka, OpenStack on a daily basis
CrÃ©dit Mutuel ArkÃ©a Brest, France
PART-TIME INTERNSHIP: SOFTWARE ENGINEER Juin 2013 - Sept. 2014
â¢
Creation of a Pig job automation backend on a Hadoop platform.
â¢
Used Java, Hadoop, Pig, Flume, ZooKeeper, Kafka, ElasticSearch on a daily basis
Pre sentation
Rediscover the known data Universe with NASA dataset
HANDS-ON TO SEARCH FOR EXOPLANETS WITH WARPSCRIPT 2018
â¢
Given at BreizhCamp, RivieraDev, DevFest Lille, Sunny Tech, Jug Summer Camp, DevFest Nantes, BDX.io
Metrics experience beyond protocols
STORY ON HOW AND WHY WE BUILT METRICS ABOVE OPEN-SOURCE TECHNOLOGIES 2017
â¢
Given at Paris Open Source Summit
Ass ociative
DECEMBER 16, 2018 PIERRE ZEMB Â· RÃSUMÃ POWERED BY LA
T
E X 

2019 Co-organizer , DevFest du Bout Du monde Brest, France
2018 Co-Founder , HelloExoWorld Brest, France
2018 Member of the Board of Directors , AndaolVras Brest, France
2017 - 2019 Teaching assistant , Introduction to Big-Data/Time-Series/Streaming Brest, France
2016 - 2019 Co-organizer , Devoxx4Kids Brest, France
2016 - 2019 Co-organizer , FinistDev JUG/GDG Brest, France
2015 - 2019 Co-organizer , Startup Weekend Brest, France
2013 - 2014 Vice-President , Electronic Club ISEN Brest Brest, France
DECEMBER 16, 2018 PIERRE ZEMB Â· RÃSUMÃ POWERED BY LA
T
E X 

"
resume," 	 	
 	
 
 	
 	
  ExpÃ©riences 	 	 	
02.20	23	 â Maintenant	 : IngÃ©nieur DÃ©veloppement 	logiciel	 (C#	/WPF	 .Net	) 	SERA (GPI) 	 - Chasselay	, France	 â Emploi 	â En cours	 	 	AmÃ©lioration de la suite logiciel	le Â« AlteSoft	 Â» (MES 	& applications Windows & 	applications 	PWA 	dÃ©diÃ©	s Ã  lâagro	-industrie qui Ã©quipe de 	nombreux sites industriels	 internationaux pour la meunerie et 	la gestion des cÃ©rÃ©ales/semences	/alimentation animale)	 	DÃ©veloppement Back	-end & Front	-end 	(C#8	-11/	.Net	/WPF/	XAML, 	Angular, 	SQL & SQL Server) en passant par les 	outils Microsoft (Azure 	DevOps, Visual Studio, TFVC)	 	Support logiciel 	& assistance technique via systÃ¨me de tickets clients 	(GMAO) 	ou en direct suivant lâurgence.	 	 TODO	 : complÃ©ter section Â«	 compÃ©tences	 Â» + site 	+ version en anglais	 	 	
03	.201	6 â 01.2023	 (h	ors 09	.201	7 â 03.2018)	 : DÃ©veloppeur C++/Qt pour lignes de productions 	de dispositifs 	
mÃ©dicaux	 	MÃ©diane SystÃ¨me 	chez Fresenius Kabi puis en interne	 (Fresenius Kabi	)  - Grenoble / BrÃ©zins	, France	 â Emploi 	â 6 ans et 6 mois	 	 	SpÃ©cification fonctionnelle / tec	hnique et mise en place de protocoles de tests pour rÃ©pondre aux besoins projets.	 	
Gestion et pilotage de prestataires.	 	
Echange technique transverse (interne & international)	. 	
Mise en place complÃ¨te dâoutils logiciels sur 6 nouvelles lignes de production	 (domaine mÃ©dical)	 	
AmÃ©lioration continue 	et Ã©volution de lâarchitecture	 logicielle	 dâe	nviron 15 lignes de production de produits 	mÃ©dica	ux 	
Gestion de lâintÃ©gration continue de lâÃ©quipe (Jenkins, Polyspace, CI/CD, scripts de build, â¦)	 	
Expert cyber	-sÃ©curit	Ã© des lignes de production	 	
Scrum Master pendant 9 mois (Ã©quipe variant de 4 Ã  8 personnes)	 	
DÃ©veloppement d'une	 multitude dâ	application	s, plugins et bibliothÃ¨ques en	 C++/Qt 	& scripts de test python	 & installeurs 	(C++11 	- 	Qt4.7.1/Qt4.8.7/Qt5.12.12 	â msvc20	08/2013/2017	 - InnoSetup	) 	
DÃ©veloppement	 des	 biblio	thÃ¨que	s de communication avec les produits de 	lâentreprise	, protocoles 	nÃ©cessaires pour la production	 (MQTT, 	TFTP, UDP, Protocol buffers	, RS232	, TWAIN scanner, Modbus	, â¦	) et IHM pour faciliter lâutilisatio	n des outils dÃ©veloppÃ©s pour les opÃ©rateurs 	sur les lignes de productions.	 	
09	.201	7 â 03.2018 	: DÃ©veloppeur systÃ¨mes embarquÃ©s 	(C/	C++/Qt	) 	Witekio GmbH 	 - Friedberg	, Allemagne	 â Emploi 	â 6 mois	 	 	4 mois	 : Conception dâune API de communication rÃ©seau pour mac	hines Ã  cafÃ© dâentreprise permettant dâenvoyer des donnÃ©es et de se 	faire c	onfigurer Ã  distance (C++11, Qt5.2	, Protocol buffers, Linux embarquÃ©, ARM)	 	
1 mois et 	demi :	 Conception d'une API REST permettant de contrÃ´ler un rÃ©glophare Ã  partir d'une applicati	on web et modification de 	l'application Qt existante pour mieux gÃ©rer l'archivage des rÃ©sultats (CivetWeb, QJson pour anciennes versions de Qt, C++	11, Qt4.6.3, 	PhyCARD	-i.MX 6, Linux embarquÃ©).	 	
15 	jours	 : Modification du driver VPU pour un BSP sous WAC 201	3 pour intÃ©grer la gestion du double Ã©cran	. 	
02.2015 	â 10.2015	 : DÃ©veloppeur C++/Qt pour serveur dâalarme	s MobiCall 	 	New Voice International Ltd	 - ZÃ¼rich	, Suisse	 â Emploi 	- 9 mois	 	 	DÃ©veloppement C++/Qt dâune application de Drag&Drop afin de simplifier lâut	ilisation du serveur 	dâalarme	s MobiCall (en Suisse pour une 	entreprise dynamique). Ma tÃ¢che principale Ã©tait le dÃ©veloppement dâune application d	epuis	 sa conception 	jusquâÃ  s	a livraison 	au 	client. Le 	panel de compÃ©tences que jâai amÃ©liorÃ© est relativement 	important	 : Travail dâÃ©quipe, qualitÃ© de code avec des tests unitaires, code review, 	code style, dÃ©veloppement dans de nombreux contextes (	fonctions dÃ©pendant	es du systÃ¨me dâexploitation aux interfaces utilisateurs), relation 	fournisseur	s. 	
10.2014 	â 01.201	5 : DÃ©veloppeur C Ã 	 Sagem	 DÃ©fense & SÃ©curitÃ© 	 	Altran	 - VÃ©lizy	, France 	â Emploi 	- 4 mois	 	 	DÃ©veloppement	 (en C)	 de la partie logicielle dâun projet de jumelle	s de rÃ©alitÃ© augmentÃ©	e connectÃ©e	 JIM	-LR. Ma tÃ¢che principale Ã©tait le 	dÃ©veloppement dâinterface e	ntre le logiciel et le matÃ©riel. Les dÃ©lais de livraisons rendaient le projet stimulant.	 	
02.2014 	â 07.2014	 : Stagiaire performance de micronoyaux pour lâembarquÃ© 	 	Thales	 - Gennevilliers	, France 	â Projet de Fin dâEtude 	- 6 mois	 	 	CrÃ©ation dâapplications sÃ©cur	isÃ©es sur ARM	-A9 (Trust	-Zone) en Bare	-Metal (+ Drivers) 	 	Conception de drivers Posix sur Hyperviseur PikeOS pour ARM	-A9	 	CrÃ©ation dâoutils de mesure de performance	s et de prÃ©sence de composants (caches, processeur, â¦)	 	
06.2013 	â 07.2013 : 	Stagiaire dans le	 domaine des systÃ¨mes embarquÃ©s	 	UXP	 â Seyssinet Pariset	, France	 - Stage 	- 2 mois	 	 	
IntÃ©gration dâun RTOS et dÃ©veloppement dâune 	bibliothÃ¨que	 pour un calculateur (	LPC17	xx) 	
  Formations 	 	 	
201	5 â Maintenant	 : Auto	-apprentissage (Langues, langages 	informatiq	ues,	 moocs	, â¦)	 	Duolingo, Sololearn, Openclassrooms, FunMooc, Udemy	, Pluralsight	 â¦ 	 	Duolingo	 : Apprentissage de langues Ã©trangÃ¨res (Anglais et Allemand)	 	
Sololearn	 & Auto	-apprentissage via des projets personnels 	: Apprentissage de langages 	informatiques	 (Flutter, VueJs, 	Python 3, C++, SQL, 	CCS3, PHP, JS, HTML5, â¦)	 	
Moocs	 : Apprentissage de sujets variÃ©s (	dÃ©veloppement, 	sÃ©curitÃ© informatique, management, Linux, robotique,	 â¦)	 	
2011 	â 2014 : 	IngÃ©nieur Ã©lectronique et informatique Ã  l'ENSEA	 	Ecole Nationale SupÃ©r	ieure de lâElectronique et de ses Applications 	- SpÃ©cialisation systÃ¨mes embarquÃ©s 	â Cergy	-Pontoise	, France	 	
Quentin Comte	-Gaz	 	
IngÃ©nieur	 en 	informatique 	et Ã©lectronique	 	
03/1991	 	
CÃ©libataire	 	
Permis de conduire	 	 	 	 	 	 	  	
TrÃ©voux (	Ain 	- 01600	)  	
E-mail: 	quentin@c	omte	-gaz.com	 	
Tel: 	HIDDEN	 
 	
Web: 	https://quentin.comte	-gaz.com	 
 	   

 	Liste 	non exhaustive des ma	tiÃ¨res Ã©tudiÃ©es	 : Ãlectronique	, informatique (J	ava	, C	, C bas niveau, VHDL	), bus et rÃ©seaux industriels,	 	noyau temps rÃ©e	l, microprocesseurs, Ã©lectromagnÃ©tisme, 	cryptologie	, identification et sÃ©curitÃ©	, RFID,	 micro	-Ã©lectronique	 â¦ 	
2009 	â 2011	 : Classe prÃ©paratoire aux grandes Ã©coles PTSI	-PT* 	(Grenoble)	 	 	      	
  CompÃ©tences 	 	 	
Informatique	, Ã©lectronique	, protocoles	 et normes	 	 	Langages de programmation	 : C++ (+++), C bas niveau (++), 	Flutter (++), 	C (+), Assembleur (+), Java (+), VHDL (+)	 	
Scripts et langages web	 : Python 2.7+/3+ (+++), 	Shell Linux (+++), 	SQL MySQL/PostgreSQL (+++), HTML5 (++), CSS3 (++), PHP (+	+), 	JavaScript (	++), Lua (+)	 	
Protocoles	 : UART	 RS232	-RS485	 (+++), UDP (+++), MQTT (+++), FTP(s) (+++), 	Modbus (+++), 	SS	L/TLS (++), CAN (++), I2C (++),	 	SSH (++),	 TFTP (++), TWAIN (++)	 	
BibliothÃ¨ques	 : Qt C++ (+++), Protocol Buffers (++), STL C++ (++), Boost C++ (+)	 	
Bibli	othÃ¨ques	 web 	: PHP Phalcon (+++), Vu	eJS	 (++), 	AngularJS	 (++), 	Angular	 (+)	, ReactJs (+)	 	
Petites bibliothÃ¨ques	 CivetWeb	 (++), QJSON (++)	, PythonQt (++)	 	
Tests automatiques	 : Google Test	 (+++), 	QTest	 (++) 	
SystÃ¨mes dâexploitation	s : Windows (+	++), Linux (++)	, RTOS VxWorks et FreeRTOS (+), Hyperviseur PikeOS (+), Android (+)	 	
Gestion de projet	s et de code	s : Github/GitLab/Bitbucket	 (+++)	, Jira/Confluence (++)	, Git/	SVN	 (+++),	 Jenkins 	(++),	 Gerrit	 (+++), 	Javadoc/	Doxygen (+++)	 	
MicrocontrÃ´leurs et cartes de dÃ©vel	oppement	 : Arduino et ESP8266 (+++), NXP LPC17xx ARM Cortex M3 (+++),	 	Raspberry Pi 1/2/3 (+++), i.MX6 ARM Cortex A9	 / PhyCARD	, STM32F1xx ARM Cortex M3 (++), Blackfin 537 (+), Spartan	-3E (+)	 	
Normes	 : Medical ISO 13485 (++), QualitÃ© ISO 9001 (++)	 	
         	        (+++ trÃ¨s bon, ++ bon, + connaissances de base)	 	
 	
Logiciels	 	 	
Logiciels de dÃ©veloppement : 	Qt Creator, 	Visual Studio Code	, Eclipse, Keil 	ÂµVision, Xilinx,	 Wind River,	 WampServer	, Dependency Walker	, 	Inno Setup	 	
Bureautique	 : Teams, 	Slac	k, LibreOffice, 	Microsoft Ofice, Lotus Notes, yED, Graphviz, LaTeX, Photoshop, Gimp, Inkscape	 	
Logiciels 	de communication 	: Wireshark, Putty, MobaXterm, Mosquitto	 	
Logiciels de modÃ©lisation	 : Balsamiq Mockups (design dâinterface graphique), 	OrCAD Pspice (Schematics),	 Eagl	e, Cadence	 	
Logiciels de 	calcul	s formel	s et numÃ©rique	s : Matlab (contenant Simulink), Maple	 	
 	
Projets techniques	 	 	
Applications Android	 (projet personnel)	: https://play.google.com/	store/apps/dev?id=7105242373770180771	 	
Site internet Â«	 CV	 Â» en HTML 5/CSS 3/	VueJs	 (projet personnel)	: http	s://quentin.comte	-gaz.com	/ 	
BibliothÃ¨que de gestion dâun module GSM en Python 2.7+/3+ (projet 	personnel	) : https://github.com/QuentinCG/GSM	-TC35	-Python	-Library	  	
BibliothÃ¨que de gestion dâampoules connectÃ©es en Python 3+ (projet 	personnel)	 : https://github.com/QuentinCG/Milight	-Wifi	-Bridge	-3.0	-	Python	-Library	 	
Nombreuses bibliothÃ¨ques Arduino C++ (projet personnel)	 : ContrÃ´leur sans fil, lecteur RFID, EEPROM I2C, capteur de tempÃ©rature, capteur 	de luminositÃ©,	 capteur de proximitÃ©, hygromÃ¨tre	 : https://github.com/QuentinCG?tab=repositories&q=arduino	 	
Nombreux	 plugins 	en Bash et Python 2.7/3+ 	pour lâassistant vocal 	OpenJarvis	 (projet personnel)	 : Envoi de mails	/SMS/appels	, camÃ©ra, calcul 	du trafic, contrÃ´le dâampoules et prises connectÃ©es, reconnaissance faciale, lecteur Youtube, contrÃ´le avec Facebook	 : 	https://github.com/QuentinCG?tab=repositories&q=jarvis	-  	
Tentative de crÃ©ation de lâentreprise CyberSeptoÃ¯d (crÃ©ation dâun prototype permettant la sÃ©curisation des bus CAN dans les v	oitures)	  	
Template de site internet de gestion de	 comptes mails OVH en PHP/HTML/CSS/	JS/API OVH (projet personnel)	 : 	https://github.com/QuentinCG/OVH	-Email	-Manager	-Website	 	
Logiciel de vÃ©rification de la sÃ©curitÃ© de serveurs FTP(s) en 	Python 2.7+/3+ (projet personnel) : 	https://github.com/QuentinCG/FTP	-Security	-	Scanner	   	
Application de chiffrement de donnÃ©es privÃ©es en 	JAVA (projet personnel	 fait pour lâENSEA	) : https://github.com/QuentinCG/Safe	-For	-All  	
SystÃ¨me dâalarme 	autonome 	Ã  base de STM32 et de X	Bee en C (projet 	de fin dâannÃ©e Ã  lâENSEA	) : https://quentin.comte	-	gaz.com/ensea/projet_2A_Systeme_d_alarme_sans_fil.pdf	  	
Programme de gestion de stock en SQL/PHP/JS (projet personnel)	 : https://github.com/QuentinCG/Stock	Malin	 	
Autres projets	 publics	 en cours 	: Disponibles sur mon Github 	https://github.com/QuentinCG	 	
 
  Langues 	 	 	
Anglais	 : Niveau professionnel	 (Travail Ã  lâÃ©tranger	 / S	core de 830 au TOEIC	 en 2012	) 	
Allemand	 : Niveau scolaire	 (Travail en suisse alÃ©manique pendant 9 mois	, travail en Allemagne pendant 	6 mois	) 	 	
  Centres d'intÃ©rÃªts 	 	 	
Sport	 : Escalade, 	Tennis	, Natation	 	
Informatique	 : Conception	 dâapplications mobiles	  

"
resume,"FranÃ§ais
Anglais
Aym eric  	L Ã G ER  A CH AR D	
DÃ©ve lo p peu r W eb  e t C ++	C onta ct
04/08/2001France : Limoges, 8710006.12.68.30.95aymeric.leger@etu.unilim.fr
C om pÃ©te n ce s	
C / C++ / Assembleur / Java / Swift
HTML / CSS / PHP / Javascript
Python / Ruby	
Programmation	 :	
Node.js / Express.js / Symfony
WordPress / PrestaShop
Electron.js / Qt / SwiftUI	
Technologies	 :	
La ng uesC en tr e s d 'i n tÃ© rÃª t	
BD	
Liv re s f a n ta stiq u es	
PC	
Je u x s tr a tÃ© gie  /  g e stio n	
Sport	
Te n nis  /  E sc a la d eF o rm atio ns
L1 F o rm atio n P ro .  A pplic a tio n W eb	2022/2 023 -  C VTIC  L im oges	C ertif ic a tio n :  H TM L5  e t C SS3	2021/2 022 -  O pen C la ssR oom s.c o m	C ertif ic a tio n :  L a ng ag e C	2 021/2 022 -  O pen C la ssR oom s.c o m	L 1 P A SS -  M Ã©d ecin e	2021/2 022 -  U niv e rs it Ã©  d e M Ã©d ecin e e t P harm acie  L im oges	B acca la urÃ© a t S cie n tif iq ue	2020/2 021 -  L yc Ã© e A ug uste  R en o ir	   L im oges	
S pÃ©cia lit Ã© s :  P hys iq ue-C him ie  /  S V T /  M ath s c o m plÃ© m en ta ir e s	E xp Ã©rie n ce s
BÃ©n Ã©vo la t C ro ix -R oug e	2019/2 021 -  C ro ix -R oug e L im oges
""V esti- b o utiq ue"" :  v e n te  d e v Ãª te m en ts	
D Ã©ve lo p pem en t C M S M in ew eb	2017/2 020 -  G it H ub  O pen S ourc e
C M S je u  "" M in ecra ft""	
T P :  la nce m en t b lo g  W ord Pre ss	2022/2 023 -  C VTIC  L im oges
W ord pre ss -  r a p po rt d e s ta g e
D Ã©ve lo ppem en t W eb  -  p ro gra m matio n W eb 

"
resume,"JOSÃ ALFREDO TORRES	
Mexico City, CDMX 04490 | Phone: 443117-3768 |  Email: josealfredotg@gmail.com
LinkedIn:  linkedin.com/in/josealfredotorres | GitHub: github.com/alfa9317 | Portfolio:  josealfredotorres.me
Front-end Web and iOS Developer with a background in Sound Design, from Tec de Monterrey University. Effective at combining rational and 
objective analysis with creative and inventive solutions to develop beautiful and efï¬cient user-friendly applications. Known for a strong wit 
and consciousness about the importance of teamwork and detail no matter the complexity of the project. Motivated by visible results, always 
trying to combine a good design within a great user experience.
Programming languages:  Java, SuperCollider, Swift, JavaScript, C#, CSS, HTML, SQL.
Technologies : Node.js, Express, React.js, jQuery , Materialize, Ant Design, Bootstrap, Xcode, Wwise, Protools, Adobe Premiere.
Languages:  Spanish, English.	
TECHNICAL SKILLS
MI FONDITA EXPRESS  |  
Project leader and Front-end developer of a web application built with React.js, Express, Node.js, Materialize and Ant Design. It is a platform 
developed to offer a cheap and healthy solution to food delivery services, where the customer would be able to order from a daily menu and 
receive the order between a certain range of time.
PROJECTS	
GitHub repo: github.com/alfa9317/WDBFinalProject   |
Deployed project:  mi-fondita-express.herokuapp.com
YOUTIFY  |  
Front-end developer and API manager of an application developed with HTML, CSS, JavaScript, jQuery and Materialize, whose main purpose 
is to translate your playlists in Spotify as YouTube videos. GitHub repo: github.com/alecvipa/Youtify
  |  
Deployed project:  alecvipa.github.io/Youtify
- iOS Development representative at ND, involved in planning and performing technical designs for the UI front-end of the apps.
- Implementation of web services inside the apps for communication with databases.
- Work closely with the technical leader and designer to deï¬ne a rich iOS experience for the user.
- Front-end developer and designer of an iOS application, built using Swift as the programming language and Xcode as the development 
environment. The appâs purpose was to administrate/control the activities and accesses of a residential property. 	
RELEVANT EXPERIENCE
iOS Developer
ND - Negocios Digitales April 2019 â December  2019
Mexico City, CDMX
Developer of an iOS app for a non-proï¬t organization called âEdnicaâ. The appâs purpose was to display Ednicaâs organizational information 
and to take, save and share custom pictures into social media. Pictures were customized using the different Ednicaâs frame designs.  Volunteer iOS Developer
Ednica May 2016
Mexico City, CDMX
Bootcamp Certiï¬cate in Full Stack Web Development (A 24-week intensive program focused on gaining technical programming skills in 
HTML5, CSS3, JavaScript,  jQuery, Bootstrap, Firebase, Node.js, MySQL, MongoDB, Express, Handlebars.js y React.js).
EDUCATION
Instituto TecnolÃ³gico y de Estudios Superiores de Monterrey (ITESM), Campus Ciudad de MÃ©xico 
Diploma in Sound Design for Visual Media. 
Vancouver Film School 
Bachelor of Science in Digital Music Production Engineering Instituto TecnolÃ³gico y de Estudios Superiores de Monterrey (ITESM), Campus Ciudad de MÃ©xico  
SUMMARY	
2019-2020
2017- 2018
2011 - 2016 

"
resume,"Martin ProtFreelance iOS developer+336 20 72 63 07  â¨martin@appricot.frI have been creating iOS apps for 10 years, from their design to their release, both as part of a team and independently. I can develop RESTFull APIs on smaller projects, or take part in its creation on larger projects. During my spare time, I like creating Unity games.	â¨	Please ask me some references if needed. SKILLS	 	Mobile development iOS development â 	Swift	 & Objective-C	 - iPhone / iPad / tvOS 	Game (2D/3D) development â 	Unity, C#	 	Backend development PHP (slim3), Vapor, Node.js, MongoDB, MySQL, PostgreSQL, Java/Tomcat API REST design, OAuth2 authentication	 	UX design UX design, storyboarding, prototyping. Unix Unix server conï¬guration (Nginx, Apache, Node.js, MySQL database, 	â¨	MongoDB, ...) Others Sketch, Adobe Photoshop Languages French: native. English: okay MISSIONS Please ï¬nd below a non-exhaustive list of my most relevant missions. Some are one-shot, others are recurrent. Chanel iOS development of internal Chanel apps Lead developer on several apps. Development, architecture, continuous integration, etc. -From	 Sept 2019 to April 2020  

 	Page 	2-Tech environment : 	iOS (swift), TFS / Azure 	Axelife iOS prototype app that calculates biomarkers	 	I created an app that converts videos of the user's ï¬nger into a heart rate signal. BLE Bracelet communication to acquire another pulsative signal. PHP (Slim3) and Java (Tomcat) backend to receive app signals and compute it with an in-house java engine. -From :	 end of 2018, updates in 2019 / 2020 	-Tech environment : 	iOS (swift), PHP (slim 3), Oauth2, Java / Tomcat, Ngnix 	CEA - Exoplanets, JWST, GÃ©nie Atomique  iOS / Android augmented reality app on Unity 	 	I developed an app that displays on-screen 3D models over printed markers. The user can move and explore models, select points of interest and read some media (text, image, video). A quiz challenges the user on what he has just learnt. -From 	2018, updates in 2019 	-Tech environment : 	Unity, C# 	Artekino iOS Streaming video app for Arte	 	I created the Artekino app, which shows every month a new movie from the selection of the Artekino festival. The user can watch the movie and vote when ï¬nished. Video content is pro-tected with Fairplay DRM.  -From :	 end of 2017, updates in 2018, 2019, 2020 	-Tech environment : 	Swift, DRM Fairplay 	Juzzy iOS Sailing app	 	Juzzy gathers information about tides and currents, and presents it in a geolocated way to the sailor. With a subscription In-App Purchase, the user can show that information in the future or in the past. -From	 end of 2017, updates in 2018, 2019 	-Tech environment : 	Swift 	Festival Scope iOS streaming video player for cinema professionals Festival Scope Pro is an iOS app for cinema professionals, allowing programs of selected fes-tivals around the world to be viewed on demand.  

 	Page 	3Movies are DRM-protected and can be downloaded to be watched later. The app allows the users to watch content on a real TV via  AppleTV and Chromecast , -from 	2016 to 2020, lots of updates 	-Tech environment : 	Objective-C, Swift, AppleTV, Chromecast, DRM Fairplay 	Scaph iOS quiz game that challenges the user on hundreds of questions. The goal of this game is to answer 100 questions of increasing difï¬culty without any mistake. I developed the whole iOS app, a Node.js backend and an Angular frontend, to manage the questions base. -From	 2015 October to 2015 December. Lots of update 2016 et 2017. 	-Tech environment : 	Objective-C, Node.js, MongoDB, Angular, websockets 	Pages Jaunes iOS yellow pages app Developer within the iOS app team. I helped developing new features, like ofï¬ine access to app data saved in a Coredata stack. -From :	 February to September 2014 	-Tech environment : 	Objective-C, CI, unit tests 	Microsoft Xbox Music iOS app, today abandoned Developer within the iOS team of the Xbox music app. I helped developing the 2.0 version of the app, which allowed users to stream music and download it on their device.	â¨	I worked within a 10-people agile team. Rich technical and organizational experiences.  I wrote a full documentation about the project, with UML modeling. -From	 mid 2013 to end of 2013 	-Tech environment : 	Objective-C 	Tikiâlabs, Paris (employee) iOS developer 	 	-From 	 2009 to 2011 	-Tech environment: 	Objective-C, Quartz 2D, webkit, MapKit 	Orange (employee) Developing and maintaining the administration UI of Orange group. -From	 2008 to 2009 	-Tech environment :	 C, C++  

 	Page 	4SNCF (	employee	) 	Developing and maintaining a software that helps SNCFs operator  handle rail trafï¬c. -From	 2007 to 2008 	-Tech environment :	 LISP, C, Ingres. 	EDUCATION Cognitics Engineering school, Bordeaux	 	computer sciences, UX, human factor, artiï¬cial intelligence. - From 	September 2004 to September 2007 	IUT Computer sciences (Bordeaux)	 	- From	 September 2002 to June 2004 

"
resume,"
Z han arb ek  O sm on alie v	
S to ny B ro ok, N Y |	
B eh an ce P ortf o lio	
|   ( 6 31) 9 85-8 237	
|
z h an arb ek .o sm onalie v @ sto nybro ok.e d u	
E D UCAT IO N
S to n y B ro ok  U niv ersit y	
S to n y B ro ok , N Y	
C om pute r S cie n ce/B ach elo r o f S cie n ce,	
C um ula tiv e G PA :	
3 .5 8/4 .0 0	
5 /2 022	
A re as o f I n te re st: F ro nt- E nd D ev elo pm en t, U I D esig n
A w ard s:	
G lo bal S ta rtu p C am pus B usin ess I d ea C am p ( 1	
s t
p la ce o ut o f 1 6 t e am s), D ean âs  L is t, R ecip ie n t o f	
M erit	
5 0 S ch ola rs h ip
W ORK  E X PE R IE N CE
A rv iz o n	
I n ch eo n , K ore a	
U I/U X D esig n I n te rn	
7 /2 019 -  1 1 /2 019	
â	
R e-d esig ned	
e x is tin g	
m obile	
a p plic atio nâs	
u se r	
i n te rfa ce	
a n d	
c re ate d	
A dobe	
X D	
p ro to ty pes,	
w hic h	
w ere	
l a te r	
a p pro ved  f o r p ro ductio n	
â	
C re ate d	
n ew	
p ro duct	
b ra n din g	
f o r	
t h e	
c o m pan yâs	
c ro w dfu ndin g	
p la tf o rm	
p ag e,	
i n cre asin g	
t h e	
a m ount	
o f	
f u nds	
d onate d  f o r u p t o  $ 100,0 00	
â	
D ir e cte d  a  p ro duct p hoto sh oot f o r d ig ita l a sse t c re atio n	
u se d  f o r t h e c o m pan yâs  a d vertis in g c am paig n	
â	
C onducte d	
r e se arc h	
o n	
o pportu nitie s	
f o r	
g ro w th	
a n d	
d esig ned	
a	
u se r	
i n te rfa ce	
f o r	
f le et	
m an ag em en t	
s y ste m	
a p plic atio n	
B usin ess O m budsm an  I n stit u te	
B is h kek , K yrg yzsta n	
W eb  D esig ner	
1 2/2 019 -  2 /2 020	
â	
C olla b ora te d	
w ith	
a	
t e am	
o f	
d ev elo pers	
i n	
p la n nin g	
a n d	
d ev elo pin g	
a	
w eb	
p la tf o rm	
w hic h	
i s	
u se d	
b y	
b usin esse s t o  f ile  c o m pla in ts  a g ain st a ctio ns o f s ta te	
b odie s	
â	
D ev elo ped	
a	
u se r	
i n te rfa ce	
d esig n	
f o r	
a	
r e sp onsiv e	
w eb site ,	
a lo ng	
w ith	
i ts	
c o lo r	
s c h em e	
a n d	
d esig n	
s y ste m	
t o	
d eliv er m odern , y et s tr ic t- d esig n p la tf o rm	
P R O JE C T E X PE R IE N CE
T in yD esk	
I n ch eo n , K ore a	
L ea d D evelo per &  D esig ner	
3 /2 021 -  6 /2 021	
â	
D ev elo ped  a n  a u to nom ous p ip elin e w hic h  h an dle s a u to m atic	
a p plic atio n b uild s, t e sts  a n d d ep lo ym en t	
â	
D ir e cte d  a  t e am  o f d ev elo pers  a n d m ain ta in ed  t h e c o deb ase	
q uality	
â	
D esig ned  a  u se r i n te rfa ce a n d o ptim iz ed  i t w ith  r e su lts	
d ra w n f ro m  u se r t e stin g a n d f e ed back .	
â	
C re ate d  a n d p re se n te d  t h e p ro je ctâs  p itc h  d eck  t o	
a n  a u die n ce o f 1 00+  p eo ple	
F  S T U DIO	
R em ote	
F ro nt- e n d D evelo per &  U I D esig ner	
2 /2 021 -  5 /2 021	
â	
B uilt a n  e -c o m merc e p la tf o rm  b ase d  o n S ho pif y  A PI	
f o r e m erg in g d esig ners  i n  N ew  Y ork	
â	
I m ple m en te d  a  p ag e f o r c u sto m ers  t o  p re v ie w  d ig ita l	
f a sh io n i te m s i n  3 D	
â	
D ev elo ped  a  d esig n s y ste m  t h at r e p re se n ts  t h e m is sio n	
o f t h e p la tf o rm  i n  a esth etic ally  p le asin g u se r i n te rfa ce	
P an dem ic  S im ula tio n	
I n ch eo n , K ore a	
R ese a rc h  D evelo per	
9 /2 020 -  1 2/2 021	
â	
C onducte d  r e se arc h  o n d if f e re n t a p pro ach es o f r e p re se n tin g	
p an dem ic  s im ula tio n a n d i ts  t e n den cie s	
â	
I m ple m en te d  g ra p h-b ase d  p ath fin din g a lg orith m  f o r	
b lo bs l iv in g i n  a  3 D  c ity  t h at t r a n sm it t h e v ir u s	
â	
I m pro ved  t h e p ro je ct i n  r e la tio n t o  H CI p rin cip le s	
u nder a  s u perv is io n o f a n  e x -A pple  e m plo yee	
S K IL LS &  I N TER EST S
A pplic a tio n s:	
A dobe I llu str a to r, P hoto sh op, P re m ie re	
P ro , L ig htr o om , F ig m a, A dobe X D, D aV in ci R eso lv e	
S kills :	
R eactJ S , G ats b yJS , W eb pack , T hre eJS , G it,	
N odeJS , H TM L5, C SS3, C om pute r V is io n	
I n te re sts :	
J iu -J its u , S w if t D ev elo pm en t 

"
resume,"AND	 	REY	 	YOSHUA MANIKBogor, Indonesiaandrey.yoshua@gmail.com+62 821 1340 2239A  d i l i g e n t   a n d   e n t h u s i a s t i c   w e b   a n d   m o b i l e  developer  with  +7  years  experience  in  creating unique  and  modern  user-focused  software application  at  the  highest  level  of  quality  and efficiencyME1994BINUS UNIVERSITY	 	School of Information System	 	GPA: 3.66EDUCATION2015-currentandreyyoshua.comSKILLS100IOS	 	DEVELOPMENTWEB	 	DEVELOPMENT80100ANDROID	 	DEVELOPMENT100PROJECT	 	MANAGEMENTPHPJAVA.NETNODE JSSWIFTOBJECTIVE CKOTLINPYTHONCSSDartHTMLJAVASCRIPTvBATAVIANET	 	Product Development Lead	 	WORKvINTEREST2016-2019PROJECTS2014-currentv2019TO K O P E D I A	 	Senior Software Engineer	 	v2020-currentTO K O P E D I A	 	Lead Software Engineer	   

"
resume,"KEVIN SCAMAN	 	
 
 
Website: 	scaman.wordpress.com	 	
Linked	-in: kevin	-scaman	-22691949	 	
 
 
PROFESSIONAL EXPERIENCE	 	
 
2018 	â present 	 	Huawei Noahâs Ark lab	, Paris, France	 	
 	Research scientist in Machine Learning.	 	
 
2016 	â 2017 	 	Microsoft Research 	- Inria Joint Center	, Palaiseau, France	 	
 	Postdoctoral  researcher  in	 the  fields  of  machine  learning  and  network 	
analysis.  Work  superbised  by  Laurent  MassouliÃ©  and  in  collaboration  with 
Francis Bach (INRIA) and SÃ©bastien Bubeck (Microsoft Research).	 	
 
2013 	â 2016	 	Ecole Centrale Paris / ENS 	Paris	-Saclay	, Paris, France	 	
 	Teachi	ng  assistant  in  probability  theory  (	âApproximation  methods  in 	
probability  theoryâ, 	ENS  Cachan),  statistics  (Ecole  Centrale  Paris)  and 	
machine learning (	âIntroduction to machine learningâ,	 Master 2 MVA	). 	
 
Mar. 	â June 2013	 	Microsoft Engineering Center	, Paris	, France	 	
 	Web developer for the Xbox Music (now Groove) website.	 	
 
July 	â Dec. 2012	 	Microsoft Engineering Center	, Paris, France	 	
Intern	 working 	on	 intelligent 	customer support 	system for Xbox	. 	
 
Apr. 	â Aug. 2011	 	MIT, Center for Biological and Computational Le	arning	, Boston, USA	 	
Intern working	 on	 classification methods for large scale object recognition	. 	
 
 
EDUCATION	 	
 
2013 	â 2016	 	ENS	 Paris	-Saclay	 (Paris	-Saclay university)	, Cachan	, France	 	
 	 	PhD in machine learning applied to social networks and diffusion processe	s,	
 	 	entitled 	 âAnalysis	  and 	 control	  of 	 diffusion 	 processes 	 in 	 networksâ	  and	 	
supervised by Ni	colas Vayatis at the âCentre de MathÃ©matiques et de Leurs 	
Applicationsâ (CMLA)	. 	
 
2011 	â 2012	 	TÃ©lÃ©com ParisTech	 / Ãcole Polytechnique	, Paris	, France	 	
Double d	egree program  in 	Engineering and 	Applied 	M	athematics	 (Masterâs	 	
program 	âMathe	matics,  Vision  and  Learning	â (MVA)	). Machine  learning 	
classes 	applied  to  various  fields  including 	vision,  biology  and  text 	
classification.	 Masterâs degree awarded with High Honors	. 	
 	
2008 	â 2011	 	Ãcole Polytechnique	, Palaiseau	, France	 	
Engineering degree with a 	major	 in Applied M	athematics in	 one of France's 	
most 	prominent universities for science.	 	
 
 
  

SCIENTIFIC PUBLICATIONS	 	
 	
ï· 	K.  Scaman, 	F.  Bach  ,  S.  Bubeck,  Y.  Lee  and  L.  MassouliÃ©  Opt	imal  algorithms  for  non	-	
smooth distributed optimization in networks. In 	NeurIPS	 (best paper award)	, 2018.	 	
 	
ï· 	M.  Draief,  K.  Kutzkov	,  K.  Scaman	 and  M.  Vojnovic	. 	KONG:  Kernels  for  ordered	-	
neighborhood graphs. In 	NeurIPS	, 2018.	 	
 	
ï· 	K. Scaman, 	A. Virmaux	. Lipschitz 	regularity of deep neural networks: analysis and efficient 	
estimation. In 	NeurIPS	, 2018.	 	
 	
ï· 	R.  Lemonnier,  K.  Scaman,  and  N.  Vayatis.  Spectral  bounds  in  random  graphs  applied  to 
spreading phenomena and percolation.	 Advances in Applied Probability	, 2018	. 	
 	
ï· 	K. S	caman, F.Bach, S. Bubeck, Y. Lee, and L. MassouliÃ©. Optimal algorithms for smooth	 	
and strongly convex distributed optimization in networks. In 	ICML	, 2017.	 	
 	
ï· 	R.  Lemonnier,  K.  Scaman,  and  A.  Kalogeratos.  Multivariate  Hawkes  Processes  for  Large	-	
scale Inference	. In 	AAAI	, 2017.	 	
 	
ï· 	K.  Scaman,  A.  Kalogeratos,  and  N.  Vayatis. 	Suppressing  Epidemics  in  Networks  using 	
Priority Planning	. In IEEE Transactions on Network Science and Engineering	, 2016	.  	
 	
ï· 	K.  Scaman,  R.  Lemonnier,  and  N.  Vayatis.  Anytime  influence  bounds  and  t	he  explosive 	
behavior of continuous	-time diffusion networks. In 	NIPS	, 2015.	 	
 	
ï· 	K.  Scaman,  A.  Kalogeratos,  and  N.  Vayatis.  A  greedy  approach  for  dynamic  control  of 
diffusion processes in networks. In 	ICTAI	, 2015.	 	
 	
ï· 	R. 	Lemonnier	, K. Scaman, and N. Vayatis. Tigh	t bounds for influence in diffusion networks 	
and application to bond percolation and epidemiology. In 	NIPS	, 2014	 	
 	
ï· 	A.  Kalogeratos,  K.  Scaman,  and  N.  Vayatis.  Learning  to  Suppress  SIS  Epidemics  in 
Networks. In	 Networks in the Social and Information Sciences 	(NIPS workshop)	, 2015.	 	
 	
ï· 	K.  Scaman,  A.  Kalogeratos,  and  N.  Vayatis.  Dynamic  treatment  allocation  for  epidemic 
control  in  arbitrary  networks.  In 	Diffusion  Networks  and  Cascade  Analytics  (WSDM 	
workshop)	, 2014.	  	
 
SCIENTIFIC AWARDS	 	
 	
ï· 	NeurIPS 2018 best paper	 awar	d (4 	best 	papers / 4865 submissions)	 	
ï· 	Huawei future star	 (awarded	 by 	other 	team members	 to promote 	local talents	) 	
ï· 	Huawei quality star	 (awarded 	to promote research transferred into products)	 	
 
LANGUAGES AND COMPUTER SKILLS	 	
 
Langu	ag	es 	
French     	 	Mother tongue	. 	
English	    	 	Fluent	, both oral and written	. 	
Japanese	   	Good notions. Level N4 of the	 Japanese Language Proficiency Test	. 	
 
Computer Skills	 	
Lang	uages      	 	Python, Matlab, 	C#, Java,	 C++	, SQL	, Php	. 	
Software	 	Desktop applications (spreadsheet, text editing) in	cluding Latex technical writing.	  

"
resume,"David Andrianavalontsalama
Co-Founder at QuÃ¦facta 
Paris, France 
Contact
david.andriana@quaefacta.com
+33 7 83 58 99 00
https://www.linkedin.com/in/david-andrianavalontsalama-437b064 (LinkedIn)
https://quaefacta.com/ (Company)
https://www.avantage-compris.com/ (Company)
Top Skills
Software Project Management
Agile Methodologies (e.g. Scrum)
Business Analysis
Languages
French (Native or Bilingual)
English (Full Professional)
Open Source Projects
avc-binding-dom: XML / YAML / JSON / Java binding, 2011 - Present
https://code.google.com/archive/p/avc-binding-dom/
Summary 
Team leader.
Problem solver.
I have a background in technical architecture and an experience in software development. 
Iâm good at lateral thinking and at telling greek mythology.
Experience 
QuÃ¦facta  
CPO & Co-Founder
January  2019 - Present
Paris, France
QuÃ¦facta provides a digital solution bringing trusted traceability in healthcare. 

We rely on organisational partnerships and future-proof technology, integrate fragmented 
information sources and provide companies with real-time data, analytics and AI to streamline 
operational activities and assist in improving workflow, patient lives and treatment outcomes.
Avantage Compris
Owner
March  2006 - Present 
Limoges, France 
Avantage Compris offers software development and software project management skills.
R&D: Traceability plaftorm
Crystalchain
CPO
October 2017 - April 2018  
Paris, France 
Chief Product Officer of a blockchain-based traceability platform.
Team of 5
Collecting & implementing customer requirements
Blockchain & Smart contracts development
Technical architecture & frameworks
MinisteÌre de l'EÌducation nationale 
Project Manager
2016 - 2017
Paris Area, France 
Project manager, team of 5
Web app that manages apprenticeship courses
JPLC
Senior Software Engineer
2014 - 2016
Limoges, France 
Development of a web app that does statistical analysis (L2-boosting for inferences on missing 
data) + data encryption 

Company training on programmation language: Java, Ruby, C
UniversiteÌ NumeÌrique Paris Ile-de-France
Senior Software Engineer
2014 - 2014
Paris, France 
iOS and Android app: âUnivmobileâ
Patients Know Best
DevOps Engineer
2013 - 2014
Limoges, France + Cambridge, UK
Automated setup for new developers: Vagrant images with the full stack (Tolven from OpenClinical,
etc.)
Capgemini
Business Analyst / Software Developer
2010 
Paris Area, France 
SNCF: Business Analyst on marketing/sales of the âMulti-Canal, Multi-Lotsâ app
Bureau Veritas: Software Developer on an app for auditors
Avantage Compris
Project director / Software Developer / Consultant
2006 - 2010
Limoges Area, France 
CNAMTS, Paris: Web app for follow-up on the A/H1N1 vaccination campagin
CNASEA, Limoges: Project director on âOsirisâ; Team of 50 people, 8 projects, MinistÃ¨re de 
lâAgriculture
Batilog, Toulouse: Startup co-founder. Batilog aided individuals track the work done on the building 
of their personal houses.
MSA, Montauban: Consultant (project auditing) + Technical Architect
Capgemini
Technical Architect / Consultant 

2006 - 2010
Bordeaux, France 
Standard&Poors, New York: Project auditing; Team of 100 people, USA/India
Education 
UniversiteÌ Paris VI, France
1996: DEA de Logique
EÌ	
cole Polytechnique, France
1995: (X92), France 

"
resume,"I	ngÃ©nieur	 	des 	M	ines	, 	7	 	ans dâexpÃ©rience,	 	Full	S	tack Developer	 	Senior	 	 	
EXP	Ã	RIENCE	S	 	PROFESSIONNELLE	S	__________________________________	_____	_____	_	______	 	
Oct.	 	201	9	 	â	 	
Maintenant	 	
Total	Energies	 	Lubrifiants 	en Freelance 	â	 	Full	Stack Developer	 	Senior	 	
  	Projet	s	 	LubPilot	, ConnectedOil et MyLubPortal	 	au sein d	e lâÃ©quipe	 	StudioLub, 	Nanterre	 	
RÃ©alisation 	de 	2	 	application	s	 	React	 	de monitoring d	e l	ubri	fia	nts	 	et d	âun site e	-	commerce B2B.	 	
 	
 	  	 	2	 	an	s	 	
 	
Sept	. 201	7	 	â	 	
Sept. 201	9	 	
Swap	 	â	 	Directeur Technique	 	
  	Start	-	up 	dÃ©veloppant 	des 	objets	 	connectÃ©s	 	pour	 	les	 	malades dâAlzheimer	 	â	 	Station F, Paris	 	
Gestion complÃ¨te de la partie technique du projet. 	RÃ©alisation	 	dâapplications 	natives 	Android	,	 	
iPhone et Web	 	(React)	 	utilisÃ©es aujourdâhui par des milliers dâutilisateurs. DÃ©veloppement 	
dâune	 	API 	N	ode.js/	M	ongoDB	 	communiquant	 	chaque 	avec 	des 	objets connectÃ©s, analyse et 	
traitement des donnÃ©es reÃ§ues	.	 	Gestion humaine, financiÃ¨re et du planning des prestataires.	 	
 	
   	 	2	 	ans	 	
 	
Avril	. 201	7	 	â	 	
AoÃ»t	.	 	201	7	 	
Thales Air Systems	 	via Alten	 	-	 	IngÃ©nieur 	dâÃ©tudes	  	 	
  	Projet de 	radar 	tridimensionnel	 	SF	-	500 pour frÃ©gate militaire	, 	Limours	 	
DÃ©veloppement du logiciel embarquÃ© du radar	 	en Ada 	et dâune 	interface graphique en Java	.	 	
 	
   	5	 	mois	 	
Oct	. 2015 	â	 	
Avril	.	 	2017	 	
Alstom Transport via Alten	 	-	 	IngÃ©nieur 	dâÃ©tudes	 	
  	Projet de signalisation Urbalis Fluence	, 	Saint	-	Ouen	 	
RÃ©alisation dâun outil dâautomatisation de tests	 	en java. Validation et vÃ©rification du logiciel 	
de signalisation 	Ã©crit en Ada. DÃ©veloppement logiciel d	u	 	modul	e	 	de localisation du train.	 	
 	
  	18	 	mois	 	
Mars 	â	 	AoÃ»t 	
2015	 	
Airbus Defence & Space 	-	 	Stage de fin dâÃ©tudes	 	
  	Service functional engineering and operation supports for space systems, Les Mureaux	 	
Ãtude et dÃ©veloppement dâun outil permettant la vÃ©rification de la cohÃ©rence entre les 
informations conte	nues dans les diagrammes UML reprÃ©sentant le programme de vol Ariane 	
6 et le code Ada 2012 associÃ©	.	 	
 	 	
  	 	6 	mois	 	
 
 	Juillet	 	201	4	 	ARC piglet 	nutrition 	-	 	Assistant informatique	 	
   	DÃ©veloppement	 	de requÃªtes SQL sous SAP Business One, Coresuite et Access	 	
 	
    	 	1 mois	 	
 	
Mars 	-	 	Juin	 	
2014	 	
Airbus 	Helicopters 	-	 	Stage Technique                     	 	
  	Service Technology and Ground Products, Marignane	 	
Ã	tude ergonomique et 	dÃ©veloppement	 	dâune IHM	 	sur 	lecteur portable et tablette pour 	
lâutilisation de la t	echnologie RFID sur hÃ©licoptÃ¨re	.	 	
 	 	         	 	
   	  	  	 	   	 	 	4 mois	 	
 
 	FÃ©vrier	 	201	3	 	STMicroelectronics 	-	 	Stage dâaccompagnement ingÃ©nieur 	 	
   	CrÃ©ation de procÃ©dures de travail et de 	spÃ©cifications pour les opÃ©rateurs	.	 	
   	 	 	1 mois	 	
 	
FORMATION	S	_________________________________________________________	_	_____	_____	 	
 
 	2015	 	
 
 
 	AoÃ»t 	-	 	DÃ©c. 	 	
 	2014	 	
DiplÃ´me 	dâingÃ©nieur 	de lâ	Ã	cole des Mines de Saint	-	Ã	tienne	 	
  	Cursus 	post	-	prÃ©pa,	 	SpÃ©cialis	ation	 	en Micro	-	Ã©lectronique, Informatique et Nouvelles technologies	.	 	
 
Ã	cole Polytechnique de MontrÃ©al	 	-	 	CANADA	 	
  	 	Cursus 	gestion Ã©conomique et financiÃ¨re de projets technologiques	 	
 	
COMP	Ã	TENCES	_____________________________________________	__	________	__	_____	_____	 	
 	Langues 	 	Anglais	 	:	 	CompÃ©tence professionnelle, TOEIC	 	: 805      	Espagnol	 	:	 	CompÃ©tence professionnelle 	limitÃ©	e	         	 	
 	Langages	 	TypeScript, 	Node.js	, 	React	/Next	, 	GraphQL, 	PostgreSQL	, MongoDB, 	Jest, 	Vu	e, 	Android	, 	Java	, 	Swift/iOS	, 	P	HP	 	
 	
Projets	 	MÃ©thode Agile, Scrum	, 	RÃ©seau	 	PERT, diagramme de Gantt	, mÃ©thode de Cycle en V	 	
CENTRES D'INTER	Ã	T	S	__________________________________________________	____	_____	___	 	
 	Associatif	 	Commission Foyer	 	:	 	TrÃ©sorier, organisation dâÃ©vÃ¨nements, 	CA	 	: 22kâ¬	                           	             	1 an	 	
Gala	 	de lâÃ©cole 	:	 	Responsable ingÃ©nieur 	son	 	et lumiÃ¨re	 	 	 	       	  	           	           	 	            	6	 	mois	  	                    	
BDE	 	:	 	TrÃ©sorier dâune liste BDE, 	organisation	 	dâÃ©vÃ¨nements, 	CA	 	:	 	8	kâ¬	  	 	    	  	           	 	        	            	6 mois	 	
      	IngÃ©nieur Solidaire en Action	 	: 	organisation dâune 	confÃ©rence pour la fÃªte de la science	 	
     	Sport                	Course Ã  pied/trail	 	(plusieurs podiums juniors)	 	
JÃ©rÃ´me 	LE CHAMPION 	 	
82	 	Boulevard Saint Marcel	, 	750	05	 	Paris	 	
+33	 	6	.	20	.	34	.	60	.	64	 	
jerome.le.champion@gmail.com	 	
jeromelechampion.fr	 	
 	
  

"
resume,"
S a nd eep	
K uria n	
S en io r	
F u ll	
S ta ck	
D eve lo p er,	
1 4	
y e a rs	
e xp .	

F re e la n ce	
F u ll	
S ta ck	
d eve lo p er	
w it h	
1 4	
y e a rs	
o f	
e xp erie n ce .	

B ein g	
a	
f r e e la n ce r,	
I
w ork e d	
o n	
d i ere n t	
k in d s	
o f	
W eb	
a n d	
A p p	
d eve lo p m en t	
p ro je c ts	
f o r	
o ve r	
5 0+	
c li e n ts	
b ase d	
o u t	
o f	
t h e	
U S,	
U K,	
A ustr a li a ,	
G erm an y,	
C an ad a	
e tc .	
E ager	
t o	
u se	
t h e	
s k ills	
I	
h ave	
d eve lo p ed	
o ve r	
t h e	
y e a rs	
a n d	
c o n tr ib ute	
t o	
t e a m	
s u cce ss	
t h ro u gh	
h ard	
w ork ,	
a tte n tio n	
t o	
d eta il	
a n d	
e xc e lle n t	
o rg an iz a tio n al	
s k ills .	
M otiv a te d	
t o	
l e a rn ,	
g ro w	
a n d	
e xc e l	
i n	
t h e	
s o ft w are	
i n d ustr y .	
P H O NE	
:
+ 1	
9 17 -7 2 2-5 62 6	
E M AIL :	
k urie n sa n deep @ gm ail.c o m	
G IT H UB	
:
h ttp s:/ /g it h ub .c o m /s a n d ee p ku rie n	

S k ills	
: N od eJS ,	
P H P,	
D yn am oD B,	
M YS Q L,	
A W S	
L a m bda,	
P yth on,	
L a ra ve l,	
R ea ct	
N ativ e ,	
R ea ctJ S ,	
E xp re ss	
J S ,	
A W S	
L a m bda,	
W ord Pre ss,	
P yth on	

P ro fe ssio n al	
E xp erie n ce	
J a n	
2 02 0	
-	P re se n t	
L e a d	
A rc h it e ct	
a n d	
F u ll- s ta ck	
D eve lo p er	
C oach Fir s t	
|
U S	
P ro d uct	
B ase d	
c o m pan y	
|
c o ach ï¬rs t.c o m	
|
I n sta g ra m	
F o u n d ers :	
T re vo r	
W hit e	
a n d	
N FL	
p la ye r	
P at	
O âD on nell	
T e ch	
s ta ck	
:
R ea ct	
N ativ e ,	
I O S	
S w if t ,	
A W S	
L a m bda,	
N od eJS ,	
D yn am od b,	
P H P/L a ra ve l,	
R ea ctJ S ,	
A W S	
C og nit o	
H ave	
b ee n	
w ork in g	
o n	
C oach Fir s t	
s in ce	
i t s	
i n ce p tio n	
f o r	
t h e	
p ast	
3	
y e a rs	
a s	
t h e	
L ea d	
a rc h it e c t,	
F u llS ta ck	
d eve lo p er.	
C oach Fir s t	
i s	
a	
p la tfo rm	
b uilt	
f o r	
S tu d io s,	
f u ll	
t im e	
a n d	
s id e	
g ig	
c o ach es	
t o	
h elp	
a u to m ate	
t h eir	
b usin ess	
â	
L ea d	
a	
t e a m	
o f	
5	
a n d	
b uilt	
t h e	
w hole	
p ro d uct	
f r o m	
s c ra tc h	
w hic h	
i n clu d es,	
2	
r e a ct	
n ativ e	
I O S/A nd ro id	
a p ps,	
3	
w eb	
p orta ls ,	
1	
I O S	
N ativ e	
A p p	
â	
W ork e d	
w it h	
o ve r	
1 5 +	
c o ach es	
t o	
u nd ers ta n d	
t h eir	
r e q uir e m en ts	
a n d	
e xp an d	
t h eir	
b usin esse s	
u sin g	
t h e	
p la tfo rm	
â	
G ath ere d	
a n d	
v a li d ate d	
o ve r	
5 0+	
r e q uir e m en ts	
f r o m	
d i ere n t	
c o ach es	
t o	
b uild	
a	
p ro d uct	
t h at	
i s	
m ost	
u se fu l	
t o	
m ost	
c o ach es	
â	
E ach	
c o ach	
b elo n ged	
t o	
d i ere n t	
ï¬ eld s,	
l i k e	
G olf ,	
F o otb all,	
T e n nis ,	
M assa ge	
S tu d io s,	
G ym	
S tu d io s,	
P ila te s	
e tc	
â	
U nd ers to od	
t h e	
r e q uir e m en t	
o f	
e a ch	
c o ach	
a n d	
b uilt	
t h e	
p ro d uct	
i n	
s u ch	
a	
w ay	
t h at	
t h e	
p la tfo rm	
w ork e d	
w ell	
f o r	
e ve ry o n e	
i n	
s p it e	
o f	
t h em	
b elo n gin g	
t o	
d i ere n t	
ï¬ eld s 


R o le s	
a n d	
R esp on sib ili t ie s	
â	
A rc h it e c t	
t h e	
p ro je c t	
a n d	
w ro te	
c lo se	
t o	
7 0 %	
o f	
t h e	
c o d eb ase	
f o r	
b oth	
b acke n d (N od eJ S ,	
P H P/L a ra ve l,	
D yn am od b)	
a n d	
f r o n te n d (R ea ct	
N ativ e ,	
R ea ctJ S ,	
H TM L/C SS).	
â	
D esig n	
t h e	
g en era l	
ï¬ o w	
o f	
e a ch	
f e a tu re	
i n	
t h e	
p ro je c t.	
â	
D esig n ed	
t h e	
D ata b ase	
u sin g	
t h e	
h ig h	
p erfo rm in g	
D yn am od b	
N oS Q L	
d ata b ase	
s o	
t h at	
t h e	
A p ps	
a n d	
t h e	
w eb	
p orta l	
c a n	
e a sily	
s c a le	
t o	
m illi o n s	
o f	
r e q uests	
p er	
d ay	
â	
C re a tin g	
A PI's	
f o r	
I O S/A nd ro id	
a p ps	
d eve lo p ed	
i n	
r e a ct	
n ativ e	
a n d	
N ativ e	
I O S.	
â	
D esig n ed	
t h e	
w hole	
b acke n d	
s y ste m	
t o	
b e	
1 0 0%	
s e rv e rle ss	
w it h	
A W S	
L a m bda,	
D yn am od b	
N oS Q L	
a n d	
N od eJ S /P H P	
L a ra ve l	
a n d	
C ogn it o ,	
s o	
t h at	
t h e	
s y ste m	
c a n	
e a sily	
s c a le	
t o	
a n	
e xp ec te d	
2 00,0 00	
a ctiv e	
u se rs .	
T h is	
a ls o	
h elp ed	
u s	
k e e p	
s e rv e r	
c o sts	
t o	
a	
b are	
m in im um	
d urin g	
t h e	
i n it ia l	
p hase	
â	
A rc h it e c t	
a n d	
c o d e	
t h e	
2	
r e a ct	
n ativ e	
a p ps	
b uilt	
f o r	
b oth	
c o ach es	
t o	
r u n	
t h eir	
b usin ess	
a n d	
a ls o	
t h e	
a p p	
b uilt	
f o r	
c li e n ts	
t o	
b ook	
a p poin tm en ts ,	
e n ro ll	
f o r	
c la sse s/C am ps,	
p ro ce ss	
p aym en ts	
e tc	
â	
A rc h it e c t	
a n d	
c o d e	
t h e	
C oach	
a n d	
C li e n t	
W eb	
p orta ls	
â	
C re a te	
a n d	
c o d e	
t h e	
A d m in	
p orta l	
t o	
m an age	
d i ere n t	
C oach es/S tu d io s,	
m an age	
s u b sc rip tio n ,	
p riv ile g es,	
p aym en ts	
e tc .	
â	
A ssig n	
T a sk s	
t o	
o th er	
m em bers	
i n	
t h e	
t e a m	
a n d	
r e vie w	
t h eir	
c o d e	
o n ce	
d on e	
â	
D esig n ed ,	
c o n ï¬g u re d	
a n d	
d eve lo p ed	
t h e	
f u lly	
a u to m ate d	
C I/ C D	
l i f e c yc le	
f o r	
t h e	
b acke n d	
A PIs	
a n d	
w eb	
p orta ls	
M ajo r	
F ea tu re s	
â	
C ale n d ar	
M an agem en t:	
A	
m ult i	
c o ach	
c a le n d ar	
f o r	
c o ach es	
a n d	
s tu d io s	
t o	
b ook	
1 :1	
s e ssio n s,	
s c h ed ule	
g ro u p	
c la sse s,	
s c h ed ule	
c a m ps	
(	
e g :	
S u m mer	
c a m ps,	
C oach in g	
c a m ps	
e tc ).	
B uilt	
t h e	
c a le n d ar	
f o r	
t h e	
r e a ct	
n ativ e	
a p p	
f o r	
t h e	
c o ach es	
a s	
w ell	
a s	
a	
s im ila r	
v e rs io n	
f o r	
t h e	
c o ach	
p orta l	
o n	
t h e	
w eb	
â	
A gora	
I m ple m en ta tio n	
f o r	
v ir tu al	
c la sse s	
â	
P u b N ub	
M essa gin g	
s e rv ic e	
i n te g ra tio n	
f o r	
T ra in ers	
a n d	
c li e n ts	
â	
S q uare	
P a ym en t	
I n te g ra tio n	
â	
C li e n t	
M an agem en t	
-	
I n vit e	
c li e n ts	
t o	
t h e	
p la tfo rm ,	
m an age	
o rd er	
h is to ry ,	
s e ssio n	
t r a ckin g	
e tc	
â	
R ec u rrin g	
m em bers h ip s	
-	
C oach es	
c a n	
s e t	
u p	
r e c u rrin g	
m em bers h ip s	
p la n s	
a n d	
a d d	
c li e n ts	
i n to	
a	
m em bers h ip	
p la n	
s o	
t h at	
t h e	
b illi n gs	
a re	
a ll	
a u to m ate d .	
M em bers h ip	
p la n s	
c a n	
b e	
c o n ï¬g u re d	
t o	
i n clu d e	
u nli m it e d /li m it e d	
s e ssio n s/c la sse s	
f o r	
c li e n ts .	
â	
I m ple m en t	
t h e	
R ea d er	
d evic e	
o n	
t h e	
c o ach	
a p p	
s o	
t h at	
c o ach es	
c a n	
t a p	
c re d it	
c a rd s	
t o	
a cce p t	
p aym en t	
e ve n	
w hen	
o n	
t h e	
ï¬ eld	
â	
C li e n t	
a p p	
a n d	
c li e n t	
w eb	
p orta l	
f o r	
c o n nec te d	
c li e n ts ,	
s o	
t h at	
t h ey	
c a n	
e a sily	
b ook	
a p poin tm en ts ,	
s ig n up	
f o r	
c a m ps,	
g ro u p	
c la sse s	
e tc	
â	
V id eo	
L ib ra ry ,	
P e rs o n ali z e d	
v id eo	
s h arin g,	
w elc o m e	
v id eo s	
â	
I n	
a p p	
p urc h ase	
f o r	
I O S	
d evic e s 

J a n	
2 010	
-	J a n	
2 02 0	
P ro p rie to r,	
T e ch n od w eep	
T e ch n olo g ie s	
F u ll	
S ta ck	
D eve lo p er	
T e ch	
s ta ck	
:
P H P/M yS Q L,	
L a ra ve l,	
N od eJS ,	
C od eig nit e r,	
Y ii2 ,	
L A M P,	
C ake P H P,	
R ea ctJ S ,	
B oots tra p ,	
W eb pack ,	
V u eJS ,	
P yth on ,	
T a ilw in d	
C SS ,	
J q uery ,	
J a va sc rip t,	
E le ctro n JS	
â	
B uilt	
a	
s o ft w are	
s e rv ic e s	
b ase d	
c o m pan y	
f r o m	
s c ra tc h	
a n d	
c re a te d	
a	
s m all	
t e a m	
o f	
p assio n ate	
d eve lo p ers	
a n d	
r a n	
t h e	
b usin ess	
f o r	
a ro u nd	
1 0	
y e a rs .	
â	
W ork e d	
o n	
m ore	
t h an	
1 0 0+	
s o ft w are	
p ro je c ts	
f o r	
o ve r	
5 0+	
c li e n ts	
a ll	
o ve r	
t h e	
w orld	
l i k e ,	
U S,	
U K,	
N orw ay,	
G erm an y,	
C an ad a,	
A ustr a li a	
e tc .	
â	
S om e	
o f	
t h e	
d eve lo p ers	
I
h ir e d	
w ere	
f r e sh	
o u t	
o f	
c o lle g e	
e n gin ee rs .	
R ea lly	
s m art	
p eo p le ,	
b ut	
d id	
n ot	
h ave	
a n y	
p ro fe ssio n al	
e xp erie n ce .	
I
h ir e d	
t h e	
b est	
g u ys	
d ir e c tly	
f r o m	
c o lle g e	
a n d	
t r a in ed	
t h em	
t o	
b e	
r e a lly	
p ro d uctiv e	
d eve lo p ers	
â	
U se d	
t h e	
b est	
t e c h nolo gy	
t h at	
s u it e d	
t h e	
g iv e n	
p ro je c t	
r e q uir e m en t.	
W ork e d	
o n	
P H P/M yS Q L,	
L a ra ve l,	
N od eJ S ,	
C od eig n it e r,	
C ake P H P,	
Y ii2 ,	
P yth on	
e tc	
M ajo r	
P ro je c ts	
P e tr o l	
P u m p	
S oft w are :	
â	
T h is	
a	
c u sto m	
m ad e	
E RP	
s y ste m	
f o r	
a	
P e tr o l	
P u m p	
d ea le r	
w ho	
w an te d	
a	
s y ste m	
t o	
m an age	
t h e	
d ay	
t o	
d ay	
w ork in gs	
o f	
t h eir	
p um p	
r e m ote ly .	
â	
M an age	
p ayro ll,	
a tte n d an ce	
o f	
t h e	
3 0+	
e m plo ye e s	
w ho	
w ork e d	
t h ere	
i n	
3	
s h if t s	
â	
R ep orts	
f o r	
d aily	
c a sh	
c o lle c tio n s,	
c re d it	
c a rd	
t r a n sa ctio n s,	
p um p	
m ete r	
r e a d in gs	
â	
M on it o r	
T e st	
r e a d in gs,	
g en era te	
d aily /m on th ly	
r e p orts	
â	
A uto m atic a lly	
r e p ort	
e rro rs	
i n	
r e a d in gs	
s o	
t h ey	
c a n	
b e	
c o rre c te d	
â	
C alc u la te	
p ro ï¬t	
a n d	
l o ss	
f o r	
t h e	
m on th /y e a r	
b ase d	
o n	
t h e	
p ric e	
o n	
w hic h	
t h e	
f u el	
w as	
b ou gh t	
a n d	
t h e	
d aily	
c h an gin g	
f u el	
r a te	
â	
M an age	
c re d it	
c u sto m ers ,	
c o lle c t	
v e riï¬ ca tio n	
d ata ,	
m an age	
v e h ic le s	
â	
S en d	
I n vo ic e s	
t o	
c re d it	
c u sto m ers	
â	
P o rta l	
f o r	
c re d it	
c u sto m er	
t o	
c re a te	
c o u p on s,	
a d d	
v e h ic le s,	
v ie w	
o u ts ta n d in g	
i n vo ic e s	
e tc	
W ooco m merc e	
P ro je cts	
â	
W ork e d	
o n	
n um ero u s	
w ooco m merc e	
p ro je c ts	
f o r	
d i ere n t	
c o m pan ie s	
a ll	
o ve r	
t h e	
w orld .	
U su ally	
t h ese	
a re	
s m all	
t o	
m ed iu m	
s iz e d	
b usin esse s	
w ho	
n eed	
t h eir	
o w n	
e c o m merc e	
w eb sit e	
w here	
p eo p le	
c a n	
b uy	
t h eir	
p ro d ucts .	
â	
C re a te d	
a	
3	
s to re	
s p ec ta cle s	
b usin ess	
i n	
L o n d on	
a n d	
i n te g ra te d	
t h e	
o n li n e	
w ooco m merc e	
s to re	
I
b uilt	
w it h	
L in nw ork s,	
a n	
E RP	
s y ste m	
t h ey	
a lr e a d y	
u se d	
i n	
t h eir	
s to re s,	
s o	
t h at	
t h ey	
c o u ld	
e a sily	
m an age	
t h eir	
i n ve n to ry	
f o r	
t h e	
w eb sit e ,	
A m azo n	
a n d	
i n	
s to re	
â	
C re a te d	
w eb sit e s	
f o r	
f u rn it u re	
s h op s,	
p ain tin g	
C anva s	
s to re ,	
S p orts	
G ood s	
s to re ,	
C lo th in g	
s to re s	
e tc	
â	
I n te g ra te	
w ooco m merc e	
w it h	
U nic e n ta	
P o in t	
o f	
S ale	
S oft w are	
â	
C re a te d	
W rit in g	
t o ols	
f o r	
a	
c o n te n t	
w rit e r	
o n	
h er	
p ers o n al	
w eb sit e	
w ho	
w an te d	
a	
t o ol	
t h at	
g en era te d	
5 00	
h ea d li n es	
a n d	
a	
f e w	
o th er	
f e a tu re s	
f o r	
c o n te n t	
w rit e rs 


â	
W ord pre ss	
p lu gin	
D eve lo p m en t,	
t h em e	
d eve lo p m en t	
C on te n t	
M an agem en t	
S ys te m s	
â	
C re a te	
c u sto m	
m ad e	
C on te n t	
M an agem en t	
s y ste m s	
f o r	
d i ere n t	
b usin esse s	
w it h	
s p ec iï¬ c	
r e q uir e m en ts .	
â	
C re a te d	
a	
s y ste m	
f o r	
a	
B ili n gu al	
s c h ool	
i n	
C an ad a.	
T h e	
c u sto m	
m ad e	
s y ste m	
m an aged	
t h e	
w hole	
s c h ool	
w eb sit e	
i n	
E ngli s h	
a n d	
F re n ch .	
P a re n ts	
c o u ld	
c h ec k	
r e p ort	
c a rd s,	
t h eir	
k id s	
p erfo rm an ce	
e tc	
â	
C re a te d	
s y ste m s	
f o r	
c a ke	
s h op s,	
a	
t o p	
d ate s	
s e lli n g	
c o m pan y	
w it h	
r e c ip e	
m an agem en t,	
r e a l	
e sta te	
c o m pan ie s	
R em ote	
T a sk	
M an agem en t	
â	
T h is	
i s	
a	
c o m ple te	
s u it e	
o f	
T e le w ork in g	
b usin ess	
a p pli c a tio n s	
u se d	
b y	
b usin esse s	
t h at	
n eed	
t o	
m an age	
E m plo ye e s,	
T a sk s,	
P a yro ll	
e tc	
r e m ote ly .	
C usto m	
S oft w are	
A ppli c a tio n s	
â	
G ST	
b illi n g	
s o ft w are	
f o r	
b usin esse s	
t o	
m an age	
t h eir	
i n vo ic e s	
â	
S erv ic e	
Q uotin g	
w eb sit e	
w here	
f r e e la n ce rs	
c o u ld	
b id	
o n	
p ro je c ts ,	
e m plo ye rs	
c o u ld	
c re a te	
p ro je c ts	
â	
S im ple	
s o ft w are	
t o	
m an age	
a	
l o ca l	
r e ta il	
s h op .	
G en era te	
d aily	
r e ve n ue,	
p ro ï¬t	
r e p orts .	
M an age	
i n ve n to ry	
e tc	
A p ril	
2 009	
-	D ec	
2 009	
S o ft w are	
T ra in ee	
R eg en esis	
T e ch n olo g ie s	
T e ch	
s ta ck	
:
P H P/M yS Q L,	
J o om la ,	
W ord pre ss,	
H TM L,	
C SS ,	
J a va sc rip t,	
J Q uery	
T h ese	
w ere	
t h e	
d ays	
w hen	
I E 6	
w as	
t h e	
m ost	
p op ula r	
b ro w se r,	
f o llo w ed	
b y	
I E 7,	
F ir e fo x	
a nd	
C hro m e.	
I t	
w as	
a	
r e a l	
s k ill	
t o	
c re a te	
H TM L/C SS	
t o	
w ork	
c o nsis te n tly	
a cro ss	
a ll	
b ro w se rs	
â	
L ea rn t	
t o	
b uild	
p ix e l	
p erfe c t	
w eb sit e s	
t h at	
w ork e d	
a cro ss	
a ll	
m ajo r	
b ro w se rs	
e ve n	
g oin g	
b ack	
t o	
I E 5	
â	
C usto m	
P H P	
w eb	
a p pli c a tio n	
d eve lo p m en t	
â	
W ork e d	
o n	
a	
u niq ue	
b id	
a uctio n	
s it e	
w hic h	
w as	
b uilt	
o n	
a	
c u sto m	
P H P	
f r a m ew ork	
â	
P S D	
t o	
H TM L	
â	
J o om la	
w eb sit e	
c h anges/F ix e s	
N atio n ali t y	
:
I n d ia n	
L o ca tio n	
:
P u ne,	
I n d ia 

"
resume,"
     
          	  
     	 

 
      	              
   
    
       	  
        
  	
 
	
 
  

	   	 
	 
	  	 	 	  	 	  	
	 !	
	 
	 	
 
      	
	
	  	   !
!
 	
 	   !	 !   	 
 ! !
   	  
          
 
      
       
""	 	 

# 
   	 $	 %! 	
 &   '(!
	 !	 
  ! #  
  			 	

 
)* '	'+ 	
 
         
'!
  ! !! 		   ,	 	
 
	 !

		  
	  	 	

	
	 
   
 	
'+   	
 		 !
 !
 

                            
-  .	 + 	 ! 	 	
  )			 ""
  	
	

  	 
		 		   

/012 	
	!	  ! 	 ! 	 	 	



         
		!  
 	 
34  ""
 	 55 	 	  	 
34 %! 		   		  	  	 %!  	
		 	
  

	 
	 
 	  !  	
              
		!   ! &  ),!


  !	 !!
	  6  	 55


	
 !&
 	 
   !
!	7 )8) !
      
9 . !:	 	 & $	 !! ;	
	 <
= 
	 55  
&

 	



9 ),!	 	 	'+ $>? . (   	
 
9 @	
  6 3 ( 	
 ?	,
9 ),!	 
	 	
 	

 
9 6! 
	 ,!	  #:	   !
 - 	
 ?8
9  ! 	 
'  A  !	   
	 !	
    
      	     
         
 
34 %! 		 B 
!		 B $	 B .  B <
 - B ' &

':	 !
 B & $	 B @	 B 	
	
                
        
             
        
             
        
                ! ""       
             
         #    
        $ 
       !            ""  


  	        
  
9 &!	
 	
  	 
 	  	

	 /01C 	
 /01/
9 &!	
 	 	 	 
  !
 ""  

/012
9 D


!

  		 	
/014 	
 /01C
9 D


! 	 	

 #
 
/012
9 $!
 	  . /014 	
9 E

!
  		 	
/01/
9 E

! 	
		
/012 	
 /01F
9 E

!
$% 
 %

/01A ;
 = /01G 	
 /01/
9 E

!

   #&"" 	
!! !	 /01C
9 . 	

 
 	 + H	 	 /01G I J	 !! !	
 	 		 K
              
         	  
    
# 	L ! !	 			   ?! 	 ""	 	 !!
 &  


7	
	 	
   		 @  
 
	 
	L   !	 ,!	 ;	= 	

  
 	  	

!!  ?! !	  	   	
 

 #  	  $  
#  

   

 !		 .	

 	   
 
 
!	  ! 	 <		    
 	
 	  
. 	   

 E

! 	 
$% 
 %

/01A 


             
 	 
 %       
<  !	 ""
 	 	 !    	 L
:	      #	 
 
 +	
 :, 	
 			
		  

	 	 	
  

		
        
' 	 



 ! 
      !    	 
 
   !	
  : : <


	  
'	
 	
 !	

 
 & '       
""- 	 !
 '!
   	  !!  (!	""&  
		 ""- 	 
 

               (      )        ' 	    
'	
 	 	 !		 	

 		 	  

! 	 #	 'D 	
 $	
 ""         $
         $ #          %
 &  '          (
 )            
                          #             * #          * 

"
resume," 	
 	
CompÃ©tences i	nformatique	s 	
    Web Back	-end	  PHP, 	Node.js	, Express, Nest.js,	 Symfo	ny, Django,	 SQL	 	
    Web Front	-end	  HTML, CSS, 	JS, React, 	Angular	, Vue, 	jQuery, Bootstrap, 	Wordpress	  	
    	Applications mobiles  	Java	, Kotlin	, Swift, Objective C, Cordova	, React Native	 	
    	Autres langages	  Python, TypeScript, 	C, C++, 	Go, 	C#	 	
    DevOps	  Docker, Kubernetes, AWS, GCP, ArgoCD	 	
CompÃ©tences acadÃ©miques	 	
    	IngÃ©nierie gÃ©nÃ©raliste	  MathÃ©matiques, Physique, Ãconomie, MÃ©canique	  	
    	Informatique thÃ©orique	  Algorithmique, M	achine Learning, Big Data, RÃ©seaux, SÃ©curitÃ©	  	
    Langues 	 Anglais (niveau C1), Espagnol (niveau B2)	 	
 	
2016	-2017	  Master informat	ique (Option IA)  	UniversitÃ© Claude Bernard, Lyon (69)	 	
2014	-2017	  Formation ingÃ©nierie gÃ©nÃ©raliste  	Ãcole Centrale de Lyon, Ãcully (69)	 	
2012	-2014	  Ãtudes en classe prÃ©paratoire (Option MP)  	LycÃ©e Louis Pasteur, Neuilly (92)	 	
 	
2022	-2023	  DÃ©veloppeur Backend	 â Yubo	 	
- DÃ©veloppement de nouvelles features & maintenance de lâ	existant, migration	s BDD	 	
- Gros enjeux de scalabilitÃ© et de performance (1M DAUs, 	Big 	Data, T	emps 	RÃ©el)	 	
- Stack	 : Node	, Docker, Kubernetes, 	MongoDB, Couchbase, PostgreSQL, ElasticSearch, 	GCP	 	
2021	-2022	  DÃ©veloppeur Full S	tack 	â Skillup	 	
- AmÃ©lioration de lâapplication (SaaS RH), maintenance et Ã©volution de lâexistant (back	-end & front	-end)	 	
- Ãvolution de lâarchitecture et des process de CI/CD po	ur amÃ©liorer la scalabilitÃ© et la robustesse du code	 	
- Stack	 : Node, React, Typescript, Docker, Neo4J, MongoDB	 	
2017	-2021	  DÃ©veloppeur Full S	tack 	â SnapCar/LeCab	 	
- Intervention Ã  divers niveau	x : back	-end, webapp, apps	 mobile, BO. Maintenance et Ã©volution de lâexistant	 	
- Participation Ã  la fusion technique suite au rachat de LeCab par SnapCar	 	
- Stack	 : PHP Symfony, Node, React, Angular, 	Java, Swift	/Objective C, 	MySQL, AWS	  	
 	
2017	  Stage de fin dâÃ©t	udes 	â Machine Learning	  Klee Group	 	
2014	-201	6  VP Web pour ÃCLAIR  	Association informatique de Centrale	 	
2010	-2023	  DÃ©veloppement projets personnels  	Mario Kart PC, Free Zap Player, Slime Volley Ball, â¦	 	
 	
COMPÃTENCES PROFESSIONNELLES	 	
FORMATION	 	
DERNIÃRES 	EXPÃRIENCES 	PROFESSIONNELLES	 	
AUTRES EXPÃRIENCES	 	
 TimothÃ© MALAHIEUDE	 	
      	  28 ans	 	
      	  92 310 SÃ¨vres	 	
       	 06 04 15 74 70	 	
      	  t.malahieude@gmail.com	 	
      	  timothe.malahieude.net	 	
DÃ©veloppeur Full Stack	 	
IngÃ©nieur centralien	 	
Tous mes projets sur 	timothe.malahieude.net	   

"
resume,"        VENISH PATIDAR 
            Email: venishpatidar@gmail.com     |    Phone:  +91 (83198) -(74206)    |  Website : www.venishpatidar.com  
            GitHub:  @venishpatidar  	
 
EDUCATION	 	
â¢ 	Arizona State University , Tempe AZ , USA	 	
Master of Science	, Computer Science 	 	
Present	 	
 	
â¢ 	Medi -Caps University,  Indore, India 	 	
Bachelor of Technology, Computer Science & Engineering 	Overall CGPA: 	3.54	 / 4.0	 	
August 2018 â  	May 2022	 
 	
 
PUBLICATIONS	 	
BOOKS : 
â¢   Venish Patidar,  1 October 2022, â Developers guide for building own N eural Network  Library : A mathematical journey in creating 
the neural network library in  C++.â   https://books.google.co.in/books?id=Ho- WEAAAQBAJ â  
RESEARCHES:  
â¢   Venish  Patidar, Prof. Preetesh Purohit  â Multidimensional Neural Network:  Heterogeneous Activation Function on Same Neural 
Layer,â  SSRN : http://dx.doi.org/10.2139/ssrn.4171522  â  
 
RESEARCH 	EXPERIENCE	 	
â¢  Emorphis Technologies,  Correlation between logs and failure of a project ,  Intern Researcher, supervisor Mr. Pavan  Patidar .  
Derived an early indication based on the pattern of log messages that can indicate the likely  imminent failure of a function. 
â¢   Medi -Caps University ,  Multidimensional Neural Network , Undergraduate Researcher, guide Prof. Preetesh Purohit .  
Proposed a new ANN architecture that made the loss converge swiftly in almost 50%  of the early epochs than traditional ANNs. 
â¢   Medi -Caps  University, Neural  Music Reminiscence,  Undergraduate Researcher, guide Prof. Preetesh Purohit . 
Devised an ANN model using GANs that could generate a musical track with a hint of the well- known singerâs voice and style. 
 
W	ORK 	EXPERIENCE	 	
Machine Learning Engineer Intern	, Emorphis Technologies	 	January 2022 	â June 2022	 	
â¢   Researched, designed, and worked on a Python module, Labeller.py, that  labels company logs and set s category tags for the error, 
warning, and standard logs.   
â¢   The  resulting labeling  reveals  the link between patterns of errors and release dates for similar p rojects. 
Machine Learning Engineer Intern	, Outworks Solution	s Pvt. Ltd.	 	May 2021 	â October 2021	 	
â¢  Worked on a sentimental analysis project that analyses employeesâ emails and slack messages.  
â¢   It r educed the time taken  to review & analyze employee happiness, positivity, and productivity by 20% while tripling efficiency.  
   
PROJECTS	 	
Portfolio link: https://venishpatidar.com/Projects  â  
Academic Projects:  
â¢  Neural Music |  Python  TensorFlow    Django (backend API)   React (front -end)    Jupyter  (Model training)  
https://github.com/venishpatidar/NeuralMusic  â  	
o  Neural Music comprises four  different  NLP  neural network models  running on the  python  Django server and is accessible through a 
webpage.  All  four  are  implemented  using TensorFlow and integrated into the Django  backend server. 
o   Each neural network  was trained  in a  Jupyter  notebook, and then the trained weights  were uploaded to the  models integrated on  the 
Django backend server, which serves the results to the Neural Music web page via REST APIs.  	
â¢  Self -driving Car in GTA V |  Python  PyCharm    OpenCV   TensorFlow  
o   Developed  n AI program that could self -drive for  2 minutes  before hitting obstacles in  the computer game  GTA V. 
o   Written in Python and used OpenCV to capture frames, generate datasets, and TensorFlow for transfer learning on AlexNet.   

Personal Projects: 
â¢  Biller.exe  | Windows exe software   electron.js  
https://venishpatidar.com/Projects/biller  â  	
o  Developed  a windows software that	 calculates the price of gold or silver items, adds the tax, substracts the discounts, and generates 
and stores the printable bill  .  
o   Used electron.js to build the software with multilingual support for printing English or Hindi bills.  	
â¢  Kukshi Info |  React-native   Firebase   MongoDB    Expo SDK  
https://play.google.com/store/apps/details?id=com.kukshiinfo.android  â 	
o  Design ed and  developed Kukshi Info during Covid- 19, containing the contact number, addres ses, and information of all the doctors, 
medical stores, government officers, and business and other town services.  
o   There are currently over 1,500+  active users utilizing the appâs benefits.  	
â¢   React -native -gradient -icon  | Typescript   node.js  open sourc e 
https://www.npmjs.com/package/react -native -gradient -icon  â  	
o  Written in  Typescript,  an open -source npm package,  aims to provide linear and radial gradient color -filled icons in react- native, 
which previously react -native lacks this functionality.  
o   The package  is used in many  projects and  has 150+  weekly downloads.  
 	
HONORS	, AWARDS 	& ACHI EVEMENTS	 	
â¢  2022 Certificate of Appreciation, Municipality of Kukshi, in recognition of converting the town into an E -town via the Kukshi Info 
mobile application, which helped 2000+ people during the covid- 19 lockdown. 
â¢   Awarded  second  place, Neural Music represented  the  Universityâs CSE department âs achievement in the intra -department  show .   
â¢   My self -driving AI in GTA V took  first place  and cash of Rs. 4,000   in the Medi-Caps University h ack-on- hackathon.   
 
LEADERSHIP  	& VOLUNTEER 	EXPERIENCE	 	
â¢  2022,  Hosted  a webinar on zoom for sophomore students ,  educating  them about  Deep learning,  AI , and its application.  
â¢   2021,  Head  of the organization committee, âCODE -TUSSLE,â a 36 -hrs hackathon for sophomores.  
â¢   2019, Volunteered as a tutor for  the NGO  â ASHA, â facilitating the growth of underprivileged kids and helping them understand Maths 
and English in a better way.   
â¢   2018- 2022, Elected four times in a  row as a class representative . 
 
SKILLS	 	
â¢   Programming Languages : C, C++, Java, Python, Prolog,  R,  Node.js (JavaScript, TypeScript), Swift, Kotlin , Jetpack Compose 
â¢   Fullstack web dev :  React, React -native, Redux, HTML -5, CSS, Express.js, Electron.js, Django, Flask,  MongoDB, MySQL  
â¢   DevOps:  Azure, AWS, Firebase, Docker, Jira, Slack, Git  
â¢   Technical Skills: Deep Learning, Machine Learning,  Unreal Engine 4, TensorFlow, CMake , Dynamic programming, Software 
engineering, Web development, UX design,  Mathematics-Statistics  
 
CERTIFICATIONS  	 	
â¢ Deep Learning Specialization  	Deeplearning.ai/Coursera	 	coursera.org/verify/specialization/3FEV2MNENLWV  â  	
â¢  Meta Android Developer  	Meta	 	coursera.org/verify/professional -cert/RX3887U3KLYP â 	
â¢  Fullstack web development 	Specialization  	Coursera	 	coursera.org/verify/specialization/53ACF96HWP6Y â 	
â¢  Unreal Engine C++ developer  	Udemy	 	UC-cafc4331 -e271- 42b9- 81f0-105a261161c4 â 	
â¢ 	Google IT Automation with Python  	Google	 	coursera.org/verify/professional-cert/WDTDK3CVN6HE â 	
â¢  DeepLearning.AI TensorFlow Developer  	Deeplearning.ai	 	coursera.org/verify/professional-cert/VA5L36WBB6SV  â  	
  

"
resume,"BenDavis
Curriculum Vitae
SummaryA passionate and motivated software developer with 7 years of experience developing iOS apps including 4 years
as the Lead iOS Developer in a fast growing, London based startup. I love creating innovative and engaging
applications and I am an advocate for high quality source code which is clean, robust and maintainable.
Technical Skills (Years experience)
LanguagesSwift (3), Objective-C (7), PHP (2), Node.js (5), Javascript (5), HTML+CSS (5)
TechnologiesXcode, Realm, Core Data, RxSwift, Fastlane, Fabric, Xcode Server, SQL, Git, L A
T
EX, and more. Experience
March-
Present 2018 Contract Developer
,Debate Mate, Stay Nimble, et al .
- Contributed to a macOS application (details undisclosed under an NDA).
- Working remotely whilst travelling, built an online platform for educational content using WordPress. Used Swift ,PHP ,Javascript ,html andsass debatemate.online
2013 - 2017 Lead iOS Developer ,Flypay Ltd .
Consumer ordering, payment and loyalty technologies for restaurants and hospitality sector.	
After joining ypay from an early stage I built and managed a team of 5 iOS developers and wrote
signicant portions of the codebase. Ãµdeveloper/ypay.
{ Used both Obj-C and Swift.
{ Test First development, Code Reviews and Paired Programming.
{	
Built a suite of iOS frameworks and libraries to enable restaurant technologies in multiple apps
(Documentation:ios.docs.ypay.io).
{ Built a variety of apps and worked with third party app developers to integrate ypay technologies.
{ Used a variety of technologies, including Apple Pay, Realm, Core Data, Web Sockets, RESTful API
{ Contributed to the backend in PHP and Node.js.	
Jun-Nov 2013	Full Stack Web Developer ,PACE Dimensions Ltd .
2010Present Independent & Contract iOS Developer ,Ben Davis Apps .
Wrote iOS apps for iPhone and iPad in my free time, including Mineral Identier and MultiPlayer Poker.
Had over 40,000 downloads in my best year. Education
2010 - 2013 Imperial College London .
BSc Mathematics and Computer Science  2.1 Honours .
Final year project -Multiple Atlas Segmentation of CT Images for Prostate Cancer
Wrote a machine learning algorithm in C++ for aligning CT scans of prostate cancer patients.
T +44 7549 960118 Bben@bendavisapps.co.uk Âbendavisapps.co.uk
Â github.com/imben123 7twitter.com/imben123 

Individual Project Work
Title Instagram for iPad
DescriptionBuilt an instagram library for eciently downloading and caching instagram feeds. I then used
this library to create a simple iPad application. Âimben123/InstagramForiPad
Title BitTorrent library in Swift
Description
Built a library for downloading BitTorrent's. Written 100% in Swift, open source.
Â imben123/BitTorrentSwift
Title Multi-Player Poker
Description
Bluetooth based poker game where an iPad displays the poker table, and iPhones display the
poker hand.Multi-player poker Achievements and Interests
2017 Won Flypay Hackathon	
- Built an iMessage app for ordering food. Competed against 10 other	
teams including external companies.
2013 Won ICHack Hackathon	
- Built a web app for travel. Judged by Facebook, Microsoft, and JP	
Morgan.
Conferences
2018 Alt Conf - The biggest WWDC alternative conference in San Jose.
2017 Try! Swift India	
- Personally funded and attended the rst international iOS conference in	
South Asia.
2016 WWDC - Represented Flypay as an attendee at WWDC in San Francisco.
Additional Courses
2016 Uncle Bob's Clean Code: Agile Software Craftsmanship	
- Three day course on clean code,	
by the man who wrote the book!
2016 Big Nerd Ranch Advanced iOS Class - Two day course, part funded by Apple.
Interests
{ Volunteered building a school in Nepal (Oct-Dec 2017).
{ Taught English as a volunteer in Peru.
{ Trained in basic rst aid to volunteer with St. John's Ambulance.
{ Completed the three peaks challenge (climbed 3 highest peaks in the UK within 24 hours).
{ Enjoys travelling, remote camping and hiking, cycling, tennis, swimming and running.
T +44 7549 960118 Bben@bendavisapps.co.uk Âbendavisapps.co.uk
Â github.com/imben123 7twitter.com/imben123 

"
resume,"ILKER	 BALTACI	   	Senior Software Engineer (Mobile)	 	Schaidlerstrasse 17, 81379 Munich	 	ilkerbaltaci@gmx.at	  	Project	 Summary	 	ProjectsRoleTechnologiesRyd	 (Tanktaler	)	Lead App	 Dev	. 	iOS (Swift & Obj-C), Android (Java, Kotlin)SS MediaMarktSaturn Internal Ticketing System native AppsLead	 Developer	 & Architect	iOS	, Android	, PHP	 Symphony	, MySQL	, API	 Design	Saturn Discount	 Hunting (MVP)	Lead	 iOS	 Developer	Swift	, Google	 Firebase	Check	24 	Finance	 iOS	 App	 (Micro	 App	)	Lead	 iOS	 Developer	Swift	, Api	 Design	, Kanban	Check	24 	iOS	 App	iOS	 Developer	Obj	-C, Swift	, In House	 libraries	 with	 CocoaPods	, 	BambooâWSDL	 to	 Swift	â Code	 generator	Java	 SE & Swift Developer	Swift, WSDL	, XSD	, XML	, Java	 SE	, Eclipse	  	driveNowLead	 iOS	 Developer	Obj	âC, SCRUM	, RestFul	 API	 Design	, SQLITE	, TDD	, 	Continious	 Integration	 (Jenkins	, Unit	 Te s t s	, UI	 	Te s t s	 etc	.)	myDriver	 Chauffeur	 Service	 App	.	Lead	 iOS	 Developer	.	myDriver	 Driver	 App	.	iSixtiOS	 Dev	. 	Sixt	 mobility	 for	 BMW	iSixt	 Leasing	driveNow	 Apple	 Wa t c h	 App	iOS	 Dev	. 	SwiftdriveNow	 Windows	 Mobile	 App	.	Windows	 Phone	. dev	.	C#, 	Visual	 Studio	, GA	, SCRUM	To o l a n iiOS	 Dev	.	Obj	-C, C, Vo I P	, SQL	forfoneiOS	 Dev	Obj	-C, C, Vo I P	, SQL	, XMPP 

Please	 visit	 the	 portfolio	 section	 on	 my	 web	 page	 for	 the	 detailed	 description	 of	 my	 projects	.  	Freelancing	 Projects	 	Liebherr	 SmartDevice	iOS	 Dev	.	Swift	, SwiftUI	, Combine	, RxSwift	Liebherr	 HNGRY	iOS	 Dev	.	Swift	, UIKit	, SwiftUI	, Combine	Liebherr	 Components	 (Beta	 phase	)	iOS	 Dev	.	Swift	, SwiftUI	, Combine	DAK	 App	iOS	 Dev	.	Swift	, SwiftUI	, Combine	, RxSwift	GreenFinder	 (iOS	 B2B App	.)	Full	 Stack	 Developer	Swift	, SwiftUI	, Firestore	, Node	.js, Google	 Cloud	ParkingLot	 Locator	 (In	 dev	.)	iOS	 Dev	.	Swift	, SwiftUI	Zodiac	 Moods	 (iOS	) 	Zodiac	 Moods	 (Android	)	React	 Native	 Developer	React	 Native	, Objective	 -C, Java	, UI	 Design	Zodiac	 Moods	 (tvOS	)	iOS	 Dev	.	Swift	, UI	 Design	, tvOS	Clevoo	 Trans	.	Full	 Stack	 Developer	SWIFT	, Parse	, Java	 Script	, HTML	, Express	, UI	 	DesignSahin	 Fruit	  CMS	 To o l	 & iOS	, Android	 	Apps	 (Beta	 Testing	) 	CTO	 & Full	 Stack	 Developer	Swift	 (iOS	), Android	 (Java	), PHP	 Symphony	, API	 	Design	, MySQL	, JQuery	, JavaScript	, Bootstrap	, 	HTMLiLensTrackeriOS	 Dev	.	Obj	âC, UI	 Design 

"
resume,"Lukas Krecan
Email:   lukas@krecan.net
Profile
Experienced  Senior Java Developer  with more than 10 years experience in 
developing backend Java applications. Expert knowledge of J2EE, Spring, Maven, 
REST, OOP and Scrum.
Expertise
ï· Java, J2EE development
ï· Spring, Spring MVC, Spring Security, Spring WS
ï· Automated testing (Unit, component and integration testing)
ï· Maven, Eclipse, IntelliJ, Tomcat, MongoDB, SQL
ï· OOP, Design Patterns, UML, REST, JSON
ï· AWS, Kubernetes, DevOps, Web security
ï· JavaScript, React (intermediate knowledge)
Skills
ï· Teaching, mentoring (taught unit, functional testing, Java 8, ...)
ï· Knowledge of Agile techniques (Scrum)
ï· API design
Emplo yment
ï· 2016 â 2019 Liftago â SW Engineer, DevOps, Backend Technical Lead
ï· 2011 â 2016 GoodData â SW Engineer, REST API architect, Scrum master
ï· 2010 â Freelancer 
ï· 2009 â 2010 â T-Mobile â Senior Software Developer/ Analyst
â Development, Design, Team coordination, Prototyping, Architecture
ï· 2008 AerLingus (Ireland),  Senior Software Engineer, J2EE programming 
â Development, Refactoring, Design, WS Implementation
ï· 2005 â 2008 Logos, Senior Software Engineer, J2EE programming in 
telecommunications and finance
â Development, Design
â Mentoring, Teaching
â Member of Java Competency Centre (knowledge sharing coordination)
ï· 2004 â 2005 CERN (Switzerland), Software Engineer, J2EE programming of 
Project Progress Tracking application
ï· 2001 â 2004 Aegis, Java programmer and consultant in banking, teaching 
Java and WebSphere courses for IBM Training Services
Education
ï· M.Sc. (Ing. degree) in Software Engineering, Czech Technical University, 
Faculty of Nuclear Sciences and Physical Engineering, Prague
ï· Erasmus studentship, ISAIP-ESAIP, Angers, France
Certificates
ï· Certified ScrumMaster
ï· Certificate in Advanced English (CAE)
Interests
ï· Sport, Slack-line, literature,  blogging about Java  

Completed projects:
ï· 2016 â 2019 â  Liftago  â taxi hailing application
â Design, architecture, implementation
â Leading Scrum adoption
â DevOps, Kubernetes, AWS
ï· 2016 â today â  ShedLock  â open-source library for task coordination
ï· 2011 â 2015 â  Webapp  - frontend component in GoodData platform 
providing REST APIs for UI and 3 rd
 parties
â Design, architecture, implementation
â Technologies used: Tomcat, Spring, MongoDB, Spring MVC
ï· 2012 â today -  JsonUnit  â open-source library for comparing JSON objects
ï· 2010 â  HOP  (ÄeskÃ¡ SpoÅitelna)
â Optimization, Refactoring, Stabilization, Support
â Stabilization of mortgage management application back-end layer
â Technologies used: Spring, JAX-WS RI, Weblogic, Oracle
ï· 2010 â  Spring WS 2.0  (Open Source)
â Implementation of WS testing framework
â API design and architecture
ï· 2009 â 2010 â  SDP  (T-Mobile, Prague)
â Service Delivery Platform â broker between third parties and European 
T - Mobiles (UK, DE, NL, CZ, ...)
â Design, Development, Testing, Team coordination
â Web Service design and implementation
â Maintenance, refactoring and evolution of existing code 
â Prototype implementation (Google Marketplace integration)
â Leading 3-5 developers, Agile evangelism
â Technologies used: EJB2, Spring, Axis, MySql, JBoss, Spring WS
ï· 2008 â  aerlingus.com  (AerLingus, Ireland)
â Development, Design
â Maintenance, enhancement and refactoring of aerlingus.com e-shop
â Implementation of Web Services for communication with third-parties
â Unit testing mentoring and implementation
â Technologies used: Struts, Spring, Spring WS, JDBC, Ant, BroadVision
ï· 2008 â  CMP Logic SK  (TelefÃ³nica O2 Slovak Republic)
â Design, Development
â High performance messaging application
â Validation and billing of premium SMS/MMS messages
â Technologies used: Spring, JMS (SwiftMQ), JDBC, Maven, JBoss
ï· 2008 â  m2eclipse  (Open Source)
â XML editor for Maven Eclipse plug-in 
ï· 2007 â  Devportal  (TelefÃ³nica O2 Czech Republic)
â Design, Development
â Migration of build system from Ant to Maven (conversion of Ant tasks to 
Maven plugins)
â Implementation of a development supporting application (releasing, 
visualization of dependencies, continuous integration) 
â Technologies used: Maven, Spring, JPA (Hibernate), Spring MVC, Tiles, 
Tomcat, Oracle  

"
resume,"
THOM AS	
P	
Z A CH ARI AH	
t z a ch ari@ berk e le y.e d u	
/	
t h o m asza ch aria h .c o m	
/	
5 45	
C ory	
H all,	
B erk e le y,	
C A	

E D U CAT IO N	

Univ ersity	
of	
California,	
Berk eley	
â	
Berk eley,	
CA	
Ph.D .	
Electrical	
Engineering	
&	
Computer	
Sciences	
,
Advisor:	
Pr abal	
Dutta	
A UG	
2017	
â	
M AY	
2023	
Univ ersity	
of	
Michigan	
â	
Ann	
Arbor ,	
MI	
M.S.E.	
Computer	
Science	
&	
Engineering	
A UG	
2013	
â	
M AY	
2017	
Lo yola	
Mar ymount	
Univ ersity	
â	
Los	
Angeles,	
CA	
B.S.E.	
Computer	
Engineering	
,
Minors:	
Applied	
Mathematics	
&	
Computer	
Science	
A UG	
2009	
â	
M AY	
2013	

E N GIN EER IN G	
&	
R ES EA RC H	
E X P ER IE N CE	

Univ ersity	
of	
California,	
Berk eley	
â	
Berk eley,	
CA	
Gr aduate	
Resear cher,	
Electrical	
Engineering	
&	
Computer	
Sciences	
A UG	
2017	
â	
A UG	
2023	
â	
W ork ed	
with	
Dr .	
Pr abal	
Dutta	
in	
Lab11	
(Embedded	
Systems)	
in	
Electrical	
Engineering	
&	
Computer	
Sciences.	
â	
Designed	
&	
implemented	
lightweight	
gatewa y	
ar chitectur es	
and	
user-inter facing	
methods	
for	
the	
Internet	
of	
Things.	
Univ ersity	
of	
Michigan	
â	
Ann	
Arbor ,	
MI	
Gr aduate	
Resear cher,	
Computer	
Science	
&	
Engineering	
M AY	
2014	
â	
A UG	
2017	
â	
W ork ed	
with	
Dr .	
Pr abal	
Dutta	
in	
Lab11	
(Embedded	
Systems)	
in	
Computer	
Science	
&	
Engineering.	
â	
Designed	
&	
implemented	
mobile-based	
gatewa y	
ar chitectur es	
t o	
enable	
connectivity	
for	
low-power	
Internet	
of	
Things.	
Vir ginia	
T ech	
â	
Blacksbur g,	
V A	
NSF	
Resear cher,	
Cognitiv e	
Communications	
REU	
JUN	
2012	
â	
A UG	
2012	
â	
De veloped	
data	
analysis	
softwar e	
&	
gr aphical	
user	
inter face	
t o	
study	
space	
weather	
eff ects	
on	
GPS	
signals	
r ecor ded	
fr om	
newly	
deplo yed	
space	
weather	
measur ement	
instrumentation	
in	
Antar ctica.	
W ork ed	
with	
Dr .	
Hy omin	
Kim.	
Lo yola	
Mar ymount	
Univ ersity	
â	
Los	
Angeles,	
CA	
Under graduate	
Resear cher,	
A pplied	
Computer	
Engineering	
SEP	
2010	
â	
M AY	
2013	
â	
De veloped	
softwar e	
t o	
pr ocess	
&	
analyz e	
climate	
model	
data	
t o	
pr edict	
impacts	
of	
climate	
change	
on	
California âs	
water	
r esour ces	
in	
the	
next	
centur y.	
W ork ed	
with	
Dr .	
Jer emy	
P al,	
member	
of	
2007	
Nobel	
Priz e-winning	
IPCC	
team.	
â	
Designed	
&	
deplo yed	
TinyOS	
wir eless	
sensor	
networks	
for	
parking	
lot	
occupancy .	
W ork ed	
with	
Dr .	
Gusta vo	
V ejar ano.	
Lo yola	
Mar ymount	
Univ ersity	
â	
Los	
Angeles,	
CA	
El	
Salv ador	
Pr oject	
Leader ,	
Engineers	
Without	
Bor ders	
J AN	
2011	
â	
J AN	
2013	
â	
Co-led	
team	
of	
engineering	
students	
in	
r eplacing	
a
waterline	
system	
for	
an	
island	
community âs	
only	
school	
and	
set	
up	
measur es	
t o	
ensur e	
water	
potability	
thr oughout	
the	
island	
(village-scale	
ï¬ltr ation	
systems	
&	
water	
education).	
â	
W ork ed	
with	
the	
islandâ s	
W omen âs	
Co-op	
t o	
get	
its	
facilities	
up	
t o	
f eder al	
code	
t o	
mak e	
goods	
t o	
sell	
acr oss	
El	
Salv ador .	
Los	
Angeles,	
CA	
Independent	
W eb	
&	
Mobile	
De veloper	
J AN	
2005	
â	
PRESEN T	
â	
W orking	
with	
H TML,	
Ja vaScript,	
PHP ,	
CSS,	
and	
gr aphic	
design	
t o	
cr eate	
captiv e	
websites	
&	
web	
apps.	
â	
De veloping	
nativ e	
&	
cr oss-platform	
(Cor dova/React)	
apps	
for	
deskt op,	
iOS	
(Objectiv e-C/Swift),	
&	
Andr oid	
(Ja va/K otlin).	

T EA CH IN G	
&	
O TH ER	
W ORK	
E X P ER IE N CE	

Univ ersity	
of	
Michigan	
â	
Ann	
Arbor ,	
MI	
Gr aduate	
Instruct or,	
EECS	
270:	
Logic	
Design	
(Fâ13,	
Wâ14)	
SEP	
2013	
â	
APR	
2014	
â	
Held	
oï¬ce	
hours,	
lectur ed	
class,	
led	
study	
sessions,	
managed	
all	
gr ades,	
cr eated	
homework	
assignments,	
hir ed	
&	
super vised	
homework	
gr aders,	
and	
wr ote	
&	
gr aded	
exams.	
Ov er	
300	
students	
during	
the	
two	
semesters.	
Lo yola	
Mar ymount	
Univ ersity	
â	
Los	
Angeles,	
CA	
T eacher/Lab	
Assistant,	
ELEC	
210/213/220/301/353/302/354:	
Cir cuits	
I/II,	
Electr onics	
I/II,	
Labs	
SEP	
2011	
â	
M AY	
2013	
â	
Guided	
students	
in	
labs,	
pr ovided	
assistance	
t o	
the	
pr of essor	
as	
needed,	
and	
held	
oï¬ce	
hours	
for	
r espectiv e	
courses.	
Lo yola	
Mar ymount	
Univ ersity	
â	
Los	
Angeles,	
CA	
Engineering	
T ut or,	
T au	
Beta	
Pi	
J AN	
2012	
â	
M AY	
2013	
â	
Off ered	
weekly	
tut oring	
ser vice	
for	
all	
engineering	
students	
in	
any	
r equested	
material.	
Lo yola	
Mar ymount	
Univ ersity	
â	
Los	
Angeles,	
CA	
E vent	
Oper ations	
Assistant	
SEP	
2009	
â	
M AY	
2013	
â	
Collabor ated	
with	
hosts	
and	
personnel	
t o	
plan,	
set	
up	
equipment	
for ,	
and	
conduct	
e vents	
held	
at	
the	
Univ ersity . 


S K IL LS	
â	
Oper ating	
Systems:	
Macint osh,	
Windows,	
Linux,	
iOS,	
Andr oid,	
TinyOS,	
Contiki,	
F reeR TOS	
â	
Pr ogr amming:	
Assembly ,	
C,	
C++,	
NesC,	
Objectiv e-C,	
Ja va,	
K otlin,	
Node.js,	
P ython,	
Swift,	
Bash,	
Hask ell,	
M ATL AB	
â	
W eb	
De velopment:	
H TML5,	
Ja vaScript,	
PHP ,	
CSS3,	
Flash,	
JQuer y,	
Bootstr ap,	
Apache	
Ma ven,	
gr aphics/animation,	
W ebXR	
â	
Embedded	
Engineering:	
Bluet ooth	
LE,	
802.15.4/Zigbee/Thr ead,	
Ar duino,	
Raspberr y	
Pi,	
Nor dic	
nRF52,	
Espr essif	
ESP32	
â	
Electrical	
Engineering:	
B
2
Logic,	
Logisim,	
Multisim,	
NI	
Elvis	
&	
LabView ,	
Xilinx	
ISE	
&	
FPGA,	
M68HC12,	
SystemV erilog	
â	
Media:	
Adobe	
Phot oshop,	
Illustr ator,	
Pr emier e	
Pr o,	
After	
E ff ects,	
Dr eamwea ver ,	
Flash;	
GIMP;	
Inkscape;	
Final	
Cut	
Pr o	
â	
Pr oductivity:	
Micr osoft	
Oï¬ce,	
Visual	
Studio;	
Google	
W orkspace;	
X code;	
Andr oid	
Studio;	
Git	

P U BLIC AT IO N S	

â	
Thomas	
Zachariah	
.
Application-Agnostic	
Connectivity	
and	
Inter action	
in	
the	
Internet	
of	
Things	
.
In	
EECS	
Dept,	
Univ ersity	
of	
California,	
Berk eley	
.
2023.	
Disser tation.	
â	
Thomas	
Zachariah	
,
Noah	
Klugman,	
Pr abal	
Dutta.	
ThingSpeak	
in	
the	
Wild:	
Exploring	
38k	
Visualizations	
of	
IoT	
Data	
.
In	
Pr oceedings	
of	
the	
Fifth	
International	
W orkshop	
on	
Data	
Acquisition	
t o	
Analysis	
.
D ATA	
â22.	
Bost on,	
M A,	
USA.	
No vember	
2022.	
W orkshop.	
â	
Thomas	
Zachariah	
,
Neal	
Jackson,	
Br anden	
Ghena,	
Pr abal	
Dutta.	
ReliaBLE:	
T owar ds	
Reliable	
Communication	
via	
Bluet ooth	
Low	
Ener gy	
Adv ertisement	
Networks	
.
In	
Pr oceedings	
of	
the	
19th	
International	
Conf erence	
on	
Embedded	
Wir eless	
Systems	
and	
Networks	
.
EWSN	
â22.	
Linz,	
A ustria.	
Oct ober	
2022.	
Conf erence.	
â	
Thomas	
Zachariah	
,
Neal	
Jackson,	
Pr abal	
Dutta.	
The	
Internet	
of	
Things	
Still	
Has	
a	
Gatewa y	
Pr oblem	
.
In	
Pr oceedings	
of	
the	
23r d	
W orkshop	
on	
Mobile	
Computing	
Systems	
and	
Applications	
.
HotMobile	
â22.	
T empe,	
AZ,	
USA.	
Mar ch	
2022.	
W orkshop.	
â	
Da vid	
E	
Culler ,	
Pr abal	
Dutta,	
Gabe	
Fierr o,	
Joseph	
E	
Gonzalez,	
Nathan	
P ember ton,	
Johann	
Schleier-Smith,	
Kaly anar aman	
Shankari,	
Alvin	
W an,	
Thomas	
Zachariah	
.
CoVista:	
A	
Uniï¬ed	
View	
on	
Priv acy	
Sensitiv e	
Mobile	
Contact	
T racing	
.
In	
IEEE	
Data	
Engineering	
Bulletin,	
V olume	
43,	
Number	
2.	
June	
2020.	
Ar ticle.	
â	
Thomas	
Zachariah	
,
Joshua	
Adkins,	
Pr abal	
Dutta.	
Br owsing	
the	
W eb	
of	
Connectable	
Things	
.
In	
Pr oceedings	
of	
the	
17th	
International	
Conf erence	
on	
Embedded	
Wir eless	
Systems	
and	
Networks	
.
EWSN	
â20.	
L yon,	
F rance.	
F ebruar y	
2020.	
Conf erence.	
â	
Thomas	
Zachariah	
,
Pr abal	
Dutta.	
Br owsing	
the	
W eb	
of	
Things	
in	
Mobile	
A ugmented	
Reality	
.
In	
Pr oceedings	
of	
the	
20th	
W orkshop	
on	
Mobile	
Computing	
Systems	
and	
Applications	
.
HotMobile	
â19.	
Santa	
Cruz,	
CA,	
USA.	
F ebruar y	
2019.	
W orkshop.	
â	
Thomas	
Zachariah	
,
Meghan	
Clark,	
Pr abal	
Dutta.	
Bluet ooth	
Low	
Ener gy	
in	
the	
Wild	
Dataset	
.
In	
Pr oceedings	
of	
the	
First	
International	
W orkshop	
on	
Data	
Acquisition	
t o	
Analysis	
.
D ATA	
â18.	
Shenzhen,	
China.	
No vember	
2018.	
W orkshop.	
â	
Thomas	
Zachariah	
,
Joshua	
Adkins,	
Pr abal	
Dutta.	
Br owsing	
the	
W eb	
of	
Things	
with	
Summon	
.
In	
Pr oceedings	
of	
the	
13th	
A CM	
Conf erence	
on	
Embedded	
Network ed	
Sensor	
Systems	
.
SenSys	
â15.	
Seoul,	
South	
K or ea.	
No vember	
2015.	
Demo.	
â	
Joshua	
Adkins,	
Br adfor d	
Campbell,	
Samuel	
DeBruin,	
Br anden	
Ghena,	
Benjamin	
K empk e,	
Noah	
Klugman,	
Y e-Sheng	
K uo,	
Deepika	
Natar ajan,	
P at	
P annut o	
,
Thomas	
Zachariah	
,
Alan	
Zhen,	
Pr abal	
Dutta.	
Michigan âs	
IoT	
T oolkit	
.
In	
Pr oceedings	
of	
the	
13th	
A CM	
Conf erence	
on	
Embedded	
Network ed	
Sensor	
Systems	
.
SenSys	
â15.	
Seoul,	
South	
K or ea.	
No vember	
2015.	
Demo.	
â	
Br ad	
Campbell,	
Noah	
Klugman,	
Thomas	
Zachariah	
.
HotMobile	
15	
.
In	
IEEE	
P er vasiv e	
Comput	
ing	
:
Mobile	
&	
Ubiquit ous	
Systems	
,
V olume	
14,	
Number	
2.	
April	
2015.	
Ar ticle.	
â	
T
homas	
Zachariah	
,
Noah	
Klugman,	
Br adfor d	
Campbell,	
Joshua	
Adkins,	
Neal	
Jackson,	
Pr abal	
Dutta.	
The	
Internet	
of	
Things	
Has	
a	
Gatewa y	
Pr oblem	
.
In	
Pr oceedings	
of	
the	
16th	
W orkshop	
on	
Mobile	
Computing	
Systems	
and	
Applications.	
HotMobile	
â15.	
Santa	
F e,	
New	
Mexico,	
USA.	
F ebruar y	
2015.	
W orkshop.	
â	
T
homas	
Zachariah	
,
Rafael	
Isaac,	
Lilian	
Kim,	
Norber to	
Rios.	
De velopment	
of	
Space	
W eather	
Monit oring	
System	
and	
Statistical	
Study	
of	
GPS	
Scintillations	
.
In	
American	
Geophysical	
Union	
F all	
Meeting	
2012	
.
A GU	
â12.	
San	
F rancisco,	
California,	
USA.	
December	
2012.	
P oster .	
â	
Hy omin	
Kim,	
Kshitija	
Deshpande,	
Calvin	
R	
Clauer ,	
Gar y	
S	
Bust,	
Geoffr ey	
Cr owle y,	
T odd	
E	
Humphr eys,	
Lilian	
Kim,	
Mar c	
Lessar d,	
Allan	
T	
W eather wax,	
Thomas	
Zachariah	
.
Ionospheric	
Irr egularities	
at	
High	
Latitudes	
During	
Geomagnetic	
St orms	
and	
Subst orms:	
Simultaneous	
Obser vations	
of	
Magnetic	
Field	
P er turbations	
and	
GPS	
Scintillations	
.	
In	
American	
Geophysical	
Union	
F all	
Meeting	
2012	
,
A GU	
â12.	
San	
F rancisco,	
California,	
USA.	
December	
2012.	
P oster .	

H ON ORS	

â	
NSF	
GRFP	
Honor able	
Mention,	
2015	
â	
Magna	
Cum	
Laude	
â	
Lo yola	
Mar ymount	
Univ ersity ,	
2013	
â	
T au	
Beta	
Pi	
(Engineering	
Honor	
Society),	
since	
2011	
â	
Bo y	
Scouts,	
1999-2009	
â	
Eagle	
Scout	
&	
Br otherhood	
Member	
of	
Or der	
of	
the	
Arr ow	
(Bo y	
Scout	
Honor	
Society) 

"
resume," 
DANIEL	 	
BLASKO	 	 	
 	 	
 	 	
daniel.blasko@	insa	-	lyon	.fr	 	
 	
/in/daniel	-	blasko	 	  	
 	
EDUCATION	 	
MSc. 	-	 	Data Science 	| 	University of Technology Vienna, Austria 	 	 	 	| 	202	2	-	202	4	 	
MEng. 	-	 	Computer Science	 	| 	National Institute of Applied 	Sciences	 Lyon	, France 	 	|	 	2020	-	2023	 	
Higher National Diploma	 	-	 	Computer Science	 	|	 	IUT Lyon 1	, France	 	 	 	| 	2018	-	2020	 	
Baccalaureate	 	in 	S	ciences	 	â	 	with distinction	 	| 	Jean Sturm, S	trasbourg	, France	 	 	| 	201	6	-	2018	 	
 	
PROFESSIONAL 	E	XPERIENCE	S	 	
INTERDISCIPLINARY PROJECT 	| 	IFT TU Wien	 	â	 	Vienna, Austria	 	 	 	| 	MARCH	-	JULY	 	2023	 	
Research  project  at  the  institute  for  manufacturing  engineering  and  photonic  technologies	.	 	Experimental 	
evaluation of industrial machine data extraction methods using different OPC	-	UA endpoint implementations 	
and industrial edge devices on Siemens and FANUC machines. 	 	
DATA SCIENCE	 	FOR IoT SECURITY 	â	 	INTERN 	| 	Cisco 	-	 	Lyon, France	 	| 	MAY	-	SEPT. 202	2	 	
On	-	premise network data	-	analysis	 	and modelling	 	for identification and tracking of components on industrial 	
IoT  networks  in  the  Cisco  Cyber  Vision  product	.  Created  client	-	dataset  analysis  and  reporting  tools  and 	
integrated machine	-	learning algorithms in the product for network component classification and clustering.	 	
CONSULTANT	 	| 	ETIC INSA Lyon (Junior business)	 	-	 	Lyon, France	 	| 	2020	â	2022	 	
Understanding	 and formalizing client needs, conceiving and implementing mock	-	ups, exchanging with clients 	
to support them in their IT projects.	 	 	
MOBILE DEVELOPMENT INTERN	 	| 	Worldline	 	Global	 	-	 	Lyon, France	 	| 	JUNE	-	AUGUST 2021	 	
Cross	-	platform  development  in  Flutter  for 	two 	mobile  banking  applications	.	 	Analyzed	 	the 	current 	in 	a 	
presentation and written article synthetizing the 	tradeoffs 	of a transition to Flutter for new projects.	 	 	
SOFTWARE DEVELOPMENT INTERN	 	| 	NatBraille	 	&	 	LIRIS lab	 	-	 	remote	 | 	APRIL	-	JULY	 	2020	 	
Modelled	, 	specified	,	 and 	implemented 	a pedagogic	al	 web application to teach Braille.	 O	rganized and	 led user 	
interviews,  specified  the  requirements  and  modelized  the  architecture. 	Implemented  Braille  application	 	in 	
PHP and JavaScript, with 	key	 	emphasis on 	web 	accessibility.	 	
R&D	 	INTERNSHIP	 	| 	Transchain	 	-	 	Strasbourg	, France	 	 	 	 	| 	JULY	-	AUGUST	 	2019	 	 	
Developed an API, multiple GUIs and different system scripts in Golang for Transchainâs blockchain	 in a scrum	-	
based environment	. 	Heavy	 usage of Docker	 containerization and creation of multiple continuous integration 	
scripts. 	 	Ensured	 	a high test	-	coverage for 	every project	. The 	created 	tools are used by clients & internally.	 	
 	
SKILLS	 	
 	
 
 
 
 	
 	
 	
Languages	:	 	
Python	 	 	
R	 	(	tidyverse	)	 	
Prolog 	(	fuzzy logic	)	 	 	
Java	: 	Swing, Spring	 	   	 	
Android	 	 	
Dart 	(	Flutter	)	 	
JavaScript,	 	PHP	 	
C	, C++	 	(system	s	 	& 	 	
parallel 	programming)	 	
Systems:	 	
Linux	 	
Windows	, 	
MacOS	 	
Docker 
co	ntainerization	 	 	
Deployment	:	 	
Gitlab CI, Jenkins 
CI, Github 
actions	 	
Data Science tools	:	 	
Frameworks	: 	Practical 	
experience with Scikit	-	
learn, Keras, PyTorch, 
OpenCV & 	data 	
visualization libraries	.	 	
Data intensive 
computing	: 	AirFlow, 	
Hadoop, Spark	.	 	
Knowledge graphs	: 	
experience with ProtÃ©gÃ©, 
OpenRefine, GraphDB, KG 
embeddings	.	 	
 
 	
Databases	:	 	
SQL, PLSQL	, 	MongoDB	 	
Computer 	Network	s	: 	 	
General	 	networking 	theory	, 	
experience with	 	industrial 	
networking protocols.	 	
Languages:	 	
French	 	â	 	Native, C2	 	
Slovak 	â	 	Native, C2	 	
English	 	â	 	TOEIC 990	, C1	 	
German	 	â	 	DSD2, C	1	 	certificate	 	
 
 
  

"
resume,"RÂ´obert CsordÂ´as
e-mail: robert@idsia.ch tel: +41 76 4961457 website:
https://robertcsordas.github.io/
EDUCATION IDSIA http://idsia.ch, Lugano, Switzerland
PhD student 2018 April - 2023 September
Supervised by Prof. JÂ¨urgen Schmidhuber. Worked on systematic generalization.
Budapest University of Technology and Economics , Budapest, Hungary
Electrical Engineering. MSc (grad. 2015) and BSc (grad. 2012). Grade: excellent.
WORK
EXPERIENCE IDSIA
http://idsia.ch 2023 October -
Postdoctoral Researcher Lugano, Switzerland
Working on systematic generalization.
DeepMind -https://www.deepmind.com 2022 June - 2022 October
Research Scientist Intern London, United Kingdom
I worked on graph neural networks, improving generalization on algorithmic problems
and external memory for Transformers.
AImotive (formerly AdasWorks) -https://aimotive.com 2015-2018
AI Research Scientist Budapest, Hungary
Worked on deep neural networks for self driving cars. â¢Monocular depth prediction using neural networks.
â¢ Neural stereo matching - predicting robust depth map with neural network.
â¢ Recurrent network research - Convolutional LSTMs for stabilizing detections,
free space detection, etc.
â¢ Ob ject detection, semantic segmentation
Hungarian Academy of Sciences - Institute for Computer Science and
Control -https://www.sztaki.hu 2015
Software Engineer Budapest, Hungary
Worked on classical computer vision pro jects. For example: â¢Detecting ob jects thrown over the fence; detecting human leaving a car.
â¢ Autonomous forklift control system.
Innomed Medical Inc. -http://innomed.hu 2007 - 2015
Embedded Software/Hardware Engineer Budapest, Hungary
â¢Designed the software architecture of Linux based patient monitor (C++, QT).
â¢ Maintained the software of the InnoCare-S patient monitor (C++).
â¢ Wrote low level hardware drivers for InnoCare-T12.
PUBLICATIONS Kazuki Irie*, RÂ´obert CsordÂ´as *, JÂ¨urgen Schmidhuber:
Topological Neural Dis-
crete Representation Learning `a la Kohonen - We show that VQ used in VQ-
VAEs is a special case of SOMs, which are more robust and converge faster.
arXiv preprint https://arxiv.org/abs/2302.07950
Anian Ruoss, GrÂ´egoire DelÂ´etang, Tim Genewein, Jordi Grau-Moya, RÂ´obert CsordÂ´as ,
Mehdi Bennani, Shane Legg, Joel Veness: Randomized Positional Encodings 

Boost Length Generalization of Transformers
- We propose randomized, or-
dered positional encodings to improving length generalization on algorithmic tasks.
ACL 2023 https://arxiv.org/abs/2305.16843
RÂ´obert CsordÂ´as , Kazuki Irie, JÂ¨urgen Schmidhuber:
CTL++: Evaluating Gener-
alization on Never-Seen Compositional Patterns of Known Functions, and
Compatibility of Neural Representations - We extend the CTL dataset to test
systematicity and show how NNs develop incompatible representations and fail to
generalize.
EMNLP 2022 https://arxiv.org/abs/2210.06350
Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani,
RÂ´obert CsordÂ´as , Andrew Dudzik, Matko BoËsnjak, Alex Vitvitskyi, Yulia Rubanova,
Andreea Deac, Beatrice Bevilacqua, Yaroslav Ganin, Charles Blundell, Petar VeliËckoviÂ´c:
A Generalist Neural Algorithmic Learner - We show that graph neural networks
are capable of learning many algorithms together and they can generalize to larger
problem instances.
LoG 2022 https://arxiv.org/abs/2209.11142
Kazuki Irie*, RÂ´obert CsordÂ´as *, JÂ¨urgen Schmidhuber:
The Dual Form of Neural
Networks Revisited: Connecting Test Time Predictions to Training Pat-
terns via Spotlights of Attention - We investigate dual form representations of
NNs to get insights into how their behaviour depends on the training samples.
ICML 2022 https://arxiv.org/abs/2202.05798
Kazuki Irie, Imanol Schlag, RÂ´obert CsordÂ´as , JÂ¨urgen Schmidhuber:
A Modern Self-
Referential Weight Matrix That Learns to Modify Itself
ICML 2022 https://arxiv.org/abs/2202.05780
RÂ´obert CsordÂ´as , Kazuki Irie, JÂ¨urgen Schmidhuber:
The Neural Data Router:
Adaptive Control Flow in Transformers Improves Systematic Generaliza-
tion - We propose to improve data routing in Transformers by gating and geometric
attention, achieving systematic generalization on algorithmic tasks.
ICLR 2022 https://arxiv.org/abs/2110.07732
RÂ´obert CsordÂ´as , Kazuki Irie, JÂ¨urgen Schmidhuber:
The Devil is in the Detail:
Simple Tricks Improve Systematic Generalization of Transformers - We
significantly improve the systematic generalization of Transformers on a variety of
systematic generalization datasets using simple tricks.
EMNLP 2021 https://arxiv.org/abs/2108.12284
Kazuki Irie, Imanol Schlag, RÂ´obert CsordÂ´as , JÂ¨urgen Schmidhuber:
Going Beyond
Linear Transformers with Recurrent Fast Weight Programmers - We we
explore the recurrent Fast Weight Programmers (FWPs), which exhibit advantageous
properties of both Transformers and RNNs.
NeurIPS 2021 https://arxiv.org/abs/2106.06295
RÂ´obert CsordÂ´as , Sjoerd van Steenkiste, JÂ¨urgen Schmidhuber:
Are Neural Nets
Modular? Inspecting Functional Modularity Through Differentiable Weight
Masks - We develop a method for analyzing emerging functional modularity in neu-
ral networks based on differentiable weight masks and use it to point out important
issues in current-day neural networks.
ICLR 2021 https://openreview.net/forum?id=7uVcpu-gMD 

RÂ´obert CsordÂ´as , JÂ¨urgen Schmidhuber:
Improving Differentiable Neural Com-
puters Through Memory Masking, De-allocation, and Link Distribution
Sharpness Control - Addresses 3 different issues with the original DNC architec-
ture. Also proposes a new, better content-based lookup mechanism.
ICLR 2019 https://openreview.net/forum?id=HyGEM3C9KQ
RÂ´obert CsordÂ´as , LÂ´aszlÂ´o Havasi, and TamÂ´as SzirÂ´anyi:
Detecting ob jects thrown
over fence in outdoor scenes - A new technique for detecting ob jects thrown over
a critical area of interest in a video sequence made by a monocular camera.
VISAPP 2015 http://goo.gl/ZDkk4g
WORKSHOP
PAPERS Kazuki Irie, Imanol Schlag, RÂ´obert CsordÂ´as , JÂ¨urgen Schmidhuber:
Improving Base-
lines in the Wild
NeurIPS 2021 DistShift https://openreview.net/forum?id=9vxOrkNTs1x
HIGH SCHOOL
PUBLICATIONS CallTheTux
- Development of CallTheTux, a universal GSM stack for Linux.
Petnica Papers, 2007 https://goo.gl/QTCy5U
RealVM - Development of a new type of virtual machine which would allow parallel
execution and fast switching between different operating systems.
Petnica Papers, 2006 https://goo.gl/8TNHf5
PrologAPI - Enabling the usage of Prolog constructs from C++.
Petnica Papers, 2005 https://goo.gl/KpV3sF
PATENTS RÂ´obert CsordÂ´as ,
Â´
Agnes Kis-Benedek, BalÂ´azs Szalkai: Method and Apparatus for
Generating a Displacement Map of an Input Dataset Pair - A neural network
based method for fast and robust stereo matching for depth map generation.
US10380753 https://pimg-fpiw.uspto.gov/fdd/53/807/103/0.pdf
TECHNICAL
STRENGTHS Python, PyTorch, TensorFlow, C, C++, CUDA, OpenCV, Algorithms, Linux, JavaScript,
Bash, Matlab, Assembly
OTHER
SKILLS Machine learning frameworks:
PyTorch, JAX, TensorFlow, Torch
Parallel programming: CUDA, numba
Electronics: KiCAD, Eagle, PIC, PIC32, AVR, AVR32, ARM, XMOS, Xilinx
Databases: MySQL, MongoDB, Sphinx search
JavaScript technologies: NodeJS, jQuery
Mobile development: Android, iOS (Swift)
Operating systems: Linux, OS X, Windows
Markup languages: LA
T
E X, XML, Markdown
HOBBY
PROJECTS MobilECG II
-https://github.com/robertcsordas/MobilECG-II 2014 - 2016
Open source Holter ECG. Designed the schematic diagram and the firmware.
engineerjs.com -http://engineerjs.com 2013 - 2015
Extendable online computing environment for engineers, with physical quantity, com-
plex numbers and linear algebra support.
LANGUAGES Hungarian (native); English, Serbian (fluent); Italian (intermediate); German (be-
ginner) 

"
resume," 
RO M AN K IS IL
Â  Â 
Fu ll- S ta ck J a va Sc rip t /  T y p eSc rip t D eve lo p er &  D evO ps E n g in e er
Â 
Â 
Exp erie nc e d  S o ft w are  E n g in e e r w it h  a  s tro ng  p a ssio n f o r
Â  Â 
Nod eJS , S y ste m s A rc hit e ctu re  &  S e curit y ,  O pen-S o urc e , L e a rn in g  a nd  M ento rin g .
Â 
Â 
ro m an@ kis il. c od es
â â  
âhttp s:/ /
âkis il. c od es 
ââ â lin ke d in .c om /in /ro m ankis il  
ââ 
âgit h u b .c om /n o d efu l
Â 
Â 
KEY   S K IL LS
Â  â¢ Maste r a t 
âJa va Sc rip t
â a nd  
âTy p eSc rip t
â p ro g ra m min g  la ng ua ges.
Â 
â¢ Adva nc e d  e xp ert is e  w it h  b uild in g  lo w  la te nc y 
âsc a la ble  M ic ro se rv ic e s 
âand
â R ES Tfu l
Â 
APIs  
âon t h e  
âClo ud  
â usin g
â N od eJS , D ocke r C onta in e rs  
âand
â K ub ern e te s
â. Â 
â¢ Exte nsiv e  e xp erie nc e  D esig nin g  a nd  Im ple m entin g  c o m ple x 
âSy ste m  a nd
Â 
Netw ork in g  A rc hit e ctu re s
Â 
â¢ Exc e lle nt k n o w le d ge o f 
âAng ula r, Io nic , E le c tr o n
â F ro nt-e nd  F ra m ew ork s.
Â 
â¢ Exp erie nc e  o f u sin g  a nd  m ain ta in in g  s to ra ge s o lu tio ns lik e :
Â 
    N oSQ L: M ong oD B,  R e d is  
âand
â S Q L: M ySQ L, P o stg re SQ L,  T im esc a le D B
Â 
â¢ Contin uo us In te g ra tio n &  D eliv e ry
â u sin g  s e rv ic e s lik e  G it L a b,  T ra vis -C I  o r J e nkin s
Â 
â¢ Ba ckg ro un d  in  w rit in g  
âUnit  T e st
â w it h  
âKa rm a
â a nd  
âJa sm in e
Â 
â¢ Not a  s tra ng er t o  m any o th e r o b je ct o rie nte d  p ro g ra m min g  la ng ua ges,
Â 
pa rt ic ula rly  
âSw if t
â a nd  
âObje ctiv e -C
â. Â  Â 
â¢ Stro ng  
âAlg orit h m  im ple m enta tio n
â a nd  
âop tim is a tio n
â s k il ls .
Â 
â¢ Exc e lle nt c o d in g  s ty le  a nd  
âso urc e c o ntr o l
â h a bit s .
Â  Â 
â¢ Ta le nt t o  p ic k u p  n e w  s u b je cts , t e chn o lo g ie s a nd  s c ie nc e s in  a  b re eze .
Â 
Â 
Â 
RELE V A NT
Â 
WORK
Â 
EX PER IE N C E
Â  Â 
Â 
â Le a d o f B a ck-e nd  S o ft w are  E n g in e erin g  &  D evO ps
â a t 3 7x L td .
Â 
             F e b ru a ry  2 018 -  P re se nt
Â 
Â 
At 3 7x w eâv e  b uilt  a n A dTe ch 
âSa aS p ro d uc t
â f o r M ed ia  P u b lis h e rs  a nd  A dO ps T e a m s.
Â 
37x is  a  G re enfie ld  S ta rt -u p  w it h  a  s m all  d eve lo p m ent t e a m . H avin g  a  s m all  t e a m  h a s p ush e d  u s t o
Â  Â 
auto m ate  a nd  o p tim iz e  o ur w ork flo w  a nd  o p era tio ns t o  t h e  v e ry  e d ge. W e u se  
âKub ern e te s C lu ste rs  
âon
Â  Â 
Goog le  C lo ud  P la tfo rm
â t o  a uto m atic a ll y  s c a le  o ur S e rv ic e s in  o rd er t o  a d apt t o  o ur c lie nt's  d em and s.
Â 
At 3 7x I  a m  r e sp onsib le  f o r:
Â 
  â¢  
âMic ro se rv ic e S y ste m  A rc hit e ctu re
Â 
  â¢  D eve lo p in g  
âAPIs
â a nd  b a ck-e nd  s e rv ic e s u sin g  
âTy p eSc rip t
â a nd  
âNod eJS
Â 
  â¢  M ana gin g  o ur 
âTim esc a le D B
â d ata ba se  c a pa ble  in g estin g  
âbillio ns o f r o w  p er d ay
Â 
  â¢  H ostin g  a ll  S e rv ic e s o n 
âKub ern e te s
â w it h  
âDocke r
Â 
  â¢  S e curin g  c o m mun ic a tio ns a nd  c lie nt d ata  w it h  t h e  la te st e nc ry p tio n t e chno lo g ie s
Â 
  â¢  D eve lo p in g  &  M ain ta in in g  o ur G it L a b 
âCI/ C D
â P ip elin e s
Â 
  â¢  P ro je ct M ana gem ent
Â 
Â 
Â 
â Le a d F u ll- S ta ck W eb  D eve lo p er
â a t 3 0 m  L td .
Â 
             F e b ru a ry  2 017 -  F e b ru a ry  2 018
Â 
Â 
I'v e  jo in e d  t h e  3 0m  t e a m  t o  a uto m ate  r e d un d ant w ork . P re vio usly , t h e  t e a m  h a d  t o  m anua lly  c o lle ct
Â  Â 
Fin a nc ia l  In fo rm atio n f ro m  a  d oze n d if fe re nt s o urc e s t o  a na ly se  a nd  m ake  c ru c ia l  b usin e ss d ecis io ns.
Â  Â 
I'v e  u se d  
âNod eJS
â, 
âTy p eSc rip t
â a nd  
âKub ern e te s
â t o  d eve lo p  a  f u ll y  a uto m ate d  s y ste m  t h a t u se d  a va ila ble
Â  Â 
API's
â a nd  
âSc ra ped
â a ll  t h e  in fo rm atio n n e ed ed  in  a  m atte r o f s e co nd s, A na ly se d  t h e  d ata  a nd  s h o w n it  in
Â  Â 
th e  m ost c o nve nie nt w ay s o  t h a t t h e  c o m pa ny c o uld  m ake  m uc h q uic ke r d ecis io ns a nd  g a in  a
Â  Â 
co m petit iv e  a d va nta ge. I  t h e n w ent a  s te p  f u rt h e r a nd  d eve lo p ed  a  
âre a l- tim e A na ly tic s E n g in e
â t h a t
Â  Â 
accura te ly  p re d ic te d  o ur A dve rt is e m ent R e ve nue  a nd  a uto m atic a lly  a d ju ste d  o ur S p end .
Â  Â 
Th is  a llo w ed  o ur b usin e ss t o  in sta ntly  a d ju st t o  t h e  m ark e t a nd  h a ve  a  c o nsis te nt r e tu rn  o n in ve stm ent.
Â  

 
â Fu ll- S ta ck W eb  D eve lo p er
â a t P o te ntia l  In ve stm ents  L td .
Â 
             N ove m ber 2 016 -  F e b ru a ry  2 01 7
Â 
Â 
I  w as in it ia lly  h ir e d  b y P o te ntia l  t o  d eve lo p  m ob ile  a pps f o r t h e ir  e xis tin g  a nd  f u tu re  p ro je cts . A ft e r a
Â 
co up le  o f m ob ile  &  w eb  a pps w ere  r e le a se d  I  w as p ro m ote d  t o  a  L e a d  D eve lo p er f o r o ne  o f t h e  in te rn a l
Â 
te a m s a t P o te ntia l.  M y n e w  r o le  r e vo lv e d  a ro un d  c re a tin g  a  n e w  s e t o f F in a nc ia l,  A na ly tic a l  a nd
Â 
Re p ort in g  t o ols  f o r t h e  t e a m .
Â 
At P o te ntia l  I  le a rn e d  a  lo t a bout t h e  
âClo ud
â, 
âContin uo us In te g ra tio n /  D eliv e ry
â, 
âDevO ps
â, 
âKub ern e te s
â a nd
Â 
build in g  s c a la ble  s o lu tio ns t h a t g ua ra nte e d ata  in te g rit y .
Â 
Â 
Â 
â Fu ll- S ta ck S o ft w are  E n g in e er
â a t P u b lic  S e cto r S o ft w are  L td .
Â 
             S e p te m ber 2 015 -  O cto b er 2 016
Â 
Â 
At "" P u b lic  S e cto r S o ft w are  L td .""  I  w as r e sp onsib le  f o r d eve lo p in g  a nd  m ain ta in in g  a  s e t o f
Â  Â 
Mob ile  ( iO S &  A nd ro id ), W eb  A ppli c a tio ns a nd  
âAPIs
â ( u sin g  
âAng ula rJ S
â, 
âIo nic
â, 
â Nod eJS
â a nd  
âSQ L
â) Â 
Â 
I  w as a ls o  m ain ta in in g  a n e xis tin g  S Q L D ata ba se  w it h  m ore  t h a n 
â100M  H ig hly  r e la tio na l  r o w  e ntr ie s
â. Â 
Whe n I  jo in e d  t h e  c o m pa ny I  w as w ork in g  a lo ng sid e a  v e ry  e xp erie nc e d  S e nio r D eve lo p er,
Â  Â 
fro m  w ho m  I  a cq uir e d  a  lo t o f e xp erie nc e  in  S e curit y , S c a la bilit y  a nd  D ata ba se  M ana gem ent.
Â 
Aft e r s ix  m onth s, I  h a d  t o  r e p la ce  h im  a nd  t a ke  t h e  p osit io n o f a  L e a d  D eve lo p er.
Â 
Â 
Â 
PER SO NAL
Â 
PR O JE C T
Â 
PO RTF O LIO
Â  Â 
Durin g  t h e  p a st 5  y e a rs  I  d eve lo p ed  a  c o up le  o f in d iv id ua l  a nd  c o m merc ia l  p ro je cts .
Â 
Ple a se  v ie w  s o m e o f m y O pen-S o urc e d  c o d e o n 
âhtt p s:/ /g it h ub .c o m /n o d efu l
Â 
Here  is  a  q uic k b rie f a bout t h e  a pplic a tio ns Iâ v e  w ork e d  o n:
Â 
Â 
eq M ac
â â  is  a n O pen-S o urc e  S y ste m -W id e A ud io  E q ua liz e r f o r t h e  m acO S P la tfo rm . F ir s t v e rs io n w as
Â 
deve lo p ed  in  m id  2 015 u sin g  
âC++
â a nd  
âObje ctiv e -C
â  a nd  r e le a se d  t o  t h e  p ub lic  a s a  f re e d ow nlo a d  f ro m
Â 
my w eb sit e . 
âSo m eho w  "" e q M ac"" g ot v e ry  p op ula r, I  m ana ged  t o  g et 1 20 k+  d ow nlo a d s ju st f ro m  m y
Â 
web sit e , w hic h p ush e d  m e t o  d eve lo p  a  s e co nd  it e ra tio n o f t h e  a pp.
Â 
""e q M ac2"" w as r e le a se d  in  J u ly  2 017 a s a n o p en-s o urc e  p ro je ct a nd  o n t h e  t im e o f w rit in g  h a s 
â380k +
Â 
dow nlo a ds
â a nd  
â1.2 k +  s ta rs  o n G it H ub
â. Â  Â 
Curre ntly , I  a m  in  a  p ro ce ss o f r e w rit in g  e q M ac in  
âSw if t
â in to  a  n e w  a nd  im pro ve d  v e rs io n a nd  m akin g  it
Â 
co m merc ia lly  s u c ce ssfu l
â. Y o u c a n c he ck o ut e q M ac2 b y g oin g  t o  
âhttp s:/ /b it g ap p.c o m /e q m ac
Â 
Â 
Â 
ED UC ATIO N
Â  Â 
Fir s t C la ss B a che lo r o f E n g in e erin g  d eg re e
Â  Â 
âC om pute r H ard w are  a nd  S o ft w are  E n g in e erin g â
Â 
Cove ntry  U niv e rs it y  ( C ove ntry , U K) 2 012-2 015
Â 
Su b je cts  s tu d ie d :
Â 
Â 
â Obje ct O rie nte d  P ro g ra m m in g
Â 
â Com pute r A rc hit e ctu re
Â 
â iO S D eve lo p m ent
Â 
â Netw ork in g  A rc hit e ctu re
Â 
â Dig it a l  S y ste m s,
Â 
â Ele ctric a l  E n g in e erin g
Â 
â En g in e erin g  M ath e m atic s
Â 
Â 
Â 
ABO UT M E
Â  Â 
I  a m  
âve ry
â  âpassio na te
â a bout S o ft w are  D eve lo p m ent. I  c o d e d urin g  t h e  d ay, e ve nin g
Â 
and  n ig ht. T h ro ug ho ut m y li f e  I  h a ve  b een a lw ays k n o w n t o  h a ve  
âouts ta nd in g  lo g ic a l
Â 
th o ug ht p ro cess .
Â 
I  w ould  s a y m y b est c ha ra cte ris tic  is  m y a bilit y  t o  
âta ke  o n a bso lu te ly  a ny c ha lle ng e
â,  n o  m atte r t h e
Â 
co m ple xit y  o r e xp erie nc e  I  h a ve  in  t h e  f ie ld . I  n e ve r f e el  a fra id  t o  t ry  s o m eth in g  n e w , a s it  o nly  e xc it e s m e.
Â 
I  a m  a  
âstr o ng  t e a m  p la ye r
â. U su a lly  I  t a ke  o n a  r o le  o f t h e  
âte a m  le a der
â. H ow eve r, I  a lw ays w elc o m e p eo p le
Â 
wit h  m ore  e xp erie nc e  a nd  in sig ht t o  t a ke  c o ntro l  o ve r t h e  t e a m  in  o rd er t o  a chie ve  b ette r r e su lt s .
Â 
I  c a n e a sily  w ork  u n d er t im e p re ssu re  a nd  p ro d uc e  g ood  q ua lit y  r e su lt s  a t t h e  s a m e t im e.
Â 
I  liv e  in  S u tto n C old fie ld , r id e m oto rc yc le s a nd  f lu e nt in  3  la ng ua ges.
Â  

"
resume,"George Sun
STARTUP FOUNDER & PHD, BiOLOGiCAL ENGiNEERiNG
New Lab, 19 Morris Ave. Bldg. 128, Brooklyn, NY, 11205
ï(310) 985-5901 |
ï george.lele.sun@gmail.com
| ïwww.mrsunny.tech
| ïmrsunny0
| ïgeorge-lele-sun
| ïGeorge L. Sun
Work Experience
Nextiles, Inc. Brooklyn, NY
CEO Jan. 2020 â Present
â¢
Secured >$1M on nonâdilutive grants, preâseed round, and established a hardware manufacturing space in Brookyln, NYC.
â¢
Collaborated with several industries in the textile, sports apparel, and medical technologies sectors.
â¢
Led the business strategy and oversaw financials to guide the company into a sustainable and profitable trajectory.
COâFOUNDER & CTO June 2019 â Dec 2019
â¢
Tasked with developing new wearable technologies based on fabricâbased sensors. Sensors are built to measure force directly from compresâ
sion/bending of threads.
â¢
Patented 3 unique inventions on the design, manufacturability, and application of fabricâbased sensors, all approved by the USPTO.
â¢
Completed programs such as MIT Delta V accelerator, Hubweek Demo Day, NSF IâCorps, and several other pitch and showcase events.
FOUNDER April 2018 â May 2019
â¢
Founded Nextiles , a startup built on reâthinking wearable technologies through advanced sewing technologies.
â¢
Inspired from past work in electrical engineering and material science, and motivated to merge the two together in unique ways in fabric.
Puma XDesign Lab
Cambridge & Nuremberg
LEAD EMBEDDED ENGiNEER Jan. 2017 â Jan. 2018
â¢
Led a team of engineers and designers with Pumaâs Innovation Team to redesign and instrument their line of athletic shoes
â¢
Focused on embedding forceâsensitive materials into the shoe to track gait and power using machine models.
Communication Lab, MIT Cambridge, MA
COMMUNiCATiON FELLOW & INSTRUCTOR Jun. 2015 â May. 2019
â¢
Facilitated workshops and seminars on effective communication and scientific presentation.
â¢
Worked with MITâs GELprogram and helped teach
Leading Creative Teams while developing course content for MIT.
Education
Ph.D. Biological Engineering Cambridge, MA
MASSACHUSETTS INSTiTUTE OF TECHNOLOGY, GPA 4.90/5.00 Aug 2014 â June 2019
B.S. Biological Engineering and
Electrical Engineering & Computer Science
Berkeley, CA
UNiVERSiTY OF CALiFORNiA BERKELEY, GPA 3.96/4.00 Aug 2010 â 2014
Research Experience
Biomolecular Materials Group, MIT Cambridge, MA
PHD, GRADUATE RESEARCHER â BELCHER LAB Sept. 2014 â June. 2019
â¢
Engineered yeast as a bioremediation agent to consume and recycle heavy metals, particularly from electronics and mining runoff.
â¢
Authored and coâauthored several research articles in Nature, with several utility patents emerging from this unique invention.
â¢
Utilized laboratory techniques ranging from material science (ICP, EDX, XRD), molecular biology (PCR, genetic circuits, transformations), chemâ
istry (chromatography, electrochemistry), and analytical tools (matplotlib, scikitâlearn, tidyverse).
â¢
Awarded several grants (NSF, Bose, CEHS) and honors for scientific talks and presentations on environmental remediation technologies.
Molecular Engineering Imaging and Control Group, Berkeley & Caltech Berkeley & Pasadena, CA
RESEARCH ASSiSTANT â SHAPiRO LAB Jan. 2011 â Aug. 2014
â¢
Conducted research in biomolecular tools such as stem cell therapy and biological contrast agents for medical imaging.
â¢
Independently researched the effects of metalloâenzymes on enhancing the magnetic properties of neurological systems for NMR and MRI.
â¢
Transitioned to Caltechâs Chemical Engineering department in the last year of college to finalize research.
Microfluidics for PointâofâCare Diagnostics Group, Columbia New York, NY
AMGEN RESEARCH SCHOLAR â SiA LAB June 2013 â September 2013
â¢
Created new experimental protocols to create PDMS scaffolds for organoid growth.
â¢
Focused on recreating synthetic extracellular matrixes and vasculature for brown fat and hair follicle growth.
MARCH 31, 2022 GEORGE L. SUN 1 


Publications
ACADEMiC JOURNALS
Pandit, Shalmalee,
Sun, George L., and Angela M. Belcher. âYeast Platform Technology for Sustainable Production of Industrial Molecules.â (2020).
in submission .
Sun, George L. , and Angela M. Belcher. âEngineering supramolecular forming proteins to chelate heavy metals for waste water remediation.â
(2020). in submission .
Sun, George L. , Erin E. Reynolds, and Angela M. Belcher. âUsing yeast to sustainably remediate and extract heavy metals from waste waters.â
Nature Sustainability (2020): 1â9.
Gilbert, C., Tang, T. C., Ott, W., Dorr, B. A., Shaw, W. M., Sun, G. L., ... & Ellis, T. âLiving materials with programmable functionalities grown from
engineered microbial coâcultures.â bioRxiv. (2019).
Sun, George L. , Erin E. Reynolds, and Angela M. Belcher. âDesigning yeast as plantâlike hyperaccumulators for heavy metals.â Nature communiâ
cations 10.1 (2019): 1â12.
Shapiro, M. G., Ramirez, R. M., Sperling, L. J., Sun, G., Sun, J., Pines, A., ... & Bajaj, V. S. (2014). âGenetically encoded reporters for hyperpolarized
xenon magnetic resonance imaging.â Nature chemistry6.7 (2014): 629.
WEB PUBLiCATiONS
Sun, George L. . âFile Structureâ. Mechanical Engineering Communication Lab, MIT . (2019).
https://mitcommlab.mit.edu/meche/commkit/file- structure/ .
McLean, K., Peters J., Ramamoorthy, D., Sun, G., Toth T., Triassi A., Prerna B. âAwesome BECL Resourcesâ. Biological Engineering Communication
Lab, MIT . (2019).
https://github.com/MIT- BECL/awesome- becl- resources .
Sun, G. , Wang, D., Gerarld, K. âAir Guitarâ. Instructables. (2016).
https://www.instructables.com/id/Air- Guitar/ .
Patents
Sun, George L. . âConnectors for integrating conductive threads to nonâcompatible electromechanical devices.â US Patent 17,559,815 (inâreview).
22 December 2022.
Sun, George L. . âDevices for static and dynamic body measurements.â US Patent 10,605,680. 31 March 2020.
Sun, George L. . âMethods of manufacturing devices for static and dynamic body measurements.â US Patent 10,458,866. 29 October 2019.
Sun, George L. . âSystems, methods, and devices for static and dynamic body measurements.â US Patent 10,378,975. 13 August 2019.
Sun, George L. , and Angela M. Belcher. âEngineered yeast as a method for bioremediation.â U.S. Patent 15/887,305. 18 August 2018
Honors & Awards
2021â Curr
Member , Council of Fashion Designers of America New York, NY
2019â Curr
Member , New Lab â Brooklyn Navy Yard New York, NY
2019 Member , Delta V Accelerator New York, NY
2019 Member , NSF IâCorps Program Philadelphia, PA
2018â2019 Recipient , CEHS Pilot Grant Cambridge, MA
2016â2019 Recipient , Amar G. Bose Research Grant Cambridge, MA
2014â2019 Recipient , NSF Graduate Research Fellowship Program Cambridge, MA
2011â2014 Recipient , IMSD NIH Research Fellow Berkeley, CA
2010â2014 Recipient , Regent & Chancellor Scholarship Berkeley, CA
Skills
Machinery Solder/Reflow, 2â3DoF CNC, Vinyl Cutters, Laser Cutters, 3D Printing, Molding /Casting, Screen Printing, Vacuum Forming
Digital Fabrication Eagle PCB, Fusion CAD/CAM, Techpacker, Multimeter/Oscilloscope, TTL/UART/I 2
C/ISP Communication
Administration Microsoft Suite, GSuite, Airtable, Coda, Slack, Asana, Docsend, Git/hub/lab, GoDaddy, Webflow, Heroku
Programming AndroidOS (Java), iOS (Swift), Javascript (Node.js), Python (Matplotlib, Numpy, Scipy, Pandas, Scikitâlearn, Notebooks),
C (AVR firmware), UNIX (bash, awk, grep, sed), GO, LaTeX
DevOps AWS (EC2, S3, Lambdas, Kinesis, Cognito), GCP (Firebase, Firestore, Authentication), MongoDB Atlas
Backâend Express, MongoDB, Websockets, BLE Stack, REST, CRUD
Frontâend D3.js, Three.js, Leaflet.js, Gulp, Yeoman, HTML5, JQuery, Bootstrap, SCSS, Jekyll
MARCH 31, 2022 GEORGE L. SUN 2 

"
resume," 	
!!!!!!!!!!!!!!	!	
!!!!!!!!!!!!!!!!!!!!!!	SUMMARY	'	
!	!	
!""#$	""a	""h	ardworking	.""	+,/0+&1'""+,'+2+'3#4""(+5%""#	,""1#-1&,1//""	5)""41#&,	.""!""-#+,1'""$7""0#//+),""8)&""	software	""9#:*""+,""	
;<=>.""%#2+,-""/5#&5+,-""3,+21&/+57""#,'""	5%1,""	-&#'3#5+,-""	with a high 2nd Class Honors 	)8""#	""?#/51&/"")8""@41:5&+:#4""	
A""@41:5&),+:""@,-+,11&+,-6""B+,:	e	""then	,""!""%#'"":)	-	8)3,'1'""	built	""#""$)9+41""#004+:#5+),""/5#&530.""	(%+4/5""/5+44	""/5&+2+,-""	
8)&""1D:1441,:1""+,""#"":#&11&""#/""#""B)85(#&1""@,-+,11&6""E851&""91+,-""+,2)421'""+,""$345+041""51#$/""#,'""$#,#-+,-""	5%1""	
'1214)0$1,5"")8""	'+881&1,5""#004+:#5+),/	.""!""#$""4))*+,-""5)""83&5%1&""0&)014""$7/148""+,""5%1""+,'3/5&7""97""5#*+,-""),"",1(""	
#,'"":%#441,-+,-"")00)&53,+5+1/	6""	""	
!	
TECHNICAL	!	SKILLS	!	
!	
â¢	 	ReactJS	(	â¢	 	AWS	(	
â¢	 	NodeJS	(	â¢	 	CI/CD	(	
â¢	 	23'(1""4""5,67""8%	(	â¢	 	9""$:82$#5(;<,=""$%(>#8#?""7""8%(	(	
â¢	 	',@%A#<""()<$:2%""$%B<""(	(	â¢	 	9""$:82$#5(*<2%28?	(	
â¢	 	Docker	((	â¢	 	MongoDB	(	
!	
EXPERIENCE	!	
 
FE@""B7/51$/""E004+1'""!,5144+-1,:1""	â	""B)85(#&1""@,-+,11&	""	""	[	I:5)91&"";<=J	""-	""Current	]	""	
M#2+,-""/5#&51'""#/""#""-&#'3#51""/)85(#&1""1,-+,11&.""!""%#21""0&)-&1//1'""+,""9)5%""51:%,+:#4""#,'""0&)81//+),#4""#/01:5/.""
&1-34#&47""91+,-"":)$$1,'1'""8)&""9)5%""$7""()&*.""#,'""$7""(+44+,-,1//""5)""/%#&1""$7""*,)(41'-1""#,'""1D01&+1,:1""
(+5%""5%)/1""#&)3,'""$16""	N)&*+,-""#9)21""1D01:5#5+)	,/""%#/""41'	""5)""	0&)$)5+),/""#,'""	/121&#4""5%#,*""7)3""#(#&'/.""	
+,:43'+,-"")851,""&1:1+2+,-""5%1""$#D+$3$""&#5+,-""+,""71#&47""01&8)&$#,:1""&12+1(/""8)&""O()&*+,-""%+-%""#9)21""
expectationsâ	6""	""	
â¢	 	E&:%+51:5+,-""#,'""'1214)0+,-""8	ull	-	/5#:*""	applications	""8)&""*17""/5&#51-+:"":4+1,5/.""'14+21&	+,-""5)""21&7""%+-%""	
/5#,'#&'/.""&1:1+2+,-""1D:1441,5""811'9#:*""8&)$""/5#*1%)4'1&/""#,'""$#,#-1$1,5""),""5%1""Q3#4+57"")8""()&*6	""	
â¢	 	K)$0415+),"")8""R&)81//+),#4""S1#:5TB"":)3&/1""5)""91""5%1,""+$041$1,51'""+,"")5%1&""0&)U1:5/	.""	
â¢	 	B3::1//834""$#,#-1$1,5"")8"":)'19#/1/""#,'""'14+21&7"")8""0&)'3:5	+),""-&#'1""81#53&1/	.""/3::1//834""	
&1$1'+#5+),"")8""1#&47""'151:51'"":)'1""234,1&#9+4+5+1/	.""	
â¢	 	E4-)&+5%$""'1/+-,""#,'""#004+:#5+),""#&:%+51:53&1"")8""#""4#&-1	-	/:#41""'#5#""0&):1//+,-""#004+:#5+),6	""	
â¢	 	@D51,/+21""3/1"")8""ENB""#,'""'1214)0+,-""(+5%""ENB""/1&2+:1/""/3:%""#/""@K;.""BVB.""BWB.""X#$9	'#.""@YB.""#,'""	
Z+,I0/""$#,#-1$1,56	""	
â¢	 	M)/5+,-""#,'""$#+,5#+,+,-""93/+,1//	-	(+'1""#004+:#5+),/""8)&""[<<<\"3/1&/""+,""/148	-	""$#,#-1'.""),	-	0&1$+/1""	
servers.	""	
â¢	 	!$041$1,5#5+),"")8""]12I0/""0)4+:+1/.""+,:43'+,-""^R^""#35%1,5+:#5+),""#,'""F+593:*15""/$#&5""%))*/	""	
â¢	 	On	-	0&1$+/1""%)/5+,-""#,'""+,	-	dep	5%""3/#-1"")8""5%1""E54#//+#,""/3+51""/3:%""#/""T+&#.""K),8431,:1.""F+593:*15""15:6	""	
â¢	 	R4#,,+,-""#,'""&3,,+,-""&1-34#&""	on/off	-	/+51""	^!`""5&#+,+,-""/1//+),/""#,'""()&*/%)0/""5)"",1(""/5#&51&/""#,'""	
1#&47"":#&11&/""+,'+2+'3#4/6	""	
â¢	 	S1-34#&""'14+21&7"")8""),	-	the	-	U)9""	5&#+,+,-.""+,:43'+,-	""&1-34#&""	/300)&5""8)&""U3,+)&""#,'""1D01&+1,:1'""	
1,-+,11&/""),""9)5%""	0&)-&#$$+,-""#,'""93/+,1//""*,)(41'-1	.""	
"" 

 
""
V:35""E00""X`]""	-	""Co	-	Z)3,'1&""A""	X1#'""	Developer	""	""	""	""	[	E3-3/5"";<=J	""-	""August	""2021	]	""	
E""8344	-	/5#:*""$)9+41""#004+:#5+),""/5#&530""5%#5""#44)(1'""3/1&/""5)"")&'1&""'&+,*/""5%&)3-%""5%1""	#00""#,'"":)441:5""#5""5%1""	
9#&""(%1,""&1#'7""	â	""(1""/+-,1'""$345+041""9+-"",#$1/""+,""5%1""X11'/"",+-%54+81""+,'3/5&7""935""(1&1""#881:51'""97""KIa!]	-	=b""	
#,'""%#21""/+,:1""/%35""')(,""5%1"":)$0#,76""!,:43'+,-""91+,-""#"":)	-	8)3,'1&.""!""'1/+-,1'""#,'""$#,#-1'""5%1""51:%"")8""5%1""	
1,5+&1""#004+:#5+),	.""	
â¢	 	Built	""#""0&)'3:5+),""&1#'7""$)9+41""#004+:#5+),""8)&""5%1"",+-%54+81""+,'3/5&76	""	
â¢	 	Z344""/5#:*""'1214)0$1,5""3/+,-""S1#:5TB.""B(+85.""I9U1:5+21	-	K.""W)'1TB.""?),-)]F.""#,'""$)&16	""	
â¢	 	N)&*+,-""(+5%""[&'""0#&57""51#$/""5)""$#,#-1""#,'""'14+21&""Y)54+,""9#/1'""#004+:#5+),/6	""	
â¢	 	N)&*1'""(+5%""	+,'3/5&7""1D01&5/""5+1&""=""4#(""8+&$/""5)""$#&*15""#,'""'14+21&"")3&""0&)'3:5""#:&)//""X11'/6	""	
â¢	 	K4+1,5""+,51&#:5+),""8)&""/300)&5""#,'""'14+21&7"")8""5%1""V:35""E006	""	
!	
!
EDUCATION	!	
""	
!""#$%&'#()*+,*-%%.'/*	UK	*	Projects	*	
(	""	
@41:5&),+:""@,-+,11&+,-""c?#/51&/""]1-&11d""	[2014	-	2018]	""	
M+-%"";e=""Z+,#4""-&#'1	""	
J<f\"Z)&""1$91''1'""/7/51$/""#,'""0&)-&#$$+,-	""	
Panacea	""â	""Custom	-	93+45""S39+*/""K391""B)42+,-""	
?#:%+,1""	""	
'E)>(!,F,%	â	""B+$345#,1)3/""X):#4+/#5+),""#,'""	
?#00+,-""	""	
Dissertation	""-	""?#:%+,1""X1#&,+,-""#,'"",13&#4""	
networks	""	
INTERESTS	!	
!""#$""#,"")35-)+,-""	01&/),""(+5%""+,51&1/5/""+,"":)$015+5+21""@,-4+/%""R))4.""1,U)7+,-""4):#4""'&+,*/""(+5%""8&+1,'/.""#,'""	
91+,-""	#""$#//+21""?#&214""-11*6	""!"")851,""0&#:5+:1""/)85(#&1""'1214)0$1,5""+,""$7""/0#&1""5+$1""#,'""()&*+,-""	(+5%"",1(""	
technologies	.""#,'""1,U)7""4),-""(#4*/""#/""9&1#*/""8&)$""5%1"":)$0351&""/:&11,6""""	""	
REFERENCES	!	
E:#'1$+:""S181&1,:1	""	
]&6""K&#+-""E6""@2#,/.""g,+21&/+57"")8""X11'/6	""	
@$#+4e""	C.A.Evans@leeds.ac.uk	""	
""	
Personal	""Reference	""	
?&6""B5121""N#4*1&.""BZN""R&)01&576	""	
@$#+4e""	sfwproperty@gmail.com	""	
""	
! 

"
resume,"!1! !!Curriculum VitÃ¦ MERCIER Denis      Situation professionnelle actuelle Professeur  des  universitÃ©s en  gÃ©ographie  physique Ã  lâUFR  de  GÃ©ographie  et  dâAmÃ©nagement, UniversitÃ© Paris-Sorbonne (Paris IV) depuis le 1er septembre 2015.  Adresse professionnelle : UniversitÃ© Paris-Sorbonne  191, rue Saint-Jacques 75 005 Paris  denis.mercier@paris-sorbonne.fr https://www.researchgate.net/profile/Denis_Mercier  Cursus universitaire 2010  Habilitation  Ã   Diriger  des  Recherches  en  GÃ©ographie,  UniversitÃ©  Blaise Pascal, Clermont-Ferrand II Titre du mÃ©moire inÃ©dit : Â« La  gÃ©omorphologie  paraglaciaire.  Analyse  de  crises  dâorigine  climatique  dans  les environnements englacÃ©s et sur leurs marges Â». Jury :  Marie-Francoise AndrÃ© (universitÃ© Blaise Pascal, Clermont-Ferrand II, garante) Bernard Etlicher (universitÃ© Jean Monet, Saint Etienne, rapporteur) Monique Fort (universitÃ© Denis Diderot, Paris 7, rapporteur) Jean-Luc Peiry (universitÃ© Blaise Pascal, Clermont-Ferrand II, rapporteur) Jean-Pierre Peulvast (universitÃ© Paris-Sorbonne, Paris IV, prÃ©sident) Dominique Sellier (universitÃ© de Nantes, examinateur)  1998  Doctorat de GÃ©ographie, UniversitÃ© Blaise Pascal, Clermont-Ferrand II ThÃ¨se :  Â« Le  ruissellement  au  Spitsberg.  L'impact  d'un  processus  azonal  sur  les  paysages  d'un  milieu  polaire (presqu'Ã®le de BrÃ¸gger 79Â°N) Â». Jury :  Marie-Francoise AndrÃ© (universitÃ© Blaise Pascal, Clermont-Ferrand II, directrice),  Jean-Paul Bravard (universitÃ© Paris IV-Sorbonne, rapporteur) Thierry Brossard (CNRS, BesanÃ§on) Alain Godard (universitÃ© Paris 1 PanthÃ©on-Sorbonne, rapporteur et prÃ©sident) RenÃ© Neboit-Guilhot (universitÃ© Blaise Pascal, Clermont-Ferrand II, examinateur) Dominique Sellier (universitÃ© de Nantes, examinateur)  1993 D.E.A. de gÃ©ographie, UniversitÃ© de Nantes MÃ©moire  :  Â« La  quantification  des  processus  morphodynamiques  agissant  sur  les  versants  dans  les  milieux arctiques : approche mÃ©thodologique et bilan Â» (sous la direction de B. Bousquet et de M.-F. AndrÃ©).  1992 AgrÃ©gation de gÃ©ographie et CAPES dâhistoire-gÃ©ographie  1991 MaÃ®trise de gÃ©ographie, UniversitÃ© de Nantes MÃ©moire  : Â« L'Ã©tagement  des  paysages  dans  les  Monts  Ross,  Highlands  d'Ãcosse Â»  (sous  la  direction  de D.  Sellier).  

!2! 1990 Licence de gÃ©ographie,  UniversitÃ© de Nantes MÃ©moire : Â« L'Ã©boulisation dans la vallÃ©e du Smedbotn, NorvÃ¨ge Â» (sous la direction de D. Sellier).   RÃ©capitulatif de la carriÃ¨re professionnelle  Postes occupÃ©s :   2015 -  Professeur  des  UniversitÃ©s Ã   lâUFR  de  GÃ©ographie  et  dâAmÃ©nagement, UniversitÃ© Paris-Sorbonne (Paris IV)  2011-2015 Professeur  des  UniversitÃ©s Ã   lâInstitut  de  GÃ©ographie  et  dâAmÃ©nagement RÃ©gional de lâUniversitÃ© de Nantes (IGARUN).   Du  14/04/2014  au  05/07/2014 :  chercheur  invitÃ©  par  lâinstitut  polaire  allemand Alfred  Wegener  Institute  Helmholtz-Zentrum  fÃ¼r  Polar  und  Meeresforchung (AWI) Ã  Potsdam.  2009-2015 Directeur  du  Laboratoire  LETG-Nantes-GÃ©olittomer â UMR  6554  CNRS Littoral,  Environnement,  TÃ©lÃ©dÃ©tection,  GÃ©omatique,  Ã©lu  le  10  septembre  2009 pour deux ans, rÃ©Ã©lu le 7 novembre 2011, fin du mandat : 31 aoÃ»t 2015.   2009-2014     Membre Junior de lâInstitut Universitaire de France (IUF).  2009-2011 MaÃ®tre  de  confÃ©rences Ã  lâInstitut de GÃ©ographie et dâAmÃ©nagement RÃ©gional de lâUniversitÃ© de Nantes (IGARUN).  2007-2009 Chercheur  en  dÃ©lÃ©gation  CNRS,  laboratoire LETG-Nantes-GÃ©olittomer,  UMR 6554 Littoral, Environnement, TÃ©lÃ©dÃ©tection, GÃ©omatique.  2006-2007 MaÃ®tre  de  confÃ©rences Ã  lâInstitut de GÃ©ographie et dâAmÃ©nagement RÃ©gional de lâUniversitÃ© de Nantes (IGARUN).  1999 â 2006 MaÃ®tre  de  confÃ©rences Ã   lâUFR  de  GÃ©ographie  de lâUniversitÃ©  Paris  IV â Sorbonne.  1999 â 1998 Professeur  AgrÃ©gÃ© (PRAG)  Ã   lâInstitut  de  GÃ©ographie  et  dâAmÃ©nagement RÃ©gional de lâUniversitÃ© de Nantes (IGARUN).  1994 â 1998 AttachÃ©  Temporaire  dâEnseignement  et  de  Recherche (ATER)  Ã   lâInstitut  de GÃ©ographie et dâAmÃ©nagement RÃ©gional de lâUniversitÃ© de Nantes (IGARUN).  1993 â 1994 Scientifique  du  Contingent Ã  l'Ãcole de SpÃ©cialisation du MatÃ©riel de l'ArmÃ©e de Terre (ESMAT) de ChÃ¢teauroux (36).  1992 â 1993 Professeur  agrÃ©gÃ©  stagiaire au collÃ¨ge RenÃ© Bernier de Saint-SÃ©bastien-sur-Loire et au LycÃ©e des BourdonniÃ¨res de Nantes.     

!3!SynthÃ¨se Enseignement : 23  ans  dâexpÃ©rience, gÃ©ographie  environnementale,  gÃ©ographie  physique (gÃ©omorphologie,  hydrologie,  climatologie),  gÃ©ographie  des  risques,  initiation  Ã   la  recherche, Ã  tous  les  niveaux, encadrement  de  sorties  et  stages  de  terrain, de  la  premiÃ¨re  annÃ©e  de  licence  au doctorat et Ã  la prÃ©paration aux concours.  Encadrement  de  travaux  de  recherche : 7 thÃ¨ses, 9 M2 et 19 M1 ; organisation de 3 colloques internationaux, 5 nationaux, 14 sÃ©minaires.  Direction de laboratoires :  2002-2006 : directeur adjoint du DEPAM (EA 2579 â Paris IV-Sorbonne). 2009-2015 : directeur du laboratoire LETG-Nantes-GÃ©olittomer (UMR 6554).  Distinctions : prix de la SociÃ©tÃ© de GÃ©ographie de Paris (2002).  Reconnaissance internationale :  IUF junior (2009-2014). ConfÃ©rencier invitÃ© Ã  lâEGU (2013) et Ã  lâIAG (2013).  Chercheur invitÃ© par lâinstitut polaire allemand (AWI) Ã  Potsdam (avril-juillet 2014).  Membre de lâEditorial board de la revue Geomorphology depuis janvier 2015. Membre cooptÃ© du comitÃ© exÃ©cutif de lâIAG depuis 2013. Relecteur pour de nombreuses revues internationales.  Production scientifique (1997-2015) Supports de publication Nombre Travaux acadÃ©miques 2 Ouvrages et direction dâouvrages 7 Chapitres dâouvrages 25 NumÃ©ro spÃ©cial de revue Ã  comitÃ© de lecture (dir.) 6 Articles dans des revues Ã  comitÃ© de lecture (ACL) 44 Articles dans des revues sans comitÃ© de lecture (non ACL) 13 Recensions  13 Posters 15 ConfÃ©rences Nombre Communications dans des confÃ©rences internationales 25 Communications dans des confÃ©rences nationales 13 Communications dans des sÃ©minaires 9 ConfÃ©rencier invitÃ© Ã  lâÃ©chelle internationale 3 ConfÃ©rencier invitÃ© Ã  lâÃ©chelle nationale 13      

!4! ActivitÃ©s scientifiques  Encadrement et animation de la recherche  Direction de laboratoire Sur  le  plan  de  mes  responsabilitÃ©s  institutionnelles,  jâai  assumÃ©  de 2009 Ã   2015 la  direction  du laboratoire LETG-Nantes-GÃ©olittomer,  antenne  nantaise  de  lâUMR  6554  CNRS â Littoral Environnement  TÃ©lÃ©dÃ©tection  GÃ©omatique  (LETG).  Ce  laboratoire  compte  en  2015, 45 personnes  (17  enseignants  chercheurs,  2  chercheurs CNRS,  1 ITA CNRS,  24  doctorants,  1 personnel contractuel CNRS). Jâai  Ã©tÃ© impliquÃ©  dans  les  instances  de  lâuniversitÃ©  de  Nantes  au  sein  desquelles  le  laboratoire  est partenaire,  telles  lâInstitut  Universitaire  Mer  et  Littoral  (IUML),  lâObservatoire  des  Sciences  de lâUnivers Nantes Atlantique (OSUNA), la Maison des Sciences de lâHomme Ange GuÃ©pin, Ecole Doctorale DEGEST.   PrÃ©alablement,  jâavais  eu  une  expÃ©rience  de  directeur  adjoint  de  lâÃ©quipe  dâaccueil  DEPAM  de Paris  IV-Sorbonne  de  2002  Ã   2006  et  de  responsable  de  lâaxe  Â« vitesse  de  lâÃ©rosion Â»  au  sein  de GÃ©olab (UMR 6042 â CNRS).   Sur le plan national, je suis trÃ¨s investi au sein du Groupe FranÃ§ais de GÃ©omorphologie (GFG) et j'ai Ã©tÃ© secrÃ©taire de 2008 Ã  2014.  Jâai assumÃ© aussi des fonctions Ã©lectives au sein de lâUFR de gÃ©ographie de Paris IV-Sorbonne de 2000  Ã   2006  et  comme  membre  du  conseil  de  lâAssociation  de  GÃ©ographes  FranÃ§ais  (AGF)  de 2004 Ã  2010, association dans laquelle jâai reprÃ©sentÃ© activement  la gÃ©ographie physique.  De 2014 Ã  2015, jâai Ã©tÃ©  membre Ã©lu du collÃ¨ge A au CA de l'IGARUN.  Organisation de colloques et de confÃ©rences  *  2015 â JournÃ©es des Jeunes GÃ©omorphologues â Groupe FranÃ§ais de GÃ©omorphologie, 29-30 janvier, UniversitÃ© de Nantes. Responsable du comitÃ© dâorganisation. *  2013 â ConfÃ©rence  internationale  Â« Geomorphology  and  sustainability Â»,  27-31 aoÃ»t  2013,  CitÃ©  des Sciences et de l'Industrie, Paris. Membre du comitÃ© dâorganisation de cette confÃ©rence organisÃ©e par le GFG pour lâIAG. * 2013 â Colloque Â« Risques littoraux et maritimes Â», 7 juin 2013, CitÃ© des CongrÃ¨s, Nantes. Responsable du comitÃ© dâorganisation de ce colloque organisÃ© par le Laboratoire LETG-Nantes-GÃ©olittomer dans le cadre des JournÃ©es Scientifiques de l'universitÃ© de Nantes. * 2011 â ConfÃ©rence internationale Â« Mondes polaires sciences environnementales et sociales pour comprendre les changements observÃ©s â Polar worlds Environmental and social sciences to understand observed changes Â», 26-28 janvier  2011,  Centre  National  de  la  recherche  scientifique  (CNRS)  3  rue  Michel-Ange  75016 Paris. Membre  du  comitÃ©  dâorganisation  de cette  confÃ©rence  organisÃ©e  par  le  GDR  3062  CNRS ""Mutations polaires : environnement et sociÃ©tÃ©s"". * 2010 - Â« La violence de la mer : lâimpact de la tempÃªte Xynthia du 28 fÃ©vrier 2010 sur le littoral atlantique Â» Sous  lâÃ©gide  de  la  Maison  des  Sciences  de  lâHomme  Ange  GuÃ©pin  et  du  laboratoire LETG-Nantes-GÃ©olittomer  (UMR  6554),  3  dÃ©cembre  2010.  Organisation  du  colloque  sur  le  plan scientifique et matÃ©riel, porteur du projet pour les demandes de subventions. *  2009 - Coordinateur pour lâUMR 6554 LETG de lâexposition de 22 posters en provenance du laboratoire au 20e Festival International de GÃ©ographie (FIG) de Saint-DiÃ©-des-Vosges, du 1er au 4 octobre. *  2007 - Coordinateur pour lâUMR 6554 LETG de lâexposition de 13 posters en provenance du  

!5!laboratoire au 18e Festival International de GÃ©ographie (FIG) de Saint-DiÃ©-des-Vosges, du 4 au 7 octobre. *  2007 - Organisation  de  la  sÃ©ance  Â« Paraglaciaire  et  changements  climatiques Â» de  lâAssociation  de GÃ©ographes FranÃ§ais, Paris, Institut de GÃ©ographie, 17 novembre 2007. *  2004 - Organisation  matÃ©rielle  du  colloque  international Ny-SMAC  (Ny-Ãlesund  Scientific Management Comitee), 3 et 4 novembre 2004, Institut OcÃ©anographique et Sorbonne, Paris. *  2002 - Co-organisation (avec  M.-F.  AndrÃ©)  dâune  sÃ©ance  de  lâAssociation  de GÃ©ographes FranÃ§ais, 11 mai 2002, Institut de GÃ©ographie, Paris Â« La recherche actuelle en milieux polaires et sub-polaires Â». *  De  2000 Ã  2003 - Organisation  de  14  confÃ©rences - dÃ©bats  du  DEPAM  (Dynamique  et Ã©volution  des  paysages  atlantiques  et  mÃ©diterranÃ©ens),  Laboratoire  de  gÃ©ographie  physique  de Paris IV - Sorbonne, Institut de GÃ©ographie, Paris.  Direction de thÃ¨ses : 7, dont une soutenue, deux soutenances fixÃ©es en 2015 et quatre en cours *  FEUILLET  Thierry  Â« Les  formes  pÃ©riglaciaires  dans  les PyrÃ©nÃ©es  centrales  franÃ§aises :  analyse  spatiale, chronologique et valorisation Â», soutenue en 2010. Situation actuelle : MCF Ã  lâuniversitÃ© de Paris 8. * COQUIN  Julien Â« La  dÃ©glaciation  dans  la  rÃ©gion  littorale  du SkagafjÃ¶rÃ°ur Â», dÃ©but  du  doctorat  : octobre 2012 (bourse ministÃ©rielle), co-directeur : Olivier Bourgeois (Pr de gÃ©ologie Ã  lâuniversitÃ© de Nantes, laboratoire LPGN). Soutenance : le 20 octobre 2015. *  CREACH  Axel  Â« Cartographie  et  Ã©valuation  Ã©conomique  de  la  vulnÃ©rabilitÃ©  du  littoral  atlantique  franÃ§ais face au  risque  de  submersion  marine Â»,  dÃ©but  du  doctorat  :  octobre  2011  (bourse  ministÃ©rielle),  co-encadrante :  Sophie  Pardo  (Mcf  Ã   lâuniversitÃ©  de  Nantes,  laboratoire  LEMNA,  Ã©conomie). Soutenance : le 13 novembre 2015. * RAGARU Etienne Â« Les  falaises  de  Safi  (Maroc)  :  approche  gÃ©omorphologique  du  risque  littoral Â», dÃ©but du doctorat : octobre 2011 (bourse ministÃ©rielle), co-directeur : Mohamed Chaibi (Pr de gÃ©ologie Ã   lâuniversitÃ©  de  Safi, Maroc),  co-encadrant :  Mohamed  Maanan  (Mcf  Ã   lâuniversitÃ©  de  Nantes, laboratoire LETG-Nantes-GÃ©olittomer). * CHEVILLOT-MIOT Elie Â« La  rÃ©silience  des  territoires  littoraux  face  au  risque  de  submersion  marine Â», dÃ©but  du  doctorat  :  octobre  2013  (bourse  ministÃ©rielle),  co-encadrante :  CÃ©line  Chadenas  (Mcf  Ã  lâuniversitÃ© de Nantes, laboratoire LETG-Nantes-GÃ©olittomer). *  COQUET  Marie Â« Evaluation  et  acceptabilitÃ©  du  risque  de  submersion  marine  par  les  populations  des littoraux  franÃ§ais :  effet  des  biais  dâoptimisme  spatial  et  temporel  Â», dÃ©but  du  doctorat  :  octobre  2014 (financement  sur  programme  de  la Fondation  de France),  co-directrice :  Ghozlane  Fleury-Bahi (Pr de psychologie environnementale Ã  lâuniversitÃ© de Nantes). * BOURRIQUEN Marine Â« Evolution  du  littoral  du  Kongsfjorden  (Svalbard)  face  aux changements climatiques Â», dÃ©but du doctorat : octobre 2015 (bourse ministÃ©rielle), co-directrice : AgnÃ¨s Baltzer (Pr Ã  lâuniversitÃ© de Nantes, laboratoire LETG-Nantes-GÃ©olittomer).  Direction dâHabilitation Ã  Diriger des Recherches : 1 * DECAULNE  Armelle Â« Datation  des  gÃ©odynamiques  de  pente  dans  les  environnements  froids  des hautes latitudes  et  Ã©chelles  de  temps  considÃ©rÃ©es.  Exemples  islandais Â», UniversitÃ© de Nantes. Soutenue le 26 aoÃ»t 2015.  RÃ©seaux de recherche :  * De 2009 Ã  2014, membre junior de lâInstitut Universitaire de France (IUF) http://iuf.amue.fr/ *  Depuis  aoÃ»t  2013,  membre  cooptÃ©  du  comitÃ©  exÃ©cutif  de  lâAssociation  Internationale  des GÃ©omorphologues (AIG), prÃ©sident : Eric Fouache http://www.geomorph.org/ *  Depuis  2006 Ã   2015,  membre  de  lâUMR  6554  Littoral  Environnement TÃ©lÃ©dÃ©tection GÃ©omatique (LETG)  du  CNRS  (directeur :  Marc  Robin)  et  de  son  antenne  nantaise GÃ©olittomer (directeur de lâantenne nantaise de 2009 Ã  2015).  

!6!http://letg.univ-nantes.fr/ ; http://geolittomer.univ-nantes.fr/ *  Membre  du GDR  3062  Â« Mutations polaires Â» de  BesanÃ§on  (ex-GDR  49  ""Recherches Arctiques""), directeur : Daniel Joly. http://thema.univ-fcomte.fr/ *  Membre  Ã©lu  du  conseil  et  secrÃ©taire  (de  2008  Ã   2014)  du Groupe  FranÃ§ais  de GÃ©omorphologie (GFG), prÃ©sidente : Monique Fort http://www.gfg.cnrs.fr/ *  Membre  de  la  commission  Â« Ãtudes  des  phÃ©nomÃ¨nes  pÃ©riglaciaires Â»  du  ComitÃ©  National FranÃ§ais de GÃ©ographie (prÃ©sident : FranÃ§ois Costard).  http://www.univ-st-etienne.fr/afdp * De  2001  Ã   2006 : Membre  Ã©lu  du  Conseil  de  lâAssociation  de  GÃ©ographes  franÃ§ais  (AGF), prÃ©sident : Roland Pourtier.  http://association-de-geographes-francais.fr/site/ * De 2003 Ã  2006, membre du RÃ©seau europÃ©en SEDIFLUX, Sedimentary source-to-sink-fluxes in cold environments, de lâEuropean Science Foundation (ESF), Coordonnateur : Dr Achim A. Beylich. http://www.ngu.no/sediflux *  De  1993  Ã   2006 :  membre  de  l'UMR  6042  du  CNRS  ""GÃ©odynamique  des  milieux  naturels  et anthropisÃ©s"" de Clermont-Ferrand, directeur : Jean-Luc Peiry.  http://www.univ-bpclermont.fr/LABOS/geolab/ *  De  1999 Ã   2006,  membre  et  directeur-adjoint (2002-2006) du  DEPAM  (Dynamiques  et Ãvolutions  des  Paysages  Atlantiques  et  MÃ©diterranÃ©ens)  EA  2579  de  lâUniversitÃ©  de  Paris  IV, directeur : FranÃ§ois CarrÃ©. http://www.univ-paris4.fr    Deux grands axes de la recherche  Sur le plan de la recherche, mes problÃ©matiques scientifiques se sont concentrÃ©es sur deux grands axes :  (i) le changement global des environnements froids  (ii) les  territoires  de  lâenvironnement  sous  lâangle  des  risques  (inondation  et  submersion marine).  Le changement global des environnements froids  Ma  premiÃ¨re  expÃ©rience  de  recherche  a  eu  pour  cadre  la  NorvÃ¨ge  centrale  et  le  massif  des Rondane  (1989)  avec  une  thÃ©matique  centrÃ©e  les  dynamiques  pÃ©riglaciaires,  essentiellement lâÃ©boulisation.  Ensuite,  lâexpÃ©rience  Ã©cossaise  (1990)  a  eu  pour  problÃ©matique  lâanalyse  des Ã©tagements  morphodynamiques  le  long  des  versants  de  cette  montagne  atlantique  europÃ©enne. Fort  de  ces  deux  expÃ©riences  de  terrain  en  gÃ©ographie  des  milieux  froids,  menÃ©es  sous  la direction de Dominique Sellier de lâUniversitÃ© de Nantes, jâai dÃ©butÃ© des recherches au Spitsberg en 1993 sous la direction de Marie-FranÃ§oise AndrÃ©. Elles allaient me permettre de soutenir une thÃ¨se  de  doctorat  devant  lâuniversitÃ©  Blaise  Pascal  de  Clermont-Ferrand  en  1998  intitulÃ©e :  Â« Le ruissellement  au  Spitsberg.  L'impact  d'un  processus  azonal  sur  les  paysages  d'un  milieu  polaire  (presqu'Ã®le  de BrÃ¸gger  79Â°N) Â». La  premiÃ¨re  partie  de  cette  thÃ¨se  est  de  la  climatologie  et  de  lâhydrolologie  Ã  travers  lâanalyse  du  bilan  hydrologique  de  la  presquâÃ®le  de  BrÃ¸gger.  Les  deuxiÃ¨me  et  troisiÃ¨me parties  dÃ©montrent  lâemprise  spatiale  de  cette  dynamique  hydrologique  Ã   travers  des  bilans dâÃ©rosion. Cette thÃ¨se dÃ©montre lâimportance du ruissellement dans les phases de rÃ©chauffement climatique  et  de  fonte  des  glaciers  et  du  pergÃ©lisol  dans  lâÃ©volution  des  paysages  des  milieux polaires. Ainsi  dÃ©buta  un  approfondissement  de  lâimpact  des  changements  climatiques  sur  les  milieux froids, des versants aux littoraux, et une approche holistique de la gÃ©omorphologie paraglaciaire, discipline  en  pleine  expansion  sur  le  plan  international.  Dans  ce  contexte,  jâai  poursuivi  mes recherches  sur  le  terrain  dans  le  cadre  dâun  programme  soutenu de  2002  Ã   2004 par  lâInstitut  

!7!Polaire FranÃ§ais Paul-Ãmile Victor (IPEV). Jâai intÃ©grÃ© des groupes de recherche dans le cadre de la Fondation EuropÃ©enne pour la Science (ESF), Sedimentary  source-to-sink-fluxes  in  cold  environments (SEDIFLUX),  qui  fÃ©dÃ¨rent  des  scientifiques  intÃ©ressÃ©s  par  lâanalyse  des  milieux  froids  face  aux changements  climatiques.  Cette  expÃ©rience  des  milieux  polaires  et  les  problÃ©matiques scientifiques  dÃ©veloppÃ©es  depuis  le  dÃ©but  des  annÃ©es  1990  mâont  valu  la  reconnaissance  de lâInstitut  Universitaire  de  France  (IUF),  oÃ¹  jâai  Ã©tÃ©  nommÃ©  en  2009  pour  cinq  annÃ©es, afin  de dÃ©velopper  un  programme  de  recherche  sur  lâArctique  face  aux  changements  climatiques.  J'ai soutenu devant  lâuniversitÃ©  Blaise  Pascal  de  Clermont-Ferrand une  habilitation  Ã   diriger  des recherches  le  8  octobre  2010  sur  la  gÃ©omorphologie  paraglaciaire,  thÃ©matique  sur  laquelle  je travaille  maintenant  depuis  de  nombreuses  annÃ©es.  Depuis  2010,  je  travaille  Ã©galement  sur  les dynamiques  paraglaciaires  en  Islande  septentrionale  dans  la  rÃ©gion  du SkagafjÃ¶rÃ°ur.  Je  co-dirige la  thÃ¨se  de  Julien  Coquin  sur  la  dÃ©glaciation  de  la  rÃ©gion  littorale  de  ce  fjord  islandais. Dans  ce cadre  des  recherches  menÃ©es  dans  les  environnements  froids,  j'ai  Ã©tÃ© chercheur  invitÃ©  par lâinstitut polaire allemand (AWI) Ã  Potsdam dâavril Ã  juillet 2014.   Implication dans des programmes de recherche  Quatre programmes concernent lâimpact des changements climatiques sur les milieux froids : des versants aux littoraux.  Programme  400  Â« GÃ©omorphoclim Â»  financÃ©  par  lâInstitut  Polaire  FranÃ§ais  Paul-Ãmile Victor (IPEV) de 2002 Ã  2004  Porteur du projet : Denis Mercier  Participants : M.-F. AndrÃ© (univ. Blaise Pascal)  J. Dupont (MusÃ©um National dâHistoire  naturelle de Paris) S. Ãtienne (univ. Blaise Pascal) D. Laffly (univ. de Pau) D. Mercier (univ. Paris IV) M. Moreau (univ. Blaise Pascal) A. Prick (UNIS, NorvÃ¨ge) G. Rachlewicz (univ. Poznam, Pologne) D. Sellier (univ. de Nantes)  Kronebreen vue depuis le sommet du Feringfjellet.  Mots  clÃ©s : Changements  climatiques,  SystÃ¨me  paraglaciaire,  Crises  morphoclimatiques,  Milieux polaires, Colonisation vÃ©gÃ©tale, Littoraux, Marges proglaciaires.  Objectifs et intÃ©rÃªts scientifiques du programme L'objectif  principal Ã©tait de  comprendre  l'impact  du  changement  climatique  contemporain (postÃ©rieur au Petit Ãge glaciaire) sur la conquÃªte vÃ©gÃ©tale et les processus morphodynamiques Ã  l'Åuvre sur les marges glaciaires des milieux polaires. Le terrain retenu est celui de la presqu'Ã®le de BrÃ¸gger  au  Spitsberg.  La  thÃ©matique  s'inscrit  dans  le  cadre  international  de  l'impact  du changement global sur les environnements polaires.     

!8! Programme  Â« LâArctique  face  aux  changements  climatiques :  analyse  des  crises gÃ©omorphologiques  paraglaciaires Â»  financÃ©  par  lâInstitut  Universitaire  de  France  (IUF) de 2009 Ã  2014  Porteur du projet : Denis Mercier Participants :  Ã. SÃ¦mundsson (Centre de Recherche NÃ¡ttÃºrustofa  NorÃ°urlands vestra, SauÃ°Ã¡rkrÃ³kur, Islande) H. P. JÃ³nsson (Centre de Recherche NÃ¡ttÃºrustofa NorÃ°urlands vestra, SauÃ°Ã¡rkrÃ³kur, Islande) Armelle Decaulne (LETG-Nantes-GÃ©olittomer,  UMR 6554 CNRS, Nantes) Etienne Cossart (universitÃ© Paris 1 PanthÃ©on-Sorbonne, Prodig) Thierry Feuillet (universitÃ© de Nantes)       The HÃ¶fÃ°ahÃ³lar rock avalanche           Mots clÃ©s : HolocÃ¨ne, dynamique des versants, Islande, Changement climatique.  Objectifs et intÃ©rÃªts scientifiques du programme Lâobjectif  principal Ã©tait dâÃ©tudier  la  rÃ©ponse  des  versants  islandais  face  aux  changements climatiques  sur  le  temps  long de  la  dÃ©glaciation  holocÃ¨ne.  Le  cadre  spatial  correspond  au SkagafjÃ¶rÃ°ur dans  la  partie  septentrionale  de  lâIslande.  Dans  ce  secteur,  la  richesse  des  horizons de  tÃ©phras  assure  un  calage  chronologique  fin  sur  la  pÃ©riode  considÃ©rÃ©e.  Le  secteur  dâÃ©tude  est effectivement  une  des  rares  zones  des  hautes  latitudes  Ã   offrir  une  telle  chronologie  dâorigine volcanique, couvrant une pÃ©riode aussi longue.  Ce  programme  a  permis  de  dater  les  glissements  de  terrain  et  les  effondrements  gravitaires  en relation  avec  la  dÃ©glaciation  du  fjord  au  dÃ©but  de  lâHolocÃ¨ne,  de  dÃ©montrer  le  rÃ´le  de  la dÃ©glaciation (rebond glacio-isostatique) dans le dÃ©clenchement des glissements de terrain.   Programme  Â« Glissements  de  terrain  paraglaciaires  islandais  et  martiens :  analyse comparÃ©e  Â»  financÃ©  par  lâObservatoire  des  Sciences  de  lâUnivers  Nantes  Atlantique (OSUNA â UMS 3281 CNRS) de 2012 Ã  2013  Porteur du projet : Denis Mercier Participants :  Armelle Decaulne et Julien Coquin (LETG-Nantes-GÃ©olittomer, UMR 6554 CNRS, Nantes) Olivier Bourgeois, Marine Gourronc, Marion MassÃ© (LPGN, UMR 6112 â CNRS)          VallÃ©e de Langidalur, Islande  Mots clÃ©s : PlanÃ©tologie comparÃ©e, Mars, Islande, glissements de terrain, dÃ©glaciation.  Objectifs et intÃ©rÃªts scientifiques du programme Lâobjectif principal de ce programme Ã©tait de dÃ©montrer le rÃ´le majeur de la dÃ©glaciation dans le dÃ©clenchement des mouvements de masse sur les versants des deux planÃ¨tes Mars et la Terre.    

!9!Programme  Â« SeiSpitz11 : Lecture  des  variations  climatiques  hautes  frÃ©quences  dans  les archives sÃ©dimentaires holocÃ¨ne d'un fjord du Spitsberg : le Kongsfjorden Â» financÃ© par lâInstitut Polaire FranÃ§ais (IPEV) de 2011 Ã  2013  Porteur  du  projet : AgnÃ¨s  Baltzer  (universitÃ©  de  Caen,  puis puis universitÃ© de Nantes)  Participants :  AgnÃ¨s Baltzer, Denis Mercier, Laurent Perez,   Sylvain Haquin          Kongsfjorden, Svalbard  Mots clÃ©s : Arctique, Littoral, SÃ©dimentation, Changement climatique.  Objectifs et intÃ©rÃªts scientifiques du projet Lâobjectif  scientifique Ã©tait  de reconnaÃ®tre  l'enregistrement  des  variations  climatiques  haute frÃ©quence  dans  les  archives  sÃ©dimentaires  d'un  fjord  du  Spitsberg  :  le  Kongsforden. La  logique est  celle  du  Â« Source-to-Sink  fluxes Â»  en  partant  du  recul  des  glaciers  depuis  la  fin  du  Petit  Ãge glaciaire,  analysÃ©  comme  origine  des  flux  liquides,  lâabandon  des  sÃ©diments  glaciaires  lors  des retraits  des  glaciers,  leur  reprise  par  le  rÃ©seau  hydrographique,  la  progradation  des  littoraux meubles,  et  la  sÃ©dimentation off-shore.  Il  sâagit  donc  dâune  logique  de  flux  sÃ©dimentaires  dans  un continuum Terre-Mer avec ici uniquement des forÃ§ages naturels sans perturbation anthropique.     

!10! Les territoires de lâenvironnement sous lâangle des risques naturels  Ce  second  axe  de  mes  recherches  est  nÃ©  de  lâintÃ©rÃªt portÃ©  par des  Ã©tudiants  parisiens  pour  les risques naturels lorsque jây enseignais comme maÃ®tre de confÃ©rences. Mes racines nantaises mâont donc sensibilisÃ©  dÃ¨s  le  dÃ©but  des  annÃ©es  2000  Ã   la  problÃ©matique  du  risque  inondation  dans  la vallÃ©e de la Loire. Jâai donc dÃ©veloppÃ© dans un premier temps des recherches sur la mÃ©moire des crues puis,  dans  un  second  temps, une  Ã©valuation  de  la  vulnÃ©rabilitÃ©  des  biens  et  des  personnes face  aux  inondations.  Cette  approche  gÃ©ographique  sur  des  territoires  sensibles  rÃ©pondait  Ã   une demande  sociale  et  me  permettait  dâapporter  aux  responsables  publics  des  informations  prÃ©cises et scientifiques utiles Ã  la gestion des risques.  Cette  expÃ©rience  ligÃ©rienne  est  aujourdâhui  utile  pour  lâanalyse  des  consÃ©quences  de  la  tempÃªte Xynthia  du  28  fÃ©vrier  2010  qui  a  touchÃ©  le  littoral  atlantique.  Ãtant  directeur  du  laboratoire LETG-Nantes-GÃ©olittomer  (UMR  6554),  les  demandes  sociÃ©tales  dâexpertise  se  font  de  plus  en plus pressantes pour lâanalyse des territoires littoraux face Ã  ces submersions marines. Je travaille donc depuis cinq ans sur le risque de submersion marine et une grande partie de ma production scientifique de ces derniÃ¨res annÃ©es est consacrÃ©e Ã  cette thÃ©matique. Je co-dirige la thÃ¨se d'Axel Creach sur ce sujet avec une collÃ¨gue Ã©conomiste de l'universitÃ© de Nantes. Depuis octobre 2013, je  co-dirige  la  thÃ¨se  d'Elie  Chevillot-Miot  sur  la  rÃ©silience  des  territoires  littoraux  face  au  risque de  submersion  marine.  Depuis  octobre  2014,  je  co-dirige  la  thÃ¨se  de  Marie  Coquet  sur  la perception  du  risque  de  submersion  marine  par  les  populations  des  littoraux  franÃ§ais,  avec  une collÃ¨gue psychologue.  Implication dans des programmes de recherche   La  recherche  sur  ce  thÃ¨me  a  dâabord  Ã©tÃ©  lancÃ©e  dans  le  cadre  dâun  programme  de  recherche centrÃ© sur la vulnÃ©rabilitÃ© du Val nantais. Puis, cette thÃ©matique de la vulnÃ©rabilitÃ© a Ã©tÃ© au centre dâun  programme  ANR  sur  les  littoraux  franÃ§ais  (VULSACO).  Les  rÃ©cents  Ã©vÃ©nements  de  la tempÃªte Xynthia nous invitent Ã  transposer les thÃ©matiques, et le savoir faire mÃ©thodologique des fleuves au littoral dans des programmes locaux (universitÃ© de Nantes) et nationaux (Fondation de France, ANR).   Programme  Â«  VulnÃ©rabilitÃ©  de  la  zone  inondable  du  Val  nantais  Â»,  sans  financement spÃ©cifique (2003-2009)  Porteurs du projet :  Denis Mercier et ValÃ©rie Jousseaume  (UniversitÃ© de Nantes)  Participants :  Julie Landrein (universitÃ© Paris IV-Sorbonne) Thibault Meunier (UniversitÃ© Paris IV-Sorbonne)   La Loire, la digue et lâhabitat  en zone inondable dans le Val nantais  Mots clÃ©s - HydrosystÃ¨mes, Risque dâinondation, AlÃ©a, VulnÃ©rabilitÃ©, Gestion, AmÃ©nagement du territoire, MaraÃ®chage, PÃ©riurbanisation, Loire.  

!11!Objectifs et intÃ©rÃªts scientifiques du programme Le Val nantais, qui Ã©tait une zone dâexpansion des crues de la Loire, a Ã©tÃ© protÃ©gÃ© par une digue au  milieu  du  XIXe siÃ¨cle.  Puis,  durant  toute  la  seconde  moitiÃ©  du  XXe siÃ¨cle,  il  a  Ã©tÃ©  assÃ©chÃ© jusquâÃ  devenir un bassin de production maraÃ®cher. Aujourdâhui, cet espace amÃ©nagÃ© par et pour lâagriculture,  est  convoitÃ©  pour  lâurbanisation  du  fait  de  lâexpansion  de  lâaire  urbaine  nantaise. Lâaccroissement  rÃ©cent  de  la  vulnÃ©rabilitÃ©  des  biens  et  des  personnes  dans  le  Val  nantais  met  en Ã©vidence les conflits dâintÃ©rÃªts entre les individus et la collectivitÃ©, entre le local et le national. Cet exemple  pose  concrÃ¨tement  la  question  de  lâapplication  des  politiques  nationales  Ã   lâÃ©chelle communale. Cette  recherche  sâinscrit  dans  le  cadre  de  la  rÃ©flexion,  menÃ©e  en  France  et  en  Europe  sur  la cindynique  en  gÃ©nÃ©ral  et  sur  les  inondations  en  particulier.  LâÃ©tude  minutieuse  Ã   lâÃ©chelle  des deux  communes  principales  du  Val  nantais,  La  Chapelle-Basse-Mer  et  Saint-Julien-de-Concelles, permet  de  quantifier  lâÃ©volution  de  la  vulnÃ©rabilitÃ©  dans  cette  zone  inondable  depuis  la construction, au milieu du XIXe siÃ¨cle, de la levÃ©e qui le protÃ¨ge.    Programme  âVULSACO :  VULnerability  of  SAndy  COast  systems  to  climatic  and anthropic  changesâ  financement  par  lâAgence  Nationale  de  la  Recherche (2007-2010)  Porteur du projet :  DÃ©borah Idier (BRGM) Participants :  LETG (Marc Robin, Mohamed Maanam,  Paul Fattal, Christine Lambert,  Denis Mercier)  BRGM ; ARN, UniversitÃ© Bordeaux 1   EPOC, UniversitÃ© du littoral cÃ´te dâopale GEODAL, UniversitÃ© Perpignan LEGEM, UniversitÃ© Montpellier 2  DL â ISTEEM, BRL IngÃ©nierie, UniversitÃ© Joseph Fourier Grenoble, LEGI  Institut Symolg de France.    CÃ´te occidentale de lâÃ®le de Noirmoutier  Mots clÃ©s : Changements climatiques, Littoraux, VulnÃ©rabilitÃ©, France.  Objectifs et intÃ©rÃªts scientifiques du programme Les  Ã©tudes  du  GIEC  sur  lâimpact  des  changements  climatiques  soulignent  la  forte  vulnÃ©rabilitÃ© des  cÃ´tes  sableuses  (31%  des  cÃ´tes  mÃ©tropolitaines)  Ã   lâÃ©rosion  et  aux  submersions  marines. Lâobjectif du projet VULSACO Ã©tait :  (1)  dâidentifier  et  dâestimer,  sur  les  cÃ´tes  sableuses  basses,  les  indicateurs  de  la  vulnÃ©rabilitÃ© physique (Ã©rosion et submersion marine) Ã  court terme (tempÃªte) et long terme (2030), (2) dâidentifier lâinfluence de lâoccupation humaine du littoral sur cette vulnÃ©rabilitÃ©.  Quatre  sites (Noirmoutier,  Truc  Vert, SÃ¨te,  Dewulf) ont Ã©tÃ© Ã©tudiÃ©s,  constituÃ©s  de  plages sableuses  linÃ©aires  et  basses,  avec  des  environnements  hydrodynamiques  et  des  contextes  socio-Ã©conomiques  variÃ©s.  Pour  chacun  dâeux,  il  sâagissait dâÃ©tablir  une  caractÃ©risation  physique  et socio-Ã©conomique,  de  rassembler,  complÃ©ter  et  analyser  les  donnÃ©es  sur  les  tendances  actuelles dâÃ©volution du site, et les Ã©lÃ©ments nÃ©cessaires pour Ã©tablir des scÃ©narios climatiques locaux basÃ©s sur les donnÃ©es historiques.   

!12! Programme  Â« La violence  de  la  mer  sur  le  littoral  atlantique Â»,  financement par lâUniversitÃ©  de  Nantes  (rÃ©ponse  Ã   lâappel  Ã   projet  pluridisciplinaire), hÃ©bergement :  Maison  des  Sciences  de  lâHomme  Ange  GuÃ©pin  de  Nantes (2010-2011)  Porteurs du projet :  Denis Mercier et Martine Acerra  Participants : MSH Ange-GuÃ©pin,  Laboratoires : LETG-Nantes (gÃ©ographie), CRHIA (histoire),  LEMNA (Ã©conomie),  CDMO (droit).    Mots clÃ©s : TempÃªte, Submersion marine, Littoral, VulnÃ©rabilitÃ©, Risque.  Objectifs et intÃ©rÃªts scientifiques du programme Ce programme avait pour objectif de faire Ã©merger une approche pluridisciplinaire pour rÃ©soudre des  questions  scientifiques  propres  aux  dynamiques  littorales  et  maritimes  en  organisant  un colloque de lancement du 3 dÃ©cembre 2010 : La violence de la mer : l'impact de la tempÃªte Xynthia sur le littoral atlantique.  Le  littoral  Atlantique  a  Ã©tÃ©  affectÃ©  par  la  tempÃªte  Xynthia  le  28  fÃ©vrier  2010,  notamment  en Charente  maritime  et  en  VendÃ©e,  mais  Ã©galement  en  Loire-Atlantique.  L'objectif  de  ce colloque pluridisciplinaire  Ã©tait d'apporter  un  regard  scientifique  sur  cet  Ã©vÃ©nement  mÃ©tÃ©o-marin  et  ses consÃ©quences  en  matiÃ¨re  d'amÃ©nagement  de  l'espace  littoral.  Les  historiens  ont  mis  en perspective  cet  Ã©vÃ©nement  de  2010  au  regard  de  l'amÃ©nagement  de  l'espace  littoral  depuis l'Ã©poque  mÃ©diÃ©vale  et  la  poldÃ©risation  des  marais  maritimes.  Les  gÃ©ographes  ont  apportÃ©  une vision  de  l'amÃ©nagement  postÃ©rieur  Ã   la  seconde  guerre  mondiale  de  l'espace  littoral  dans  une logique  d'urbanisation  et  de  balnÃ©arisation  du  littoral,  du  choix  architectural  des  habitations  au regard des Plans de PrÃ©vention des Risques d'inondation, de l'impact de l'alÃ©a mÃ©tÃ©o-marin (force des  vents,  coefficients  de  marÃ©es,  dÃ©pression  baromÃ©trique, effet  de  site...).  Les  juristes  ont prÃ©sentÃ© les aspects rÃ©glementaires de la loi littoral et des lois rÃ©gissant les zones inondables et les enjeux  fonciers.  Les  Ã©conomistes  ont  apprÃ©ciÃ©  les  impacts  financiers  sur  les  activitÃ©s  littorales.!Lors de ce colloque du 3 dÃ©cembre, des acteurs politiques et de la sociÃ©tÃ© civile Ã©taient prÃ©sents.!Les actes de ce colloque ont fait l'objet de publications.     

!13!  Programme  COSELMAR  Â« ComprÃ©hension  des  socio-Ã©cosystÃ¨mes  littoraux  et  marins pour l'amÃ©lioration de la valorisation des ressources marines, la prÃ©vention et  la  gestion  des  risques Â»,  financement par  la RÃ©gion  des  Pays  de  la  Loire (2013-2016) : http://www.coselmar.fr/  Porteur du projet :  Sophie Pardo (universitÃ© de Nantes) et  Philippe Hess (ifremer) Porteur de lâaction 3.1 :  Denis Mercier   Participants :  Laboratoire GÃ©olittomer (UMR 6554  LETG) : gÃ©ographie Laboratoire CRHIA : histoire Laboratoire LEMNA : Ã©conomie Laboratoire CDMO : droit Laboratoire Groupe d'Etudes et de  Recherches d'Histoire en Centre Ouest!(GERHICO-Cerhilim EA 4270) : histoire     Mots clÃ©s : littoraux, vulnÃ©rabilitÃ©, submersion marine, enjeux, cartographie.  Objectifs et intÃ©rÃªts scientifiques du programme Lâobjectif  principal  du  projet  est  de  cartographier  les  zones  de  submersions  marines  potentielles sur le littoral atlantique franÃ§ais en prenant principalement la RÃ©gion des Pays de la Loire comme zone  dâÃ©tude  afin  de  dÃ©finir  les  enjeux.  Il  sâagit  dâintÃ©grer  dans  cette  cartographie  lâÃ©vÃ©nement rÃ©cent de la tempÃªte Xynthia qui correspond aux plus hautes eaux connues (PHEC) Ã  ce jour et dây intÃ©grer pour le futur un potentiel dâÃ©lÃ©vation du niveau marin en relation avec le changement climatique  en  cours.  Cette  cartographie  fine est facilitÃ©e  par  lâutilisation  des  donnÃ©es  Lidar rÃ©cemment  acquises  par  la  rÃ©gion. Les  zones  sensibles  de  danger  potentiel  une  fois  dÃ©limitÃ©es serviront de support Ã  la quantification des enjeux en terme dâoccupation du sol (habitat, activitÃ©s Ã©conomiques,  infrastructures,  espaces  de  natureâ¦).  La  vulnÃ©rabilitÃ©  potentielle  sera  alors quantifiÃ©e.  Enfin  des  propositions  de  gestion  de  ces  espaces  seront  proposÃ©es  selon  plusieurs scÃ©narios de danger potentiel. Les enjeux en terme dâamÃ©nagement du littoral sont fondamentaux pour le devenir de cette frange territoriale trÃ¨s attractive de la RÃ©gion des Pays de la Loire.     

!14! Programme SUBMARINE Â« Evaluation et acceptabilitÃ© du risque de submersion marine par  les  populations  sur  les  littoraux  franÃ§ais  :  effet  des  biais  dâoptimisme spatial  et  temporel  Â»,  financement  par  la  Fondation  de  France,  suite  Ã  lâappel dâoffre Â« Quels littoraux pour demain ? Â» (2014-2017)   Porteur du projet :  Denis Mercier   Participants :  Marie  Coquet,  doctorante  en gÃ©ographie, membres du laboratoire  LETG-Nantes-GÃ©olittomer  (UMR  CNRS  6554) et Ghozlane Fleury-Bahi (Pr de psychologie  sociale et environnementale Ã  lâuniversitÃ© de  Nantes et membre du Laboratoire de  Psychologie des Pays de la Loire - LPPL â  UPRES EA 4638).          La Faute-sur-Mer aprÃ¨s la tempÃªte Xynthia  Mots clÃ©s : Risque, Submersion marine, Evaluation, AcceptabilitÃ©, VulnÃ©rabilitÃ©, Perception.  Objectifs et intÃ©rÃªts scientifiques du programme Lâobjectif principal de ce programme est dâanalyser lâÃ©valuation du risque par des populations des littoraux  franÃ§ais  sous  lâangle  dâune  approche  participative.  Ce projet  cherche  Ã   montrer  en  quoi lâÃ©tude de lâÃ©valuation du risque de submersion marine par les individus permet Ã  ceux-lÃ  mÃªmes dâentreprendre une dÃ©marche dâÃ©valuation de leur propre vulnÃ©rabilitÃ©. Nous faisons lâhypothÃ¨se de  lâÃ©mergence  dâune  plus  grande  acceptabilitÃ©  du  risque  par  les  populations  si  celles-lÃ   mÃªmes entreprennent  cette  dÃ©marche.  Ainsi,  nous  pensons  que  lâacceptabilitÃ©  du  risque  est  une composante essentielle dans lâÃ©valuation de la vulnÃ©rabilitÃ© humaine. LâoriginalitÃ© de ce projet est quâil aborde ces notions au moyen dâune approche pluridisciplinaire (gÃ©ographie et psychologie).  Ce  travail  se  base  notamment  sur  lâanalyse  des  biais  dâoptimisme  spatial  et  temporel  qui interviennent  dans  lâÃ©valuation  du  risque  de  submersion  marine  par  les habitants  des  communes concernÃ©es  et  qui  entrainent  diffÃ©rents  degrÃ©s  dâacceptabilitÃ©.  Ainsi,  les  habitants  interrogÃ©s pourraient  faire  preuve  dâun  optimisme  spatial,  câest-Ã -dire  la  perception  que  les  choses  vont mieux  ici  quâailleurs,  ou  dâun  pessimisme temporel,  lâidÃ©e  que  les  choses  vont  sâempirer  dans  le futur.  Cette  Ã©tude  sâappuiera  sur  des  territoires  littoraux  qui  nâont  pas  connu  les  mÃªmes  Ã©pisodes  de submersion  marine  et  dont  il  est  supposÃ©  que  lâÃ©valuation  et  lâacceptabilitÃ©  du  risque  par  les habitants  en  sera  diffÃ©rente.  Il  sâagira  notamment  de  comparer  les  zonages  reprÃ©sentÃ©s  dans  les documents  rÃ©glementaires  de  type  PPRL  et  les  zones  Ã   risques  Ã©valuÃ©es  et  reprÃ©sentÃ©es  par  les habitants au moyen de la cartographie participative, couplÃ©e Ã  des enquÃªtes par questionnaire afin  de  dÃ©gager  des  biais  qui  opÃ¨rent  dans  lâÃ©valuation  du  risque  de  submersion  marine  par  les habitants et de lâacceptabilitÃ© qui en dÃ©coule.    

!15!Projet CLIMARisk Â« Sentiment de vulnÃ©rabilitÃ© et stratÃ©gies dâadaptation face aux effets du  changement  climatique.  Le  cas  de  la  perception  des  risques  cÃ´tiers  Â», Financement obtenu en 2015 auprÃ¨s de lâANR (2015-2018)  Porteur du projet :  Oscar Navarro  (Mcf en psychologie Ã  lâuniversitÃ© de Nantes)    Participants :  Ghozlane Fleury-Bahi (Pr de psychologie  sociale et environnementale Ã  lâuniversitÃ©  de Nantes),  Aurore Marcouyeux  (Mcf en mÃ©thodologie),  Delphine Rommel (Mcf en psychologie  clinique) et Colin LemÃ©e (doctorant),  tous membres du Laboratoire de Psychologie  des Pays de la Loire - LPPL - UPRES EA 4638) ;  Denis Mercier (Pr de gÃ©ographie) et  Marie Coquet (doctorante en gÃ©ographie),  tous les deux membres du laboratoire LETG- Nantes-GÃ©olittomer (UMR CNRS 6554).       Faute-sur-Mer aprÃ¨s la tempÃªte Xynthia. Mots clÃ©s : Submersion marine, Littoral, VulnÃ©rabilitÃ©, Risque, Perception.  Objectifs et intÃ©rÃªts scientifiques du programme Lâobjectif principal de ce programme est dâanalyser la capacitÃ© dâadaptation des sociÃ©tÃ©s face aux changements  environnementaux  en  partant  dâune  analyse  fine  de  la  vulnÃ©rabilitÃ©  subjective. LâhypothÃ¨se  est  que  la  vulnÃ©rabilitÃ©  perÃ§ue  ou  ressentie  par  une  population  (individus,  groupes, organisations)  est  la  rÃ©sultante  du  croisement  entre  la  perception  du  risque  (Ã©valuation  du potentiel  destructeur  de  la  menace)  et  lâÃ©valuation  quâils  font  de  leur  capacitÃ©  Ã   faire  face,  Ã  sâadapter ou  se  reconstruire. Lâobjectif  final  sera  la  construction  dâun  Â« indice  de  vulnÃ©rabilitÃ© perÃ§ue Â».     

!16! Programme Ta Keo â Sauvegarde des Temples dâAngkor, Cambodge Â» (2006-2010)  Porteur du projet :  M.-F. AndrÃ© (GÃ©olab, UMR 6042 â CNRS).  Participants :  Membres de lâÃ©quipe vitesse de lâÃ©rosion de GÃ©olab : M.-F. AndrÃ©, S. Ãtienne, D. Mercier, F. Vautier,  O. Voldoire.  Membres de lâEcole FranÃ§aise dâExtrÃªme Orient  (EFEO) : P. RoyÃ¨re, J. DumarÃ§ay, C. Pottier,  I. Poujol.  Collaboration internationale : Ã©quipe du Prof.  H. Leisen, leader du German Apsara  Conservation Project (GACP).   Temple Ta Keo  Mots  clÃ©s  - Quantification  de  lâÃ©rosion,  patrimoine  culturel,  stratÃ©gie  de  conservation,    site dâAngkor, dÃ©veloppement durable.  Objectifs et intÃ©rÃªts scientifiques du programme Les objectifs de ce programme Ã©taient de conduire, en Ã©troite collaboration avec les architectes et archÃ©ologues  de  lâÃcole  FranÃ§aise  dâExtrÃªme-Orient  (EFEO)  dÃ©jÃ   sur  place,  une  opÃ©ration  Ã  double entrÃ©e : 1 â recherche  fondamentale :  quantifier  la  tranche  dâablation  rocheuse  intervenue  Ã   la  surface dâun monument inscrit au Patrimoine Mondial de lâHumanitÃ© depuis sa construction. Le Ta Keo possÃ¨de  des  surfaces  de  rÃ©fÃ©rence  (Ã©pannelÃ©es,  moulurÃ©es,  sculptÃ©es)  permettant  cette quantification, ce que dâautres sites ne possÃ¨dent pas. 2 â recherche  finalisÃ©e :  effectuer  un  diagnostic  prospectif  sur  lâaggravation  contemporaine  de lâÃ©rosion  des  surfaces  sculptÃ©es  et  procÃ©der  Ã   un  zonage  des  risques  de  maniÃ¨re  Ã   orienter  les stratÃ©gies  de  conservation  de  lâUNESCO-APSARA.  Le  Ta KÃ©o  constitue  lâarchÃ©type  du monument soumis Ã  un stress climatique du fait de lâouverture du couvert forestier. LâUNESCO-APSARA est en attente dâune Ã©valuation de lâimpact de la dÃ©forestation sur la dÃ©tÃ©rioration de la pierre monumentale dans la perspective  dâune gestion durable du parc archÃ©ologique dâAngkor, car une centaine de structures archÃ©ologiques est encore enfouie sous la forÃªt.    En somme, sur le plan de la recherche, je me sens donc un gÃ©ographe en phase avec les grandes problÃ©matiques  scientifiques  qui  interpellent  les  hommes  du  21e siÃ¨cle :  Ã   la  fois  lâimpact  des changements  climatiques  dans  le  cadre  de  la  rÃ©flexion  sur  le  changement  global  et  la  gestion  des territoires  de  lâenvironnement  avec  les  problÃ©matiques  sociÃ©tales  associÃ©es  aux  risques  naturels. La  liste  de  mes  travaux  ci-dessous  tÃ©moigne  de  mon  investissement  scientifique  sur  ces  deux grandes questions.    

!17!Liste classÃ©e des publications   Travaux acadÃ©miques (n = 2) Mercier D., 1998 - Le ruissellement au Spitsberg. L'impact d'un processus azonal sur les paysages d'un milieu polaire,  presqu'Ã®le  de  BrÃ¸gger  (79Â°N), ThÃ¨se de Doctorat, UniversitÃ© Blaise Pascal, Clermont-Ferrand II, 532 p.  Mercier  D.,  2010 - Â« La  gÃ©omorphologie  paraglaciaire.  Analyse  de  crises  dâorigine  climatique  dans  les environnements  englacÃ©s  et  sur  leurs  marges Â»,  Habilitation  Ã   Diriger  des  Recherches,  volume  3  inÃ©dit, 262 p.   Direction ou co-direction d'ouvrages (n = 7) 7 - Mercier D. (sous la direction de), 2013 â GÃ©omorphologie de la France, Paris, Dunod, 320 p.  6 - Mercier  D.,  2011. La  gÃ©omorphologie  paraglaciaire.  Changements  climatiques,  fonte  des  glaciers  et  crises Ã©rosives associÃ©es, Editions universitaires europÃ©ennes, 256 p. 5 - Penven  M.-J.,  Regnauld  H.,  Mercier  D.  (sous  la  direction  de),  2011 â MobilitÃ©  des formes  et surfaces  terrestres.  Des  changements  passÃ©s  aux  changements  actuels,  Rennes,  Presses  Universitaires  de Rennes, 220 p. 4 - AndrÃ© M.-F., Etienne S., Lageat Y., Le Coeur C., Mercier D. (sous la direction de), 2007 â Du continent  au  bassin  versant.  ThÃ©ories  et  pratiques  en  gÃ©ographie  physique  (Hommage  au  Professeur  Alain Godard), Clermont-Ferrand, Presses universitaires Blaise Pascal, Collection Nature & SociÃ©tÃ©s, vol. 4, 592 p. 3 - Mercier  D.  (sous  la  direction  de),  2004 â Le  commentaire  de  paysages en  gÃ©ographie  physique. Documents et mÃ©thodes, Paris, Armand Colin, collection U, 256 p. (rÃ©impression en 2010). 2 - Jousseaume V., Croix N., Mercier D. - 2003 â La Chapelle-Basse-Mer : commune ligÃ©rienne, guide de gÃ©ographie locale, Presses Universitaires de Rennes, collection Espace, Territoires, 95 p. 1 - Mercier  D.,  2001 - Le  ruissellement  au  Spitsberg.  Le  monde  polaire  face  aux  changements  climatiques, Presses Universitaires Blaise Pascal, Clermont-Ferrand, Collection Nature et SociÃ©tÃ©s, 278 p.  Coordination de numÃ©ros spÃ©ciaux de revues avec comitÃ© de lecture (n = 6) 6 - Mercier D. (coordinateur), 2012 â Xynthia : regards de la gÃ©ographie, du droit et de l'histoire, Norois, nÂ°222. 5 - Mercier  D.  (coordinateur),  2008 â Recent  advances  in  Paraglacial  Geomorphology â AvancÃ©es  rÃ©centes  en  GÃ©omorphologie  paraglaciaire, GÃ©omorphologie  :  relief,  processus, environnement, 4, pp. 219-272. 4 - Mercier  D.  (coordinateur),  2008 - Paraglaciaire  et  changements  climatiques, Bulletin  de l'Association de GÃ©ographes FranÃ§ais, 2, pp. 131-208. 3 - Mercier  D.,  Etienne  S.  (ed.),  2008 â Paraglacial  Geomorphology  :  Processes  and  paraglacial context, Geomorphology, Special Issue, vol. 95, nÂ°1-2, pp. 1-102. 2 - Mercier  D.  (coordinateur),  2005 â Les  milieux  polaires  et  subpolaires  de  lâAtlantique  Nord, Norois, nÂ°194, pp. 49-148. 1 - AndrÃ©  M.-F.,  Mercier  D.  (coordinateurs),  2003 â La  recherche  franÃ§aise  actuelle  dans  les milieux polaires et subpolaires, Bulletin de l'Association de GÃ©ographes FranÃ§ais, 4, pp. 343-416.  Chapitres dâouvrages collectifs (n = 25) 25 - Mercier  D.,  2015. LâArctique  face  aux  crises  gÃ©omorphologiques  paraglaciaires,  in  Joly  D. (ed.), LâArctique face au changement climatique : des milieux, des territoires et des hommes, Paris, Presses de lâEPHE (acceptÃ©, sous presses). 24 - Costa  S.,  Gueben-Veniere  S.,  Goeldner-Gianella  L.,  Mercier  D.,  2015.  Mouvements  de  la  

!18!surface  des  mers  et  des  ocÃ©ans  et  consÃ©quences  Ã   lâinterface  Terre-mer, in Escach N.  (dir.), GÃ©ographie des mers et des ocÃ©ans, Paris, Dunod, pp. 102-133. 23 - Mercier  D.,  2013 â Inondations, in Croix  A. et  al.,  (eds) â Dictionnaire  de  Nantes, Presses Universitaires de Rennes, pp. 538-540. 22 - Cossart E., Feuillet T., Mercier D., Ravanel L., Monnier S., 2013 â La cryosphÃ¨re, in Mercier D., (ed) â GÃ©omorphologie de la France, Paris, Dunod, pp. 97-112. 21 - Etienne  S. et  Mercier  D.,  2013 â Les  outre-mer,  in  Mercier  D.,  (ed) â GÃ©omorphologie  de  la France, Paris, Dunod, pp. 81-95. 20 - Mercier D., Maquaire O., Suanez S., Costa S., Vinet F., Fressard M., Lissak C., Fressard M., Thiery  Y.,  2013 â GÃ©omorphologie  et  risques  naturels,  in  Mercier  D.,  (ed) â GÃ©omorphologie  de la France, Paris, Dunod, pp. 173-186. 19 - Mercier D., 2010 â As paisagens da FranÃ§a nos Polos, in De Castro Panizza A. (ed), Paisagens Francesas, terroirs, cidades e litorais, Editora da Fecilcam, Campo MourÃ£o, pp. 103-110. 18 - Lantuit  H.,  Overduin  P.P.,  Solomon  S.,  Mercier  D.,  2010 - Coastline  dynamics  in  polar systems  using  remote  sensing,  in  Maanan  M. and Robin  M.  (eds) - Geomatic  solutions  for  coastal environments, Nova Science Publishers, pp. 163-174. 17 - Jousseaume V., Mercier D., 2009 - Ãvaluer la vulnÃ©rabilitÃ© architecturale de lâhabitat en zone inondable.  Lâexemple  du  Val  nantais,  in  S.  Becerra  et  A.  Peltier  (ed.), Risques  et  environnement: recherches interdisciplinaires sur la vulnÃ©rabilitÃ© des sociÃ©tÃ©s, Paris, L'Harmattan, 575 p., pp. 199-214. 16 - EtzelmÃ¼ller  B.,  Warburton  J  .,  Mercier  D.,  Etienne  S.,  Frauenfelder  R.,  2007 - Chapter  2 - Analysis of Sediment Storage: Geological and geomorphological context, in Achim A. Beylich and  Jeff  Warburton  (eds), SEDIFLUX  Manual,  Analysis  of  Source-to-Sink-Fluxes  and  Sediment Budgets  in  Changing  High-Latitude  and  High-Altitude  Cold  Environments,  Norges  Geologiske UndersÃ¸kelse, NGU-report nÂ°53, 158 p., pp. 37-60. 15 - Mercier D., 2007 â Le paraglaciaire : Ã©volution dâun concept, in Du  continent  au  bassin  versant. ThÃ©ories et pratiques en gÃ©ographie physique (Hommage au Professeur Alain Godard), Clermont-Ferrand, Presses universitaires Blaise Pascal, Collection Nature & SociÃ©tÃ©s, vol. 4, pp. 341- 353. 14 - Mercier  D.,  2005 â Changements  climatiques  et  mÃ©tamorphoses  des  paysages  polaires,  in AndrÃ©  M.-F.  (dir.) Le  monde  polaire  :  mutations  et  transitions,  Paris,  Ãditions  Ellipses,  collection Carrefours, 187 p., (Ch. 2, pp. 25-38). 13 - Mercier  D.,  Laffly  D. - 2005 â Actual  paraglacial  progradation  of  the  coastal  zone  in  the Kongsfjorden  area,  western  Spitsbergen  (Svalbard),  in  C.  Harris,  J.  Murton  (eds), Cryospheric Systems:  Glaciers  and  Permafrost,  Geological  Society,  London,  Special  publications  nÂ°  242,  pp. 111-117. 12 - Mercier  D.,  2004 â MÃ©thodologie  du  commentaire  de  paysages,  in  Mercier  D.  (dir.),  Le commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et  mÃ©thodes,  Paris,  Armand Colin, collection U, pp. 5-14. 11 - Mercier D., 2004 â Les paysages de lâeau solide sous les hautes latitudes, in Mercier D. (dir.), Le commentaire de paysages en gÃ©ographie physique. Documents et mÃ©thodes, Paris, Armand Colin, collection U, pp. 36-39. 10 - Mercier D., Jacob N., 2004 â Les paysages continentaux de lâÃ©rosion par lâeau, in Mercier D. (dir.),  Le  commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et  mÃ©thodes,  Paris, Armand Colin, collection U, pp. 48-51. 9 - Mercier D., 2004 â Les paysages fluviaux des monts Mackenzie (Canada), in Mercier D. (dir.), Le commentaire de paysages en gÃ©ographie physique. Documents et mÃ©thodes, Paris, Armand Colin, collection U, pp. 52-55. 8 - Mercier D., 2004 â LâÃ©tagement des paysages de toundras en NorvÃ¨ge, in Mercier D. (dir.), Le commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et  mÃ©thodes,  Paris,  Armand Colin, collection U, pp. 60-63. 7 - Mercier  D.,  2004 â Lâeau  et  les  roches  :  les  modelÃ©s  pÃ©riglaciaires,  in  Mercier  D.  (dir.),  Le commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et  mÃ©thodes,  Paris,  Armand  

!19!Colin, collection U, pp. 92-95. 6 - Mercier D., 2004 â Les paysages du Spitsberg : eau solide, eau liquide et formes associÃ©es, in Mercier  D.  (dir.),  Le  commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et mÃ©thodes, Paris, Armand Colin, collection U, pp. 96-99. 5 - Mercier  D.,  Peulvast  J.-P.,  2004 â Les  temps  quaternaires  du  paysage  :  plage  soulevÃ©e,  plage perchÃ©e,  in  Mercier  D.  (dir.),  Le  commentaire  de  paysages  en  gÃ©ographie  physique. Documents et mÃ©thodes, Paris, Armand Colin, collection U, pp. 130-133. 4 - Mercier  D.,  Amat  J.-P.,  2004 â La  reconquÃªte  vÃ©gÃ©tale  holocÃ¨ne  du  grand  nord  canadien,  in Mercier  D.  (dir.),  Le  commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et mÃ©thodes, Paris, Armand Colin, collection U, pp. 134-137. 3 - Mercier  D.,  2004 â Risques  en  montagne  :  le  cas  de  lâIslande  du  Nord-Ouest,  in  Mercier  D. (dir.),  Le  commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et  mÃ©thodes,  Paris, Armand Colin, collection U, pp. 188-191. 2 - Mercier D., 2004 â Risque dâinondation de plaine : la Loire dans le Val nantais, in Mercier D. (dir.),  Le  commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et  mÃ©thodes,Paris, Armand Colin, collection U, pp. 196-199. 1 - Mercier  D.,  Galochet  M.,  Gramond  D.,  2004 â Des  paysages  incendiÃ©s  en  Corse, in  Mercier D.  (dir.),  Le  commentaire  de  paysages  en  gÃ©ographie  physique.  Documents  et  mÃ©thodes, Paris, Armand Colin, collection U, pp. 208-211.  Articles dans des revues avec comitÃ© de lecture (n = 44) 44 - Coquin  J.,  Mercier  D.,  Bourgeois  O., Decaulne  A., 2015 â A  paraglacial  origin  for  cirques: example from TindastÃ³ll ridge, Northern Iceland, Geomorphology (accepted, in Press). 43 - Decaulne  A., Cossart Ã., Mercier  D.,  Coquin  J., Feuillet  T.,  JÃ³nsson  H.P., 2015 â Early Holocene  dating  of  the  Vatn  landside (SkagafjÃ¶rÃ°ur,  central north  Iceland)  and  Holocene slope development, The Holocene (accepted, in Press). 42 - Creach A., Chevillot-Miot E., Mercier D., Pourinet L., 2015 â Vulnerability map to sea-flood risk  of  buildings  on  Noirmoutier  Island  (France), Journal  of  Maps, DOI: 10.1080/17445647.2015.1027041 41 - Creach  A.,  Pardo  S.,  Guillotreau  P.,  Mercier  D.,  2015 â The  use  of  a  micro-scale  index  to identify potential death risk areas due to sea-flood surges: lessons from Storm Xynthia on the French Atlantic coast, Natural Hazards, DOI 10.1007/s11069-015-1669-y 40 - Coquin  J.,  Mercier  D.,  Bourgeois  O.,  Cossart  Ã.,  Decaulne  A.,  2015 - Gravitational spreading  of  mountain  ridges  coeval  with  Late  Weichselian  deglaciation:  impact  on  glacial landscapes in TrÃ¶llaskagi, northern Iceland, Quaternary Science Reviews, 107, 1, pp. 97-213 39 - Chevillot-Miot E., Mercier D., 2014 â La vulnÃ©rabilitÃ© face au risque de submersion marine : exposition  et  sensibilitÃ©  des  communes  littorales  de  la  rÃ©gion  Pays  de  la  Loire  (France), VertigO La revue Ã©lectronique en science de lâenvironnement, vol. 14, nÂ°2, http://vertigo.revues.org/15110 38 - Feuillet  T.,  Coquin  J.,  Mercier  D.,  Cossart  Ã.,  Decaulne  A.,  JÃ³nsson  H.P.,  SÃ¦mundsson Ã., 2014 - Focusing  on  the  spatial  non-stationarity  of  landslide  predisposing  factors  in  northern Iceland:  Do  paraglacial  factors  vary  over  space?, Progress  in  Physical  Geography.  38,  3,  pp.  354-377. DOI:10.1177/0309133314528944  37 - Gourronc  M.,  Bourgeois  O.,  Mege  D.,  Pochat  S.,  Bultel  B.,  Masse  M.,  Le  Deit  L.,  Le Mouelic  S.,  Mercier  D.,  2014 - One  million  cubic  kilometers  of  fossil  ice  in  Valles  Marineris: Relicts  of  a  3.5  Gy  old  glacial  landsystem  along  the  Martian  equator, Geomorphology,  204,  pp. 235-255,  DOI : 10.1016/j.geomorph.2013.08.009 36 - Cossart  Ã.,  Mercier  D.,  Decaulne  A.,  Feuillet  T.,  JÃ³nsson  H.P.,  SÃ¦mundsson Ã.,  2014 -Impacts of post-glacial rebound on landslide spatial distribution at a regional scale in northern Iceland  (SkagafjÃ¶rÃ°ur), Earth  Surface  Processes  and  Landforms,  39,  3,  pp.  336-350,  DOI: 10.1002/esp.3450  

!20!35 - Chadenas C., Creach A., Mercier D., 2014 - The impact of storm Xynthia in 2010 on coastal flood  prevention  policy  in  France, Journal  of  Coastal  Conservation. 18, 5,  pp.  529-538. DOI: 10.1007/s11852-013-0299-3 34 - Mercier D., Cossart Ã., Decaulne A., Feuillet T., JÃ³nsson H.P., SÃ¦mundsson Ã., 2013 - The HÃ¶fÃ°ahÃ³lar  rock avalanche  (sturzstrÃ¶m):  Chronological  constraint  of  paraglacial  landsliding on an Icelandic hillslope, The Holocene, 23, 3, pp. 431-445. 33 - Cossart Ã., Mercier D., Decaulne A., Feuillet T., 2013 - Paraglacial adjustment of mountains slopes: typology, timing and contribution to cascading fluxes, Quaternaire, 24, 1, pp. 13-24. 32 - Mercier D., Chadenas C., 2012 - La tempÃªte Xynthia et la cartographie des Â« zones noires Â» sur  le  littoral  franÃ§ais  :  analyse  critique  Ã   partir  de  l'exemple  de  La  Faute-sur-Mer  (VendÃ©e), Norois, nÂ°222, pp. 45-60. 31 - Feuillet  T.,  Mercier  D.,  2012 - Post-Little  Ice  Age  patterned  ground  development  on  two Pyrenean proglacial areas: from deglaciation to periglaciation, Geografiska  Annaler, 94, pp. 363-376. . 30 - Feuillet  T.,  Mercier  D.,  Decaulne  A.,  Cossart  E.,  2012 - Classification  of  sorted  patterned ground  areas  based  on  their  environmental  characteristics  (SkagafjÃ¶rÃ°ur,  Northern  Iceland), Geomorphology, 139-140, pp. 577-587. 29 - Chauveau  E.,  Chadenas  C.,  Comentale  B.,  Pottier  P.,  Blanloeil  A.,  Feuillet  T.,  Mercier  D., Pourinet  L.,  Rollo  N.,  Tillier I.,  Trouillet  B.,  2011 - Xynthia  :  leÃ§ons  dâune  catastrophe, CybergÃ©o. 538. http://cybergeo.revues.org/23763 28 - Fattal  P.,  Robin  M.,  Paillart  M.,  Maanan  M.,  Mercier  D.,  Lamberts  C.,  Costa  S.,  2010 â Effets  des  tempÃªtes  sur  une  plage  amÃ©nagÃ©e  et  Ã   forte  protection  cÃ´tiÃ¨re  : la  plage  des  Ãloux (cÃ´te de Noirmoutier, VendÃ©e, France), Norois, nÂ°215, pp. 101-114. 27 - Jousseaume  V.,  Mercier  D.,  2009 â Â¿El  agua  domesticada?  Procesos,  actores  y vulnerabilidades del ordenamiento territorial en el valle del Loira, Reflexiones  GeogrÃ¡ficas,13, pp. 105-121. 26 - Mercier  D.,  Ãtienne  S.,  Sellier  D.,  AndrÃ©  M.-F.,  2009 - Paraglacial  gullying  of  sediment-mantled slopes: a case study of ColletthÃ¸gda, Kongsfjorden area, West Spitsbergen (Svalbard), Earth Surface Processes and Landforms, 34, pp. 1772-1789. 25 - Mercier  D.,  2008 - Paraglacial  geomorphology:  Conceptual  and  methodological  revival, GÃ©omorphologie : relief, processus, environnement, 4, pp. 219-222. 24 - Mercier D., 2008 - Paraglacial and paraperiglacial landsystems: concepts, temporal scales and spatial distribution, GÃ©omorphologie : relief, processus, environnement, 4, pp. 223-234. 23 - AndrÃ©  M.-F.,  Ãtienne  S.,  Mercier  D.,  Vautier  F.,  Voldoire  O.,  2008 â Assessment  of sandstone  deterioration  at  Ta  Keo  temple  (Angkor)  :  first  results  and  future  prospects, Environmental Geology, 56, pp. 677-688. 22 - Mercier D., 2008 - Le gÃ©osystÃ¨me paraglaciaire face aux changements climatiques, Bulletin  de lâAssociation de gÃ©ographes FranÃ§ais, 2, pp. 131-140. 21 - Ãtienne S., Mercier D., 2008 â Reconstitution de lâhistoire paraglaciaire dâune marge glaciaire face aux changements climatiques : le cas du glacier Baron au Spitsberg, Bulletin  de  lâAssociation de gÃ©ographes FranÃ§ais, 2, pp. 199-208. 20 - AndrÃ©  M.-F.,  Mercier  D.,  Ãtienne  S.,  Voldoire  O.,  Vautier  F.,  2008 - Approche gÃ©ographique de l'Ã©rosion des temples d'Angkor : enjeux et perspectives, Bulletin de lâAssociation de GÃ©ographes FranÃ§ais, 1, pp. 105-117. 13 - Ãtienne  S.,  Mercier D.,  Voldoire  O.,  2008 - Temporal  scales  and  deglaciation  rhythms  in  a polar  glacier  margin,  Baronbreen,  Svalbard, Norsk  Geografisk  Tidsskrift â Norwegian  Journal  of Geography, 62, 2, pp. 102-114. 19 - Mercier D., Ãtienne S., 2008 â The Paraglacial concept: new approaches to glacial landscape evolution, Geomorphology, Special Issue, 95, 1-2, pp. 1-2. 18 - Moreau M., Mercier D., Laffly D., Roussel E., 2008 â Impacts of recent paraglacial dynamics on  plant  colonization  :  A  case  study  on  midtre  LovÃ©nbreen  foreland,  Spitsbergen  (79Â°N),  

!21!Geomorphology, 95, 1-2, pp. 48-60. 17 - Ãtienne S., Mercier D., Voldoire O., 2006 â Paraglacial evolution of Conway glacier complex foreland, Northwestern Spitsbergen, Svalbard, Norsk Geologisk Forening, 4, pp. 36-37. 16 - Ãtienne S., Mercier D., AndrÃ© M.-F., 2005 â Chronique polaire, Norois, 194, pp. 125-148. 15 - Mercier  D.,  2005 â Norois  et  lâArctique  :  cinquante  annÃ©es  de  publications  (1954-2004), Norois, 194, pp. 51-58. 14 - Jousseaume  V.,  Landrein  J.,  Mercier  D.,  2004 â La  vulnÃ©rabilitÃ©  des  hommes  et  des habitations  face  au  risque  dâinondation  dans  le  Val  nantais  (1841-2003)  :  entre  lÃ©gislation nationale et pratiques locales, Norois, 192, pp. 29-45. 13 - Mercier  D.,  Ãtienne  S.,  Sellier  D.,  2004 â Recent  paraglacial  slope  deformation  in  the Kongsfjorden area, West Spitsbergen (Svalbard), NÃ¡ttÃºrustofa Nordurlands Vestra, 3, pp. 46-47. 12 - Moreau  M.,  Mercier  D.,  Laffly  D.,  2004 â Un  siÃ¨cle  de  dynamiques  paraglaciaires  et vÃ©gÃ©tales sur les marges du Midre LovÃ©nbreen, Spitsberg nord occidental, GÃ©omorphologie : relief, processus, environnement, 2, pp. 157-168. 11 - Ãtienne  S.,  Mercier  D. â 2003 â Le  volcanisme  des  milieux  englacÃ©s, Photo-InterprÃ©tation,  39, 3-4, pp. 55-64. 10 - Mercier D., 2003 - Glaciaire, paraglaciaire, pÃ©riglaciaire : relais et combinaisons de processus, Bulletin de l'Association de GÃ©ographes FranÃ§ais, 4, pp. 349-354. 9 - Mercier  D.,  2003 - Les  gÃ©ographes  franÃ§ais  et  les  milieux  polaires  et  subpolaires, Bulletin  de l'Association de GÃ©ographes FranÃ§ais, 4, pp. 407-416. 8 - Mercier  D.,  Laffly  D.,  2003 â La  progradation  des  littoraux  meubles  au  Spitsberg  :  une rÃ©ponse  sÃ©dimentaire  paraglaciaire  au  changement  climatique  contemporain, Bulletin  de l'Association de GÃ©ographes FranÃ§ais, 4, pp. 362-368. 7 - Laffly D., Mercier D., 2002 - Global change and paraglacial morphodynamic modification in Svalbard, International Journal of Remote Sensing, 43, 21, pp. 4 743-4 760. 6 - Mercier  D.,  2002 - La  dynamique  paraglaciaire  des  versants  du  Svalbard, Zeitschrift  fÃ¼r Geomorphologie, 46, 2, pp. 203-222. 5 - Mercier D., 2001 - Les piÃ©monts des hautes latitudes : rythmes et crises morphogÃ©niques, Sud-Ouest EuropÃ©en, 10, pp. 3-21. 4 - Mercier  D.,  2000 - Du  glaciaire  au  paraglaciaire  :  la  mÃ©tamorphose  des  paysages  polaires  au Svalbard, Annales de GÃ©ographie, 616, pp. 580-596. 3 - Laffly D., Mercier D., 1999 - RÃ©flexions mÃ©thodologiques sur les observations de terrain et la tÃ©lÃ©dÃ©tection  (cartographie  des  sandurs  en  Baie  du  Roi,  Spitsberg  nord-occidental), Photo-InterprÃ©tation, 2, pp. 15-28 et 48-58. 2 - Mercier  D.,  1998 - Un  siÃ¨cle  d'Ã©rosion  sur  les  moraines  du  Petit  Ãge  glaciaire  au  Spitsberg nord-occidental, Bulletin de l'Association de GÃ©ographes FranÃ§ais, 1, pp. 96-108. 1 - Mercier  D.,  1997 - L'impact  du  ruissellement  sur  les  moraines  latÃ©rales  du  Glacier  du  Roi (ColletthÃ¸gda, Spitsberg, 79Â°N), Norois, 175, pp. 549-566.  Actes de colloques avec comitÃ© de lecture (n = 5) 5 - Creach  A.,  Mercier  D.,  Pardo  S.,  2014 - Identification  et  cartographie  des  zones  Ã   risque potentiellement  mortel  face  Ã   la  submersion  marine  :  lâindice  de  V.I.E.  appliquÃ©  Ã   La  Faute-sur-Mer  (VendÃ©e,  France), Actes  du  colloque  international  Â« Connaissance  et  comprÃ©hension des risques cÃ´tiers : alÃ©as, enjeux, reprÃ©sentations, gestion Â», Brest, 3-4 juillet, pp. 214-223. 4 - Chevillot-Miot  E.,  Mercier  D.,  2014 - La  vulnÃ©rabilitÃ©  face  au  risque  de  submersion  marine. Exposition et sensibilitÃ© des communes littorales de la RÃ©gion Pays de la Loire (France). Actes du colloque international Â« Connaissance et comprÃ©hension des risques cÃ´tiers : alÃ©as, enjeux, reprÃ©sentations, gestion Â», Brest, 3-4 juillet, pp. 289-297. 3 - Mercier  D.,  AndrÃ©  M.-F.,  Ãtienne  S.,  Laffly  D.,  Moreau  M.,  Sellier  D.,  Dupont  J.,  Prick  A., Rachlewicz  G.,  2005 - Paraglacial  dynamics  in  Svalbard,  Pr.  400 â Geomorphoclim â French Polar Institute Paul-Ãmile Victor, in S. Ãtienne (Ã©diteur), 2005 â Shifting  lands.  New  insights  into  

!22!periglacial  geomorphology,  Clermont-Ferrand,  Ã©ditions  Seteun,  collection  GÃ©oenvironnement,  pp. 43-45. 2 - Moreau M., Mercier D., Laffly D., Roussel E., 2005 â Runoff impact on plant colonization on the forefield of midre LovÃ©nbreen, Spitsbergen (79Â°N) since the end of the little ice age, in S. Ãtienne  (Ã©diteur),  2005 â Shifting  lands.  New  insights  into  periglacial  geomorphology,  Clermont-Ferrand, Ã©ditions Seteun, collection GÃ©oenvironnement, p. 26. 1 - Costard  F.,  Forget  F.,  Mangold  N.,  Mercier  D.,  Peulvast  J.-P.,  2001 -Debris  flows  on  Mars  : analogy  with  terrestrial  periglacial  environment  and  climatic  implications,  Geophysical detection  of  subsurface  water  on  Mars, Proceedings  of  the  32nd  Lunar  and  Planetary  Science Conference, Houston, USA, 2 p.  Articles dans des revues sans comitÃ© de lecture (n = 13) 13 - Chevillot-Miot  Ã.,  Creach  A.,  Mercier  D.,  2013 â La  vulnÃ©rabilitÃ©  du  bÃ¢ti  face  au  risque  de submersion  marine :  premiers  essais  de  quantification  sur  lâÃ®le  de  Noirmoutier  (VendÃ©e), Les Cahiers Nantais, 1, pp. 5-14. 12 - Mercier  D.,  Acerra  D.,  2011 â Xynthia,  une  tragÃ©die  prÃ©visible,  Introduction, Place Publique, Hors SÃ©rie, pp. 5-8. 11 - Chadenas  C.,  Pottier  P.,  Mercier  D.,  Chauveau  E.,  Pourinet  L.,  2011 â Le  prix  dâune urbanisation abusive, Place Publique, Hors SÃ©rie, Xynthia, une tragÃ©die prÃ©visible, pp. 24-29 10 - Mercier D., 2010 - Le rÃ©chauffement climatique et les Ã©cosystÃ¨mes polaires, Bulletin  dâÃtudes de la Marine nÂ°47, pp. 83-95. 9 - Mercier D., 2007 â Les PÃ´les, Un observatoire privilÃ©giÃ©, Textes et Documents pour la classe, TDC nÂ° 942, Centre National de Documentation PÃ©dagogique, pp. 6-13. 8 - Mercier D., 2007 â Au chevet des Apsaras dâAngkor. Contribution Ã  lâÃ©tude de la maladie de la pierre monumentale, GÃ©ographies Nantaises, AmÃ©nagement Recherche, Formation, 3, p. 3. 7 - Jousseaume  V.,  Mercier  D.,  2005 â Processus  et  acteurs  de  lâamÃ©nagement de  la  zone inondable  du  Val  nantais.  RÃ©flexions  sur  la  prise  de  risque  dâune  sociÃ©tÃ©  promÃ©thÃ©enne, Cahiers Nantais, 64, pp.23-42. 6 - Mercier    D.,  2004 â La  mÃ©moire  des  crues  dans  le  Val  nantais, La  Loire  et  ses  terroirs,  49,  pp. 19-23. 5 - AndrÃ© M.-F., Mercier  D., 2003 â Lâeau dans les dÃ©serts polaires : source de vie et moteur de la transformation des paysages, Emenir, 28-29, pp. 18-22. 4 - Mercier    D.,  2001 - Le  poids  des  dynamiques  paraglaciaires  dans  l'Ã©volution  des  versants arctiques, Environnements PÃ©riglaciaires, 8, pp. 70-84. 3 - Mercier  D., 1999 - Ruissellement et pergÃ©lisol au Spitsberg, Environnements  PÃ©riglaciaires, 6, pp. 53-61. 2 - Mercier    D.,  1998 - Rythmes  et  vitesses  de  l'Ã©rosion  en  milieu  polaire  :  l'incision  des  plages soulevÃ©es holocÃ¨nes au Spitsberg nord-occidental, Cahiers nantais, 49, pp. 151-158. 1 - Mercier  D., Marlin C., Laffly D., 1998 - Ruissellement et Ã©rosion en milieu polaire ocÃ©anique. Exemple du basin-versant du Zeppelinfjellet, Presqu'Ã®le de BrÃ¸gger, Spitsberg nord-occidental 79Â° N, Cahiers nantais, 49, pp. 159-179.  Colloques internationaux (n = 25) 25 - 2015 Â« 3rd Planetary cryosphere workshop Â», Nantes, 26-28 January. Communication : After the Ice : geomorphological studies in Iceland 24 - 2014  Â« Connaissance  et  comprÃ©hension  des  risques  cÃ´tiers :  alÃ©as,  enjeux, reprÃ©sentations, gestion Â», Brest, 3-4 juillet Co-auteur  dâune  communication prÃ©sentÃ©e  par  Axel  Creach  :  Identification  et  cartographie des  zones  Ã   risque  potentiellement  mortel  face  Ã   la  submersion  marine  :  lâindice  de  V.I.E. appliquÃ© Ã  La Faute-sur-Mer (VendÃ©e, France), avec Sophie Pardo. 23 - 2014  Â« Connaissance  et  comprÃ©hension  des  risques  cÃ´tiers :  alÃ©as,  enjeux,  

!23!reprÃ©sentations, gestion Â», Brest, 3-4 juillet Co-auteur  dâune  communication prÃ©sentÃ©e  par  Elie  Chevillot-Miot  :  La  vulnÃ©rabilitÃ©  face  au risque  de  submersion  marine.  Exposition  et  sensibilitÃ©  des  communes  littorales  de  la  RÃ©gion Pays de la Loire (France). 22 - 2014 Â« 31th Geological nordic winter meeting Â», Lund, SuÃ¨de, 8-10 janvier Co-auteur  dâune  communication prÃ©sentÃ©e  par  Julien  Coquin  :  Morphologic  evidence  for  a sackung  event  in  TrÃ¶llaskagi  mountain  (Northern  Iceland),  avec  Olivier  Bourgeois,  Etienne Cossart. 21 - 2012 Â« VariabilitÃ©  spatiale  des  environnements  quaternaires  contraintes,  Ã©chelles  et temporalitÃ©s, Q8 Â», Clermont-Ferrand, France, 29 fÃ©vrier-2 mars. Co-auteur  dâune  communication :  VariabilitÃ©  des  rÃ©ponses  paraglaciaires  dans  les  montagnes de  lâhÃ©misphÃ¨re  Nord :  facteurs  de  contrÃ´le,  empreintes  gÃ©omorphologiques  et  transferts sÃ©dimentaires Etienne Cossart, Denis Mercier, Armelle Decaulne, Thierry Feuillet 20 - 2013 Â« European Geosciences Union Â», Vienne, Autriche, 7â12 avril  1 - Key-note  lecture,  invitÃ©  par  les  organisateurs  de  la  session GM9.2/HS9.8/NH3.15: Geomorphic  and  hydrological  processes  in  proglacial  areas  under  conditions  of  (rapid) deglaciation;  Convener:  Tobias  Heckmann;  Co-Conveners:  Andrew  Kos,  Christian  Briese, Reynald Delaloye, Volkmar Mair, Samuel McColl, David Morche, Philip Owens, Tim Stott Titre de la communication : Paraglacial processes during rapid deglaciation: a question of time and space.  2 â Co-auteur  de  la  communication  dâArmelle  Decaulne  â  The  Vatn  landslide,  SkagafjÃ¶rÃ°ur, northern Iceland: evidence of an early Holocene paraglacial crisis and impact on further slope developmentâ. 19 - 2013  Â« Risques  littoraux  et  maritimes  Â», JournÃ©es scientifiques de lâUniversitÃ© de Nantes, Colloque international, CitÃ© des CongrÃ¨s, 7 juin  Organisateur du colloque.  Communication :  Analyse  de  la  vulnÃ©rabilitÃ©  du  littoral  de  la  rÃ©gion  des  Pays  de  la  Loire  face au risque de submersion marine.  Co-auteur avec Elie Chevillot-Miot, Axel Creach et Sophie Pardo 18 - 2013 Â« Colloque Franco-IndonÃ©sien Â», Padang, IndonÃ©sie, 24-25 juin  ConfÃ©rencier invitÃ© par l'Ambassade de France en IndonÃ©sie au sÃ©minaire franco-indonÃ©sien Ã  Padang sur l'Ã®le de Sumatra les 24 et 25 juin 2013. Le thÃ¨me de cette rencontre Ã©tait ""Land use planning  in  coastal  regions  facing  natural  risks.  How  to  think  natural  risks  and  mitigating strategies in land use planning processes of coastal regions?""   Le titre de la communication : ""How to mitigate the risk of marine submersion on the French coast"" 17 - 2013 Â« 8th International Conference on Geomorphology Â», Paris, aoÃ»t PrÃ©sentation dâune communication :   1 :  Key-note  lecture,  invitÃ©  par  les  organisateurs  Jasper  Knight  &  Stephan  Harrison  de  la session Paraglacial geomorphology,   Titre : Toward a comprehensive paraglacial model: case studies from Iceland  Denis  Mercier,  Etienne  Cossart,  Armelle  Decaulne,  Thierry  Feuillet,  Julien  Coquin,  Olivier Bourgeois, StÃ©phane Pochat, Helgi PÃ¡ll JÃ³nsson, Ãorsteinn SÃ¦mundsson   2 â Co-auteur de la communication prÃ©sentÃ©e par Armelle Decaulne   The  Vatn  landslide, SkagafjÃ¶rÃ°ur,  northern  Iceland:  early  Holocene  dating  and  Holocene palaeoenvironmental reconstitution potentialities   Decaulne A., Mercier D., Cossart E., Feuillet T., JÃ³nsson H.P., SÃ¦mundsson Ã.  3 â Co-auteur de la communication prÃ©sentÃ©e par Etienne Cossart  Impacts  of  post  glacial  rebound  on  landsliding  at  a  regional  scale  in  Northern  Iceland (SkagafjÃ¶rÃ°ur): spatial distribution and mechanisms involved   Cossart E., Mercier D., Decaulne A., Feuillet T., JÃ³nsson H., SÃ¦mundsson Ã.  

!24! 4 â Co-auteur du poster sur le Maroc prÃ©sentÃ©e par Etienne Ragaru,   Lithological control on coastal rock cliffs erosion of Safi, Morocco   Ragaru E., Mercier D., Chaibi M., Maanan M. 16 - 2012 Â« Nordic  WorkShop  on  cosmogenic  Nuclide  dating Â»,  May  21st - 22nd 2012, Trondheim, Geological Survey of Norway  Communication  prÃ©sentÃ©e  par  Armelle  Decaulne  (co-auteurs :  Armelle  Decaulne,  Denis Mercier,  Etienne  Cossart,  Thierry  Feuillet, Ãorsteinn  SÃ¦mundsson,  Helgi  PÃ¡ll  JÃ³nsson) : Dating  post-glacial  geoprocesses  in  SkagafjÃ¶rÃ°ur  (Central  North Iceland)  by  geomorphology, tephrochronology, radiocarbon and cosmogenic nucleids. 15 - 2012 Â« Â« 32th  International  Geographic  Union Â»,  26-30  aoÃ»t  2012,  Cologne,  Allemagne, Co-auteur dâune communication prÃ©sentÃ©e par CÃ©line Chadenas et Axel Creach intitulÃ©e : The impact of storm Xynthia (2010) on French coastal risk management. 14 - 2012 - Â« 30th  Nordic  geological  winter  meeting Â», Reykjavik (Islande) du 9 au 12 janvier 2012   2 communications :   Reconstructing  chronology  of  post-glacial  mass  movements  int  the  SkagafjÃ¶rÃ°ur (Northern Iceland) from radiocarbon, tephrochronological and geomorphological results  Typology  of  sorted  patterned  ground  sites  in  SkagafjÃ¶rÃ°ur (Northern  Iceland)  by  using  a factor analysis of mixed data 13 - 2011 Â« European Geosciences Union Â», Vienne, Autriche, 3 â 9 avril PrÃ©sentation  dâune  communication  :  Dating  of  a  rock  avalanche  in SkagafjÃ¶rÃ°ur,  Northern Iceland: pieces of evidence of a paraglacial origin. Avec  Denis  Mercier,  Armelle  Decaulne,  Etienne  Cossart,  Thierry  Feuillet,  Thorsteinn SÃ¦mundsson and Helgi PÃ¡ll JÃ³nsson 12 - 2011 Â« Mondes polaires sciences environnementales et sociales pour comprendre les changements  observÃ©s â Polar  worlds  Environmental  and  social  sciences  to understand observed  changes Â»,  26-28  janvier  2011,  Centre  National  de  la  recherche  scientifique (CNRS) 3 rue Michel-Ange 75016 Paris. - PrÃ©sentation dâune communication : Baltzer A., Mercier D., Laffly D., Deloffre J., Lafite R., Glacial,  fluvial,  coastal  and  offshore  cascading-system  to  assess  deglaciation  in  a  polar environment, a case study in the Kongsffjorden area, Svalbard. - SynthÃ¨se des dÃ©bats et clÃ´ture du colloque pour les sciences environnementales. 11 - 2010 Â« Qualitative and Quantitative Analysis of Sedimentary Fluxes and Budgets in Changing Cold Climate Environments: Field-Based Approaches and Monitoring Â», 5th I.A.G./A.I.G. SEDIBUD Workshop Sediment Budgets in Cold Environments, SaudarkrokÃ¼r, Islande, 19 â 25 septembre PrÃ©sentation  dâune  communication  :  The  Hofdaholar  rock  avalanche  in SkagafjÃ¶rÃ°ur, Northern  Iceland:  geomorphological  characteristics  and  relative  dating,  pieces  of  evidence  of a paraglacial origin. Avec Armelle Decaulne, Denis Mercier, Etienne Cossart, Thierry Feuillet, ?orsteinn SÃ¦mundsson and Helgi PÃ¡ll JÃ³nsson 10 - 2009 Â« Ancient Landscapes â Modern Perspectives Â», 7th International Conference on Geomorphology, Melbourne, 6-11 juillet. Communication : Quantify paraglacial adjustment of sediment-mantled slopes in a warming polar environment, Svalbard. 9 - 2007 Â« NySMAC  Ny-Ãlesund  and  the  International  Polar  Year Â», Cambridge, UK, 16-17 octobre, http://www.antarctica.ac.uk/nysmac/ PrÃ©sentation  dâune  communication  :  Paraglacial geomorphology  in  Kongsfjorden  area, Svalbard. 8 - 2006 Â« Sedimentary  source-to-sink  fluxes  in  cold  environments Â», Trondheim, NorvÃ¨ge,  

!25!29  octobre- 2  novembre,  4e  colloque  dans  le  cadre  du  rÃ©seau  SEDIFLUX  de  la  Fondation EuropÃ©enne pour la Science (ESF), http://www.ngu.no/sediflux Communication : Paraglacial modification on Conwaybreen forefield, Spitsberg (avec S. Ãtienne). 7 - 2005 Â« Second  International  Conference  on  Arctic  Research  Planning  (ICARP  II) Â», Copenhagen, Danemark, 10-13 novembre, http://www.icarp.dk/ ReprÃ©sentant du RÃ©seau Arctique du CNRS (ex. GDR 049 Recherches arctiques). PrÃ©sentation des posters du groupe et participation aux working groups. 6 - 2005 Â« Joint  French-German  collaborations  for  science  in  Svalbard Â»  Workshop AWIPEV, Strasbourg, 2-3 mars. Ã la demande de lâIPEV : communication dans le cadre des Key-lectures :  - Geosciences research in Svalbard : a review. PrÃ©sentation dâun poster : Paraglacial dynamics in Svalbard. Chairman of the working group : Soils, geology and geormorphology. PrÃ©sentation des rÃ©sultats du working group en sÃ©ance plÃ©niÃ¨re. Lien avec la page du Workshop : http://www.ipev.fr/awipev/Events_Reports.htm 5 - 2005 Â« Shifting lands: new insights into periglacial geomorphology Â», Clermont-Ferrand, 20-22  janvier,  2e  colloque  dans  le  cadre  du  rÃ©seau  SEDIFLUX  de  la  Fondation  EuropÃ©enne pour la Science (ESF). http://www.ngu.no/sediflux 2 Communications : - Paraglacial dynamics in Svalbard (poster). - Runoff  impact  on  plant  colonisation  on  the  forefield  of  midre  LovÃ©nbreen,  Spitsbergen (79Â°N) since the end of the little ice age (avec M. Moreau, D. Laffly et E. Roussel). - Paraglacial session (Chairman). 4 - 2004 Â« Sedimentary source-to-sink fluxes in cold environments Â», Saudarkrokur, Islande, 1e colloque dans le cadre du rÃ©seau SEDIFLUX de la Fondation EuropÃ©enne pour la Science (ESF), 17-22 juin, http://www.ngu.no/sediflux Communication  :  Recent  paraglacial  slope  deformation  in  the  Kongsfjorden  area,  West Spitsbergen (Svalbard). 3 - 2003 Â« Cryospheric systems: glaciers and permafrost Â», Londres, UK, 13-14 janvier. Communication : Paraglacial progradation of the coastal zone in Svalbard since the end of the Little Ice Age (avec D. Laffly). 2 - 2002 Â« Seventh Circumpolar Symposium on Remote Sensing of Polar Environments Â», Longyearbyen, Svalbard, NorvÃ¨ge, 24-27 juin. Communication  :  Shoreline  paraglacial  progradation  on  Svalbard  since  the  end  of  the  Little Ice Age (avec D. Laffly). 1 - 2000 Â« Sixth  Circumpolar  Symposium  on  Remote  Sensing  of  Polar  Environments Â», Yellowknife, N.W.T., Canada, 12-14 juin. Communication  :  Global  change  and  paraglacial  morphodynamic  modification  in  Svalbard (avec D. Laffly).  Colloques nationaux (n = 14) 14 - 2013 Â« 1993-2013 :  20  ans  de  la  MSH  Ange-GuÃ©pin :  20  ans de  projets interdisciplinaires Â», Nantes, 3 & 4 octobre 2013 Participation Ã  la table ronde : Ã©largissement des Ã©chelles Ã  partir dâun objet local : la santÃ© et lâeau AnimÃ©e  par  Yannick  Lemarchand  et  prÃ©sence  de  Patrick  Chaumette,  Anne-Chantal  Hardy  et Maryvonne Bodiguel. 13 - 2011 Â« Colloque  de  l'Association  franÃ§aise  du  PÃ©riglaciaire Â»,  Institut  de  GÃ©ographie, Paris, 29 janvier PrÃ©sentation dâune communication : CaractÃ©ristiques gÃ©omorphologiques et datation relative  

!26!dâune avalanche rocheuse : le cas de HÃ¶fdahÃ³lar, SkagafjÃ¶rÃ°ur, Islande septentrionale. Avec Armelle Decaulne, Denis Mercier, Etienne Cossart, Thierry Feuillet, Thorsteinn SÃ¦mundsson and Helgi PÃ¡ll JÃ³nsson. 12 - 2010 Â« La  violence  de  la  mer  :  lâimpact  de  la  tempÃªte  du  28  fÃ©vrier  2010  sur  le littoral atlantique Â» Sous  lâÃ©gide  de  la  Maison  des  Sciences  de  lâHomme  Ange  GuÃ©pin  et  du  laboratoire GÃ©olittomer  (UMR  6554 â LETG),  3  dÃ©cembre  2010.  Organisateur  du  colloque  et prÃ©sentation dâune communication : Â« Xynthia : une tempÃªte banale ? Â» 11 - 2010 Â« Eau  et  urbanisme Â»  Colloque  organisÃ©  par  lâAgence  de  lâeau  Adour-Garonne. ConfÃ©rencier invitÃ©. PrÃ©sentation dâune communication : Â« constructibilitÃ© en zones inondables sur le littoral Â». 11 - 2010 Â« Mer  agitÃ©e,  mer  Ã   gÃ©rer.  Vers  un  amÃ©nagement  de  l'espace  maritime Â» Colloque dans le cadre des JournÃ©es Scientifiques de lâUniversitÃ© de Nantes et sous lâÃ©gide des Â« JournÃ©es maritimes europÃ©ennes 2010 Â». PrÃ©sentation dâune communication : Â« Le littoral inondÃ©. Retour sur la tempÃªte Xynthia du 28 fÃ©vrier 2010 Â» avec Etienne Chauveau, Thierry Guineberteau, Denis Mercier 10 - 2008 Â« Changements  climatiques  et  vulnÃ©rabilitÃ©s Â», Angers, 26-27 mai, SÃ©minaire de la Zone Atelier Loire (ZAL). ConfÃ©rence invitÃ©e par les coordinateurs de la ZAL. ExposÃ©  scientifique introductif  :  D.  Mercier â Entre  alÃ©a,  vulnÃ©rabilitÃ©  et  gestion.  Questions posÃ©es  par  les  changements  climatiques,  lâÃ©valuation  et  la  gestion  de  la  vulnÃ©rabilitÃ©  socio-Ã©conomique des extrÃªmes hydrologiques dans la vallÃ©e de la Loire. 9 - 2008 Â« VulnÃ©rabilitÃ© sociÃ©tales, risques et environnement : comprendre et Ã©valuer Â», Toulouse, 14-16 mai. Communication  :  V.  Jousseaume  et  D.  Mercier â Ãvaluer  la  vulnÃ©rabilitÃ©  architecturale  de lâhabitat en zone inondable. Lâexemple du Val nantais. 8 - 2007 Â« Paraglaciaire et changements climatiques Â» Organisateur  D.  Mercier  de  la  sÃ©ance  de  lâAssociation  de  GÃ©ographes  franÃ§ais,  Paris,  Institut de GÃ©ographie, 17 novembre. 2 Communications : D. Mercier : Le gÃ©osystÃ¨me paraglaciaire face aux changements climatiques. S.  Ãtienne et  D.  Mercier  :  Reconstitution  de  lâhistoire  paraglaciaire  dâune  marge  glaciaire  face aux changements climatiques : le cas du glacier Baron au Spitsberg. 7 - 2007 Â« La  dÃ©gradation  de  la  pierre  monumentale  :  apports  de  la  recherche gÃ©ographique Â» SÃ©ance  de lâAssociation  de  GÃ©ographes  franÃ§ais,  Paris,  Institut  de GÃ©ographie, 12 mai, co-organisation : S. Ãtienne et D. Sellier Communication : AndrÃ©  M.-F.,  Mercier  D.,  Ãtienne  S.,  Voldoire  O.,  Vautier  F.  :  Approche  spatiotemporelle multiscalaire de l'Ã©rosion historique des grÃ¨s d'Angkor : l'exemple du temple de Ta Keo. 6 - 2005 Â« Risques  et  problÃ©matiques  gÃ©ographiques Â», Colloque en hommage au Professeur Bernard Bousquet, UniversitÃ© de Nantes, 3 fÃ©vrier. Communication  :  Deux  siÃ¨cles  dâamÃ©nagement  de  la  zone  inondable  du  Val  nantais  (avec  V. Jousseaume). 5 - 2002 Â« La recherche actuelle dans les milieux polaires et subpolaires Â». Co-organisateur avec M.-F. AndrÃ© de la sÃ©ance de lâAssociation de GÃ©ographes franÃ§ais, Paris, Institut de GÃ©ographie, 11 mai. 3 Communications : - Glaciaire, pÃ©riglaciaire, paraglaciaire : combinaisons et relais de processus. - La  progradation  des  littoraux  meubles  au  Spitsberg  :  une  rÃ©ponse  sÃ©dimentaire  paraglaciaire au changement climatique contemporain. (avec D. Laffly). - Les gÃ©ographes franÃ§ais et les milieux polaires et subpolaires. 4 - 2001 Â« Colloque de l'Association franÃ§aise du PÃ©riglaciaire Â», Fontainebleau-Avon, 20-21  

!27!janvier. Communication  :  Le  poids  des  dynamiques  paraglaciaires  dans  l'Ã©volution  des  versants arctiques. 3 - 1999 Â« Colloque de l'Association franÃ§aise du PÃ©riglaciaire Â», Fontainebleau-Avon, 16-17 janvier Communication : Ruissellement et pergÃ©lisol au Spitsberg. 2 - 1998 Â« La  vitesse  de  l'Ã©rosion  rÃ´le  de  la  durÃ©e,  des  Ã©chelles  gÃ©ographiques  et  des contextes climatiques Â», SÃ©ance de l'Association de GÃ©ographes FranÃ§ais,10 janvier, Institut de GÃ©ographie, Paris, organisation Y. Lageat et B. Lemartinel. Communication : Un siÃ¨cle d'Ã©rosion sur les moraines du Petit Ãge glaciaire au Spitsberg. 1 - 1996 Â« PremiÃ¨res JournÃ©es des Jeunes GÃ©omorphologues Â», Groupe FranÃ§ais de GÃ©omorphologie, 14 dÃ©cembre Ã  Lille. Communication : Le ruissellement au Spitsberg.  SÃ©minaires (n = 9) 9 - 2013 Â« sÃ©minaire  du  laboratoire  LARMAUR  de  Rennes  1 Â»  confÃ©rencier  invitÃ©  avec Samuel Etienne le  3  avril  :  Â« InterprÃ©tations  palÃ©oenvironnementales  de  microformes glaciaires : les stries des substrats siliceux Â». 8 - 2013 Â« sÃ©minaire  du  laboratoire  EPHE â Dinard,  laboratoire  de  gÃ©omorphologie littorale Â»  confÃ©rencier  invitÃ©  le  3  juin  :  Â« La tempÃªte  Xynthia et  lâamÃ©nagement  des littoraux Â». 7 - 2010 Â« sÃ©minaire  du  laboratoire  de  planÃ©tologie  et  de  gÃ©odynamique  de  Nantes (LPGN) Â» confÃ©rencier invitÃ© le 22 juin : Â« Les crises paraglaciaires au Svalbard Â». 6 - 2006 Â« Recherches franÃ§aises en Arctique Â» Ãcole thÃ©matique du CNRS, Chamonix, 2-7  avril, organisÃ©e par Madeleine Griselin du laboratoire THÃMA de BesanÃ§on.  2 communications :  - Le bilan des recherches internationales en gÃ©osciences au Spitsberg.  - Les dynamiques paraglaciaires au Spitsberg. 4 - 2005 Â« Le  paysage  :  temporalitÃ©,  spatialitÃ©,  risques Â»  ConfÃ©rence-dÃ©bat  du  DEPAM â Institut de GÃ©ographie de Paris, 16 mars.  PrÃ©sentation de lâouvrage Â« Le commentaire de paysages en gÃ©ographie physique. Documents et mÃ©thodes Â». 3 - 2004 Â« AssemblÃ©e GÃ©nÃ©rale de GÃ©olab - UMR 6042 CNRS Â», Clermont-ferrand, 4-5 juin,  2 communications :  - Le programme 400 GÃ©omorphoclim au Spitsberg.  - Lâidentification  des  marques  de  processus  morphogÃ©niques  en  milieux  froids  par microscopie  Ã©lectronique  Ã   balayage.  Application  aux  surfaces  glaciaires  dâabandon  de  la presquâÃ®le de BrÃ¸gger (Spitsberg) (avec D. Sellier). 2 - 2001 Â« Perceptions  et  prises  en  compte  du  temps  dans  les  recherches environnementalesÂ»,  JournÃ©e  Ãcole  doctorale,  UniversitÃ©  Blaise  Pascal,  Clermont-Ferrand, 26 avril.  Communication  :  Des  temps  holocÃ¨nes  aux  temps  instantanÃ©s  :  les  mÃ©thodes  pour  mesurer les rythmes de la morphogenÃ¨se dans les hautes latitudes. 1 - 1998 Â« AssemblÃ©e  gÃ©nÃ©rale  de  l'UPRES-A  6042  CNRS Â»,  Clermont-Ferrand,  30 -31 janvier. Communication  :  Bilan  hydrologique-Bilan  d'Ã©rosion,  application  Ã   un  petit  bassin-versant polaire non englacÃ© au Spitsberg nord-occidental.     

!28!Posters (n = 15) 15 - 2014 â  Â« Connaissance  et  comprÃ©hension  des  risques  cÃ´tiers :  alÃ©as,  enjeux, reprÃ©sentations, gestion Â», Brest, 3-4 juillet.  Chevillot-Miot  E.,  Creach  A.,  Mercier  D.,  Diagnostic  de  la  vulnÃ©rabilitÃ©  du  bÃ¢ti  de  lâÃ®le  de Noirmoutier  (VendÃ©e)  face  au  risque  de  submersion  marine :  premiers  essais  de quantification. 14 - 2013 - Â« 8Ã¨me confÃ©rence internationale des gÃ©omorphologues Â», Paris 27-31 aoÃ»t  Ragaru  E.,  Mercier  D.,  Chaibi  M.,  Maanan  M.,  Lithological  control  on  coastal  rock  cliffs erosion of Safi, Morocco. 13 - 2013 -  Â« Prospective Arctique Â», CollÃ¨ge de France, 3-6 juin. Mercier  D.  Decaulne  A.,  Cossart  E.,  Feuillet  T.,  Coquin  J.,  Les  mouvements  de  terrain postglaciaires dans le SkagafjÃ¶rÃ°ur (Islande septentrionale). 12 - 2013 -  Â« Prospective Arctique Â», CollÃ¨ge de France, 3-6 juin.  Feuillet  T.,  Mercier D.  Decaulne  A.,  Cossart  E.,  Aspects  intrinsÃ¨ques  et  contextuels  des cercles de pierres en Islande du Nord. 11 - 2009 Â«  20e Festival  International  de  GÃ©ographie (FIG) Â», Saint-DiÃ©-des-Vosges, 1er au 4 octobre, thÃ¨me : Â« GÃ©ographie des mers Â»  Mercier D., Fonte des glaces et Ã©lÃ©vation du niveau marin. 10 - 2007 - Â« 18e Festival  International  de  GÃ©ographie (FIG) Â», Saint-DiÃ©-des-Vosges, 4 au 7 octobre. ThÃ¨me : Â« La PlanÃ¨te en mal dâÃ©nergies Â» Mercier  D.,  RÃ©chauffement  climatique  de  lâArctique  et  gaz  Ã   effet  de  serre  :  une  boucle  de rÃ©troaction positive. 9 - 2007 - Â« 18e Festival  International  de  GÃ©ographie (FIG) Â»,  Saint-DiÃ©-des-Vosges,  4  au  7 octobre. ThÃ¨me : Â« La PlanÃ¨te en mal dâÃ©nergies Â» Mercier D., Les clathrates de lâArctique : Ã©nergie du XXIe siÃ¨cle ? 8 - 2005 - Â« Second  International  Conference  on  Arctic  Research  Planning  (ICARP  II) Â», November 10 - 13 2005, Copenhagen, Denmark.   Decaulne  A.  SÃ¦mundsson  S.,  Mercier  D.,  Vulnerability  and  resilience  of  local  population confronted with natural hazards, due to geomorphic processes on slopes in Icelandic fjords. 7 - 2005 - Â« Second  International  Conference  on  Arctic  Research  Planning  (ICARP  II) Â», November 10 - 13 2005, Copenhagen, Denmark. Griselin  M.,  Marlin  C.,  Ferrandez  C.,  Nagelseisen  S.,  Mercier  D.,  Hydro-Sensor-Flows  (FLux Of  Water  and  Sediments)  an  in  situ  sensing  program  to  survey  a  glacier  basin  (East  LovÃ©n Glacier, 79Â°N, Spitsbergen).  6 - 2005 - Workshop  AWIPEV â Â« Joint  French-German  collaborations  for  science  in Svalbard Â», Strasbourg, 2-3 mars 2005. Mercier D., AndrÃ© M.-F., Ãtienne S., Laffly D., Moreau M.,  Sellier  D.,  Dupont  J.,  Prick  A.,  Rachlewicz  G.,  Paraglacial  dynamics  in  Svalbard, programme 400 Geomorphoclim (2002-2005), French Polar Institute Paul-Ãmile Victor. 5 - 2005 - colloque Â« Shifting lands Â» (Clermont-Ferrand, 20-22 janvier 2005). Mercier  D.,  AndrÃ©  M.-F.,  Ãtienne  S.,  Laffly  D.,  Moreau  M.,  Sellier  D.,  Dupont  J.,  Prick  A., Rachlewicz  G.,  Paraglacial  dynamics  in  Svalbard,  programme  400  Geomorphoclim  (2002-2005), French Polar Institute Paul-Ãmile Victor. 4 - 2003 - Â« 14e Festival  International  de  GÃ©ographie Â»,  St  DiÃ©, Griselin  M.,  AndrÃ©  M.-F., Marlin  C.,  Laffly  D.,  Mercier  D.,  Moreau  M.,  Vandercruyssen  O.,  La  fusion  des  glaciers polaires : signal climatique et moteur de mutations paysagÃ¨res. 3 - 2002 - Participation  Ã   lâexposition  Â« Nous,  La  Chapelle-Basse-Mer Â»  organisÃ©e  par lâuniversitÃ© de Nantes. RÃ©alisation de posters sur lâhistoire gÃ©ologique et gÃ©omorphologique, la mise en place des paysages et la dynamique fluviale. 2 - 2001â2003 - Participation  Ã   lâexposition  Â« La  recherche  polaire  franÃ§aise  au  Spitsberg. Histoire  et  actualitÃ©  dâune  coopÃ©ration  scientifique  avec  la  NorvÃ¨ge Â»  organisÃ©e  par lâAmbassade  de  France  Ã  Oslo.  Exposition  itinÃ©rante  :  UNIS  (Longyearbyen,  Svalbard),  

!29!Polaria  (TromsÃ¸,  NorvÃ¨ge),  Fram  Museum  (Oslo,  NorvÃ¨ge),  UniversitÃ©  de  Toulouse  1 (France),  MusÃ©um  national  dâhistoire  naturelle  de  Paris  sous  le  titre  Â«  ExpÃ©ditions  polaires  au Spitsberg : entre science et aventure Â». http://www.france.no/science/ 1 - 1999 - Participation  Ã   lâexposition  photographique  Â« LumiÃ¨res  arctiques Â»,  DÃ©lÃ©gation RÃ©gionale CNRS, Nancy (GDR 49 â Recherches arctiques â).  Recensions (n = 13) 13 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  de  Freddy  VINET,  2010, Le  risque inondation.  Diagnostic  et  gestion,  Editions  Tec  &  Doc  Lavoisier,  collection  Science  du  Risque  et du Danger (SRD), 318 p., Norois, 2011, nÂ°212, pp. 74-76. 12 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  de  Philippe  DEBOUDT  (Ã©d.),  2010 â InÃ©galitÃ©s  Ã©cologiques,  territoires  littoraux  &  dÃ©veloppement  durable,  Presses  Universitaires  du Septentrion, collection Environnement et SociÃ©tÃ©, 409 p., Norois, 2011, nÂ°212, pp. 73-74. 11 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  coordonnÃ©  Wilfried  ENDLICHER  & Friedrich-Wilhelm  GERSTENGARBE,  2010. â Continents  under  Climate  Change,  Nova  Acta Leopoldina, Neue Folge, Nummer 384, Band 112, 317 p., Norois, 2012, nÂ°223, pp. 123-124. 10 - Compte-rendu  rÃ©digÃ©  par  Denis  Mercier  de  lâouvrage  de  Olav  SLAYMAKER  &  Richard E.J. KELLY, 2007 â The cryosphere and global environmental change, Blackwell Publishing, 261 p., GÃ©omorphologie : relief, processus, environnement, 2008, 4, p. 273-274. 9 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  de  Laurent  TOUCHART  (L.)  (sous  la direction  de),  2007,  GÃ©ographie  de  lâÃ©tang.  Des  thÃ©ories  globales  aux  pratiques  locales,  Paris, LâHarmattan, 228 p., Norois, 204, pp. 95-97. 8 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  de  Frank  Rudolph,  2006 - Strandsteine, Sammeln & Bestimmen von Steine an der OstseekÃ¼ste, Wachholtz Verlag, 160 p., Norois, 203, p. 85. 7 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  de  Robert  Vivian,  2005 â Les  glaciers  du Mont-Blanc,  MontmÃ©lian,  La  Fontaine  de  SiloÃ©,  319  p., GÃ©omorphologie  :  relief,  processus, environnement, 2006, 3, pp. 213-215. 6 - Compte-rendu rÃ©digÃ© par D. Mercier de lâouvrage de Chantal EDEL, 2004 â Sur les routes du pÃ´le Nord, GlÃ©nat, Grenoble, 128 p., publiÃ© en 2006 dans Historiens & GÃ©ographes, 393, p. 415-416. 5 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâAtlas  CÃ©dÃ©rom,  Environnement  et  pratiques paysannes  Ã   Madagascar,  collectif  dirigÃ©  par  Florent  Lasry,  2005,  Paris,  IRD  Ã©ditions,  site  des cafÃ©s gÃ©ographiques, http://www.cafe-geo.net/article.php3?id_article=807 4 - Compte-rendu rÃ©digÃ© par D. Mercier de lâouvrage de Michel VARAGNE, 2003 â Le jour oÃ¹ la  Loire  dÃ©borderaâ¦,  Romorantin,  Communication-Presse-Ãdition,  224  p., Norois,  192,  p. 144-145. 3 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  collectif  dirigÃ©  par  Christian  Bouchardy, 2002 â La  Loire.  VallÃ©es  et  vals  du  grand  fleuve  sauvage,  Paris,  Delachaux  et  NiestlÃ©,  288 p., Norois, 192, p. 143-144. 2 - Compte-rendu  rÃ©digÃ©  par  D. Mercier  de  lâouvrage  de  Michel  DâArcangues,  2002 â Dictionnaire  des  explorateurs  des  pÃ´les,  SÃ©guier,  Paris,  693  p.,  publiÃ©  en  2003  dans Historiens & GÃ©ographes, 384, p. 560. 1 - Compte-rendu rÃ©digÃ© par D. Mercier de lâouvrage de Martin Gude, 2000 â Ereignissequenzen und  Sedimenttransporte  im  fluvialen  Milieu  kleiner  Einzugsgebiete  auf  Spitzbergen, Heidelberger  Geographische  Arbeiten,  Heft  110,  124  p.,  publiÃ©  en  2003  dans  les Annales  de GÃ©ographie, 632, p. 445.    

!30! Valorisation de la recherche  Un  gÃ©ographe  universitaire  se  doit  de  rÃ©pondre  aux  demandes  sociÃ©tales  et  jâai  donc  acceptÃ©  de donner  de  nombreuses  confÃ©rences  grand  public  et de  rÃ©pondre  Ã   de  nombreuses interventions dans les mÃ©dias.  Dans le  cadre  du  changement  global,  du  rÃ©chauffement  climatique  et  des  milieux polaires : A â interview - Interview pour lâhebdomadaire, LâHebdo de SÃ¨vre et Maine, publiÃ©e le 29 dÃ©cembre 2005. - Interview pour le magazine mensuel Eureka, Bayard Presse, publiÃ©e dans le numÃ©ro 4 de mars 2006, pages 24-28, sous le titre Â« Arctique Danger immÃ©diat Â». - Interview  pour France-Info diffusÃ©e  le  16  mars  2006  dans  la  chronique  Â« Profession chercheur Â» de Marie-Odile Monchicourt,  http://www.radiofrance.fr/chaines/France-info/chroniques/chercheur/index.php - Interview pour lâhebdomadaire, LâHebdo de SÃ¨vre et Maine, publiÃ©e le 22 mars 2007. - Tribune  intitulÃ©e  Â« Le  rÃ©chauffement  climatique :  un  enjeu  politique ? Â»,  publiÃ©e  dans  les quotidiens Presse OcÃ©an, Le Courrier de lâOuest et Le Maine libre, le 26 mars 2007. - Interview  pour  la  tÃ©lÃ©vision, France  3  Pays-de-la-Loire sur  le  rÃ©chauffement  climatique, diffusion aux informations rÃ©gionales du 19-20 heures le 18 juin 2007. B â confÃ©rencier invitÃ© - 22  septembre  2004 - ConfÃ©rencier  invitÃ©  par  lâuniversitÃ©  permanente  de  Nantes,  antenne  de Saint-BrÃ©vin-les-Pins. ConfÃ©rence intitulÃ©e : Â« Les paysages polaires du Spitsberg Â». - 24 octobre 2007 â ConfÃ©rencier invitÃ© par lâuniversitÃ© permanente de Nantes, antenne de Saint-BrÃ©vin-les-Pins, confÃ©rence intitulÃ©e : Â« RÃ©chauffement climatique, fonte des glaces et Ã©lÃ©vation du niveau de la mer Â». - 15 mars 2008 â ConfÃ©rencier invitÃ© par la mÃ©diathÃ¨que de lâagglomÃ©ration troyenne (10) dans le cadre  de  lâannÃ©e  polaire  internationale,  confÃ©rence  intitulÃ©e :  Â« Les  PÃ´les.  Enjeux  climatiques, paysagers et gÃ©opolitiques Â». - 7  juin  2013 â ConfÃ©rencier  invitÃ©  dans  le  cadre  des  JournÃ©es  Scientifiques de  lâUniversitÃ©  de Nantes  Ã   la  citÃ©  des  congrÃ¨s  de  Nantes.  ThÃ¨me :  le  rÃ©chauffement  climatique  est-il  irrÃ©versible ? Table ronde avec Jean-Louis Etienne, HervÃ© Le Treut, FranÃ§ois Collart-Dutilleul http://webtv.univ-nantes.fr/fiche/3456/jean-louis-etienne-herve-le-treut-le-rechauffement-climatique-est-il-irreversible  C â expositions - En  2007 : Participation  Ã   la  prÃ©paration  scientifique  de  lâexposition  avec  Mme  Bellec, Â« Aventure  aux  PÃ´les.  Dans  les  pas  de  Paul-Ãmile  Victor,  vers  un  rÃ©chauffement  climatique ? Â», Palais  de  la  dÃ©couverte,  avenue  Franklin  Roosevelt,  75  008  Paris,  du  11  juin  2007  au 7  janvier 2008. - En 2007 : Participation Ã  l'exposition ""Le CNRS aux pÃ´les"" qui sâest tenue dans le couloir de la RATP Ã  Montparnasse Ã  Paris du 1er novembre au 31 dÃ©cembre 2007. Elle est consultable en ligne Ã  cette adresse :  http://www.cnrs.fr/cnrs-images/multimedia/cnrs-ratp/   DÃ©bats autour du risque dâinondation dans la rÃ©gion nantaise : A â interview - Interview pour lâhebdomadaire, LâHebdo de SÃ¨vre et Maine, publiÃ©e le 23 octobre 2003. - Interview  pour le quotidien  Ouest France, publiÃ©e le 26 fÃ©vrier 2004.  

!31!- Interview pour lâhebdomadaire, LâHebdo de SÃ¨vre et Maine, publiÃ©e le 26 fÃ©vrier 2004. - Interview pour le quotidien  Presse OcÃ©an, publiÃ©e le 3 mars 2004. - Interview pour  lâhebdomadaire, LâHebdo de SÃ¨vre et Maine, publiÃ©e le 4 mars 2004. - Interview  pour le quotidien  Ouest France, publiÃ©e le 5 mars 2004. - Interview pour le quotidien  Presse OcÃ©an, publiÃ©e le 10 mars 2004. - Interview  pour le quotidien  Ouest France, publiÃ©e le 11 mars 2004. - Interview pour le quotidien  Presse OcÃ©an, publiÃ©e le 30 mars 2004. - Interview pour le quotidien Presse OcÃ©an, publiÃ©e le 24 janvier 2009. - Interview pour le quotidien Ouest France, publiÃ©e le 3 dÃ©cembre 2010. - Interview pour la tÃ©lÃ©vision France 3 Pays de la Loire, diffusÃ©e le 1er dÃ©cembre 2010. - Interview pour la tÃ©lÃ©vision Nantes 7, diffusÃ©e le 2 dÃ©cembre 2010.  B - confÃ©rencier invitÃ© - 26 fÃ©vrier 2004 Â« RÃ©union publique Â», La Chapelle-Basse-Mer. ConfÃ©rence invitÃ©e : Â« Les crues et les inondations dans le Val nantais : alÃ©a, vulnÃ©rabilitÃ©, gestion Â». - 8 juin 2010 â ConfÃ©rencier invitÃ© par les archives dÃ©partementales de Loire-Atlantique (ADLA) dans  le  cadre  dâune  sÃ©rie  de  confÃ©rences  ayant  pour  titre  Â« Visages  dâeaux Â».  ConfÃ©rence  Â« la mÃ©moire des crues et des inondations dans le Val nantais Â». - 15 octobre 2010 â ConfÃ©rencier invitÃ© par lâUniversitÃ©-sur-Lie, musÃ©e du vignoble nantais, dans le cadre du centenaire de la crue de 1910, Ã  Saint-Julien-de-Concelles, ConfÃ©rence Â« la  vulnÃ©rabilitÃ© du Val nantais 1910-2010 Â». - 11 janvier 2011 â ConfÃ©rencier invitÃ© par la ville de Nantes, Lieu Unique, confÃ©rence : Â« Le  Val nantais face aux inondations : mÃ©moire vivante ou dÃ©ni du risque ? Â». - 30  novembre  2013 â ConfÃ©rencier  invitÃ©  par  les  coordinateurs  du Dictionnaire  de  Nantes,  publiÃ© aux  Presses  Universitaires  de  Rennes,  Lieu  Unique,  Nantes,  confÃ©rence  sur  les  inondations Ã  Nantes.  Suite Ã  la tempÃªte Xynthia du 28 fÃ©vrier 2010 : A â interview - 1er mars  2010 :  Interview  pour  la  tÃ©lÃ©vision, France  3,  dans  le  cadre  du  Soir  3  national, sur  la construction  en  zone  inondable  sur  le  littoral.  InvitÃ©  sur  le  plateau  en  duplex  de  Nantes  et  en direct  avec  Chantal  Jouanno,  secrÃ©taire  dâEtat  Ã   lâEcologie,  et  le  journaliste  Francis  Letellier  Ã  Paris. - 3  mars  2010 :  Interview  publiÃ©e  dans  le  quotidien Direct  matin en  page  6,  sous  le  titre Â« lâurbanisation en cause Â». - 4 mars 2010 : Interview pour la tÃ©lÃ©vision en direct dans le journal du soir sur la chaÃ®ne Nantes 7 http://www.nantes7.fr/emission/le-journal-4490 - CafÃ© gÃ©o, le 19 mars 2010 Â«  le littoral inondÃ© Â», Nantes, AlterâcafÃ©. - Interview le 4 mars 2010 par Ouest France : publiÃ© le 5 mars, p. 12. - Interview le 4 mars 2010 sur la Radio Hit west - Interview le 10 avril 2010 par La Croix : publiÃ© le 12 avril. - Interview le 12 avril 2010 par La Croix : publiÃ© le 13 avril. - Interview le 20 mai 2010 Ã  La Faute-sur-mer (85) pour lâÃ©mission consacrÃ©e Ã  l'environnement Â« La Terre pour Avenir Â», diffusÃ©e sur France 3 Bretagne et Pays de la Loire le 22 mai. Lâinterview a  Ã©tÃ©  rÃ©alisÃ©e  par  Christelle  ProutiÃ¨re,  RÃ©dactrice  en  chef  adjointe  de  France  3  ouest  pour  un numÃ©ro consacrÃ© Ã  la loi Â« Littoral Â». - Interview  le  23  fÃ©vrier  2011  Ã   La  Faute-sur-mer  (85)  en  direct  du  journal  rÃ©gional  Pays  de  la Loire 19-20h sur France 3. - 1er mars 2011 : Interview pour la tÃ©lÃ©vision diffusÃ©e dans le journal du soir sur la chaÃ®ne Nantes 7 http://www.nantes7.fr/emission/le-journal-5310/ - janvier  2011 :  Interview  pour  le  magazine  GÃ©o,  publiÃ©e  dans  le  numÃ©ro  387  de  mai  2011  dans  

!32!un dossier sur le risque dâinondation en France. - Interview  le  28  fÃ©vrier  2012  Ã   LâAiguillon-sur-mer  (85)  en  direct  du  journal  rÃ©gional  Pays  de  la Loire 19-20h sur France 3. - Interview le 2 fÃ©vrier 2012 par Presse OcÃ©an : publiÃ© le 3 mars. - Interview le 28 fÃ©vrier 2012 par Presse OcÃ©an : publiÃ© le 1er mars. - Interview le 28 fÃ©vrier 2012 par Ouest France : publiÃ© le 3-4 mars. - Interview le 7 juin 2013 en direct sur la radio Prun http://www.prun.net/labo-des-savoirs - Interview le 20 janvier 2014 sur Euroradionantes dans lâÃ©mission intitulÃ©e ""Les causeries de la Msh"" http://www.euradionantes.eu/emission/les-causeries-de-la-msh - Interview le 9 septembre 2014 par Ouest France : publiÃ© le 11 septembre. - Interview le 12 septembre 2014 par France bleu Loire OcÃ©an : publiÃ© le 15 septembre. - Interview  le  15  septembre  2014  par TÃ©lÃ©  Nantes :  diffusÃ©  le  15  septembre, www.telenantes.com/actualite/article/xynthia-t-tire-les-enseignements - Interview le 2 octobre 2014 par France 3 Pays de la Loire : diffusÃ© le 2 octobre  - Interview le 17 mars 2015 par France 3 Pays de la Loire : diffusÃ© le 19 mars   B â confÃ©rencier invitÃ© - 1er octobre 2010 : ConfÃ©rencier invitÃ© au colloque Â« eau et urbanisme Â» Ã  Bayonne par lâAgence de lâeau Adour-Garonne, intervention Â« constructibilitÃ© en zones inondables sur le littoral Â». - 19 mars 2011 â ConfÃ©rencier invitÃ© par lâassociation Â«  ECLAT Â», Ã  Pornichet (44), confÃ©rence : Â« De l'eau et des hommes : le risque d'inondation dans la vallÃ©e de la Loire et de submersion marine sur le littoral atlantique Â». - 19 dÃ©cembre 2012 â ConfÃ©rencier invitÃ© par le Conseil Ã©conomique social et environnemental, CESER des rÃ©gions de la faÃ§ade atlantique de la France, confÃ©rence : Â« Changements  climatiques  sur le littoral atlantique et enjeux fonciers Â», Nantes, HÃ´tel de la RÃ©gion des Pays de la Loire. - 19 septembre 2014 â ConfÃ©rencier invitÃ© par le Conseil Ã©conomique social et environnemental, CESER des rÃ©gions de la faÃ§ade atlantique de la France, confÃ©rence : Â« La vulnÃ©rabilitÃ© des communes littorales de Pays de la LoireÂ», Nantes, HÃ´tel de la RÃ©gion des Pays de la Loire.   Rayonnement international  2014 - Chercheur invitÃ© par lâinstitut polaire allemand (AWI) Ã  Potsdam (avril-juillet 2014).  2013 Â« Colloque Franco-IndonÃ©sien Â», Padang, IndonÃ©sie, 24-25 juin ConfÃ©rencier  invitÃ©  par  l'Ambassade  de  France  en  IndonÃ©sie  au  sÃ©minaire  franco-indonÃ©sien  Ã  Padang  sur  l'Ã®le  de  Sumatra  les  24  et  25  juin  2013.  Le  thÃ¨me  de  cette  rencontre  Ã©tait  ""Land  use planning in coastal regions facing natural risks. How to think natural risks and mitigating strategies in land use planning processes of coastal regions?""  Le titre de la communication : ""How to mitigate the risk of marine submersion on the French coast""  2013 Â« European Geosciences Union Â», Vienne, Autriche, 7â12 avril  Key-note  lecture  invitÃ©  par  les  organisateurs  de  la  session  GM9.2/HS9.8/NH3.15: Geomorphic and hydrological processes in proglacial areas under conditions of (rapid) deglaciation;  Convener:  Tobias  Heckmann;  Co-Conveners:  Andrew  Kos,  Christian  Briese,  Reynald  Delaloye, Volkmar Mair, Samuel McColl, David Morche, Philip Owens, Tim Stott Titre de la communication : Paraglacial processes during rapid deglaciation: a question of time and space.  2013 Â« 8th International Conference on Geomorphology Â», Paris, 27-31 aoÃ»t  Key-note  lecture  invitÃ© par  les  organisateurs  de  la  session Paraglacial  geomorphology,  Jasper Kinght  &  Stephan  Harrison,  Titre  de  la  communication  : Toward  a  comprehensive  paraglacial  model:  

!33!case studies from Iceland.   Membre des commissions de spÃ©cialistes ou de comitÃ©s de sÃ©lection 2001-2006 : UniversitÃ© de Paris IV-Sorbonne (titulaire et assesseur) 2001-2007 : UniversitÃ© de Pau et des Pays de lâAdour (titulaire) 2002-2006 : UniversitÃ© de Limoges (supplÃ©ant), puis titulaire en 2007. 2008 : UniversitÃ© de Nantes (supplÃ©ant) 2012 : UniversitÃ© de Nantes, Angers et Toulouse 2013 : UniversitÃ© de Nantes (prÃ©sident du comitÃ©), Paris IV-Sorbonne et La Rochelle 2014 : UniversitÃ© de Nantes  2015 : UniversitÃ© de Nantes  Expertises Ã©ditoriales Membre du comitÃ© de lecture du Bulletin de lâAssociation de GÃ©ographes FranÃ§ais â GÃ©ographies.  Membre  Ã©lu  en  2008  au  ComitÃ©  de  rÃ©daction  de  la  revue GÃ©omorphologie :  relief,  processus, environnement, du Groupe FranÃ§ais de GÃ©omorphologie (GFG), comme rÃ©dacteur adjoint. Membre de lâEditorial Board de la revue Geomorphology depuis janvier 2015.  Relecteur pour des articles dans les revues suivantes : Revues Ã©trangÃ¨res :  Permafrost and Periglacial Processes Geografia Fisica e Dinamica Quaternaria Quaternary Science Reviews Quaternary Research Geografiska Annaler Serie A : physical geography Natural Hazards and Earth System Sciences Paleogeography, Paleoclimatology, Paleaoecology Advances in Geosciences Geomorphology Polish Polar Research  Revues franÃ§aises : LâInformation GÃ©ographique Bulletin de lâAssociation de GÃ©ographes FranÃ§ais  Annales de GÃ©ographie GÃ©ocarrefour Karstologia Physio-GÃ©o GÃ©omorphologie : relief, processus, environnement Norois Cahiers nantais  Expert sollicitÃ© par lâANR, lâAxa research found, LâAgence nationale de la Recherche polonaise    

!34! ActivitÃ©s pÃ©dagogiques     Sur  le  plan  pÃ©dagogique,  jâai  une  longue  expÃ©rience  de  lâenseignement  puisque  jâexerce  depuis lâobtention  de  lâagrÃ©gation  de  gÃ©ographie  en  1992. Jâai  enseignÃ©  devant  des  publics  variÃ©s : collÃ©giens,  lycÃ©ens,  militaires,  Ã©tudiants  Ã   lâuniversitÃ©.  Je  me  suis  enrichi  par  une  mobilitÃ© gÃ©ographique avec des postes Ã  Nantes et Ã  Paris IV - Sorbonne, mais Ã©galement des expÃ©riences Ã  Clermont-Ferrand. Jâai essentiellement enseignÃ© la gÃ©ographie physique et plus particuliÃ¨rement la  gÃ©omorphologie,  mais  aussi la  climatologie,  lâhydrologie, la  biogÃ©ographie et les  risques naturels.  La  dimension  globale  et  lâapproche  paysagÃ¨re  a  toujours  Ã©tÃ©  une  prioritÃ©  dans  mes enseignements.  RÃ©pondant  Ã   une  demande  des  Ã©tudiants  de  Paris  IV - Sorbonne que  jâavais  en TD,  jâai  coordonnÃ©  un  manuel  de  commentaires  de  paysages  publiÃ©  chez  Armand  Colin  leur permettant  de  sâexercer  Ã   cette  mÃ©thodologie  de  base  en  gÃ©ographie.  Progressivement, lâenseignement des risques naturels a pris une place importante au fur et Ã  mesure de lâÃ©volution des  maquettes  pÃ©dagogiques.  Jâai  guidÃ©  des  Ã©tudiants  dans  leurs  premiers  pas  en  recherche (master, doctorat). Jâai eu Ã  diriger des Ã©tudiants sur des thÃ©matiques variÃ©es mais essentiellement sur  les  deux  centres  dâintÃ©rÃªts  principaux  de  mes  recherches :  les  milieux  froids  et  les risque dâinondation et de  submersion  marine.  En  cela,  la  double  fonction  du  statut  de  maÃ®tre  de confÃ©rences et de professeur des universitÃ©s trouve tout son sens, Ã  la fois dans lâenseignement et la recherche, oÃ¹ les deux aspects se nourrissent mutuellement.  

!35!MatiÃ¨re Niveau Type de cours Nombre dâheures Lieu GÃ©ographie de  lâenvironnement L1 CM 22  UniversitÃ© de Nantes GÃ©omorphologie L1 CM 24  UniversitÃ© Paris IV â  Sorbonne GÃ©ologie L1 CM 20  UniversitÃ© Paris IV â  Sorbonne Analyse de documents gÃ©ographiques L1 TD 24 par groupe UniversitÃ© de Nantes GÃ©ographie physique tropicale L1  non historiens CM et TD 20 CM et  24 par groupe UniversitÃ© de Nantes Climatologie L2 TD 24 par groupe UniversitÃ© de Nantes Climatologie L2 TD 18 par groupe UniversitÃ© Paris IV â  Sorbonne Hydrologie L2 TD 18 par groupe UniversitÃ© de Nantes BiogÃ©ographie L2 CM 24  UniversitÃ© de Nantes BiogÃ©ographie L2 TD 18 par groupe UniversitÃ© de Nantes BiogÃ©ographie L2 TD 18  UniversitÃ© Paris IV â  Sorbonne GÃ©omorphologie structurale L2 TD 24  par groupe UniversitÃ© de Nantes GÃ©omorphologie climatique L2 TD 18  par groupe UniversitÃ© Paris IV â  Sorbonne Stage de terrain L2 Encadrement 3 Ã  5 jours UniversitÃ© Paris IV â  Sorbonne GÃ©ologie L2 TD 20  par groupe UniversitÃ© de Nantes GÃ©ographie thÃ©matique :  les vallÃ©es franÃ§aises L2 non  gÃ©ographes CM et TD 20 CM et  20 TD UniversitÃ© de Nantes GÃ©ographie thÃ©matique :  les risques L2 non  gÃ©ographes CM et TD 20 CM et  20 TD UniversitÃ© de Nantes GÃ©ographie de lâeau L3  CM  18  UniversitÃ© Paris IV â  Sorbonne GÃ©omorphologie thÃ©matique L3  TD  18  UniversitÃ© Paris IV â  Sorbonne GÃ©ographie physique zonale L3  TD  18  UniversitÃ© Paris IV â  Sorbonne Risques naturels et sociÃ©tÃ©s L3  TD  18  UniversitÃ© Paris IV â  Sorbonne GÃ©omorphologie L3 Pro â  paysages  CM et TD 10 CM et  10 TD UniversitÃ© de Nantes Sortie de terrain L3 Pro â  paysages  Diagnostique territorial 8  UniversitÃ© de Nantes Risques cÃ´tiers M1  CM  10 CM  UniversitÃ© de Nantes Risque inondation M1  CM  10 CM  UniversitÃ© de Nantes ÃpistÃ©mologie et  mÃ©thodologie de la recherche M1 â  Dynamique  des milieux et  risques   CM  10 CM  UniversitÃ© Paris IV â  Sorbonne Initiation Ã  la recherche M1 CM 10 CM UniversitÃ© de Nantes  Stage de terrain M1 â  Dynamique  des milieux  et risques Encadrement 5 jours UniversitÃ© Paris IV â  Sorbonne Risque de submersion marine M2 GAEM  CM  8 CM  UniversitÃ© de Nantes Changement global M2 GAEM  CM  8 CM  UniversitÃ© de Nantes Les crises environnementales M2  Dynamique  des milieux et  risques  CM  8 CM  UniversitÃ© Paris IV â  Sorbonne Les crises dans les milieux  M2  CM  6 CM UniversitÃ© Blaise Pascal  

!36!   UniversitÃ© de Nantes  1992-93 : ChargÃ© d'enseignement Ã  l'UniversitÃ© de Nantes (IGARUN) DEUG 1 : TD  Analyse du document gÃ©ographique  1994-1998 : ATER Ã  l'UniversitÃ© de Nantes (IGARUN) Concours : AgrÃ©gation, CM sur l'Ã©rosion et prÃ©paration Ã  l'oral hors programme DEUG 2 : TD GÃ©ographie physique (gÃ©omorphologie, hydrologie, climatologie, biogÃ©ographie) DEUG 2 : TD de gÃ©ologie DEUG 2 : CM de biogÃ©ographie DEUG 1 : TD Initiation Ã  la gÃ©ographie physique et humaine  1998-1999 : Professeur agrÃ©gÃ© (PRAG) Ã  l'UniversitÃ© de Nantes (IGARUN) Concours : AgrÃ©gation, CM sur l'Ã©rosion Cet  enseignement  portait  sur  la  question  au  programme  de l'agrÃ©gation,  avec  des  thÃ©matiques dÃ©veloppÃ©es sur le Petit Ãge glaciaire, les captures, les montagnes, l'azonalitÃ©, ... AgrÃ©gation, prÃ©paration Ã  l'oral hors programme et commentaire de cartes. CAPES,  prÃ©paration  Ã   l'oral,  leÃ§on  pour  gÃ©ographes  et commentaires  de  documents gÃ©ographiques pour historiens. DEUG  2,  TD  GÃ©ographie  physique (gÃ©omorphologie,  hydrologie,  climatologie, biogÃ©ographie) En  50  heures  de  travaux  dirigÃ©s,  les  Ã©tudiants  doivent  maÃ®triser  le  commentaire  de    carte  en gÃ©omorphologie structurale (relief en structures plissÃ©es et faillÃ©es), le commentaire de la carte de la vÃ©gÃ©tation, l'analyse hydrographique Ã  partir de cartes, le calcul de base en hydrologie, l'analyse synoptique et la description analytique en climatologie. DEUG 2, TD GÃ©ologie  En 20 heures de TD, les Ã©tudiants apprennent Ã  identifier une trentaine de roches magmatiques, sÃ©dimentaires et mÃ©tamorphiques. Ils Ã©tudient les reliefs associÃ©s Ã  ces diffÃ©rentes roches. DEUG 2, CM et TD Les vallÃ©es franÃ§aises Les  Ã©tudiants  non gÃ©ographes  apprennent  dans  cette  option,  les  bases  de  la  gÃ©omorphologie,  de l'hydrologie, de la climatologie et de la biogÃ©ographie Ã  travers l'Ã©tude des vallÃ©es franÃ§aises. DEUG 1, TD Initiation Ã  la gÃ©ographie physique et humaine En  50  heures  de  travaux dirigÃ©s,  les  Ã©tudiants  en  gÃ©ographie  s'initient  Ã   l'analyse  des  documents gÃ©ographiques (cartes topographiques et gÃ©ologiques, textes, diapositives, statistiques...).  Depuis  septembre  2006, en tant que maÃ®tre  de  confÃ©rences, puis de septembre  2011 Ã   2015 en  tant  que Professeur  des  UniversitÃ©s,  jâai  enseignÃ© la  gÃ©ographie  physique  et polaires Recherche â  option  milieux  polaires   Questions au programme AgrÃ©gation CM 8   UniversitÃ© de Nantes Questions au programme Capes PrÃ©paration Ã  lâoral 12  UniversitÃ© de Nantes GÃ©ographie hors programme AgrÃ©gation PrÃ©paration Ã  lâoral 12 UniversitÃ© de Nantes  GÃ©ographie hors programme AgrÃ©gation PrÃ©paration Ã  lâoral 12  UniversitÃ© Paris IV â  Sorbonne GÃ©ographie pour historiens AgrÃ©gation PrÃ©paration Ã  lâoral 12  UniversitÃ© Paris IV â  Sorbonne  

!37!environnementale  au  sein  de  lâInstitut  de  GÃ©ographie  et  dâAmÃ©nagement  RÃ©gional  de lâUniversitÃ© de Nantes (IGARUN). En  premiÃ¨re  annÃ©e  de  Licence  (L1), jâai enseignÃ© la gÃ©ographie physique et environnementale sous  lâangle  des  grands  enjeux  (accÃ¨s  Ã   lâeau  potable,  Ã©rosion  des  sols  cultivÃ©s,  dÃ©sertification, changement  global, rÃ©chauffement  climatique,  Ã©lÃ©vation  du  niveau  de  la  mer  et  les  espaces cÃ´tiersâ¦)  en  CM  et  en  TD.  Jâai  enseignÃ©  la  gÃ©ographie  physique  de  la  zone  tropicale  pour  un public  dâhistoriens  (CM),  la  gÃ©ographie  physique  gÃ©nÃ©rale  et  environnementale  pour  gÃ©ographes et  historiens  (CM  et TD).  Les  thÃ©matiques  abordÃ©es  relÃ¨vent  de  problÃ©matiques  dâactualitÃ© (changement global, dÃ©sertification, inondation, sÃ©ismes, volcanismes, Ã©rosionâ¦). En  deuxiÃ¨me  annÃ©e  (L2), jâai  enseignÃ© lâhydrologie  (lâÃ©quation  gÃ©nÃ©rale  de  lâÃ©coulement,  les rÃ©gimes, les styles fluviaux, la mÃ©tamorphose fluvialeâ¦) et la climatologie pour gÃ©ographes (TD). La gÃ©ographie des risques (CM) en unitÃ© de dÃ©couverte pour des non gÃ©ographes. En  troisiÃ¨me  annÃ©e  (L3), jâai  enseignÃ© la  gÃ©omorphologie zonale aux  Ã©tudiants  de  Licence  de gÃ©ographie et la gÃ©omorphologie et lâanalyse des paysages aux Ã©tudiants de Licence Pro paysages. Jâai encadrÃ© Ã©galement des sorties de terrain dans la rÃ©gion nantaise (vallÃ©e de la Loire, vallÃ©e de la SÃ¨vre nantaise). En  Master  1, jâai  enseignÃ© la  gÃ©ographie  des  risques  cÃ´tiers  avec  une  approche  sur  lâalÃ©a,  la vulnÃ©rabilitÃ©  et  la  gestion des  risques  Ã   la  fois  par  une  approche  zonale  (les  littoraux  face  aux changements  climatiques)  et  en  termes  de  lÃ©gislation  pour  la  gestion  du  risque  cÃ´tier  (CM)  et  le risque  dâinondation  en  France (CM).  Jâenseigne  les  bases  de  donnÃ©es  gÃ©ographiques environnementales  (CM).  Jâassure  un  cours  dâinitiation  Ã   la  recherche  pour les  Ã©tudiants  qui  ont choisi cette filiÃ¨re (TD). En  Master  2  GÃ©ographie  et  AmÃ©nagement  des  Espaces  Maritimes  (GAEM), jâai enseignÃ© le changement global avec des dÃ©veloppements sur les milieux polaires (Ã©volution de la banquise arctique  et  ses  consÃ©quences  sur  le  climat  de  lâArctique  et  les  populations  littorales,  lâÃ©volution des  littoraux  arctiques,  â¦).  Les  changements  climatiques  sur  les  temps  longs  gÃ©ologiques,  et courts  Ã   lâÃ©chelle  des sociÃ©tÃ©s.  Jâenseigne  Ã©galement la  gÃ©ographie  des  risques  cÃ´tiers  avec  une approche sur la vulnÃ©rabilitÃ© et la gestion (CM).   UniversitÃ© de Paris IV - Sorbonne   De 1999 Ã  2006, jâai enseignÃ© comme maÃ®tre  de  confÃ©rences  Ã   l'UniversitÃ©  de  Paris IV  Sorbonne Ã   tous  les  niveaux,  de  la  premiÃ¨re  annÃ©e  de  Licence  (ex-DEUG  1)  au  M2  (ex-DEA)  et  Ã   la  prÃ©paration  des  concours. Depuis  septembre  2015,  jâenseigne  comme Professeur des UniversitÃ©s, la gÃ©omorphologie, la climatologie et les risques naturels.     Concours : prÃ©paration  Ã   l'oral  pour  les  historiens  (commentaires  de  documents)  et  la  question hors programme pour gÃ©ographes. Master  2 - Dynamique  des  milieux  et  risques :  interventions  sur  les  milieux  polaires.  Les crises paraglaciaires. Master 1 â Dynamique des milieux et risques.  UE â ÃpistÃ©mologie et mÃ©thodologie de la recherche.  CM sur  les  processus  dâÃ©rosion  Ã   la  surface  des  continents  et  les  relais  spatio-temporels  des processus  et  des  flux.  Les  zones  de  production  sÃ©dimentaire,  les  principaux  flux  et  les  zones  de transfert,  les  zones  de  stockage  et  de  dÃ©stockage.  Les  relations  entre  les  rythmes gÃ©omorphologiques  et  les  rythmes  climatiques,  les  rythmes  longs  et  les  rythmes  courts,  la surimposition des rythmes, les crises morphogÃ©niques. Encadrement du stage de terrain en gÃ©ographie physique et suivi des mÃ©moires de recherche.  

!38!Licence 3 CM  GÃ©ographie  de  lâeau :  le  cycle  de  lâeau,  les  hydrosystÃ¨mes,  la  mÃ©tamorphose  fluviale,  la gestion des hydrosystÃ¨mes, les risques dâinondation en France : alÃ©a, vulnÃ©rabilitÃ©, gestion. TD  de  gÃ©omorphologie  thÃ©matique (selon  les  annÃ©es) :  les  piÃ©monts,  les  reliefs  volcaniques,  la dynamique  des  versants,  les  palÃ©oenvironnements,  les  rÃ©seaux  de  drainage  et  les  contrÃ´les structuraux, â¦ TD  de  gÃ©ographie  physique  zonale (selon  les  annÃ©es) :  analyse  intÃ©grÃ©e  dâune  zone  climatique (froide, tempÃ©rÃ©e, chaude). TD  Risques  naturels  et  sociÃ©tÃ©s :  alÃ©as,  vulnÃ©rabilitÃ©  et  gestion  des  risques  sismiques, volcaniques, dâinondation, cÃ´tiers, de versants, â¦  Licence 2 CM  et TD  de  gÃ©omorphologie dynamique:  relief  et  climat,  les  hÃ©ritages,  les  processus dâÃ©rosion, processus dâÃ©rosion et modelÃ©s, principes de zonation des modelÃ©s, â¦ CM  et  TD de  climatologie : Ã©tude rÃ©gionale des climats terrestres, Ã©lÃ©ments de variations et de variabilitÃ© climatique (Oscillation Nord-Atlantique, El NiÃ±o, â¦). TD de biogÃ©ographie : les formations vÃ©gÃ©tales du globe.  Licence 1 CM  de  gÃ©omorphologie :  Le  triptyque  tectogenÃ¨se-morphogenÃ¨se-temps.  Les  grands ensembles  morphostructuraux : les  formes  de  relief  dans  les  bassins  sÃ©dimentaires,  en  structures plissÃ©es et faillÃ©es, en roches cristallines, le karst, les reliefs volcaniques. Les agents, processus et les formes associÃ©es (lâÃ©rosion glaciaire, fluviale, Ã©olienne, littorale, â¦). CM  de  gÃ©ologie :  Notions  de  gÃ©odynamique interne  (structure  de  la  terre,  tectonique  des plaques,  volcanisme,  sÃ©ismesâ¦).  Le  cycle  des  roches (les  roches  magmatiques,  sÃ©dimentaires, mÃ©tamorphiques).   UniversitÃ© Blaise Pascal (Clermont-Ferrand II)  De  2004  Ã   2008,  interventions  chaque  annÃ©e  dans  le Master  2  Recherche  de  lâuniversitÃ© Blaise Pascal (Clermont-Ferrand II), option Milieux polaires. Cours  sur  le  paraglaciaire,  sur  la  mÃ©tamorphose  des  milieux  polaires  face  aux  changements climatiques,  sur  les  boucles  de  rÃ©troaction  du  systÃ¨me  paraglaciaire,  sur  les  crises  et  mutations paysagÃ¨res Ã  diffÃ©rentes Ã©chelles spatiotemporelles.   Autres expÃ©riences dâenseignement dans des cadres non universitaires   1993-94 :  Scientifique  du  contingent Ã   l'Ãcole  de  SpÃ©cialisation  du  MatÃ©riel  de  l'ArmÃ©e  de Terre (ESMAT) de ChÃ¢teauroux (36). Cours de culture gÃ©nÃ©rale, de gÃ©ographie et d'analyse topographique aux sous-officiers et officiers prÃ©parant lâÃ©cole de guerre.  1992-93 :  Professeur  agrÃ©gÃ©  stagiaire au  collÃ¨ge  RenÃ©  Bernier  de  Saint-SÃ©bastien-sur-Loire (classes  de  sixiÃ¨me  et  de  quatriÃ¨me)  et  au  LycÃ©e  des  BourdonniÃ¨res  de  Nantes  (classe  de seconde).      

!39! Organisation et participation Ã  des stages de terrain   Mon engagement dans les stages de terrain a toujours Ã©tÃ© grand, Ã  la fois sur le plan administratif (montage  du  budget,  rÃ©servation  des  moyens  de  transport,  de  lâhÃ©bergement,  de  la restaurationâ¦), mais aussi sur le plan pÃ©dagogique avec en amont, une prÃ©paration en cours, sur le  terrain  directement,  et  en  aval  pour  lâaide  Ã   la  mise  en  forme  des  rapports  de  stage  et  leur notation.  Stages dâinitiation Ã  la recherche (M1) â UniversitÃ© Paris IV-Sorbonne 2006 Stage de M1 de gÃ©ographie physique en Bretagne (16 - 21 janvier).  Analyse dâun littoral Ã  grande Ã©chelle : Anse du Guesclin, Havre de Rotheneuf, Anse du Verger, et laboratoire de gÃ©omorphologie de Dinard.   Encadrement pÃ©dagogique : F. CarrÃ©, D. Gramond, D. Mercier, J.-P. Peulvast.  2004 Stage de maÃ®trise de gÃ©ographie physique en Bretagne (11 - 15 octobre). Analyse dâun littoral Ã   Ã©chelle  rÃ©gionale : Baie  du  Mont-Saint-Michel,  Pointe  du  Grouin,  Havre  de Rotheneuf, Baie de Lancieux, Baie de lâArguenon, Baie de la Fresnaye, Cap FrÃ©hel, et laboratoire de gÃ©omorphologie de Dinard.  Encadrement pÃ©dagogique  : J.-P. Amat, D. Mercier, J.-P. Peulvast.  2003 Stage de maÃ®trise de gÃ©ographie physique en Bourgogne (6 - 10 octobre).  Analyse  paysagÃ¨re  et  problÃ©matique  de  recherche  thÃ©matique :  La  CÃ´te  dâOr (gÃ©omorphologie  et  environnement),  la  ForÃªt  de  Citeaux  (analyse  biogÃ©ographique), la plaine de la SaÃ´ne (risque dâinondation).   Encadrement pÃ©dagogique : J.-P. Amat, D. Gramond, D. Mercier, J.-P. Peulvast.  2002 Stage de maÃ®trise de gÃ©ographie physique dans le Cotentin (7 â 11 octobre).   Analyse  paysagÃ¨re  et  problÃ©matique  de  recherche  thÃ©matique  :  forÃªt  de  Cerisy, marais  de  Carentan,  lande  de  Lessay  (analyses  biogÃ©ographiques)  havre  de  Saint-Germain  (amÃ©nagement  littoral),  Suisse  normande,  pointe  de  la  Hague (gÃ©omorphologie).    Encadrement pÃ©dagogique : J.-P. Amat, D. Mercier, J.-P. Peulvast.  Les  Ã©tudiants  sont  initiÃ©s  aux  pratiques  de  terrain :  apprÃ©hension  dâune  problÃ©matique  de recherche,  mise  en  place  de  stratÃ©gies  dâÃ©chantillonnage,  rÃ©alisation  de  fiches  de  terrain, techniques  de  relevÃ©s  topographiques,  analyse  des  composantes  paysagÃ¨res,  prise  en  compte  du temps  des  paysages  et  des  Ã©chelles  spatiales  du  paysage,  description  de  coupes,  techniques  de prÃ©lÃ¨vement, cartographie thÃ©matique, â¦ Ce  stage  reprÃ©sente  souvent  la  premiÃ¨re  approche  de  terrain  pour  les  Ã©tudiants  face  Ã   une problÃ©matique scientifique.  Stages de dÃ©couverte gÃ©ographique â UniversitÃ© Paris IV-Sorbonne 2002 Stage de DEUG en Alsace  (21-23 mai). Programme :  ville  de  Strasbourg,  plaine  dâAlsace,  vignoble,  ville  de  Colmar,  ballons dâAlsace.   Encadrement pÃ©dagogique : D. Gramond, D. Mercier, R. Schirmer.  2001 Stage sur le terrain pour les Ã©tudiants de licence dans la rÃ©gion nantaise (2 au 6 avril). Programme : vignoble du Muscadet, vallÃ©e de la Loire, littoral, marais de GuÃ©rande, port de Saint-Nazaire, ville de Nantes, Ã®le dâYeu.   

!40! Encadrement pÃ©dagogique : D. Mercier, J.-R. Pitte, R. Schirmer.  Dans  ces  stages,  la  diversitÃ©  des  approches  gÃ©ographiques  a  Ã©tÃ©  privilÃ©giÃ©e  en  analysant  les paysages,  les  mutations  socio-spatiales.  Les  approches  sont  thÃ©matiques  et  le  plus  large  possible, sans  cloisonnement  intradisciplinaire,  pour  donner  une  vision  globale  de  la  gÃ©ographie  aux Ã©tudiants.  Sorties de terrain pour les Ã©tudiants de Licence Pro-Paysage de lâIGARUN   De 2006 Ã  2015   - Sortie  dans  la  vallÃ©e  de  la  Loire  sur  la  commune  de  Basse-Goulaine-Saint  Julien  de Concelles  et  La  Chapelle-Basse-Mer.  ThÃ¨mes  abordÃ©s :  risque  dâinondation,  gestion  de lâeau, dynamique fluvialeâ¦)  - Sortie  dans  la  vallÃ©e  de  la  SÃ¨vre  nantaise  sur  les  communes  de  Vertou  et  Saint-Fiacre-sur-Maine. ThÃ¨mes  abordÃ©s :  les  contraintes  paysagÃ¨res,  les  dynamiques  hydrologiques,  les pollutions, le tourismeâ¦).  2015 : Sorties  de  terrain dans  la  vallÃ©e  de  la  Loire pour des  stagiaires  dans  le  cadre  de  la formation  de  formateurs risques  majeurs  Ã©ducation,  du  23  au  27  mars  2015 organisÃ©  par  lâInstitut FranÃ§ais  des Formateurs Risques Majeurs  et  protection  de  l'Environnement, 9,  rue  Jacques  Louvel-Tessier - 75 010 Paris, www.iffo-rme.fr   Participation Ã  des jurys   Participation Ã  des jurys d'HDR : 2014 â Etienne  COSSART â Â« Des  sources  sÃ©dimentaires  Ã   lâexutoire :  un  problÃ¨me  de  connectivitÃ© ? RÃ©flexions sur  le  fonctionnement  gÃ©omorphologique  des  bassins  montagnards Â»,  UniversitÃ©  Blaise  Pascal, Clermont-Ferrand II (rapporteur). 2015 â Armelle  DECAULNE â Â« Datation  des  gÃ©odynamiques  de  pente  dans  les  environnements  froids  des hautes latitudes et Ã©chelles de temps considÃ©rÃ©es. Exemples islandais Â», UniversitÃ© de Nantes. (garant). 2015 â Pascal  BARTOUT - Â«  Les  territoires  limniques.  Nouveau  concept  limnologique  pour  une  gestion gÃ©ographique des milieux lentiques Â», universitÃ© dâOrlÃ©ans. (rapporteur).  Participation Ã  des jurys de thÃ¨se : 2005 â Myrtille  MOREAU â Dynamique  des  paysages  vÃ©gÃ©taux  depuis  la  fin  du  Petit  Ãge  glaciaire  au Spitsberg  (79Â°N).  Analyse  intÃ©grÃ©e  de  la  reconquÃªte  vÃ©gÃ©tale  des  marges  proglaciaires,  UniversitÃ©  Blaise Pascal, Clermont-Ferrand II (examinateur). 2010 â Thierry  FEUILLET â Â« Les  formes  pÃ©riglaciaires  dans  les  PyrÃ©nÃ©es  centrales  franÃ§aises :  analyse spatiale, chronologique et valorisation Â», UniversitÃ© de Nantes (co-directeur). 2011 - Erwan ROUSSEL - RÃ©ponses  des  glaciers  et  sandurs  sud-islandais  au  rÃ©chauffement  climatique  post-Petit  Ãge  glaciaire. ModalitÃ©s  et  rythmes  dâajustement  du  continuum  glacio-fluvial, UniversitÃ©  Blaise Pascal, Clermont-Ferrand II (examinateur). 2013 â Mehdi MAANAN â Â« Impact des  changements  de  l'occupation  des  sols  sur  l'Ã©tat  de  l'environnement dans  les  Ã©cosystÃ¨mes  cÃ´tiers  :  cas  des  lagunes  d'Oualidia  et  de  Moulay  Bouselham  (faÃ§ade  atlantique marocaine) Â», FacultÃ© des Sciences, El Jadida, Maroc (examinateur). 2013 â Pauline LETORTU - Â« Le  recul  des  falaises  crayeuses  haut-normandes  et  les  inondations  par  la  mer en  Manche  centrale  et  orientale:  de  la  quantification  de  l'alÃ©a  Ã   la  caractÃ©risation  des  risques  induits Â»  ,  

!41!UniversitÃ© de Caen Basse-Normandie (rapporteur). 2013 â Mathieu  FRESSARD â Â« Les  glissements  de  terrain  du  Pays  dâAuge  continental  (Normandie, France) :  caractÃ©risation,  cartographie,  analyse  spatiale  et  modÃ©lisation Â»,  UniversitÃ©  de  Caen  Basse-Normandie (prÃ©sident). 2014 â Nafissa  SFAKSI â Â« Dynamique  spatio-temporelle  des  paysages  vÃ©gÃ©taux  dans  un  espace  naturel protÃ©gÃ© :  cas  du  Parc  National  de  Taza  (PNT),  AlgÃ©rie Â»,  UniversitÃ©  de  Toulouse  2  Le  Mirail (rapporteur). 2014 â Romain  PERRIER â Â« Suivi  local  et  rÃ©gional  du  pergÃ©lisol  dans  le  cadre  du  changement  climatique contemporain :  application  aux  vallÃ©es  de  la  ClarÃ©e  et  de  lâUbaye  (Alpes  du  sud,  France) Â»,  UniversitÃ© Paris Denis Diderot (Paris 7) (rapporteur). 2015 â Marine  GOURRONC â Â« Morphologie  et  remplissage  sÃ©dimentaire  des  vallÃ©es  martiennes : marqueurs  des conditions  climatiques  prÃ©-amazoniennes Â», UniversitÃ© de Nantes. Discipline : GÃ©ologie (prÃ©sident). 2015 â Julien COQUIN Â« Les  impacts  gÃ©omorphologiques  de  la  dÃ©glaciation  dans  la  rÃ©gion  du SkagafjÃ¶rÃ°ur, Islande Â», UniversitÃ© de Nantes (directeur). 2015 â Axel  CREACH  Â« Cartographie  et  Ã©valuation  Ã©conomique  de  la  vulnÃ©rabilitÃ©  du  littoral  atlantique franÃ§ais face au risque de submersion marine Â», UniversitÃ© de Nantes (directeur). 2015 â Annabelle MOATTY â Â« Les reconstructions post-catastrophe â une approche gÃ©ographique Â», UniversitÃ© de Montpellier III (rapporteur).   Participation Ã  des jurys de mÃ©moires de Master 2 2005 â Thierry  FEUILLET â Apport  de  lâÃ©tude  des  Ã©tagements  dans  les  reconstitutions palÃ©oenvironnementales :  exemple  de  lâIslande, UniversitÃ© Paris IV, 96 p., co-direction : D. Mercier et Ch. Le CÅur (Paris 1). 2005 â Nathalie  GRANGERAY â La  plaine  alluviale  comme  milieu  dâenregistrement  des  Ã©vÃ©nements climatiques et anthropiques : application Ã  la Loire bourbonnaise, direction : E. Gautier (Paris 8). 2005 - Erwan ROUSSEL â Contexte paraglaciaire et jÃ¶kulhlaups en Islande : impacts morphologiques sur les rÃ©seaux  hydrographiques  proglaciaires, UniversitÃ© Blaise Pascal, Clermont-Ferrand II, Co-direction : D. Mercier et S. Ãtienne, 69 p. 2005 â Morgan PRIET-MAHEO â Ãvolution  des  cÃ´tes  rocheuses  de  la  pÃ©ninsule  de  Reykjanes,  Islande, UniversitÃ© Blaise Pascal, Clermont-Ferrand II, direction : S. Ãtienne. 2005 â Virginie  COMTE â Ãvolution  biogÃ©omorphologique  des  moraines  en  Islande,  UniversitÃ©  Blaise Pascal, Clermont-Ferrand II, direction : M.-F. AndrÃ©. 2007 â Riwan  KERGUILLEC â LâÃ©volution  rÃ©cente  des  limites  de  lâÃ©tage  pÃ©riglaciaire  actif  dans  les montagnes  de  lâEurope  du  Nord-Ouest  (Islande-NorvÃ¨ge), UniversitÃ© de Nantes, 110 p., direction : D. Sellier. 2007 â Lauriane  LâHOUR â Ãtude  de  la  vulnÃ©rabilitÃ©  de  lâagglomÃ©ration  nantaise  face  au  changement climatique, UniversitÃ© de Nantes, 64 p., direction : E. Chauveau. 2011 â Marion HUBER â Les outils de lâadaptation au changement climatique en zone cÃ´tiÃ¨re. Lâexemple du Nouveau-Brunswick, UniversitÃ© de Nantes, 80 p. + annexes, direction : D. Mercier 2012 â Anissa  QUEMENEUR â Anthropisation  et  transformations  du  milieu  lagunaire  de  Sidi  Moussa (Maroc) :  analyse  spatiale,  sÃ©dimentaire,  gÃ©ochimique et  biologique  de  lâÃ©volution  de  lâoccupation  des  sols, UniversitÃ© de Nantes, 78 p., direction : M. Maanan. 2012 - LaurÃ¨ne GIULANI â Lâobservatoire  du  risque  inondation  de  la  vallÃ©e  de  la  Maurienne UniversitÃ© de Nantes, 86 p., direction : D. Mercier 2013 â Eliane  GROSSE â DÃ©finition  et  cartographie  des  enjeux  en  zone  inondable  dans  le  cadre  de  la Directive  Inondation.  Application  sur  le  territoire  meurthe-et-mosellan, UniversitÃ©  de  Nantes,  119  p.  + un atlas en annexes, direction : D. Mercier 2013 â Elie CHEVILLOT-MIOT â Analyse de la vulnÃ©rabilitÃ© de la rÃ©gion Pays de la Loire au risque de submersion, UniversitÃ© de Nantes, 79 p., direction : D. Mercier.  

!42!2013 â Julie DOSSMANN â Les  enjeux  humains  et  Ã©conomiques  du  littoral  de  la  Charente-Maritime  face au  risque  de  submersion  marine.  Etude  de  la  vulnÃ©rabilitÃ©  du  territoire  de  la  Charente-Maritime  au  sein  du Service  dÃ©partemental  dâincendie  et  de  secours  de  Charente-Maritime  (SDIS17),  UniversitÃ©  de  Nantes, 90 p., direction : D. Mercier. 2015 â Marine  BOURRIQUEN â Evolution  du  trait  de  cÃ´te  de  la  presquâÃ®le  de  BrÃ¸gger  (Spitsberg  nord-occidental) de 1948 Ã  2014, UniversitÃ© de Nantes, 74 p., co-direction : A. Baltzer et D. Mercier. 2015 â Etienne ROUX-RIOU â Gestion  durable  de  la  bande  cÃ´tiÃ¨re  en  Basse-Normandie, UniversitÃ© de Nantes, 64 p., direction : D. Mercier.  Participation Ã  des jurys de mÃ©moires de Master 1 1999 â Riwan KERGUILLEC - Les  formes  de  ravinement  postglaciaires  dans  le  secteur  du  Midfell  (Islande du Sud), UniversitÃ© de Nantes, direction : D. Sellier. 2001 â Adeline KILICASLAN â CaractÃ©risation de profils dâaltÃ©ration dans les hautes terres du Cap Breton (Nouvelle-Ãcosse, Canada), UniversitÃ© Paris IV, 122 p., direction : J.-P. Peulvast. 2002 â Pascal EVANO â Le  bassin-versant  de  la  Vesgre :  Ã©tude  gÃ©omorphologique, UniversitÃ© Paris IV, 245 p., direction : J.-P. Peulvast. 2003 â Julie  LANDREIN â Le  risque  dâinondation  Ã   La  Chapelle-Basse-Mer  commune  du  Val  nantais, UniversitÃ© Paris IV, 277 p., direction : D. Mercier. 2003 â Tibault  MEUNIER â Le  risque dâinondation  dans  le  Val  nantais.  Lâexemple  de  Saint-Julien  de-Concelles, UniversitÃ© Paris IV, 215 p., direction : D. Mercier. 2003 â Laure DE BUZON â La  crue  dans  le  gÃ©osystÃ¨me  mÃ©diterranÃ©en  du  bassin  versant  de  lâÃze  dans  le Luberon.  Vers  une  approche  environnementale  des  extrÃªmes  hydrologiques ?, UniversitÃ© Paris IV, 147 p., direction : D. Mercier. 2003 â Elena  STEFANESCU - Les  risques  dâinondations  et  dâÃ©coulements  de  boue  dans  la  commune dâEssÃ´mes-sur-Marne (02), UniversitÃ© Paris IV, 166 p., direction : J.-P. Amat. 2003 â Jean-Pierre  LANCKMAN â GÃ©omorphologie  des  Ã©difices  volcaniques  sous-glaciaires  en  Islande (morphologie, morphomÃ©trie et morphogÃ©nÃ¨se), UniversitÃ© Paris IV, 85 p., direction : J.-P. Peulvast. 2003 â Sandra  BERTHOU â Ãtude  gÃ©omorphologique  du  littoral  du  Petit  TrÃ©gor  (FinistÃ¨re),  UniversitÃ© Paris IV, 143 p., direction : D. Mercier. 2004 â Jordan CHARRIER â Le  risque  dâinondation  Ã   Paris, UniversitÃ© Paris IV, 191 p., direction : D. Mercier. 2004 â Nathalie GRANGERAY â Le  risque  dâinondation  dans  la  vallÃ©e  de  la  Loire  entre  Montjean-sur-Loire et Ancenis, UniversitÃ© Paris IV, 230 p. + annexes, direction : D. Mercier. 2004 â Marine BILLIARD â CrÃ©ation dâun label de qualitÃ© pour la gestion et la prÃ©vention des risques et des catastrophes naturels, UniversitÃ© Paris IV, 198 p., direction : D. Mercier. 2004 - Erwan ROUSSEL â LâÃ©volution morphologique des rÃ©seaux hydrographiques en presquâÃ®le de BrÃ¸gger, Spitsberg,  UniversitÃ©  de  Toulouse-Le  Mirail,  185 p.,  co-direction  M.  Pagealow  (universitÃ©    de Toulouse-Le Mirail) et D. Mercier. 2004 â Anatole  CRAMER â Dynamique  du  fleuve  Serayu,  Java,  IndonÃ©sie,  UniversitÃ©  Paris  IV,  84 p., direction : D. Mercier. 2005 â MÃ©lanie MARTIN â Dynamiques vÃ©gÃ©tales et fluctuations glaciaires holocÃ¨nes dans la vallÃ©e du glacier Franz  Josef  (Nouvelle  ZÃ©lande)  depuis  la  derniÃ¨re  grande  glaciation,  UniversitÃ©  Paris  IV,  92 p., direction : D. Mercier. 2005 â Sarah  JUGLAIR â ModelÃ©s  des  versants  dâune  grande  dÃ©pression  fermÃ©e :  Hebes  Chasma,  Valles Marineris, Mars, UniversitÃ© Paris IV, 105 p., direction : J.-P. Peulvast. 2005 - Julien  QUIRET â Le  Morvan  septentrional  et  sa  bordure  sÃ©dimentaire :  Ã©tude  gÃ©omorphologique, UniversitÃ© Paris IV,157 p., direction : J.-P. Peulvast. 2005 â Antoine DE GREGORI â La  dynamique  des  paysages du  bassin  versant  du  Bez  aprÃ¨s  la  crue  de dÃ©cembre 2003, UniversitÃ© Paris IV, 151 p., direction : D. Gramond.  

!43!2005 â Alban  DOMMANGET â Ãvaluation  des  programmes  de  dÃ©fense  et  restauration  des  sols  (DRS) dans  le  pÃ©rimÃ¨tre  agricole  du  Loukkos  (Maroc), UniversitÃ© Paris IV, 127 p., co-direction : R. Ragala et D. Mercier. 2005 â Fatimata KANE â Ãvolution rÃ©cente de la langue de Barbarie (Saint-Louis du SÃ©nÃ©gal) et ses impacts environnementaux et socio-Ã©conomiques, UniversitÃ© Paris IV, 181 p., direction : Florence Brondeau. 2006 â ÃvangÃ©lia DELCHANIDIS â Vers une gestion citoyenne de lâeau. Appropriation et rÃ©appropriation des connaissances relatives Ã  lâimpact de lâactivitÃ© humaine sur la qualitÃ© de lâeau dans le bassin de la Seine par la sociÃ©tÃ© civile et la communautÃ© scientifique, UniversitÃ© Paris IV, 27 p., direction : D. Mercier. 2006 â Antoine HÃNOCQ â Ãtat  des  lieux  des  retenues  dâeau  destinÃ©es  Ã   lâirrigation  dans  les  Pays  de  la Loire, UniversitÃ© Paris IV, 39 p., direction : D. Mercier. 2006 â Anthony  HOUBART â RÃ©ponse  du  littoral  de  lâArctique  Canadien  de  lâOuest  au  rÃ©chauffement climatique (Mer de Beaufort), UniversitÃ© Paris IV, 89 p., direction : D. Mercier. 2006 â Charlotte  TROSSEILLE â Contribution  au  projet  de  crÃ©ation  du  Portail  Â« eau  Alsace Â», UniversitÃ© Paris IV, 88 p., direction : D. Mercier. 2006 â Perrine SZUBA â Ãvolutions  spatiale  et  temporelle  de  la  dÃ©glaciation  du  bassin-versant  de  la  Biaysse (Alpes  du  Sud,  France)  depuis  la  fin  du  Petit  Ãge  Glaciaire, UniversitÃ© Paris IV, 98 p., co-direction : D. Mercier et E. Cossart (Paris 8). 2012 â Elie CHEVILLOT-MIOT â La  vulnÃ©rabilitÃ©  du  bÃ¢ti  face  au  risque  de  submersion  marine  sur  lâÃ®le de Noirmoutier (VendÃ©e), UniversitÃ© de Nantes, 123 p., direction : D. Mercier. 2015 â Aurore  LETELLIER-PERAS â Les  glissements  post-glaciaires  en  Islande  du  Nord-Ouest, UniversitÃ© de Nantes, direction : D. Mercier, co-direction : Armelle Decaulne. 2015 â Marie  PROVOST â Le  risque  inondation  Ã   Dublin  (Irlande) :  oÃ¹,  quand,  comment ?, UniversitÃ© de Nantes, direction : D. Mercier.  Autres responsabilitÃ©s Collectives : Outre les responsabilitÃ©s de direction de laboratoire prÃ©sentÃ©es dans la rubrique recherche.  2015 â Jury du BTS au LycÃ©e Livet de Nantes.  2007 â PrÃ©sident du Jury du BaccalaurÃ©at gÃ©nÃ©ral au LycÃ©e MendÃ¨s-France de La Roche-sur-Yon (85).  2006-2007 â Responsable pÃ©dagogique de la premiÃ¨re annÃ©e de Licence de gÃ©ographie Ã  lâIGARUN Les  Ã©tudiants  de  premiÃ¨re  annÃ©e  de  Licence  de  gÃ©ographie Ã©taient 160  inscrits en  2006-2007.  La responsabilitÃ© pÃ©dagogique consistait Ã  gÃ©rer au cas par cas, les problÃ¨mes dâinscription, de notes et  dâorientation  vis-Ã -vis  des  Ã©tudiants.  Au  niveau  du  secrÃ©tariat,  il  sâagissait  de vÃ©rifier  la  saisie des  notes  de  contrÃ´le  continu  et  dâexamen.  Il  incombait au  responsable dâannÃ©e  de  prÃ©sider le jury  de  la  premiÃ¨re  et  seconde  sessions  dâexamen.  Il  fallait  aussi gÃ©rer  les  diffÃ©rents  entre  les Ã©tudiants, les enseignants et la scolaritÃ© centrale.   2000-2006 - Membre Ã©lu au conseil de lâUFR de GÃ©ographie de Paris IV-Sorbonne.   - En 2102 : jâai Ã©tÃ© dÃ©signÃ© par le PrÃ©sident Y. Lecointe pour reprÃ©senter l'UniversitÃ© de Nantes Ã  l'Association Communautaire de l'Estuaire de la Loire et du Littoral (ACELL). Cette association rÃ©gie  par  la  loi  du  1er juillet  1901  a  pour  but  dâÃªtre  une  instance  de  concertation  entre  ses membres  pour  construire  une  vision  stratÃ©gique  partagÃ©e  durable  de  lâestuaire  de  la  Loire  et  du littoral  de  Loire-Atlantique.  Elle  est  composÃ©e  du  Conseil  RÃ©gional  des  Pays  de  la  Loire,  du Conseil gÃ©nÃ©ral de Loire-Atlantique, de la CommunautÃ© Urbaine de Nantes (Nantes MÃ©tropole), la  CARENE,  la  Chambre  de  Commerce  et  dâIndustrie  de  Nantes/Saint  Nazaire,  du  Grand  Port  

!44!Maritime  de  Nantes/Saint-Nazaire,  de  lâUnion  Maritime  Nantes  Ports,  de  Cap  Atlantique,  des CommunautÃ©s de communes Sud Estuaire et de Pornic et de lâUniversitÃ© de Nantes.  - De 2009 Ã  2014 : responsable de la commission GÃ©othÃ¨que de lâIgarun  - De 2012 Ã   2015 :  reprÃ©sentant  de  la  gÃ©ographie  au  sein  du  conseil  de  lâÃ©cole  doctorale DEGEST de lâUniversitÃ© de Nantes.  - De 2013 Ã  2015 : membre de la commission Documentation de lâuniversitÃ© de Nantes.  - 2014-2015  :  responsable  du  Sous  groupe  de  travail  sur  les  UnitÃ©s  de  recherche  (SGT3)  dans  le cadre  de  l'auto-Ã©valuation  de  l'universitÃ©  de  Nantes  (vague  B),  dÃ©signÃ©  par  la  vice-prÃ©sident recherche et innovation FrÃ©dÃ©ric Benhamou.  2000 et 2001 - Membre du jury du CAPES Externe d'Histoire et de GÃ©ographie.    

"
resume,"
J uliu s H ie ta la
F ulls ta c k S oft w are  E ngin e er
I n fo
j u li u s.h ie ta la @ gm ail. c o m
+ 3 58  4 4 5 61 0 94 6
H els in ki,  F in la n d  /  R em ote
L in ke d In  >
G it h ub  >
W ork in g  L an g uag es
F in nis h
E ngli s h
S w ed is h	
B io
I  h ave  4  y e ars  o f e xp erie nce  a s a  F ulls ta c k S oft w are  E ngin e er. F o r m ost o f
m y c are er, I  h ave  b een  d oin g a  l o t o f N od e a n d  R eac t w it h  T yp eS crip t
s p eciï¬ cally , b oth  a t w ork  a n d  i n  s id e p ro je cts . T hese  s e rv ic e s h ave  o fte n
b een  c o nta in e riz e d  a n d  d ep lo ye d  t o  t h e  c lo ud , u su ally  t o  G CP, b ut I  a m  n o
s tra n ger t o  K ub ern ete s e it h e r w he re  I  h ave  h e lp ed  d esig n  m ult ip le
m ic ro se rv ic e s. R ew rit in g a n d  m ig ra tin g l e g ac y c o d eb ase s i n to  n ew er
s ys te m s i s  s o m eth in g I  e n jo y a  l o t. I  c o nsta n tly  k e ep  a n  e ye  o ut f o r
i n cre asin g d eve lo p m en t s p eed  b y a p ply in g m od ern  b uild  a n d  C I/ C D  t o ols . I
e n jo y l e ad in g i n te r-  a n d  c ro ss -te am  e ffo rts , w it h  a n  e m phasis  o n
s u p po rtin g t h e  l e arn in g o f m ys e lf  a n d  o th e rs .
T o p  s k ills
P ro gra m min g L an gu ag es
F ro nt E nd  D eve lo p m en t
B ac k E nd  D eve lo p m en t
M ac h in e  L earn in g /  D ata  S cie nce
D ata b ase s
C lo ud  P la tfo rm s
M ob ile  D eve lo p m en t
D ev O ps
J ava S crip t, T yp eS crip t, P yth o n
R eac t, N ext.js
N od e.js , F la sk
P yT o rc h, N um py, P an d as
S QL, P ostg re S QL
G CP
S w if t , R eac t N ativ e
K ub ern ete s, C I/ C D
P ro je cts
C TO , F o und er |  h an aa.io
2 0 22 -  p re se n t
H an aa.io  i s  a  P 2P  p la tfo rm  f o r b uyin g a n d  s e lli n g u se d  m oto rc yc le s. I
h ave  b uilt  t h e  p la tfo rm  w it h  t h e  m ost m od ern  w eb  t e ch no lo gie s a n d
f r a m ew ork s.
T e ch : T yp eS crip t, R eac t, P yth o n ( ï¬ ask ),  G ra p hQ L, G CP
M ac h in e  L earn in g  E ng in e er |  V oi S co ote rs
2 0 22
I  w ork e d  o n a n  u rg en t p ro je ct h e lp in g t h e  c o m pan y a p ply  M ac h in e
L earn in g i n  t h e ir  a p p.
T e ch : i O S/S w if t , A nd ro id /J ava , C ore M L, P yT o rc h 


S oftw are  E ng in e er, F o und er |  S tra y R ob ots
2 0 21 -  2 0 22
I  b uilt  t o ols  a n d  p ro vid ed  s e rv ic e s f o r 2 D  a n d  3 D  c o m pute r v is io n
r a n gin g f r o m  l a b eli n g t o  c u sto m  C V a lg o rit h m s.
T e ch : P yth o n, C ++, P yT o rc h, D ocke r, R O S
W ork  E xp erie nce
S oftw are  E ng in e er |  N im ble  R ob otic s
2 0 22 -  p re se n t
N im ble  i s  r e im ag in in g f u lï¬ llm en t w it h  i n te lli g en t r o bots  t h at c an  p ic k a n d
p ac k a n yth in g.
T e ch : P yth o n
S oftw are  E ng in e er |  S m artly .io
2 0 18  -  2 0 22
I  w ork e d  o n t h e  v id eo  t e m pla te  r e nd erin g e n gin e  a n d  m ed ia  l i b ra ry  s id e o f
t h e  p ro d uct. L ed  m ult ip le  e ffo rts  o f m ig ra tin g l e g ac y c o d e i n to  t h e  n ew
s ys te m s.
T e ch : R eac t, R ed ux, T yp eS crip t, N od e
S olu tio ns E ng in e er |  S m artly .io
2 0 17  -  2 0 18
I  h e lp ed  s o m e o f t h e  w orld 's  l a rg est s o cia l m ed ia  a d ve rtis e rs  s u ch  a s
A ir b nb  a n d  U ber i n  s c ali n g t h e ir  s o cia l m ed ia  a d ve rtis in g b y i n te g ra tin g
t h e m  i n to  t h e  S m artly .io  a d ve rtis in g p la tfo rm .
T e ch : J ava S crip t, P yth o n
E ducatio n
M aste r o f S cie nce  /  A alt o  U niv e rs it y
2 0 12 -2 0 22 C om pute r S cie nce  /  M ac h in e  L earn in g, D ata  S cie nce  a n d
A rtiï¬ cia l I n te lli g ence 

"
resume,"Claire Le Goues
Software and Societal Systems Department +1 (412) 268-6954
School of Computer Scienceclegoues@cs.cmu.edu
Carnegie Mellon University http://www.cs.cmu.edu/~clegouesResearch Interests and ApproachMy research interests span software engineering and programming languages, and especially in how to construct,
maintain, evolve, improve/debug, and assure high-quality software systems.
Employment Carnegie Mellon University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pittsburgh, PA, USA
School of Computer Science (SCS)
Software and Societal Systems Department (S3D)
2022 â present Associate Department Head for Faculty
2021 â present Associate Professor, with indefinite tenure
2019 â 2021 Associate Professor, without indefinite tenure
2013 â 2019 Assistant Professor
Microsoft Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Redmond, WA, USA 2009 Research Intern, Research in Software Engineering (RiSE) group
IBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (various)
2006â2007 Software Engineer, XML Technologies/Compilation Cambridge, MA, USA
2005 Research Intern, Collaborative User Experience (CUE) Cambridge, MA, USA
2004 Research Intern, Architectâs Workbench Hawthorne, NY, USA
Education University of Virginia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Charlottesville, VA, USA
2013 Doctor of Philosophy in Computer Science
Thesis: Automatic Program Repair Using Genetic Programming, advised by Westley Weimer
2009 Master of Science in Computer Science Thesis: Specification Mining With Few False Positives, advised by Westley Weimer
Harvard University . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Cambridge, MA, USA 2006 Bachelor of Arts in Computer Science
Thesis: Algebraic Type Isomorphisms, advised by Gregory Morrisett
Honors and Major Leadership Leadership and Impact
2020â2026 SCS Faculty Fellowship Recognizing Diversity and Inclusion
2020 ACM SIGSOFT Early Career Researcher Award
2019 ICSE Most Influential Paper (N-10)
2019 SIGEVO Impact Award
Research Quality 2022 Best Paper, USENIX Security
2018 ACM Distinguished Paper, Intl. Conference on Software Engineering
2018 Facebook Testing and Verification Research Award
2015 Featured Article, IEEE Transactions on Software Engineering
2013 Google Faculty Research Award Claire Le Goues Curriculum Vitae July 10, 2023 Page 1 of16 

2012 Featured Article, IEEE Transactions on Software Engineering
2012Bronze, ACM SIGEVO âHumiesâ for Human-Competitive Results Produced by Genetic and Evolu-
tionary Computation
2009
Gold, ACM SIGEVO âHumiesâ for Human-Competitive Results Produced by Genetic and Evolutionary
Computation
2009 IFIP TC2 Manfred Paul Award, Intl. Conference on Software Engineering
2009 ACM Distinguished Paper, Intl. Conference on Software Engineering
2009 Best Paper, Genetic and Evolutionary Computation Conference
2009 Best Short Paper, Workshop on Search-Based Software Testing
Other honors 2022 Distinguished Reviewer, Intl. Conference on Automated Software Engineering (ASE)
2018 Reliable Rapid Response Reviewer, Intl. Conference on Software Engineering
2018 National Science Foundation CAREER Award
2016 Best Reviewer Award, Intl. Symposium on Search-Based Software Engineering
2015 Distinguished Reviewer, Intl. Conference on Automated Software Engineering (ASE)
2009â2012 Graduate Research Fellowship, National Science Foundation
Professional Service and Affiliations Local Service at Carnegie Mellon University
Leadership and University-level service
Associate Dept. Head for Faculty, S3D . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2022âpresent Member, University Committee on Faculty Appointments (Non tenure) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2021â2023
Chair, ISR Tenure Track Hiring Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2019â2022
Co-Director, REUSE@CMU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2016âpresent
Director, Undergraduate Minor in Software Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2014â2018
Other service
Member, SCS Dean Search Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2018â2019
Member, Cylab Director Search Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2018
Member, SCS Undergraduate Review Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2016â2019
Member, ISR Teaching/Tenure Track Faculty Hiring Committees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2015â2019
Member, SE PhD Graduate Admissions Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2013âpresent
International Service
Organization and Chairs â¢Area Co-Chair, Intl. Conference on Software Engineering (ICSE), 2024
â¢Newcomers Co-Chair, Intl. Conference on Software Engineering (ICSE), 2022
â¢PC Co-Chair, Intl. Conference on Automated Software Engineering (ASE), 2020
â¢	
PC Co-Chair, Tool Demonstration Track, Intl. Conference on Automated Software Engineering (ASE Demo),
2019
â¢
PC Co-chair, Foundations of Software Engineering, New Ideas and Emerging Results Track (FSE-NIER), 2018
â¢Co-organizer, Dagstuhl Seminar 18052, Genetic Improvement of Software, 2018
â¢Co-organizer, Dagstuhl Seminar 17022, Automated Program Repair, 2017
â¢PC Chair, Graduate Track, Symposium on Search Based Software Engineering (SSBSE), 2017
â¢Review Process Co-Chair, Intl. Conference on Automated Software Engineering (ASE), 2016
â¢
Local Arrangements Chair, Systems, Programming, Languages and Applications: Software for Humanity
(SPLASH), 2015
â¢PC Co-chair, Symposium on Search Based Software Engineering (SSBSE), 2014 Claire Le Goues Curriculum Vitae July 10, 2023 Page 2 of16 

Memberships
â¢Steering Committee Member, Intl. Conference on Automated Software Engineering (ASE), 2020âpresent
â¢Member, ASE Most Influential Paper (MIP) Award Committee, Intl. Conference on Automated Software
Engineering (ASE), 2020, 2021
â¢
Member, Test of Time Award Selection Committee, European Software Engineering Conference/Foundations of
Software Engineering (ESEC/FSE), 2019
â¢Member, IEEE Transactions on Software Engineering (TSE) Review Board, 2017â2020
â¢Member, DARPA ISAT study group, 2017â2020
â¢Steering Committee Member, Symposium on Search Based Software Engineering (SSBSE), 2014â2017
Journal Editorships â¢Associate Editor, Harvard Data Science Review (HDSR), 2022âpresent
â¢Guest editor for special issue of IEEE Software on Automatic Program Repair, 2021
â¢
Associate Editor, Genetic Programming and Evolvable Machines (Area Editor for Software Engineering) (GPEM),
2019âpresent
Program Committee (conference) â¢
Intl. Conference on Software Engineering (ICSE) 2016, 2017, 2018 (Rapid Response Reviewer), 2019 (Program
Board), 2022, 2023, 2024 (Area Chair, AI&SE, Auto-coding)
â¢IEEE/ACM Intl. Automated Software Engineering (ASE), 2015, 2018, 2019, 2022
â¢European Software Engineering Conference/Foundations of Software Engineering (ESEC/FSE), 2017
â¢Symposium on Search Based Software Engineering (SSBSE), 2015, 2016, 2018
â¢Working Conference on Mining Software Repositories (MSR), 2016
â¢Intl. Symposium on Software Testing and Analysis (ISSTA), 2016
â¢North American Conference on Search-Based Software Engineering (NasBASE), 2015
â¢Intl. Conference on Software Maintenance and Evolution (ICSME), 2014, 2015
Program Committee (special tracks, workshops) â¢
Intl. Workshop on Dependability of Safety-Critical Systems with Machine Learned Components (D-SyMLe),
2023
â¢Intl. Workshop on Automatic Program Repair (ICSE) (APR), 2022
â¢Intl. Workshop on Genetic Improvement (ICSE) (GI), 2018
â¢Intl. Workshop on Software Fairness (FairWare), 2018
â¢Demonstrations Track, Intl. Symposium on Software Testing and Analysis (ISSTA-Demos), 2017
â¢Intl. Workshop on Genetic Improvement (GECCO) (GECCO-GI), 2015, 2016
â¢Tools Track, Intl. Conference on Software Testing (ICST-Tools), 2015
â¢Tools Track, Intl. Conference on Software Maintenance and Evolution (ICSME/Tools), 2015
â¢Tools Track, Intl. Conference on Software Engineering (ICSE/Tools), 2015
â¢Onward! Essays (Onward!), 2015
â¢New Ideas and Emerging Results, Intl. Conference on Software Engineering (ICSE NIER), 2014
Journal Referee â¢IEEE TSE (2015âpresent)
â¢ACM TOSEM (2014, 2015, 2020âpresent)
â¢Harvard Data Science Review, 2022âpresent
â¢Empirical Software Engineering (EMSE) (2016âpresent)
â¢IEEE Software (2014, 2020)
â¢Journal of Automated Reasoning (JARS) (2016, 2017)
â¢Journal of Software: Evolution and Process (JSEP) (2014, 2015)
â¢Journal of Computing (2015)
â¢Journal of Systems and Software (JSS) (2014) Claire Le Goues Curriculum Vitae July 10, 2023 Page 3 of16 

Publications
Books, Chapters, and Proceedings
[B3]John Grundy, Claire Le Goues, and David Lo, eds.	35th IEEE/ACM International Conference on Automated	
Software Engineering Workshops, ASE Workshops 2020, Melbourne, Australia, September 21-25, 2020	. ACM,	
2020. I S B N: 978-1-4503-8128-4. D O I:10.1145/3417113 .U R L :https://doi.org/10.1145/3417113 .
[B2]
Claire Le Goues and Shin Yoo, eds.	Proceedings of the 6th International Symposium on Search-Based Software	
Engineering, SSBSE 2014, Fortaleza, Brazil, August 26-29, 2014	. Vol. 8636. Lecture Notes in Computer Science.	
Springer, 2014. I S B N: 978-3-319-09939-2. D O I:10.1007/978-3-319-09940-8 .
[B1]
Claire Le Goues, Anh Nguyen-Tuong, Hao Chen, Jack W. Davidson, Stephanie Forrest, Jason Hiser, John C.
Knight, and Matthew Van Gundy. âMoving Target Defenses in the Helix Self-Regenerative Architectureâ. In:
Moving Target Defense II - Application of Game Theory and Adversarial Modeling	. Springer, 2013, pp. 117â149.	
D O I :10.1007/978-1-4614-5416-8_7 .
Refereed Journal Articles
[J19]
Luke Dramko, Jeremy Lacomis, Pengcheng Yin, Edward J. Schwartz, Miltiadis Allamanis, Graham Neubig,
Bodan Vasilescu, and Claire Le Goues. âDIRE and its Data: Neural Decompiled Variable Renamings with respect
to Software Classâ. In: ACM Trans. Softw. Eng. Methodol. 32.2 (2022), 39:1â39:34.D O I:10.1145/3546946 .
[J18]
Juan Alfredo Cruz-Carlon, Mahsa Varshosaz, Claire Le Goues, and Andrzej W Ëasowski. âPatching Locking Bugs
Statically with Crayonsâ. In:	ACM Trans. Softw. Eng. Methodol.	(June 2022). Just Accepted.	I S S N	: 1049-331X.	
D O I :10.1145/3548684 .
[J17]
Afsoon Afzal, Claire Le Goues, and Christopher Steven Timperley. âMithra: Anomaly Detection as an Oracle
for Cyberphysical Systemsâ. In:	IEEE Trans. Software Eng.	48.11 (2022), pp. 4535â4552.	D O I	:10.1109/TSE.
2021.3120680 .
[J16]	
Claire Le Goues, Michael Pradel, Abhik Roychoudhury, and Satish Chandra. âAutomatic Program Repairâ. In:
IEEE Softw. 38.4 (2021), pp. 22â27. D O I:10.1109/MS.2021.3072577 .
[J15]
Christopher Steven Timperley, Lauren Herckis, Claire Le Goues, and Michael Hilton. âUnderstanding and
improving artifact sharing in software engineering researchâ. In:	Empir. Softw. Eng.	26.4 (2021), p. 67.	D O I	:	
10.1007/s10664-021-09973-5 .
[J14]
Cody Kinneer, David Garlan, and Claire Le Goues. âInformation Reuse and Stochastic Search: Managing
Uncertainty in Self-	*Systemsâ. In:	ACM Trans. Auton. Adapt. Syst.	15.1 (2021), 3:1â3:36.	D O I	:10 . 1145 /
3440119 .
[J13]	
Manish Motwani, Mauricio Soto, Yuriy Brun, RenÃ© Just, and Claire Le Goues. âQuality of Automated Program
Repair on Real-World Defectsâ. In:	IEEE Transactions on Software Engineering (TSE)	48.2 (2022), pp. 637â661.	
I S S N : 0098-5589. D O I:10.1109/TSE.2020.2998785 .
[J12]
Afsoon Afzal, Manish Motwani, Kathryn T. Stolee, Yuriy Brun, and Claire Le Goues. âSOSRepair: Expressive
Semantic Search for Real-World Program Repairâ. In:	IEEE Transactions on Software Engineering (TSE)	47.10	
(2021), pp. 2162â2181. I S S N: 0098-5589. D O I:10.1109/TSE.2019.2944914 .
[J11]
Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. âAutomated Program Repairâ. In:	Commun. ACM	
62.12 (Nov. 2019), pp. 56â65. I S S N: 0001-0782. D O I:10.1145/3318162 .
[J10]
Jonathan Aldrich, David Garlan, Christian KÃ¤stner, Claire Le Goues, Anahita Mohseni-Kabir, Ivan Ruchkin,
Selva Samuel, Bradley R. Schmerl, Christopher Steven Timperley, Manuela Veloso, Ian Voysey, Joydeep Biswas,
Arjun Guha, Jarrett Holtz, Javier CÃ¡mara, and Pooyan Jamshidi. âModel-Based Adaptation for Robotics Softwareâ.
In: IEEE Software 36.2 (2019), pp. 83â90. D O I:10.1109/MS.2018.2885058 .
[J9]
Claire Le Goues, Ciera Jaspan, Ipek Ozkaya, Mary Shaw, and Kathryn T. Stolee. âBridging the Gap: From
Research to Practical Adviceâ. In: IEEE Software35.5 (2018), pp. 50â57. D O I:10.1109/MS.2018.3571235 .Claire Le Goues Curriculum Vitae July 10, 2023 Page 4 of16 

[J8]Claire Le Goues, Yuriy Brun, Sven Apel, Emery Berger, Sarfraz Khurshid, and Yannis Smaragdakis. âEffec-
tiveness of Anonymization in Double-Blind Reviewâ. In:	Commun. ACM	61.6 (June 2018), pp. 30â33.	D O I	:	
10.1145/3208157
.
[J7]
Xuan-Bach D. Le, Ferdian Thung, David Lo, and Claire Le Goues. âOverfitting in semantics-based automated
program repairâ. In:	Empirical Software Engineering	23.5 (2018), pp. 3007â3033.	D O I	:10.1007/s10664- 017-
9577-2 .
[J6]	
Vinicius Paulo L. Oliveira, Eduardo F. Souza, Claire Le Goues, and Celso G. Camilo-Junior. âImproved repre-
sentation and genetic operators for linear genetic programming for automated program repairâ. In:	Empirical	
Software Engineering 23.5 (2018), pp. 2980â3006. D O I:10.1007/s10664-017-9562-9 .
[J5]
Claire Le Goues, Neal Holtschulte, Edward K. Smith, Yuriy Brun, Premkumar T. Devanbu, Stephanie Forrest,
and Westley Weimer. âThe ManyBugs and IntroClass Benchmarks for Automated Repair of C Programsâ. In:
IEEE Trans. Software Eng. 41.12 (2015), pp. 1236â1256. D O I:10.1109/TSE.2015.2454513 .
[J4]
Claire Le Goues, Stephanie Forrest, and Westley Weimer. âCurrent challenges in automatic software repairâ. In:
Software Quality Journal 21.3 (2013), pp. 421â443. D O I:10.1007/s11219-013-9208-0 .
[J3]
Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer. âGenProg: A Generic Method for
Automatic Software Repairâ. In:	IEEE Trans. Software Eng.	38.1 (2012), pp. 54â72.	D O I	:10.1109/TSE.2011.
104 .
[J2]	
Claire Le Goues and Westley Weimer. âMeasuring Code Quality to Improve Specification Miningâ. In:	IEEE	
Trans. Software Eng. 38.1 (2012), pp. 175â190. D O I:10.1109/TSE.2011.5 .
[J1]
Westley Weimer, Stephanie Forrest, Claire Le Goues, and ThanhVu Nguyen. âAutomatic program repair with
evolutionary computationâ. In:	Communications of the ACM Research Highlight	53.5 (May 2010), pp. 109â116.	
D O I :10.1145/1735223.1735249 .
Refereed Conference Publications
[C46]
Alex Groce, Rijnard van Tonder, Goutamkumar Tulajappa Kalburgi, and Claire Le Goues. âMaking no-fuss
compiler fuzzing effectiveâ. In: 31st ACM SIGPLAN International Conference on Compiler Construction (CC) .
ACM, 2022, pp. 194â204. D O I:10.1145/3497776.3517765 .
[C45]
Kevin Leach, Christopher Steven Timperley, Kevin Angstadt, Anh Nguyen-Tuong, Jason Hiser, Aaron Paulos,
Partha P. Pal, Patrick Hurley, Carl Thomas, Jack W. Davidson, Stephanie Forrest, Claire Le Goues, and Westley
Weimer. âSTART: A Framework for Trusted and Resilient Autonomous Vehicles (Practical Experience Report)â.
In:	IEEE 33rd International Symposium on Software Reliability Engineering, ISSRE	. IEEE, 2022, pp. 73â84.	
D O I :10.1109/ISSRE55969.2022.00018 .
[C44]
Christopher S. Timperley, Tobias DÃ¼rschmid, Bradley Schmerl, David Garlan, and Claire Le Goues. âROS-
Discover: Statically Detecting Run-Time Architecture Misconfigurations in Robotics Systemsâ. In:	19th IEEE	
International Conference on Software Architecture (ICSA) . ICSA â22. IEEE, 2022, pp. 112â123.
[C43]
Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Claire Le Goues, Graham Neubig, and Bogdan Vasilescu.
âAugmenting Decompiler Output with Learned Variable Names and Typesâ. In:	31st USENIX Security Symposium	.	
2022. D O I:10.48550/arXiv.2108.06363 .
[C42]
Qibin Chen, Jeremy Lacomis, Edward J. Schwartz, Graham Neubig, Bogdan Vasilescu, and Claire Le Goues.
âVarCLR: Variable Semantic Representation Pre-training via Contrastive Learningâ. In:	44th IEEE/ACM Interna-	
tional Conference on Software Engineering (ICSE) . 2022, pp. 2327â2339.D O I:10.1145/3510003.3510162 .
[C41]
Chu-Pan Wong, Priscila Santiesteban, Christian KÃ¤stner, and Claire Le Goues. âVarFix: balancing edit expres-
siveness and search effectiveness in automated program repairâ. In:	Proceedings of the 29th ACM Joint European	
Software Engineering Conference and Symposium on the Foundations of Software Engineering	. Athens, Greece:	
ACM, Aug. 2021, pp. 354â366. D O I:10.1145/3468264.3468600 .
[C40]
Zhen Yu Ding and Claire Le Goues. âAn Empirical Study of OSS-Fuzz Bugsâ. In:	2021 IEEE/ACM 18th	
International Conference on Mining Software Repositories	. MSRâ21. Madrid, Spain: IEEE Computer Society,	
May 2021, pp. 131â142. D O I:10.1109/MSR52588.2021.00026 .Claire Le Goues Curriculum Vitae July 10, 2023 Page 5 of16 

[C39]Ansong Ni, Daniel Ramos, Aidan Z. H. Yang, InÃªs Lynce, Vasco M. Manquinho, Ruben Martins, and Claire Le
Goues. âSOAR: A Synthesis Approach for Data Science API Refactoringâ. In:	43rd IEEE/ACM International	
Conference on Software Engineering (ICSE)	. IEEE, 2021, pp. 112â124.	D O I	:10.1109/ICSE43902.2021.00023	.	
[C38]	Afsoon Afzal, Deborah S. Katz, Claire Le Goues, and Christopher Steven Timperley. âSimulation for Robotics
Test Automation: Developer Perspectivesâ. In:	14th IEEE Conference on Software Testing, Verification and	
Validation (ICST)	. Porto de Galinhas, Brazil: IEEE, Apr. 2021, pp. 263â274.	D O I	:10.1109/ICST49551.2021.
00036 .
[C37]	
Zack Coker, Joshua Sunshine, and Claire Le Goues. âFrameFix: Automatically Repairing Statically-Detected
Directive Violations in Framework Applicationsâ. In:	28th IEEE International Conference on Software Anal-	
ysis, Evolution and Reengineering (SANER)	. Honolulu, HI: IEEE, Mar. 2021, pp. 201â212.	D O I	:10 . 1109 /
SANER50967.2021.00027 .
[C36]	
Cody Kinneer, Rijnard van Tonder, David Garlan, and Claire Le Goues. âBuilding Reusable Repertoires for
Stochastic Self-* Plannersâ. In:	IEEE International Conference on Autonomic Computing and Self-Organizing	
Systems (ACSOS)	. Washington, DC: IEEE, Aug. 2020, pp. 222â231.	D O I	:10.1109/ACSOS49614.2020.00045	.	
[C35]	Sophia Kolak, Afsoon Afzal, Claire Le Goues, Michael Hilton, and Christopher Steven Timperley. âIt Takes
a Village to Build a Robot: An Empirical Study of The ROS Ecosystemâ. In:	IEEE International Conference	
on Software Maintenance and Evolution (ICSME)	. Adelaide, Australia: IEEE, Sept. 2020, pp. 430â440.	D O I	:	
10.1109/ICSME46990.2020.00048 .
[C34]
Thomas Durieux, Claire Le Goues, Michael Hilton, and Rui Abreu. âEmpirical Study of Restarted and Flaky
Builds on Travis CIâ. In:	IEEE/ACM 17th International Conference on Mining Software Repositories (MSR)	.	
Seoul, Republic of Korea: ACM, June 2020, pp. 254â264. D O I:10.1145/3379597.3387460 .
[C33]
Deborah S. Katz, Casidhe Hutchison, Milda Zizyte, and Claire Le Goues. âDetecting Execution Anomalies
as an Oracle for Autonomy Software Robustnessâ. In:	Proceedings of the 2020 International Conference on	
Robotics and Automation (ICRA)	. Paris, France: IEEE, May 2020, pp. 9366â9373.	D O I	:10.1109/ICRA40945.
2020.9197060 .
[C32]	
Rijnard van Tonder and Claire Le Goues. âTailoring Programs for Static Analysis via Program Transformationâ.
In:	Proceedings of the 42nd IEEE/ACM International Conference on Software Engineering (ICSE)	. ACM, 2020.	
D O I :10.1145/3377811.3380343 .
[C31]
Afsoon Afzal, Claire Le Goues, Michael Hilton, and Christopher Steven Timperley. âA Study on Challenges
of Testing Robotic Systemsâ. In:	2020 IEEE International Conference on Software Testing, Verification and	
Validation (ICST)	. IEEE, 2020, pp. 96â107.	D O I	:10.1109/ICST46399.2020.00020	.U R L	:https://doi.org/
10.1109/ICST46399.2020.00020 .
[C30]	
Cody Kinneer, Ryan Wagner, Fei Fang, Claire Le Goues, and David Garlan. âModeling Observability in
Adaptive Systems to Defend against Advanced Persistent Threatsâ. In:	Proceedings of the 17th ACM-IEEE	
International Conference on Formal Methods and Models for System Design (MEMOCODE)	. New York, NY,	
USA: Association for Computing Machinery, 2019. I S B N: 9781450369978. D O I:10.1145/3359986.3361208 .
[C29]
Zack Coker, David G. Widder, Claire Le Goues, Christopher Bogart, and Joshua Sunshine. âA Qualitative Study
on Framework Debuggingâ. In:	2019 IEEE International Conference on Software Maintenance and Evolution	
(ICSME) . Cleveland, OH, Sept. 2019, pp. 568â579. D O I:10.1109/ICSME.2019.00091 .
[C28]
Jeremy Lacomis, Pengcheng Yin, Edward Schwarts, Miltiadis Allamanis, Claire Le Goues, Graham Neubig,
and Bogdan Vasilescu. âDIRE: A Neural Approach to Decompiled Identifier Namingâ. In:	Proceedings of the	
34th IEEE/ACM International Conference on Automated Software Engineering (ASE) . 2019, pp. 628â639.D O I:
10.1109/ASE.2019.00064 .
[C27]
Rijnard van Tonder and Claire Le Goues. âLightweight Multi-language Syntax Transformation with Parser
Parser Combinatorsâ. In:	Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design	
and Implementation (PLDI) . 2019, pp. 363â378.D O I:10.1145/3314221.3314589 .
[C26]
Rijnard van Tonder, John Kotheimer, and Claire Le Goues. âSemantic crash bucketingâ. In:	Proceedings of the	
33rd ACM/IEEE International Conference on Automated Software Engineering (ASE)	. Montpellier, France, 2018,	
pp. 612â622. D O I:10.1145/3238147.3238200 .Claire Le Goues Curriculum Vitae July 10, 2023 Page 6 of16 

[C25]Rijnard van Tonder and Claire Le Goues. âCross-Architecture Lifter Synthesisâ. In:	Proceedings of the 16th	
International Conference on Software Engineering and Formal Methods (SEFM) Held as part of STAF 2018	.	
Vol. 10886. Lecture Notes in Computer Science. Springer, 2018, pp. 155â170.	D O I	:10 . 1007 / 978 - 3 - 319 -
92970-5_10 .
[C24]	
Eduardo Faria de Souza, Claire Le Goues, and Celso Goncalves Camilo-Junior. âA Novel Fitness Function for
Automated Program Repair Based on Source Code Checkpointsâ. In:	Proceedings of the Genetic and Evolutionary	
Computation Conference (GECCO)	. Kyoto, Japan: ACM, July 2018, pp. 1443â1450.	D O I	:10.1145/3205455.
3205566 .
[C23]	
Alan Jaffe, Jeremy Lacomis, Edward Schwartz, Claire Le Goues, and Bogdan Vasilescu. âMeaningful Variable
Names for Decompiled Code: A Machine Translation Approachâ. In:	Proceedings of the 26th IEEE International	
Conference on Program Comprehension (ICPC)	. Gothenburg, Sweden: ACM, May 2018, pp. 20â30.	D O I	:	
10.1145/3196321.3196330 .
[C22]
Rijnard van Tonder and Claire Le Goues. âStatic Automated Program Repair for Heap Propertiesâ. In:	Proceed-	
ings of the 40th IEEE/ACM International Conference on Software Engineering (ICSE)	. Gothenburg, Sweden:	
ACM, May 2018, pp. 151â162. D O I:10.1145/3180155.3180250 .
[C21]
Casidhe Hutchison, Milda Zizyte, Patrick E. Lanigan, David Guttendorf, Michael Wagner, Claire Le Goues,
and Philip Koopman. âRobustness Testing of Autonomy Softwareâ. In:	Proceedings of the 40th International	
Conference on Software Engineering: Software Engineering in Practice (ICSE SEIP)	. Gothenburg, Sweden:	
ACM, May 2018, pp. 276â285. D O I:10.1145/3183519.3183534 .
[C20]
Cody Kinneer, Zack Coker, Jiacheng Wang, David Garlan, and Claire Le Goues. âManaging Uncertainty in Self-
Adaptive Systems with Plan Reuse and Stochastic Searchâ. In:	Proceedings of the 12th IEEE/ACM International	
Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)	. Gothenburg, Sweden:	
ACM, May 2018, pp. 40â50. D O I:10.1145/3194133.3194145 .
[C19]
Mauricio Soto and Claire Le Goues. âUsing a probabilistic model to predict bug fixesâ. In:	Proceedings of the	
25th IEEE International Conference on Software Analysis, Evolution, and Reengineering (SANER)	. Campobasso,	
Italy, Mar. 2018, pp. 221â231. D O I:10.1109/SANER.2018.8330211 .
[C18]
Christopher Steven Timperley, Afsoon Afzal, Deborah Katz, Jam Marcos Hernandez, and Claire Le Goues.
âCrashing simulated planes is cheap: Can simulation detect robotics bugs early?â In:	Proceedings of the 11th IEEE	
Conference on Software Testing, Validation and Verification (ICST)	. VÃ¤sterÃ¥s, Sweden, Apr. 2018, pp. 331â342.	
D O I :10.1109/ICST.2018.00040 .
[C17]
Zack Coker, Kostadin Damevski, Claire Le Goues, Nicholas A. Kraft, David Shepherd, and Lori Pollock.
âBehavior Metrics for Prioritizing Investigations of Exceptionsâ. In:	Proceedings of the 2017 IEEE International	
Conference on Software Maintenance and Evolution (ICSME, Industry Track)	. Shanghai, China: IEEE Computer	
Society, Sept. 2017, pp. 554â563. D O I:10.1109/ICSME.2017.62 .
[C16]
Christopher Steven Timperley, Susan Stepney, and Claire Le Goues. âAn investigation into the use of mutation
analysis for automated program repairâ. In:	Proceedings of the 9th International Symposium on Search Based	
Software Engineering (SSBSE)	. Vol. 10452. Lecture Notes in Computer Science. Paderborn, Germany: Springer,	
Sept. 2017, pp. 99â114. D O I:10.1007/978-3-319-66299-2_7 .
[C15]
Xuan-Bach D. Le, Duc Hiep Chu, David Lo, Claire Le Goues, and Willem Visser. âS3: Syntax- and Semantic-
Guided Repair Synthesis via Programming by Examplesâ. In:	Proceedings of the 11th Joint Meeting on Founda-	
tions of Software Engineering (ESEC/FSE) . ACM, 2017, pp. 593â604.D O I:10.1145/3106237.3106309 .
[C14]
Cyrus Omar, Ian Voysey, Michael Hilton, Joshua Sunshine, Claire Le Goues, Jonathan Aldrich, and Matthew
Hammer. âToward Semantic Foundations for Program Editorsâ. In:	Proceedings of the 2nd Summit on Advances	
in Programming Languages (SNAPL)	. Asilomar, CA, USA: Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik,	
May 2017, 11:1â11:12. D O I:10.4230/LIPIcs.SNAPL.2017.11 .
[C13]
Vinicius Paulo L. Oliveira, Eduardo F. D. Souza, Claire Le Goues, and Celso G. Camilo-Junior. âImproved
Crossover Operators for Genetic Programming for Program Repairâ. In:	Proceedings of the 8th International	
Symposium on Search Based Software Engineering (SSBSE)	. Vol. 9962. Lecture Notes in Computer Science.	
Raleigh, NC, USA, Oct. 2016, pp. 112â127. D O I:10.1007/978-3-319-47106-8_8 .Claire Le Goues Curriculum Vitae July 10, 2023 Page 7 of16 

[C12]Tien-Duy B. Le, David Lo, Claire Le Goues, and Lars Grunske. âA Learning-to-rank Based Fault Localization
Approach Using Likely Invariantsâ. In:	Proceedings of the 25th International Symposium on Software Testing and	
Analysis (ISSTA)
. SaarbrÃ¼cken, Germany: ACM, July 2016, pp. 177â188. D O I:10.1145/2931037.2931049 .
[C11]
Yuan Tian, Dinusha Wijedasa, David Lo, and Claire Le Goues. âLearning to rank for bug report assignee
recommendationâ. In:	Proceedings of the 24th IEEE International Conference on Program Comprehension	
(ICPC)	. Austin, TX, USA: IEEE Computer Society, May 2016, pp. 1â10.	D O I	:10.1109/ICPC.2016.7503715	.	
[C10]	Xuan-Bach D. Le, David Lo, and Claire Le Goues. âHistory Driven Program Repairâ. In:	Proceedings of the	
23rd IEEE International Conference on Software Analysis, Evolution, and Reengineering (SANER)	. Vol. 1. Osaka,	
Japan: IEEE Computer Society, Mar. 2016, pp. 213â224. D O I:10.1109/SANER.2016.76 .
[C9]
Zack Coker, Michael Maass, Tianyuan Ding, Claire Le Goues, and Joshua Sunshine. âEvaluating the Flexibility
of the Java Sandboxâ. In:	Proceedings of the 31st Annual Computer Security Applications Conference (ACSAC)	.	
Los Angeles, CA, USA: ACM, Dec. 2015, pp. 1â10. D O I:10.1145/2818000.2818003 .
[C8]
Yalin Ke, Kathryn T. Stolee, Claire Le Goues, and Yuriy Brun. âRepairing Programs with Semantic Code Searchâ.
In:	Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)	.	
Lincoln, NE, USA: IEEE Computer Society, Nov. 2015, pp. 295â306. D O I:10.1109/ASE.2015.60 .
[C7]
Edward K. Smith, Earl Barr, Claire Le Goues, and Yuriy Brun. âIs the Cure Worse than the Disease? Overfitting
in Automated Program Repairâ. In:	Proceedings of the 10th Joint Meeting of the European Software Engineering	
Conference and ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/FSE)	. Bergamo,	
Italy: ACM, Sept. 2015, pp. 532â543. D O I:10.1145/2786805.2786825 .
[C6]
Claire Le Goues, Stephanie Forrest, and Westley Weimer. âRepresentations and Operators for Improving
Evolutionary Software Repairâ. In:	Proceedings of the Genetic and Evolutionary Computation Conference	
(GECCO) . Philadelphia, PA, USA: ACM, July 2012, pp. 959â966. D O I:10.1145/2330163.2330296 .
[C5]
Claire Le Goues, Michael Dewey-Vogt, Stephanie Forrest, and Westley Weimer. âA Systematic Study of
Automated Program Repair: Fixing 55 out of 105 bugs for $8 Eachâ. In:	Proceedings of the 34th International	
Conference on Software Engineering (ICSE)	. Zurich, Switzerland: IEEE Computer Society, June 2012, pp. 3â13.	
D O I :10.1109/ICSE.2012.6227211 .
[C4]
Ethan Fast, Claire Le Goues, Stephanie Forrest, and Westley Weimer. âDesigning better fitness functions for
automated program repairâ. In:	Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)	.	
Portland, OR, USA: ACM, July 2010, pp. 965â972. D O I:10.1145/1830483.1830654 .
[C3]
Stephanie Forrest, Westley Weimer, ThanhVu Nguyen, and Claire Le Goues. âA genetic programming approach to
automated software repairâ. In:	Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)	.	
Montreal, QuÃ©bec, Canada: ACM, July 2009, pp. 947â954. D O I:10.1145/1569901.1570031 .
[C2]
Westley Weimer, ThanhVu Nguyen, Claire Le Goues, and Stephanie Forrest. âAutomatically Finding Patches
Using Genetic Programmingâ. In:	Proceedings of the 31st International Conference on Software Engineering	
(ICSE) . Vancouver, Canada: IEEE, May 2009, pp. 364â374. D O I:10.1109/ICSE.2009.5070536 .
[C1]
Claire Le Goues and Westley Weimer. âSpecification Mining with Few False Positives.â In:	Proceedings of the	
15th Annual Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS), Held as
Part of the Joint European Conferences on Theory and Practice of Software (ETAPS)	. Vol. 5505. Lecture Notes	
in Computer Science. York, UK: Springer, Mar. 2009, pp. 292â306. D O I:10.1007/978-3-642-00768-2_26 .
Refereed Short Publications
[S17]
Alex Groce, Kush Jain, Rijnard van Tonder, Goutamkumar Tulajappa Kalburgi, and Claire Le Goues. âLooking
for Lacunae in Bitcoin Coreâs Fuzzing Effortsâ. In:	44th IEEE/ACM International Conference on Software	
Engineering: Software Engineering in Practice, ICSE (SEIP)	. IEEE, 2022, pp. 185â186.	D O I	:10.1109/ICSE-
SEIP55303.2022.9794086 .
[S16]	
Milda Zizyte, Casidhe Hutchison, Raewyn Duvall, Claire Le Goues, and Philip Koopman. âThe Importance of
Safety Invariants in Robustness Testing Autonomy Systemsâ. In:	Proceedings of the 51st Annual IEEE/IFIP	
International Conference on Dependable Systems and Networks - Supplemental Volume	. Taipei, Taiwan: IEEE,	
June 2021, pp. 41â44. D O I:10.1109/DSN-S52858.2021.00028 .Claire Le Goues Curriculum Vitae July 10, 2023 Page 8 of16 

[S15]Deborah Katz, Milda Zizyte, Casidhe Hutchison, David Guttendorf, Patrick Lanigan, Eric Sample, Philip
Koopman, Michael Wagner, and Claire Le Goues. âRobustness Inside Out Testingâ. In:	Proceedings of the	
50th IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2020) â Industry Track	.	
DSN-Industryâ20. Valencia, Spain: IEEE, June 2020, pp. 1â4.	D O I	:10.1109/DSN-S50200.2020.00013	.U R L	:	
https://doi.org/10.1109/DSN-S50200.2020.00013
.
[S14]
Rijnard van Tonder, Asher Trockman, and Claire Le Goues. âA Panel Data Set of Cryptocurrency Development
Activity on GitHubâ. In:	2019 IEEE/ACM 16th International Conference on Mining Software Repositories	
(MSR)	. Montreal, Canada: IEEE/ACM, May 2019, pp. 186â190.	D O I	:10 . 1109 / MSR . 2019 . 00037	.U R L	:	
https://doi.org/10.1109/MSR.2019.00037 .
[S13]
Christopher Steven Timperley, Susan Stepney, and Claire Le Goues. âPoster: BugZoo: A Platform for Studying
Software Bugsâ. In:	Proceedings of the 40th International Conference on Software Engineering: Companion	
Proceedings (ICSE Poster)	. Gothenburg, Sweden: ACM, May 2018, pp. 446â447.	D O I	:10 . 1145 / 3183440 .
3195050 .
[S12]	
Mauricio Soto and Claire Le Goues. âCommon Statement Kind Changes to Inform Automatic Program Repairâ.
In:	Proceedings of the 15th International Conference on Mining Software Repositories (MSR Challenge)	.	
Gothenburg, Sweden, May 2018, pp. 102â105. D O I:10.1145/3196398.3196472 .
[S11]
Afsoon Afzal and Claire Le Goues. âA Study on the Use of IDE Features for Debuggingâ. In:	Proceedings of the	
15th International Conference on Mining Software Repositories (MSR Challenge)	. Gothenburg, Sweden, May	
2018, pp. 114â117. D O I:10.1145/3196398.3196468 .
[S10]
Claire Le Goues, Yuriy Brun, Stephanie Forrest, and Westley Weimer. âClarifications on the Construction and
Use of the ManyBugs Benchmark (Comment Paper)â. In:	IEEE Trans. Software Eng.	43.11 (2017), pp. 1089â	
1090. D O I:10.1109/TSE.2017.2755651 .
[S9]
Xuan-Bach D. Le, Duc Hiep Chu, David Lo, Claire Le Goues, and Willem Visser. âJFix: Semantics-based repair
of Java programs via Symbolic PathFinderâ. In:	Proceedings of the 26th ACM SIGSOFT International Symposium	
on Software Testing and Analysis (ISSTA Tools)	. Santa Barbara, CA, USA: ACM, July 2017, pp. 376â379.	D O I	:	
10.1145/3092703.3098225 .
[S8]
Mauricio Soto, Zack Coker, and Claire Le Goues. âAnalyzing the Impact of Social Attributes on Commit
Integration Successâ. In:	Proceedings of the 14th International Conference on Mining Software Repositories	
(MSR Challenge)	. Buenos Aires, Argentina: IEEE Computer Society, May 2017, pp. 483â486.	D O I	:10.1109/
MSR.2017.34 .
[S7]	
Xuan-Bach D. Le, David Lo, and Claire Le Goues. âEmpirical Study on Synthesis Engines for Semantics-
based Program Repairâ. In:	Proceedings of the 32nd IEEE International Conference on Software Maintenance	
and Evolution (ICSME ERA)	. Raleigh, NC, USA: IEEE Computer Society, Oct. 2016, pp. 423â427.	D O I	:	
10.1109/ICSME.2016.68 .
[S6]
Xuan-Bach D. Le, Quang Loc Le, David Lo, and Claire Le Goues. âEnhancing Automated Program Repair with
Deductive Verificationâ. In:	Proceedings of the 32nd IEEE International Conference on Software Maintenance	
and Evolution (ICSME ERA)	. Raleigh, NC, USA: IEEE Computer Society, Oct. 2016, pp. 428â432.	D O I	:	
10.1109/ICSME.2016.66 .
[S5]
Rijnard van Tonder and Claire Le Goues. âDefending against the attack of the micro-clonesâ. In:	Proceedings of	
the 24th IEEE International Conference on Program Comprehension (ICPC Short)	. Austin, TX, USA: IEEE	
Computer Society, May 2016, pp. 1â4. D O I:10.1109/ICPC.2016.7503736 .
[S4]
Mary Beth Kery, Claire Le Goues, and Brad A. Myers. âExamining Programmer Practices for Locally Handling
Exceptionsâ. In:	Proceedings of the 13th International Conference on Mining Software Repositories (MSR	
Challenge) . Austin, TX, USA: ACM, May 2016, pp. 484â487. D O I:10.1145/2901739.2903497 .
[S3]
Mauricio Soto, Ferdian Thung, Chu-Pan Wong, Claire Le Goues, and David Lo. âA Deeper Look into Bug
Fixes: Patterns, Replacements, Deletions, and Additionsâ. In:	Proceedings of the 13th International Conference	
on Mining Software Repositories (MSR Challenge)	. Austin, TX, USA: ACM, May 2016, pp. 512â515.	D O I	:	
10.1145/2901739.2903495 .Claire Le Goues Curriculum Vitae July 10, 2023 Page 9 of16 

[S2]Zack Coker, David Garlan, and Claire Le Goues. âSASS: Self-Adaptation Using Stochastic Searchâ. In:	Proceed-	
ings of the 10th IEEE/ACM International Symposium on Software Engineering for Adaptive and Self-Managing
Systems (SEAMS)	. Florence, Italy: IEEE Computer Society, May 2015, pp. 168â174.	D O I	:10.1109/SEAMS.
2015.16 .
[S1]	
Claire Le Goues, K. Rustan M. Leino, and Michal Moskal. âThe Boogie Verification Debugger (Tool Paper)â.
In:	Proceedings of the 9th International Conference on Software Engineering and Formal Methods (SEFM)	.	
Vol. 7041. Lecture Notes in Computer Science. Montevideo, Uruguay: Springer, Nov. 2011, pp. 407â414.	D O I	:	
10.1007/978-3-642-24690-6_28 .
Refereed Workshop Publications
[W7]
Alexander G. Shypula, Pengcheng Yin, Jeremy Lacomis, Claire Le Goues, Edward J. Schwartz, and Graham
Neubig. âLearning to Superoptimize Real-World Programsâ. In:	Deep Learning for Code Workshop	. DL4C â22.	
2022.
[W6]
Zhen Yu Ding, Yiwei Lyu, Christopher Timperley, and Claire Le Goues. âLeveraging Program Invariants to
Promote Population Diversity in Search-Based Automatic Program Repairâ. In:	2019 IEEE/ACM International	
Workshop on Genetic Improvement (GI) . May 2019, pp. 2â9.D O I:10.1109/GI.2019.00011 .
[W5]
Rijnard van Tonder and Claire Le Goues. âTowards s/engineer/bot: Principles for Program Repair Botsâ. In:	2019	
IEEE/ACM 1st International Workshop on Bots in Software Engineering (BotSE)	. May 2019, pp. 43â47.	D O I	:	
10.1109/BotSE.2019.00019 .
[W4]
Afsoon Afzal, Jeremy Lacomis, Claire Le Goues, and Christopher S. Timperley. âA Turing Test for Genetic
Improvement (Position Paper)â. In:	Proceedings of the 4th International Genetic Improvement Workshop	. GI â18.	
Gothenburg, Sweden: ACM, 2018, pp. 17â18. D O I:10.1145/3194810.3194817 .
[W3]
Westley Weimer, Stephanie Forrest, Miryung Kim, Claire Le Goues, and Patrick Hurley. âTrusted Software
Repair for System Resiliencyâ. In:	Proceedings of the 46th Annual IEEE/IFIP International Conference on	
Dependable Systems and Networks Workshops (DSN Workshops)	. Toulouse, France: IEEE Computer Society,	
July 2016, pp. 238â241. D O I:10.1109/DSN-W.2016.64 .
[W2]
Claire Le Goues, Stephanie Forrest, and Westley Weimer. âThe case for software evolutionâ. In:	Proceedings of	
the Workshop on Future of Software Engineering Research (FoSER), at the 18th ACM SIGSOFT International
Symposium on Foundations of Software Engineering	. Santa Fe, NM, USA: ACM, Nov. 2010, pp. 205â210.	D O I	:	
10.1145/1882362.1882406 .
[W1]
ThanhVu Nguyen, Westley Weimer, Claire Le Goues, and Stephanie Forrest. âUsing Execution Paths to
Evolve Software Patchesâ. In:	Second International Conference on Software Testing Verification and Vali-	
dation, Workshops Proceedings	. Denver, CO, USA: IEEE Computer Society, Apr. 2009, pp. 152â153.	D O I	:	
10.1109/ICSTW.2009.35 .
Non-Refereed Publications [N5]
Deborah S. Katz, Christopher Steven Timperley, and Claire Le Goues. âUsing Dynamic Binary Instrumentation
to Detect Failures in Robotics Softwareâ. In:	CoRR	abs/2201.12464 (2022). arXiv:	2201.12464	.U R L	:https:
//arxiv.org/abs/2201.12464 .
[N4]	
Justyna Petke, Claire Le Goues, Stephanie Forrest, and William B. Langdon. âGenetic Improvement of Software
(Dagstuhl Seminar 18052)â. In:	Dagstuhl Reports	8.1 (2018). Ed. by Justyna Petke, Claire Le Goues, Stephanie	
Forrest, and William B. Langdon, pp. 158â182. I S S N: 2192-5283. D O I:10.4230/DagRep.8.1.158 .
[N3]
Xuan-Bach D. Le, Ferdian Thung, David Lo, and Claire Le Goues. âOverfitting in semantics-based automated
program repairâ. In:	Proceedings of the 40th International Conference on Software Engineering (Journal First)	.	
ICSE (Journal First) 2018. Gothenburg, Sweden: ACM, May 2018, p. 163. D O I:10.1145/3180155.3182536 .
[N2]
Claire Le Goues and Shin Yoo. âGuest editorial for special section on research in search-based software
engineeringâ. In:	Empirical Software Engineering	22.2 (2017), pp. 849â851.	D O I	:10.1007/s10664-017-9504-
6 . Claire Le Goues Curriculum Vitae July 10, 2023 Page 10 of16 

[N1]Sunghun Kim, Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. âAutomated Program Repair
(Dagstuhl Seminar 17022)â. In:	Dagstuhl Reports	7.1 (2017). Ed. by Sunghun Kim, Claire Le Goues, Michael	
Pradel, and Abhik Roychoudhury, pp. 19â31.
I S S N: 2192-5283. D O I:10.4230/DagRep.7.1.19 .
Invited Tutorials [T1]
Stephanie Forrest and Claire Le Goues. âEvolutionary software repair (Invited Tutorial)â. In:	Genetic and	
Evolutionary Computation Conference (GECCO): Companion Material Proceedings	. Philadelpha, PA, USA:	
ACM, July 2012, pp. 1345â1348. D O I:10.1145/2330784.2330943 .
Formal and Invited Presentations Correctness Matters: Program repair at the intersection of heuristic and semantic analyses
â SnT Center, University of Luxembourg
Luxembourg City, Luxembourg , January 2023
Let me fix that for you: An Overview of Automated Program Repair âSummer School, 30th ACM International Symposium on Software Testing and Analysis (ISSTA, co-located with ECOOP)
Virtual, Covid-19 , July 2021
Challenges and Opportunities in Automatic Program Repair â Volkswagen AG
Virtual, Covid-19 , November 2020
Do What I Mean, Not What I Say: An Introduction to Automatic Program Repair for Early-Career Researchers âDoctoral Symposium/Summer School, 29th ACM International Symposium on Software Testing and Analysis (ISSTA)
Virtual, Covid-19 , July 2020
It Does What You Say, Not What You Mean: Lessons from 10 Years of Program Repair âPlenary Session, N-10 Award, 41st ACM/IEEE International Conference on Software Engineering (ICSE)
Montreal, Canada , May 2019
âUniversity of Virginia, Charlottesville, VA, Sep 2019
Fault Localization and Program Repair âLorentz Center Workshop, In-Vivo Analytics for Big Software Quality
Leiden, Netherlands , Sept 2018
Fixed That For You: Scalable Semantic Code Search for High-Quality Program Repair âWilliams College, Williamstown, MA, Sept 2018
Evolving Software Quality (keynote) â4th Intl. Genetic Improvement Workshop (GI), co-located with ICSE 2018
Gothenburg, Sweden , June 2018
From PhD Candidate to Early-Career Researcher: Reflections on Science and Other Useful Stuff (keynote) âDoctoral Symposium, 32nd IEEE/ACM Symposium on Automated Software Engineering (ASE)
Urbana Champaign, IL, USA , Nov 2017
Advances in automated software repair âFaceTAV 2017 Symposium, Facebook, London, UK, Nov 2017
Video available: https://facetavlondon2017.splashthat.com/
FTFY: Research Advances in Automatic Bug Repair (keynote) âOâReilly Velocity NY, NYC, NY, Sep 2017
Research Advances in Automatic Program Repair âAmazon, Seattle, WA , Sep 2017
Scalable Semantic Code Search for High-Quality Program Repair âUniversity of Washington, Seattle, WA, Jan 2017
âMicrosoft Research, Redmond, WA, Jan 2017
âDagstuhl Seminar 17022, Automated Program Repair, Wadern, Germany, Jan 2017
Overview on Search-based Program Patching âDagstuhl Seminar 17022, Automated Program Repair, Wadern, Germany, Jan 2017Claire Le Goues Curriculum Vitae July 10, 2023 Page 11 of16 

Automatic patch generation (keynote)
âPWLConf, co-located with StrangeLoop 2016
St. Louis, MO , Sep 2016
Video available: https://www.youtube.com/watch?v=sRkfMe0_5cA
Passing tests is easy: when full coverage isnât enough (keynote) â9th Intl. Workshop on Search Based Software Testing (SBST), co-located with ICSE 2016
Austin, TX , May 2016
Automatic Program Repair Using Genetic Programming âUniversity of Massachusetts, Amherst, Amherst, MA, Jan 2014
âVirginia Polytechnic Institute and State University (Virginia Tech), Blacksburg, VA, Sep 2012
Bloat vs. overfitting in test-driven GP for program repair â28th Crest Open Workshop, Genetic Programming for Software Engineering
University College London, London, UK, Oct 2013
Question your assumptions: the bleeding edge of search-based program repair âLille 1 University/INRIA Lille Norde-Europe, Lille, France, Oct 2013
Specification Mining with few false positives âKingâs College London, Nov 2009
Invited Panels Will AI Render Programming Obsolete?
âCEO Innovation Network
New York, New York , April 2023
New Faculty Symposium â40th Intl. Conference on Software Engineering (ICSE)
Gothenburg, Sweden , May 2018
Thirty Years of Automated Software Engineering (ASE) â30th IEEE/ACM Intl. Conference on Automated Software Engineering (ASE)
Lincoln, NE, USA , Nov 2015
Moderated by Lars Grunske
Educational Contributions Leadership
CMU SCS Faculty Fellowship Recognizing Diversity and Inclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2020â2026
Co-Director, REUSE@CMU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2016âpresent
Director, Undergraduate Minor in Software Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2014â2018
Member, SCS Undergraduate Review Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2016â2019REUSE@CMU.	I am co-director and co-founder of REUSE@CMU,	reuse.cs.cmu.edu	, which provides summer	
undergraduate research opportunities in Interdisciplinary Software Engineering. REUSE trains students in all elements
of research. We specifically seek out students early in their undergraduate educations and those who do not have access
to traditional research at their home institutions.	
Since 2016, approximately 55% of the 198 students who have passed through the program identify as non-male,	
and approximately 20% as members of other under-represented minority groups in computing (groups overlap). Of
the students who have graduated, more than 60% are pursuing research as a career, either in a PhD program (the vast
majority) or other lab or research environment. (One way to understand these numbers is to consult the CRA Taulbee
survey, which as of 2018 indicated that women made up 22.3% of the CS PhD population, and members of other URM
groups, about 4%.)
Program analysis.	With Jonathan Aldrich and Rohan Padhye, I have designed an upper-division elective course in	
Program Analysis and written an associated set of course notes that we use as a textbook. The most recent version of
the course and its book are available at https://cmu-program-analysis.github.io/ .Claire Le Goues Curriculum Vitae July 10, 2023 Page 12 of16 

Teaching
Instructor of Record Carnegie Mellon University
I also co-teach/have co-taught 17-808, Software Engineering Research (Ph.D. level), Fall 2013âpresent.
17-355 Program Analysis (cross-listed, 17-655, 17-819) (undergraduate, graduate) Spring 2023
17-214/514 Principles of Software Construction (undergraduate, masters) Fall 2022
17-214/514 Principles of Software Construction (undergraduate, masters) Spring 2022
17-313 Foundations of Software Engineering (undergraduate) Fall 2020
17-355 Program Analysis (cross-listed, 17-655, 17-819) (undergraduate, graduate) Spring 2020
17-313 Foundations of Software Engineering (undergraduate) Fall 2019
17-355 Program Analysis (cross-listed, 17-655, 17-819) (undergraduate, graduate) Spring 2018
17-356 Software Engineering for Startups (undergraduate) Spring 2018
15-313 Foundations of Software Engineering (undergraduate) Fall 2017
17-654 Analysis of Software Systems (Masters) Spring 2017
15-313 Foundations of Software Engineering (undergraduate) Fall 2016
15-819O Special Topics in Programming Languages: Program Analysis (Ph.D.) Spring 2016
15-313 Foundations of Software Engineering (undergraduate) Fall 2015
17-654 Analysis of Software Systems (Masters) Spring 2015
15-313 Foundations of Software Engineering (undergraduate) Fall 2014
17-654 Analysis of Software Systems (Masters) Spring 2014
University of Virginia
CS4444/6444 High Performance and Parallel Computation (undergraduate/graduate) Spring 2013
Supervision Listings include first position post-PhD/PostDoc.
Postdoctoral Advisor â¢Milda Zizyte (2020â2021), Lecturer in Computer Science at Brown University
â¢Christopher Timperley (2016â2017), Systems Scientist at Carnegie Mellon University
PhD Advisor â¢Nikitha Rao, (2023âpresent), joint with Vincent Hellendoorn
â¢Kush Jain (2021âpresent)
â¢Tobias DÃ¼rschmid (2022âpresent), joint with David Garlan
â¢Sophia Kolak (2021âpresent), joint with Ruben Martins
â¢Aidan Yang (2021âpresent), joint with Ruben Martins
â¢Luke Dramko (2020âpresent), joint with Bogdan Vasilescu
â¢Trenton Tabor (2020âpresent)
â¢Daniel Ramos (2020âpresent), joint with Ruben Martins and Vasco Manquinho
â¢Jeremy Lacomis, PhD in Software Engineering, 2023. âAutomatically Annotating Decompiled Code with
Meaningful Names and Typesâ. Joint with Bogdan Vasilescu. Mark Stehlik Postdoctoral Teaching Fellow
â¢
Afsoon Afzal, PhD in Software Engineering, 2021. âAutomated Testing of Robotic and Cyberphysical Systemsâ.
Software Engineer, Nuro
â¢
Cody Kinneer. PhD in Software Engineering, 2021. âSearch-based Plan Reuse in Self-* Systemsâ. Joint with
David Garlan. Trader, IMC Trading
â¢Mauricio Soto. PhD in Software Engineering, 2021. âImproving Patch Quality by Enhancing Key Components of Automatic Program Repairâ. Research Scientist, ABB/Hitachi
â¢
Deborah Katz. PhD in Computer Science, 2020. âIdentification of Software Failures in Complex Systems Using
Low-Level Execution Dataâ. Research Engineer, Seegrid Corp
â¢
Zack Coker. PhD in Computer Science, 2020. âAutomatic repair of framework applicationsâ. Research Scientist,
Sandia National Labs Claire Le Goues Curriculum Vitae July 10, 2023 Page 13 of16 

â¢Rijnard van Tonder. PhD in Software Engineering, 2019. âAutomated Program Transformation for Improving
Software Qualityâ. Research Scientist, Sourcegraph
PhD Thesis Committees â¢CMU, School of Computer Science âChristopher Meiklejohn (advisor Heather Miller)
â Paulo Casanova, completed 2023 (advisor David Garlan)
â Miguel Velez, completed 2021 (advisor Christian KÃ¤stner)
â Chu-Pan Wong, completed 2021 (advisor Christian KÃ¤stner)
â Gabriel Moreno, completed 2017 (advisor David Garlan)
â Jason Tsay, completed 2017 (advisors Jim Herbsleb and Laura Dabbish)
â¢CMU, College of Engineering âMilda Zizyte, completed 2020 (advisor Philip Koopman)
â Xuechen (Jerry) Lei, completed 2019 (advisor Burcu Akinci)
â¢External PhD Committees âManish Motwani, University of Massachusetts - Amherst, completed 2022 (advisor Yuriy Brun)
â Mozhan Soltani, Leiden University, NL, completed 2020 (advisor Felienne Hermans)
â David Kelk, University of Ontario Institute of Technology, completed 2014 (advisor Jeremy Bradbury)
Masters Advisor â¢Qibin Chen, completed 2022. Language Technologies Institute, MLT, CMU
â¢Jon Kotheimer, completed 2017. Heinz College of Public Policy, CMU
â¢Edward (Ted) Smith, completed 2016. University of Massachusetts - Amherst, joint with Yuriy Brun
Research Funding Listed amounts denote the CMU portion of multi-institutional awards.
HPSTA: Hardware in the loop Perception Safety Testing for Autonomy US Army Research Office/US Army Futures Command, 2022-2023, $730,000 (solo PI, via NREC)
RINGS: Language-Agnostic Resilience Engineering at the Edge with WebAssembly	
National Science Foundation, 2022â2025, with Heather Miller and Benjamin Titzer (both at CMU), $929,959, and
REU supplements totaling $16,000
BAST: Build-in Adaptive System Testing Naval Air Warfare Center Aircraft Division, 2021â2025, solo PI (via NREC), $3.1M
CMU REU Site in Interdisciplinary Software Engineering (REUSE)
National Science Foundation, 2016â2025 (renewed twice), with Joshua Sunshine (at CMU), award amounts of
$360,000, $375,402, and $416,347
SHF: Small: Feedback-Driven Mutation Testing for Any Language National Science Foundation, 2021â2024, with Alex Groce (at NAU), $255,457
SHF: Small: Idiomatic Decompilation
National Science Foundation, 2019â2023, with Graham Neubig (at CMU), $425,000, and REU supplement of $8,000
Improving Search-Based and Semantic Automated Program Repair Air Force Research Lab, 2018â2023, with Stephanie Forest (ASU) and Westley Weimer (UMich), $330,000
Improving analysis via automated program transformation Facebook Testing and Verification Research Award, 2018â2019, solo PI, $50,000
Modeling Observability in Adaptive Systems to Improve their Security Cylab Seed Funding, 2018â2019, with Fei Fang and David Garlan (both at CMU), $110,000 Claire Le Goues Curriculum Vitae July 10, 2023 Page 14 of16 

CAREER: Quality Matters: Dynamic, Static and Proactive Analyses for Automated Program Repair
National Science Foundation, 2018â2023, Solo PI, $525,000 and REU Supplements totaling $81,352
SHF: Small: Evolution of Self-adaptive Systems using Stochastic SearchNational Science Foundation, 2016â2020, with David Garlan (CMU), $499,948 and REU Supplements totaling
$52,575
Trusted and Resilient Mission Operation
Air Force Research Lab, 2017â2019, with Stephanie Forrest (ASU), Westley Weimer (UMich), and Jack Davidson
(UVA), $447,252
SHF: Medium: Semi and fully automated program repair and synthesis via semantic code search
National Science Foundation, 2016â2021, with Yuriy Brun (UMass-Amherst) and Kathryn Stolee (NCSU), $411,996
and REU Supplements totaling $19,050
Robust Inside Out Testing (RIOT)
Army Test Resource Management Center, 2016â2019, with Philip Koopman and Michael Wagner (CMU/NREC),
$617,798
Intelligent Model-Based Adaptation for Mobile Robotics
Defense Advanced Research Projects Agency, 2015 â 2019, with Jonathan Aldrich (lead PI), David Garlan, Christian
Kaestner, Manuela Velosa (all at CMU), and Joydeep Biswas (UMass-Amherst), $7.8M
Cooperative, Trusted Repair for Cyber Physical System Resiliency
Air Force Research Lab, 2015â2017, with Stephanie Forrest (UNM), Miryung Kim (UCLA), and Westley Weimer
(UVA), $185,202
Automated Code Repair Software Engineering Institute, 2015â2016, with Christian Kaestner (CMU) and William Klieber (SEI), $50,000
EAGER: Demonstrating the Feasibility of Automatic Program Repair Guided by Semantic Code Search
National Science Foundation, 2014â2016, with Yuriy Brun (UMass-Amherst) and Kathryn Stolee (Iowa State),
$111,864
Human-friendly automatic bug repair via source code and repository mining Google Faculty Research Award, 2014â2015, Solo PI, $81,924
Selected Software Artifacts Other code and data can be found at
https://github.com/squaresLab andhttp://squareslab.github.io .
Comby: Parser parser combinators for language-agnostic code transformation.
https://comby.dev
JarFly: Heuristic repair for Java programs.
https://github.com/squaresLab/genprog4java
JFix: Semantics-based repair for Java programs. Implements S3.
https://xuanbachle.github.io/semanticsrepair/
BugZoo: A framework for performing empirical studies on automated repair of C programs.
https://github.com/squaresLab/BugZoo
SearchRepair: A semantic-search-based automated program repair technique.
https://github.com/ProgramRepair/SearchRepair
ManyBugs and IntroClass: benchmarks for research in automated repair of C programs.
http://repairbenchmarks.cs.umass.edu
GenProg: framework for search-and evolutionary-computation-based repair of C programs.
https://squareslab.github.io/genprog-code/
Boogie Verification Debugger (BVD): tool to assist in debugging failed program verification activities.
http://boogie.codeplex.com/
Professional Associations Claire Le Goues Curriculum Vitae July 10, 2023 Page 15 of16 

ACMAssociation for Computing Machinery, Senior Member
ACM SIGSOFTACM Special Interest Group on Software Engineering IEEEThe Institute of Electrical and Electronics Engineers, Senior Member
IEEE TCSEIEEE Technical Community on Software Engineering
Miscellaneous Selected media:
â¢â10 challenges of using simulators for testing robotsâ, The Robot Report (https://www.therobotreport.com/10-
challenges-simulators-robotics-testing/), 2020
â¢
Software Engineering Radio, Episode 379: Automatic Program Repair (https://www.se-radio.net/2019/09/episode-
379-claire-le-goues-on-automated-program-repair/), 2019
â¢Interview, People of PLDI, (http://abstract.ece.cmu.edu/peopleOfPLDI/claire.html), 2019
â¢
Times Higher Ed, âObjections to double-blind peer review âunfoundedââ (https://www.timeshighereducation.com/news/
objections-double-blind-peer-review-unfounded), 2018
I was on maternity leave the Spring 2019 and Spring 2021 semesters. I am a Dual US-French citizen. Claire Le Goues Curriculum Vitae July 10, 2023 Page 16 of16 

"
resume,"Personalia
Nationality
Day of birth
Ruben Nijhuis	
Dutch
November 21st, 2002	
E-mail
Portfolio	
contact@rubennijhuis.com
rubennijhuis.com	
Education
2020	 â 2021	
2015	 â 2018	
2018	 â 2020	
Software Developer
Pre University Secondary (VWO)
Vocational Secondary (VMBO)
ROC Amstelland, Amsterdam
Het Amsterdams Lyceum, Amsterdam
Media College, Amsterdam	
Languages	Dutch, Fluent
English, Fluent
French, Intermediate	
HTML5
Javascript
CSS3
OOCSS
BEM
Webpack
GIT
NPM
Yarn	
JQuery
CSS3
SASS
Pug
Swift
SQL	
React.js
Node.js
p5.js
Contentful
Wordpress
Netlify
Gatsby.js	Java	
Anime.js
React Native	
Technologies	
9/2020 â 	present	
Founder 	Louder Minds	
Digital agency focused on creating unique digital 
experiences with the newest web technologies.
Experiences
6/2019 â 8/2019
Front-end developer 	Devign.it	
At this internship Iâve developed websites with 
HTML5, CSS3, React, Gatsby, and GSAP, focusing 
on user interaction and revamping older cases.
6/2018 â 8/2018
App developer 	Superhero Cheesecake	
Working with React Native and discovering the 
possibilities of app development.
6/2017 â 	8/2017	
Front-end developer 	A friend of mine	
With the guidance of my internship supervisor I 
build the foundation of planning and executing a 
project, eventually creating my first portfolio site.
10/2020 â 02/2021
Teacher of Design
At my former school I got the opportunity to 
teach students how to dissect, understand 
and design websites. Further elevating my 
knowledge of design.
Extras
6/2016 â 	8/2016	
Front-end developer 	Ogilvy	
During my first internship I worked on the basics 
of web development. Mainly working with HTML5 
and CSS3.	
Creating unique 
digital experiences 
is what I like and do	
Creative front-end developer	
Capabilities
Project management
Web & app development
Digital product development
SEO
Interactive prototyping
Creative & art direction	
Sketch
Photoshop
InDesign
Illustrator	Slack	
Figma	
Trello
Slack
Notion	
Phone	+31 6 28 63 42 44
Styled
Components	
2021	 â present	Graphics Developer  (BoCS)Codam Coding College, Amsterdam
Git 

"
resume,"â£SummaryEntrepreneurial engineer with strong digital focus and chemical engineering background.â¨â£Personal projectsUNBLND, M	EETING	 PLATFORM	 â 2019 - PRESENT (	WWW.UNBLND.COM	)	- Blind ground dating to make friends based on interests, partner of AWS Activate and Flanders Investment;â¨- Available on 	Google Play	 and 	App Store	;â¨	- Project/team management, frontend and backend development, cloud management, Laravel, ReactNative.Extra: â¨Best2Nest: neighbourhood search engine based on personal interests (2015 - killed) using Google Places API, PHP, GIS, PostgreSQL, maps;â¨Virtual Whisky Tastings: 	thewhiskyhouse.com/shop	 (2020).	â£Employment*BACKEND ENGINEER, ROXI (S	TA R T U P	); SINT-NIKLAAS	, B	ELGIUM	 â AUG 2020 - OCT 2020	- Graphql API, nodeJS, AWS cloud management (mobile app, contractual, on-hold)CTO A	D INTERIM	, RELOTRUST (S	TA R T U P	); BRUSSELS	, B	ELGIUM	 â OCT 2019 - MAR 2020	- Building a corporate relocation management system from scratch (	www.relotrust.com	);â¨	- Infrastructure from backend to frontend & manage student interns;	â¨	- AngularJS, NodeJS, ReactNative (javascript frameworks), setup AWS services.SOFTWARE ENGINEER	, BAKERONLINE (S	CALEUP	); UTRECHT	, N	E â AUG 2018 - FEB 2019	- Accomplished reduction of on-boarding time by automating workï¬ow (	www.bakeronline.com	);â¨	- Business strategy, Sales & Marketing, Laravel PHP, Docker, keynotes (Young entrepreneur Intern).VALUE	 STREAM	 ENGINEER	, PALL (D	ANAHER	); HOEGAARDEN	, B	ELGIUM	 â FEB 2017 - FEB 2018	- Communicator between Production, Quality, Sales and Logistics (	www.pall.com	);â¨	- Solved production breakdown by redesigning the procedure (assembly of single-use biocontainers);	â¨	- SAP, Visual Basic, Problem Solving Process (PSP).GENERAL	 PROCESS 	OPTIMIZATION 	INTERN	, BASF SE; 	LUDWIGSHAFEN	, G	ERMANY	 â JUN 2015 - AUG 2015	- Interpret historical data of plant processes (	www.basf.com	);â¨	- Formulate real-time energy efï¬ciency in function of external and internal parameters;	â¨	- Programatic conditions, KPI visualization, large datasets (MES Siemens).*Side-note: Independent from Sep 2018 - presentâ£EducationKULEUVEN, 	LEUVEN	, B	E; POLYTECHNIQUE DE MONTRÃAL, 	MONTRÃAL	, C	A â SEP 2011 - JUN 2016	- Major:	 (MSc) Master of Science of Engineering (Chemical Technology)â¨	- Programming:	 Matlab, Python, Java, Algorithms, Networks, Data structure & analysis, CFD Simulationsâ¨	- GE:	 Computer Science, Decision making, Numerical Mathematics, Problem solving, Innovationâ¨	- Language of instruction:	 English (1y), French (1y) and Dutch (3y)	â£Skills- Software:	 advanced	: JavaScript, Git, Cloud (AWS), PHP, MySQL, HTML, CSS, SAP, VBAâ¨	familiar: MES, BoxCox, Machine Learning, Liquid, Python, Swift, C++	â¨	- Language:	 distinguished	: Dutch, English, French / 	advanced	: German, Spanish / 	novice	: Chinese 	IR. MS	C  A	DRIAAN	 D	E	 B	OLLE	+32 485 66 04 32adriaandebolle@gmail.comwww.adriaandebolle.com  

"
resume,"Angla isF ra n Ã§a isC rÃ© o leR usseL A N GU ESC 2C2B2A 2	Lan gag es d e p ro gra m matio n
M on ta g es d es v id Ã©o s
M oto  c ro ss/ E nduro
v id eo s G am es	
B A C  P R O  E LE ECLycÃ© e P ro fe ssio n nel P riv Ã©
d e B la n ch et,  G ou rb ey re ,	 	
G uad elo u p e.	
2 0 16 - 2 0 18	D on  F re d  H ecto r952 20 ,  H erb la y -su r-se in efr e d .d ev @ d-h ecto r.c o m0  7 8 3  7 9  3 9  6 3P erm is  BV Ã©h ic u lÃ©F O RM AT IO NSCEN TRE D 'I N TÃR ÃT SPR ÃSE N TAT IO NDan s l e  c a d re  d 'u n  B ach elo r
D Ã©v elo p peu r W eb  f u ll s ta ck ,	 	
je  s u is  Ã   l a  r e ch erc h e d 'u n  c o n tr a t e n
alt e rn an ce .	 	
R yth m e d âa lt e rn an ce  : 	4  j o u rs e n	
en tr e p ris e  /  1  j o u r e n  f o rm atio n
E X PÃ R IE N CES P R O FE SSIO NNEL LESD Ã©v elo p pem en t W eb  2 0 23	C rÃ© er p orte fo li o  e n  H TM L &  C SS
C od ag e d e s it e  w eb  e n  H TM L &  C SS
M od if ic a tio n  d e p ag e w eb  H TM L &  C SS
C rÃ© atio n  d e p ag e v it r in e e n  H TM L &  C SS
D Ã©v elo p pem en t d e n ou velle s f o n ctio n nali t Ã© s
C on ce v o ir  s it e  w eb  a v ec W ord p re ss
C rÃ© er d es p ag es w eb  d es s it e s c li e n ts
C rÃ© atio n  d âu n  c a h ie r d es c h arg es t e ch n iq u e
A naly se  d es b eso in s u tili s a te u rs
C orre ctio n  d e f o n ctio n nali t Ã© s	
D Ã©v elo p pem en t W eb  2 0 22A utr e s ( P arc o u rs e n  E ntr e p ris e s)	U D T -  B obig n y 2 0 22/2 0 23 ( C D I)
IT Q  G RO UP -  L ogn es 2 0 21/ 2 0 22 ( C D I)
C LEA N EO L -  B ezo n s 2 0 21 ( C D D)
AIR  &  E A U  -  N ois y -L e-G ra n d 2 0 20  (  C D I)	
      	T ech n ic ie n	
      	T ech n ic ie n	
      	T ech n ic ie n	
      	T ech n ic ie n	
F R ED  C H AR LE S
DÃV EL O PPE U R W EB  &  A PPL IC AT IO N M OBIL EV S C od eG it H ubB oots tr a pN od e j sJ a v aS crip tR eact j sP hpM yS Q LPyth onC SSH TM L	 	Ja v aC O M PÃ T EN CESSw if tX co d eK otli n 

"
resume,"Remi Pace
Software engineer
Montpellier, France, remi.pace@protonmail.com
My work
I am a software engineer: from user needs, I create reliable and scalable software solutions.I strive to fully understand the needs of my users to find the most effective solutions. I take care of the
interfaces and documentation of my applications to make them attractive and accessible.
I intervene on all aspects of the software: from the server infrastructure to the user experience. I create
platforms with technologies adapted to the problems and constraints of the projects.
I seek maximum quality and productivity. I implement tools to allow developers to focus on adding value to
products (continuous integration, static code analyses, debug tools, development aids, etc.).
If necessary, I support my team membersâ skills development using good development practices and group
work sessions (pair coding and more). These days I work with these technologies:
â¢	
Server infrastructure: Google Cloud Platform, Amazon Web Services, Kubernetes, Docker, Ansible,
Terraform, GNU/Linux (Debian, Ubuntu, Centos), . . .
â¢ Databases: PostgreSQL, MySQL/MariaDB, MongoDB, Elasticsearch, . . .
â¢ Languages: Javascript, Typescript, Java
â¢ Interfaces: React, Angular, Redux/Ngrx, Bootstrap, CSS, SASS, HTML5, Electron.js, React Native. . .
My last evening project is online, try it: https://abc-map.fr.
Professional experiences
Project Lila
From January 2019, company Believe IT	
Lila is a virtual hostess. Lila welcomes visitors, answers their questions, and notifies the members of his
company.
Technologies and tasks: â¢Typescript, Express, Lerna backend development
â¢ Dialogflow V2 API
â¢ Conversations and interactions as code
â¢ Frontend development React, Redux, Jest, Cypress, ThreeJS
â¢ React Native mobile app, Expo EAS
â¢ MongoDB database
â¢ Docker, Google Kubernetes Engine, Alpine Linux
â¢ Continuous integration and strict code reviews, with Gitlab CI, Gitlab CI Kubernetes Executor
â¢ Simple CLI tooling for a productivity-oriented development experience
Position held: lead developer.
Connected Fleet
From January 2019 to January 2021, companies Believe IT and Groupama Support and Services
Vehicle fleet management software for Groupama Support and Services. The objective of Connected Fleet
is to encourage safe and eco-responsible driving, while respecting the privacy of users.
Technologies and tasks: â¢Java backend development, Spring, Maven, Vavr, Lombok, Mapstruct, PMD, Angular 7,8,9
1 

â¢
Frontend development Angular 9, Ngrx, Jasmine, Typescript, Leaflet
â¢ End-to-end testing with Cypress
â¢ Data processing clustering with JGroups and RabbitMQ
â¢ MariaDB, PostgreSQL (Amazon RDS), Elasticsearch (Amazon ES) databases
â¢ Deployment in Infrastructure as Code on Amazon EKS (Kubernetes) with Terraform and Helm
â¢Continuous integration and strict code reviews, with Gitlab CI, Gitlab CI Kubernetes Executor, and
Python scripting for deployment
â¢ Performance measurements with Prometheus and Micrometer
â¢ Simple CLI tooling for a productivity-oriented development experience
â¢ On-board device (IOT) telemetry data
â¢ Data on the French road network OpenStreetMap
â¢ Data from the file BAAC
Position held: lead developer.
SÃ©lÃ©nÃ©e
From June 2018 to January 2019, company Agysoft.	
Within a publisher specializing in public procurement management, Fullstack developer integrated into the
team of development of the SÃ©lÃ©nÃ©e project, a platform for connecting public buyers and suppliers.
Technologies and tasks:
â¢Architecture in micro-services, Spring Cloud framework and Spring Boot
â¢ Redeployment of all the infrastructure with Docker-Compose and Ansible, in Infrastructure as Code
â¢ Deployment of a Gitlab / Gitlab CI instance
â¢ Continuous integration and deployment with Gitlab CI (internally hosted)
â¢ Validation and deployment process involving non-technical personnel
â¢ Data import and processing pipelines to test and develop with production data
â¢ Reorganization of development Git repositories
â¢ Administration of PostgreSQL, MongoDB, Elasticsearch, RabbitMQ, Postfix, Nginx, Bind9 servers
â¢	
Management of secrets with fine permissions (SSH keys, passwords, TLS certificates and internal
authority)
â¢ Monitoring with Prometheus and Grafana
â¢ Slack deployment bot in Python
â¢ Simple CLI tooling for a productivity-oriented development experience
Position held: fullstack developer, devops developer.
Bee Library
From July 2017 to June 2018, company Bee Buzziness.	
Within an innovative startup specialized in dematerialization of documents, integrated back-end developer
to the team in charge of clustering the server infrastructure.
Technologies and tasks: â¢Typescript, Javascript, NodeJS development.
â¢ Microservices architecture
â¢ Orchestration of Docker containers with Kubernetes
â¢ Infrastructure automation with Terraform and Ansible
â¢ Development of web administration interfaces (VueJS, ReactJS)
â¢ Development of a permissions system
â¢ Administration of GNU/Linux Debian, CentOS systems
â¢ MongoDB database
Position held: fullstack developer, devops developer.
Silverpeas
From May 2016 to September 2016, company Silverpeas.
2 

Creation of an instant multimedia communications system easily integrated into a web interface. This
system allows conversation by text or video, collaborative text editing, screen sharing.
Technologies and tasks: â¢XMPP, Openfire, WebRTC
â¢ NodeJS, JQuery, Sass, Gulp, Grunt
â¢ Ubuntu Server 16.04 LTS, PostgreSQL, Docker
â¢ Google Chrome extension
Position held: intern developer.
Previous career: Army, Navy
Infantry soldier for the 92nd Infantry Regiment, Clermont-Ferrand (69), France. Sailor on board the
Light Frigate Furtive La Fayette, Toulon (83), France. Permanent officer for the Watchtower of Homet,
Cherbourg-Octeville (50), France.
Several missions abroad, several distinctions.
Training
Masterâs degree IC2A DCISS (Computer Development)
2016 - Pierre MendÃ¨s France University, Grenoble (38), France
Masterâs degree in computer engineering at the University of Grenoble Alpes, obtained with honours.
imss-www.upmf-grenoble.fr
Bachelorâs degree in Geography
2014 - University of Rennes II, Rennes (35), France
univ-rennes2.fr
TOEIC
2012 - French Navy, LanvÃ©oc Poulmic (29)
International English test. Language practiced regularly. 810 dots.
etsglobal.org
3 

"
